{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1c65321c",
      "metadata": {
        "id": "1c65321c",
        "outputId": "d2b22f21-82be-4de4-fd61-801caf9c6996",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32512"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import os\n",
        "os.system('cmd command')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "4842b918",
      "metadata": {
        "id": "4842b918",
        "outputId": "9033443c-5d1e-4482-8253-3f599c80d05e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "31ecb10e",
      "metadata": {
        "id": "31ecb10e",
        "outputId": "a885aa23-65f9-4c99-a311-6861d5555806",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/HiDePrompt\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/HiDePrompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "cd565e2a",
      "metadata": {
        "id": "cd565e2a"
      },
      "outputs": [],
      "source": [
        "!pip install -q condacolab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "6a2473b5",
      "metadata": {
        "id": "6a2473b5",
        "outputId": "e435931a-a6cb-4f7a-f09c-fd79432ddd86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⏬ Downloading https://github.com/jaimergp/miniforge/releases/download/24.11.2-1_colab/Miniforge3-colab-24.11.2-1_colab-Linux-x86_64.sh...\n",
            "📦 Installing...\n",
            "📌 Adjusting configuration...\n",
            "🩹 Patching environment...\n",
            "⏲ Done in 0:00:11\n",
            "🔁 Restarting kernel...\n"
          ]
        }
      ],
      "source": [
        "import condacolab\n",
        "condacolab.install()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/HiDePrompt/HiDePrompt"
      ],
      "metadata": {
        "id": "4aBTrmkLHGUx",
        "outputId": "c58ee341-cb5e-4a42-a240-1aed8b238398",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "4aBTrmkLHGUx",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/drive/MyDrive/HiDePrompt/HiDePrompt'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "59ddd81a",
      "metadata": {
        "id": "59ddd81a",
        "outputId": "2d958915-469d-49c8-90be-f3d2d67d4cfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm (from -r requirements.txt (line 1))\n",
            "  Downloading timm-1.0.20-py3-none-any.whl.metadata (61 kB)\n",
            "Collecting pillow (from -r requirements.txt (line 2))\n",
            "  Downloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting matplotlib (from -r requirements.txt (line 3))\n",
            "  Downloading matplotlib-3.10.6-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
            "Collecting torchprofile (from -r requirements.txt (line 4))\n",
            "  Downloading torchprofile-0.0.4-py3-none-any.whl.metadata (303 bytes)\n",
            "Collecting torch (from -r requirements.txt (line 5))\n",
            "  Downloading torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
            "Collecting torchvision (from -r requirements.txt (line 6))\n",
            "  Downloading torchvision-0.23.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (2.3.0)\n",
            "Collecting scipy (from -r requirements.txt (line 8))\n",
            "  Downloading scipy-1.16.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
            "Collecting scikit-learn (from -r requirements.txt (line 9))\n",
            "  Downloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
            "Collecting numpy (from -r requirements.txt (line 10))\n",
            "  Downloading numpy-2.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "Collecting pyyaml (from timm->-r requirements.txt (line 1))\n",
            "  Downloading pyyaml-6.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)\n",
            "Collecting huggingface_hub (from timm->-r requirements.txt (line 1))\n",
            "  Downloading huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting safetensors (from timm->-r requirements.txt (line 1))\n",
            "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib->-r requirements.txt (line 3))\n",
            "  Downloading contourpy-1.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 3))\n",
            "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib->-r requirements.txt (line 3))\n",
            "  Downloading fonttools-4.60.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (111 kB)\n",
            "Collecting kiwisolver>=1.3.1 (from matplotlib->-r requirements.txt (line 3))\n",
            "  Downloading kiwisolver-1.4.9-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 3)) (24.2)\n",
            "Collecting pyparsing>=2.3.1 (from matplotlib->-r requirements.txt (line 3))\n",
            "  Downloading pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting python-dateutil>=2.7 (from matplotlib->-r requirements.txt (line 3))\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting filelock (from torch->-r requirements.txt (line 5))\n",
            "  Downloading filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting typing-extensions>=4.10.0 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting sympy>=1.13.3 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting networkx (from torch->-r requirements.txt (line 5))\n",
            "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting jinja2 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting fsspec (from torch->-r requirements.txt (line 5))\n",
            "  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.9.90 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Collecting nvidia-nccl-cu12==2.27.3 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.8.90 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.4.0 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/site-packages (from triton==3.4.0->torch->-r requirements.txt (line 5)) (65.6.3)\n",
            "Collecting joblib>=1.2.0 (from scikit-learn->-r requirements.txt (line 9))\n",
            "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn->-r requirements.txt (line 9))\n",
            "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting six>=1.5 (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 3))\n",
            "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch->-r requirements.txt (line 5))\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/site-packages (from huggingface_hub->timm->-r requirements.txt (line 1)) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/site-packages (from huggingface_hub->timm->-r requirements.txt (line 1)) (4.67.1)\n",
            "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface_hub->timm->-r requirements.txt (line 1))\n",
            "  Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch->-r requirements.txt (line 5))\n",
            "  Downloading markupsafe-3.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 1)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 1)) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 1)) (2024.12.14)\n",
            "Downloading timm-1.0.20-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m141.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.10.6-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m179.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchprofile-0.0.4-py3-none-any.whl (7.7 kB)\n",
            "Downloading torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl (888.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m888.1/888.1 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m116.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "Downloading triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.5/155.5 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.23.0-cp311-cp311-manylinux_2_28_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m101.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.16.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.9/35.9 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m107.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m112.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading contourpy-1.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (355 kB)\n",
            "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading fonttools-4.60.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (5.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m96.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
            "Downloading kiwisolver-1.4.9-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
            "Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
            "Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\n",
            "Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
            "Downloading huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyyaml-6.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (806 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.6/806.6 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
            "Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markupsafe-3.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\n",
            "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: nvidia-cusparselt-cu12, mpmath, typing-extensions, triton, threadpoolctl, sympy, six, safetensors, pyyaml, pyparsing, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, kiwisolver, joblib, hf-xet, fsspec, fonttools, filelock, cycler, scipy, python-dateutil, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, jinja2, huggingface_hub, contourpy, scikit-learn, nvidia-cusolver-cu12, matplotlib, torch, torchvision, torchprofile, timm\n",
            "Successfully installed MarkupSafe-3.0.3 contourpy-1.3.3 cycler-0.12.1 filelock-3.19.1 fonttools-4.60.0 fsspec-2025.9.0 hf-xet-1.1.10 huggingface_hub-0.35.3 jinja2-3.1.6 joblib-1.5.2 kiwisolver-1.4.9 matplotlib-3.10.6 mpmath-1.3.0 networkx-3.5 numpy-2.3.3 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 pillow-11.3.0 pyparsing-3.2.5 python-dateutil-2.9.0.post0 pyyaml-6.0.3 safetensors-0.6.2 scikit-learn-1.7.2 scipy-1.16.2 six-1.17.0 sympy-1.14.0 threadpoolctl-3.6.0 timm-1.0.20 torch-2.8.0 torchprofile-0.0.4 torchvision-0.23.0 triton-3.4.0 typing-extensions-4.15.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "cycler",
                  "dateutil",
                  "kiwisolver",
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy",
                  "pyparsing",
                  "six"
                ]
              },
              "id": "5e9c917c5f81486e8bc6a6c737f1745b"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/HiDePrompt/HiDePrompt/"
      ],
      "metadata": {
        "id": "nTIVenS1Ih7Q",
        "outputId": "891587c2-3bd7-41b1-d5c1-87345480c78c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "nTIVenS1Ih7Q",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "383f6206",
      "metadata": {
        "id": "383f6206",
        "outputId": "9e397e74-061c-4172-f6c6-c0c421491b7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(subparser_name='cifar100_hideprompt_5e', batch_size=24, epochs=1, original_model='vit_small_patch16_224.dino', model='vit_small_patch16_224.dino', input_size=224, pretrained=True, drop=0.0, drop_path=0.0, opt='adam', opt_eps=1e-08, opt_betas=(0.9, 0.999), clip_grad=1.0, momentum=0.9, weight_decay=0.0, reinit_optimizer=True, sched='constant', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=0, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, unscale_lr=True, color_jitter=None, aa=None, smoothing=0.1, train_interpolation='bicubic', reprob=0.0, remode='pixel', recount=1, data_path='./datasets/', dataset='Split-CIFAR100', shuffle=False, output_dir='./output/cifar100_full_dino_1epoch', device='cuda', seed=20, eval=False, num_workers=4, pin_mem=True, world_size=1, dist_url='env://', num_tasks=10, train_mask=True, task_inc=False, use_g_prompt=False, g_prompt_length=5, g_prompt_layer_idx=[], use_prefix_tune_for_g_prompt=False, use_e_prompt=True, e_prompt_layer_idx=[0, 1, 2, 3, 4], use_prefix_tune_for_e_prompt=True, larger_prompt_lr=False, prompt_pool=True, size=10, length=20, top_k=1, initializer='uniform', prompt_key=False, prompt_key_init='uniform', use_prompt_mask=True, mask_first_epoch=False, shared_prompt_pool=True, shared_prompt_key=False, batchwise_prompt=False, embedding_key='cls', predefined_key='', pull_constraint=True, pull_constraint_coeff=1.0, same_key_value=False, global_pool='token', head_type='token', freeze=['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed'], crct_epochs=30, train_inference_task_only=True, original_model_mlp_structure=[2], ca_lr=0.005, milestones=[10], trained_original_model='', prompt_momentum=0.01, reg=0.01, not_train_ca=False, ca_epochs=30, ca_storage_efficient_method='multi-centroid', n_centroids=10, print_freq=10, config='cifar100_hideprompt_5e')\n",
            "| distributed init (rank 0): env://\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "[rank0]:[W929 16:34:00.549764268 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.\n",
            "/usr/local/lib/python3.11/site-packages/timm/models/helpers.py:7: FutureWarning: Importing from timm.models.helpers is deprecated, please import via timm.models\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
            "/usr/local/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "/usr/local/lib/python3.11/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:886: UserWarning: Overwriting vit_tiny_patch16_224 in registry with vits.hide_prompt_vision_transformer.vit_tiny_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:895: UserWarning: Overwriting vit_tiny_patch16_384 in registry with vits.hide_prompt_vision_transformer.vit_tiny_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:904: UserWarning: Overwriting vit_small_patch32_224 in registry with vits.hide_prompt_vision_transformer.vit_small_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:913: UserWarning: Overwriting vit_small_patch32_384 in registry with vits.hide_prompt_vision_transformer.vit_small_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:922: UserWarning: Overwriting vit_small_patch16_224 in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:932: UserWarning: Overwriting vit_small_patch16_384 in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:942: UserWarning: Overwriting vit_base_patch32_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:952: UserWarning: Overwriting vit_base_patch32_384 in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:962: UserWarning: Overwriting vit_base_patch16_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:972: UserWarning: Overwriting vit_base_patch16_384 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:982: UserWarning: Overwriting vit_base_patch8_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:992: UserWarning: Overwriting vit_large_patch32_224 in registry with vits.hide_prompt_vision_transformer.vit_large_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1001: UserWarning: Overwriting vit_large_patch32_384 in registry with vits.hide_prompt_vision_transformer.vit_large_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1011: UserWarning: Overwriting vit_large_patch16_224 in registry with vits.hide_prompt_vision_transformer.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1021: UserWarning: Overwriting vit_large_patch16_384 in registry with vits.hide_prompt_vision_transformer.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1031: UserWarning: Overwriting vit_large_patch14_224 in registry with vits.hide_prompt_vision_transformer.vit_large_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1040: UserWarning: Overwriting vit_huge_patch14_224 in registry with vits.hide_prompt_vision_transformer.vit_huge_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1049: UserWarning: Overwriting vit_giant_patch14_224 in registry with vits.hide_prompt_vision_transformer.vit_giant_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1058: UserWarning: Overwriting vit_gigantic_patch14_224 in registry with vits.hide_prompt_vision_transformer.vit_gigantic_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1067: UserWarning: Overwriting vit_tiny_patch16_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_tiny_patch16_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1078: UserWarning: Overwriting vit_small_patch32_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_small_patch32_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1089: UserWarning: Overwriting vit_small_patch16_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1100: UserWarning: Overwriting vit_base_patch32_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1111: UserWarning: Overwriting vit_base_patch16_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1122: UserWarning: Overwriting vit_base_patch8_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_base_patch8_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1133: UserWarning: Overwriting vit_large_patch32_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_large_patch32_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1144: UserWarning: Overwriting vit_large_patch16_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_large_patch16_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1155: UserWarning: Overwriting vit_huge_patch14_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_huge_patch14_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1166: UserWarning: Overwriting vit_base_patch16_224_sam in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_sam. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1175: UserWarning: Overwriting vit_base_patch32_224_sam in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_224_sam. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1184: UserWarning: Overwriting vit_small_patch16_224_dino in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_224_dino. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1193: UserWarning: Overwriting vit_small_patch8_224_dino in registry with vits.hide_prompt_vision_transformer.vit_small_patch8_224_dino. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1211: UserWarning: Overwriting vit_base_patch8_224_dino in registry with vits.hide_prompt_vision_transformer.vit_base_patch8_224_dino. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1220: UserWarning: Overwriting vit_base_patch16_224_miil_in21k in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_miil_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1230: UserWarning: Overwriting vit_base_patch16_224_miil in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_miil. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1242: UserWarning: Overwriting vit_base_patch32_plus_256 in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_plus_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1251: UserWarning: Overwriting vit_base_patch16_plus_240 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_plus_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1260: UserWarning: Overwriting vit_base_patch16_rpn_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_rpn_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1271: UserWarning: Overwriting vit_small_patch16_36x1_224 in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_36x1_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1282: UserWarning: Overwriting vit_small_patch16_18x2_224 in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1294: UserWarning: Overwriting vit_base_patch16_18x2_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1331: UserWarning: Overwriting vit_base_patch16_224_dino in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_dino. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "100\n",
            "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, 13, 14, 15, 16, 17, 18, 19], [20, 21, 22, 23, 24, 25, 26, 27, 28, 29], [30, 31, 32, 33, 34, 35, 36, 37, 38, 39], [40, 41, 42, 43, 44, 45, 46, 47, 48, 49], [50, 51, 52, 53, 54, 55, 56, 57, 58, 59], [60, 61, 62, 63, 64, 65, 66, 67, 68, 69], [70, 71, 72, 73, 74, 75, 76, 77, 78, 79], [80, 81, 82, 83, 84, 85, 86, 87, 88, 89], [90, 91, 92, 93, 94, 95, 96, 97, 98, 99]]\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Creating original model: vit_small_patch16_224.dino\n",
            "[Sequential(\n",
            "  (0): Linear(in_features=384, out_features=768, bias=True)\n",
            "  (1): GELU(approximate='none')\n",
            "  (2): Dropout(p=0.0, inplace=False)\n",
            "), Sequential(\n",
            "  (0): Linear(in_features=768, out_features=384, bias=True)\n",
            "  (1): Dropout(p=0.0, inplace=False)\n",
            ")]\n",
            "model.safetensors: 100% 86.7M/86.7M [00:00<00:00, 245MB/s]\n",
            "Namespace(subparser_name='cifar100_hideprompt_5e', batch_size=24, epochs=1, original_model='vit_small_patch16_224.dino', model='vit_small_patch16_224.dino', input_size=224, pretrained=True, drop=0.0, drop_path=0.0, opt='adam', opt_eps=1e-08, opt_betas=(0.9, 0.999), clip_grad=1.0, momentum=0.9, weight_decay=0.0, reinit_optimizer=True, sched='constant', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=0, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, unscale_lr=True, color_jitter=None, aa=None, smoothing=0.1, train_interpolation='bicubic', reprob=0.0, remode='pixel', recount=1, data_path='./datasets/', dataset='Split-CIFAR100', shuffle=False, output_dir='./output/cifar100_full_dino_1epoch', device='cuda', seed=20, eval=False, num_workers=4, pin_mem=True, world_size=1, dist_url='env://', num_tasks=10, train_mask=True, task_inc=False, use_g_prompt=False, g_prompt_length=5, g_prompt_layer_idx=[], use_prefix_tune_for_g_prompt=False, use_e_prompt=True, e_prompt_layer_idx=[0, 1, 2, 3, 4], use_prefix_tune_for_e_prompt=True, larger_prompt_lr=False, prompt_pool=True, size=10, length=20, top_k=1, initializer='uniform', prompt_key=False, prompt_key_init='uniform', use_prompt_mask=True, mask_first_epoch=False, shared_prompt_pool=True, shared_prompt_key=False, batchwise_prompt=False, embedding_key='cls', predefined_key='', pull_constraint=True, pull_constraint_coeff=1.0, same_key_value=False, global_pool='token', head_type='token', freeze=['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed'], crct_epochs=30, train_inference_task_only=True, original_model_mlp_structure=[2], ca_lr=0.005, milestones=[10], trained_original_model='', prompt_momentum=0.01, reg=0.01, not_train_ca=False, ca_epochs=30, ca_storage_efficient_method='multi-centroid', n_centroids=10, print_freq=10, config='cifar100_hideprompt_5e', rank=0, gpu=0, distributed=True, dist_backend='nccl', nb_classes=100)\n",
            "number of params: 630244\n",
            "Start training for 1 epochs\n",
            "Train: Epoch[1/1]  [  0/209]  eta: 0:08:42  Lr: 0.000047  Loss: 2.2565  Acc@1: 8.3333 (8.3333)  Acc@5: 70.8333 (70.8333)  time: 2.4997  data: 0.9594  max mem: 196\n",
            "Train: Epoch[1/1]  [ 10/209]  eta: 0:01:01  Lr: 0.000047  Loss: 2.2859  Acc@1: 12.5000 (12.8788)  Acc@5: 62.5000 (61.7424)  time: 0.3106  data: 0.0889  max mem: 217\n",
            "Train: Epoch[1/1]  [ 20/209]  eta: 0:00:40  Lr: 0.000047  Loss: 2.1882  Acc@1: 12.5000 (14.2857)  Acc@5: 58.3333 (60.3175)  time: 0.0994  data: 0.0024  max mem: 217\n",
            "Train: Epoch[1/1]  [ 30/209]  eta: 0:00:31  Lr: 0.000047  Loss: 2.0816  Acc@1: 16.6667 (17.8763)  Acc@5: 66.6667 (64.2473)  time: 0.0983  data: 0.0024  max mem: 217\n",
            "Train: Epoch[1/1]  [ 40/209]  eta: 0:00:25  Lr: 0.000047  Loss: 2.1953  Acc@1: 29.1667 (21.0366)  Acc@5: 79.1667 (67.8862)  time: 0.0846  data: 0.0016  max mem: 217\n",
            "Train: Epoch[1/1]  [ 50/209]  eta: 0:00:21  Lr: 0.000047  Loss: 2.0984  Acc@1: 29.1667 (23.3660)  Acc@5: 79.1667 (70.5882)  time: 0.0805  data: 0.0012  max mem: 217\n",
            "Train: Epoch[1/1]  [ 60/209]  eta: 0:00:19  Lr: 0.000047  Loss: 2.0825  Acc@1: 37.5000 (26.2978)  Acc@5: 79.1667 (72.7459)  time: 0.0809  data: 0.0010  max mem: 217\n",
            "Train: Epoch[1/1]  [ 70/209]  eta: 0:00:16  Lr: 0.000047  Loss: 1.9405  Acc@1: 45.8333 (30.2230)  Acc@5: 87.5000 (75.0000)  time: 0.0809  data: 0.0008  max mem: 217\n",
            "Train: Epoch[1/1]  [ 80/209]  eta: 0:00:15  Lr: 0.000047  Loss: 1.8399  Acc@1: 50.0000 (32.7675)  Acc@5: 91.6667 (77.0576)  time: 0.0810  data: 0.0005  max mem: 217\n",
            "Train: Epoch[1/1]  [ 90/209]  eta: 0:00:13  Lr: 0.000047  Loss: 1.6933  Acc@1: 54.1667 (35.4853)  Acc@5: 91.6667 (78.9377)  time: 0.0811  data: 0.0005  max mem: 217\n",
            "Train: Epoch[1/1]  [100/209]  eta: 0:00:11  Lr: 0.000047  Loss: 1.7572  Acc@1: 58.3333 (37.3762)  Acc@5: 91.6667 (80.1155)  time: 0.0809  data: 0.0007  max mem: 217\n",
            "Train: Epoch[1/1]  [110/209]  eta: 0:00:10  Lr: 0.000047  Loss: 1.7481  Acc@1: 58.3333 (39.6396)  Acc@5: 91.6667 (81.1937)  time: 0.0809  data: 0.0006  max mem: 217\n",
            "Train: Epoch[1/1]  [120/209]  eta: 0:00:09  Lr: 0.000047  Loss: 1.7529  Acc@1: 66.6667 (41.9077)  Acc@5: 91.6667 (82.1625)  time: 0.0813  data: 0.0005  max mem: 217\n",
            "Train: Epoch[1/1]  [130/209]  eta: 0:00:08  Lr: 0.000047  Loss: 1.4117  Acc@1: 70.8333 (44.3066)  Acc@5: 95.8333 (83.3015)  time: 0.0812  data: 0.0008  max mem: 217\n",
            "Train: Epoch[1/1]  [140/209]  eta: 0:00:06  Lr: 0.000047  Loss: 1.4223  Acc@1: 70.8333 (46.2470)  Acc@5: 95.8333 (84.0721)  time: 0.0813  data: 0.0011  max mem: 217\n",
            "Train: Epoch[1/1]  [150/209]  eta: 0:00:05  Lr: 0.000047  Loss: 1.4347  Acc@1: 70.8333 (48.0408)  Acc@5: 95.8333 (84.8234)  time: 0.0836  data: 0.0013  max mem: 217\n",
            "Train: Epoch[1/1]  [160/209]  eta: 0:00:04  Lr: 0.000047  Loss: 0.9774  Acc@1: 75.0000 (49.9224)  Acc@5: 95.8333 (85.5849)  time: 0.0890  data: 0.0014  max mem: 217\n",
            "Train: Epoch[1/1]  [170/209]  eta: 0:00:03  Lr: 0.000047  Loss: 0.9328  Acc@1: 79.1667 (51.4620)  Acc@5: 95.8333 (86.3060)  time: 0.0962  data: 0.0026  max mem: 217\n",
            "Train: Epoch[1/1]  [180/209]  eta: 0:00:02  Lr: 0.000047  Loss: 1.1613  Acc@1: 75.0000 (52.7394)  Acc@5: 95.8333 (86.8554)  time: 0.0996  data: 0.0023  max mem: 217\n",
            "Train: Epoch[1/1]  [190/209]  eta: 0:00:01  Lr: 0.000047  Loss: 0.9965  Acc@1: 79.1667 (54.1449)  Acc@5: 95.8333 (87.3909)  time: 0.1014  data: 0.0013  max mem: 217\n",
            "Train: Epoch[1/1]  [200/209]  eta: 0:00:00  Lr: 0.000047  Loss: 0.8691  Acc@1: 79.1667 (55.3483)  Acc@5: 95.8333 (87.8939)  time: 0.0984  data: 0.0017  max mem: 217\n",
            "Train: Epoch[1/1]  [208/209]  eta: 0:00:00  Lr: 0.000047  Loss: 0.7511  Acc@1: 79.1667 (56.2400)  Acc@5: 95.8333 (88.2000)  time: 0.0893  data: 0.0013  max mem: 217\n",
            "Train: Epoch[1/1] Total time: 0:00:20 (0.0995 s / it)\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.7511  Acc@1: 79.1667 (56.2400)  Acc@5: 95.8333 (88.2000)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:16  Loss: 2.0977 (2.0977)  Acc@1: 87.5000 (87.5000)  Acc@5: 95.8333 (95.8333)  time: 0.3899  data: 0.3221  max mem: 217\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 2.1757 (2.1722)  Acc@1: 87.5000 (89.7727)  Acc@5: 100.0000 (97.7273)  time: 0.1083  data: 0.0326  max mem: 218\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 2.1757 (2.1881)  Acc@1: 91.6667 (90.0794)  Acc@5: 100.0000 (97.8175)  time: 0.0797  data: 0.0021  max mem: 218\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 2.1213 (2.1589)  Acc@1: 91.6667 (90.7258)  Acc@5: 95.8333 (97.7151)  time: 0.0797  data: 0.0005  max mem: 218\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 2.1630 (2.1524)  Acc@1: 91.6667 (90.7520)  Acc@5: 100.0000 (97.8659)  time: 0.0800  data: 0.0003  max mem: 218\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 2.1332 (2.1480)  Acc@1: 91.6667 (90.8000)  Acc@5: 100.0000 (97.9000)  time: 0.0833  data: 0.0003  max mem: 218\n",
            "Test: [Task 1] Total time: 0:00:03 (0.0905 s / it)\n",
            "* Acc@1 90.800 Acc@5 97.900 loss 2.148\n",
            "[Average accuracy till task1]\tAcc@1: 90.8000\tAcc@5: 97.9000\tLoss: 2.1480\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:27  Loss: 2.0977 (2.0977)  Acc@1: 87.5000 (87.5000)  Acc@5: 95.8333 (95.8333)  time: 0.6577  data: 0.5640  max mem: 343\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 2.1757 (2.1722)  Acc@1: 87.5000 (89.7727)  Acc@5: 100.0000 (97.7273)  time: 0.1398  data: 0.0562  max mem: 343\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 2.1757 (2.1881)  Acc@1: 91.6667 (90.0794)  Acc@5: 100.0000 (97.8175)  time: 0.0848  data: 0.0029  max mem: 343\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 2.1213 (2.1589)  Acc@1: 91.6667 (90.7258)  Acc@5: 95.8333 (97.7151)  time: 0.0825  data: 0.0004  max mem: 343\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 2.1630 (2.1524)  Acc@1: 91.6667 (90.7520)  Acc@5: 100.0000 (97.8659)  time: 0.0831  data: 0.0003  max mem: 343\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 2.1332 (2.1480)  Acc@1: 91.6667 (90.8000)  Acc@5: 100.0000 (97.9000)  time: 0.0816  data: 0.0003  max mem: 343\n",
            "Test: [Task 1] Total time: 0:00:04 (0.0987 s / it)\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@1 90.800 Acc@5 97.900 loss 2.148\n",
            "[Average accuracy till task1]\tAcc@1: 90.8000\tAcc@5: 97.9000\tLoss: 2.1480\n",
            "Train: Epoch[1/1]  [  0/209]  eta: 0:02:01  Lr: 0.000047  Loss: 2.3728  Acc@1: 20.8333 (20.8333)  Acc@5: 54.1667 (54.1667)  time: 0.5799  data: 0.4752  max mem: 343\n",
            "Train: Epoch[1/1]  [ 10/209]  eta: 0:00:26  Lr: 0.000047  Loss: 2.1554  Acc@1: 12.5000 (14.0152)  Acc@5: 62.5000 (60.6061)  time: 0.1319  data: 0.0441  max mem: 343\n",
            "Train: Epoch[1/1]  [ 20/209]  eta: 0:00:20  Lr: 0.000047  Loss: 2.1206  Acc@1: 12.5000 (16.0714)  Acc@5: 62.5000 (63.6905)  time: 0.0860  data: 0.0012  max mem: 343\n",
            "Train: Epoch[1/1]  [ 30/209]  eta: 0:00:18  Lr: 0.000047  Loss: 2.1744  Acc@1: 16.6667 (18.4140)  Acc@5: 70.8333 (67.7419)  time: 0.0848  data: 0.0013  max mem: 343\n",
            "Train: Epoch[1/1]  [ 40/209]  eta: 0:00:16  Lr: 0.000047  Loss: 2.0108  Acc@1: 25.0000 (21.2398)  Acc@5: 75.0000 (70.8333)  time: 0.0866  data: 0.0011  max mem: 343\n",
            "Train: Epoch[1/1]  [ 50/209]  eta: 0:00:15  Lr: 0.000047  Loss: 1.9464  Acc@1: 33.3333 (24.2647)  Acc@5: 83.3333 (73.7745)  time: 0.0913  data: 0.0009  max mem: 343\n",
            "Train: Epoch[1/1]  [ 60/209]  eta: 0:00:14  Lr: 0.000047  Loss: 1.8856  Acc@1: 41.6667 (27.2541)  Acc@5: 87.5000 (75.8880)  time: 0.0908  data: 0.0009  max mem: 343\n",
            "Train: Epoch[1/1]  [ 70/209]  eta: 0:00:13  Lr: 0.000047  Loss: 1.8084  Acc@1: 45.8333 (30.1056)  Acc@5: 87.5000 (77.5822)  time: 0.0888  data: 0.0009  max mem: 343\n",
            "Train: Epoch[1/1]  [ 80/209]  eta: 0:00:12  Lr: 0.000047  Loss: 1.5028  Acc@1: 50.0000 (32.3045)  Acc@5: 91.6667 (79.3210)  time: 0.1041  data: 0.0016  max mem: 343\n",
            "Train: Epoch[1/1]  [ 90/209]  eta: 0:00:11  Lr: 0.000047  Loss: 1.6355  Acc@1: 58.3333 (35.2106)  Acc@5: 91.6667 (80.9066)  time: 0.1108  data: 0.0019  max mem: 343\n",
            "Train: Epoch[1/1]  [100/209]  eta: 0:00:10  Lr: 0.000047  Loss: 1.4802  Acc@1: 58.3333 (37.1287)  Acc@5: 91.6667 (81.5182)  time: 0.1028  data: 0.0020  max mem: 343\n",
            "Train: Epoch[1/1]  [110/209]  eta: 0:00:09  Lr: 0.000047  Loss: 1.3909  Acc@1: 58.3333 (39.4895)  Acc@5: 91.6667 (82.7703)  time: 0.1030  data: 0.0051  max mem: 343\n",
            "Train: Epoch[1/1]  [120/209]  eta: 0:00:08  Lr: 0.000047  Loss: 1.3783  Acc@1: 62.5000 (41.5634)  Acc@5: 95.8333 (83.8499)  time: 0.1004  data: 0.0056  max mem: 343\n",
            "Train: Epoch[1/1]  [130/209]  eta: 0:00:07  Lr: 0.000047  Loss: 1.3068  Acc@1: 62.5000 (43.5433)  Acc@5: 95.8333 (84.6056)  time: 0.0922  data: 0.0025  max mem: 343\n",
            "Train: Epoch[1/1]  [140/209]  eta: 0:00:06  Lr: 0.000047  Loss: 1.3686  Acc@1: 66.6667 (45.0355)  Acc@5: 95.8333 (85.5201)  time: 0.0872  data: 0.0012  max mem: 343\n",
            "Train: Epoch[1/1]  [150/209]  eta: 0:00:05  Lr: 0.000047  Loss: 1.4347  Acc@1: 66.6667 (46.7163)  Acc@5: 100.0000 (86.2859)  time: 0.0873  data: 0.0008  max mem: 343\n",
            "Train: Epoch[1/1]  [160/209]  eta: 0:00:04  Lr: 0.000047  Loss: 1.1967  Acc@1: 70.8333 (48.3178)  Acc@5: 95.8333 (86.8789)  time: 0.0874  data: 0.0006  max mem: 343\n",
            "Train: Epoch[1/1]  [170/209]  eta: 0:00:03  Lr: 0.000047  Loss: 0.9606  Acc@1: 75.0000 (49.9025)  Acc@5: 95.8333 (87.3782)  time: 0.0873  data: 0.0005  max mem: 343\n",
            "Train: Epoch[1/1]  [180/209]  eta: 0:00:02  Lr: 0.000047  Loss: 1.0581  Acc@1: 79.1667 (51.2201)  Acc@5: 95.8333 (87.7302)  time: 0.0876  data: 0.0007  max mem: 343\n",
            "Train: Epoch[1/1]  [190/209]  eta: 0:00:01  Lr: 0.000047  Loss: 1.2466  Acc@1: 70.8333 (52.4433)  Acc@5: 95.8333 (88.2853)  time: 0.0880  data: 0.0006  max mem: 343\n",
            "Train: Epoch[1/1]  [200/209]  eta: 0:00:00  Lr: 0.000047  Loss: 1.1053  Acc@1: 70.8333 (53.2960)  Acc@5: 95.8333 (88.5987)  time: 0.0882  data: 0.0008  max mem: 343\n",
            "Train: Epoch[1/1]  [208/209]  eta: 0:00:00  Lr: 0.000047  Loss: 0.9302  Acc@1: 70.8333 (54.0600)  Acc@5: 95.8333 (88.8200)  time: 0.0852  data: 0.0007  max mem: 343\n",
            "Train: Epoch[1/1] Total time: 0:00:19 (0.0946 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.9302  Acc@1: 70.8333 (54.0600)  Acc@5: 95.8333 (88.8200)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:17  Loss: 2.1435 (2.1435)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  time: 0.4055  data: 0.3236  max mem: 343\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 2.2426 (2.2132)  Acc@1: 79.1667 (77.2727)  Acc@5: 100.0000 (98.1061)  time: 0.1226  data: 0.0359  max mem: 343\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 2.2426 (2.2269)  Acc@1: 79.1667 (77.7778)  Acc@5: 100.0000 (97.8175)  time: 0.0908  data: 0.0040  max mem: 343\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 2.1772 (2.2005)  Acc@1: 83.3333 (79.9731)  Acc@5: 95.8333 (97.7151)  time: 0.0885  data: 0.0008  max mem: 343\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 2.1472 (2.1886)  Acc@1: 83.3333 (80.6911)  Acc@5: 95.8333 (97.6626)  time: 0.0893  data: 0.0005  max mem: 343\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 2.1426 (2.1843)  Acc@1: 83.3333 (80.7000)  Acc@5: 95.8333 (97.6000)  time: 0.0874  data: 0.0005  max mem: 343\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1004 s / it)\n",
            "* Acc@1 80.700 Acc@5 97.600 loss 2.184\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:27  Loss: 2.6363 (2.6363)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  time: 0.6629  data: 0.5485  max mem: 343\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 2.6576 (2.6482)  Acc@1: 75.0000 (73.4849)  Acc@5: 91.6667 (92.8030)  time: 0.1428  data: 0.0518  max mem: 343\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 2.6522 (2.6371)  Acc@1: 70.8333 (73.4127)  Acc@5: 91.6667 (93.0556)  time: 0.0895  data: 0.0013  max mem: 343\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 2.5796 (2.6327)  Acc@1: 70.8333 (73.7903)  Acc@5: 91.6667 (93.1452)  time: 0.0888  data: 0.0005  max mem: 343\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 2.5796 (2.6333)  Acc@1: 70.8333 (72.9675)  Acc@5: 95.8333 (94.0041)  time: 0.0898  data: 0.0003  max mem: 343\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 2.5595 (2.6281)  Acc@1: 70.8333 (73.1000)  Acc@5: 95.8333 (94.1000)  time: 0.0882  data: 0.0003  max mem: 343\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1045 s / it)\n",
            "* Acc@1 73.100 Acc@5 94.100 loss 2.628\n",
            "[Average accuracy till task2]\tAcc@1: 76.9000\tAcc@5: 95.8500\tLoss: 2.4062\tForgetting: 10.1000\tBackward: -10.1000\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([4800, 384])\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 0.9854  Acc@1: 83.3333 (85.4167)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000467  Loss: 1.0466  Acc@1: 87.5000 (86.2500)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000464  Loss: 1.1825  Acc@1: 87.5000 (86.6667)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 1.0447  Acc@1: 91.6667 (90.8333)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000448  Loss: 0.8657  Acc@1: 87.5000 (88.7500)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000437  Loss: 0.8647  Acc@1: 87.5000 (90.0000)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 0.7949  Acc@1: 87.5000 (90.8333)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000409  Loss: 0.9146  Acc@1: 91.6667 (90.4167)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000391  Loss: 0.8698  Acc@1: 87.5000 (87.0833)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 0.8043  Acc@1: 87.5000 (90.4167)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000352  Loss: 0.7207  Acc@1: 91.6667 (93.3333)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000330  Loss: 0.7628  Acc@1: 95.8333 (93.7500)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 0.8686  Acc@1: 91.6667 (92.5000)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000283  Loss: 0.6687  Acc@1: 91.6667 (89.5833)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000259  Loss: 0.6362  Acc@1: 95.8333 (95.4167)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 0.8619  Acc@1: 91.6667 (92.9167)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000210  Loss: 0.6080  Acc@1: 95.8333 (93.7500)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000186  Loss: 0.6365  Acc@1: 91.6667 (90.0000)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.8271  Acc@1: 95.8333 (95.0000)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000139  Loss: 0.8884  Acc@1: 91.6667 (92.0833)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000117  Loss: 0.6518  Acc@1: 95.8333 (94.1667)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 0.4668  Acc@1: 91.6667 (93.7500)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000078  Loss: 0.6261  Acc@1: 95.8333 (96.2500)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000060  Loss: 0.6510  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 0.6156  Acc@1: 95.8333 (93.7500)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000031  Loss: 0.5775  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000020  Loss: 0.5867  Acc@1: 95.8333 (96.6667)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.5502  Acc@1: 91.6667 (92.9167)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000005  Loss: 0.6690  Acc@1: 95.8333 (93.7500)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000001  Loss: 0.6424  Acc@1: 95.8333 (96.2500)  Acc@5: 100.0000 (100.0000)\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:16  Loss: 1.1227 (1.1227)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.3873  data: 0.3074  max mem: 349\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 1.2416 (1.2326)  Acc@1: 87.5000 (85.9849)  Acc@5: 100.0000 (99.2424)  time: 0.1207  data: 0.0372  max mem: 349\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 1.2416 (1.2396)  Acc@1: 87.5000 (86.3095)  Acc@5: 100.0000 (99.2063)  time: 0.0914  data: 0.0054  max mem: 349\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 1.2010 (1.2110)  Acc@1: 91.6667 (88.1720)  Acc@5: 100.0000 (98.7903)  time: 0.0896  data: 0.0012  max mem: 349\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 1.1512 (1.1921)  Acc@1: 91.6667 (89.0244)  Acc@5: 100.0000 (98.8821)  time: 0.0899  data: 0.0012  max mem: 349\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 1.1418 (1.1862)  Acc@1: 91.6667 (89.1000)  Acc@5: 100.0000 (98.9000)  time: 0.0884  data: 0.0012  max mem: 349\n",
            "Test: [Task 1] Total time: 0:00:04 (0.0998 s / it)\n",
            "* Acc@1 89.100 Acc@5 98.900 loss 1.186\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:23  Loss: 1.5104 (1.5104)  Acc@1: 87.5000 (87.5000)  Acc@5: 95.8333 (95.8333)  time: 0.5626  data: 0.4829  max mem: 349\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 1.5087 (1.4684)  Acc@1: 87.5000 (84.0909)  Acc@5: 100.0000 (98.1061)  time: 0.1390  data: 0.0552  max mem: 349\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 1.4387 (1.4759)  Acc@1: 87.5000 (84.5238)  Acc@5: 100.0000 (97.8175)  time: 0.0919  data: 0.0068  max mem: 349\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 1.3972 (1.4651)  Acc@1: 87.5000 (85.0806)  Acc@5: 100.0000 (97.9839)  time: 0.0876  data: 0.0008  max mem: 349\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 1.3609 (1.4578)  Acc@1: 87.5000 (85.3659)  Acc@5: 100.0000 (98.1707)  time: 0.0880  data: 0.0003  max mem: 349\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 1.3484 (1.4552)  Acc@1: 87.5000 (85.4000)  Acc@5: 100.0000 (98.2000)  time: 0.0867  data: 0.0003  max mem: 349\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1026 s / it)\n",
            "* Acc@1 85.400 Acc@5 98.200 loss 1.455\n",
            "[Average accuracy till task2]\tAcc@1: 87.2500\tAcc@5: 98.5500\tLoss: 1.3207\tForgetting: 1.7000\tBackward: -1.7000\n",
            "Train: Epoch[1/1]  [  0/209]  eta: 0:01:34  Lr: 0.000047  Loss: 2.2799  Acc@1: 16.6667 (16.6667)  Acc@5: 66.6667 (66.6667)  time: 0.4506  data: 0.3353  max mem: 349\n",
            "Train: Epoch[1/1]  [ 10/209]  eta: 0:00:27  Lr: 0.000047  Loss: 2.3458  Acc@1: 12.5000 (10.9848)  Acc@5: 54.1667 (55.6818)  time: 0.1364  data: 0.0441  max mem: 349\n",
            "Train: Epoch[1/1]  [ 20/209]  eta: 0:00:21  Lr: 0.000047  Loss: 1.9871  Acc@1: 12.5000 (16.4683)  Acc@5: 62.5000 (63.0952)  time: 0.0965  data: 0.0080  max mem: 349\n",
            "Train: Epoch[1/1]  [ 30/209]  eta: 0:00:18  Lr: 0.000047  Loss: 2.0547  Acc@1: 29.1667 (20.8333)  Acc@5: 70.8333 (66.9355)  time: 0.0885  data: 0.0011  max mem: 349\n",
            "Train: Epoch[1/1]  [ 40/209]  eta: 0:00:17  Lr: 0.000047  Loss: 1.9078  Acc@1: 33.3333 (25.6098)  Acc@5: 79.1667 (71.5447)  time: 0.0890  data: 0.0009  max mem: 349\n",
            "Train: Epoch[1/1]  [ 50/209]  eta: 0:00:15  Lr: 0.000047  Loss: 1.8443  Acc@1: 41.6667 (29.1667)  Acc@5: 91.6667 (75.1634)  time: 0.0886  data: 0.0010  max mem: 349\n",
            "Train: Epoch[1/1]  [ 60/209]  eta: 0:00:14  Lr: 0.000047  Loss: 1.7420  Acc@1: 45.8333 (32.3087)  Acc@5: 87.5000 (77.5956)  time: 0.0888  data: 0.0010  max mem: 349\n",
            "Train: Epoch[1/1]  [ 70/209]  eta: 0:00:13  Lr: 0.000047  Loss: 1.4851  Acc@1: 58.3333 (36.4437)  Acc@5: 91.6667 (79.8709)  time: 0.0889  data: 0.0009  max mem: 349\n",
            "Train: Epoch[1/1]  [ 80/209]  eta: 0:00:12  Lr: 0.000047  Loss: 1.5378  Acc@1: 58.3333 (39.1461)  Acc@5: 95.8333 (81.5844)  time: 0.0911  data: 0.0013  max mem: 349\n",
            "Train: Epoch[1/1]  [ 90/209]  eta: 0:00:11  Lr: 0.000047  Loss: 1.4223  Acc@1: 58.3333 (41.4377)  Acc@5: 95.8333 (82.8297)  time: 0.0994  data: 0.0019  max mem: 349\n",
            "Train: Epoch[1/1]  [100/209]  eta: 0:00:10  Lr: 0.000047  Loss: 1.3384  Acc@1: 66.6667 (44.2244)  Acc@5: 95.8333 (83.9934)  time: 0.1018  data: 0.0020  max mem: 349\n",
            "Train: Epoch[1/1]  [110/209]  eta: 0:00:09  Lr: 0.000047  Loss: 1.2319  Acc@1: 70.8333 (46.7342)  Acc@5: 95.8333 (85.2477)  time: 0.1029  data: 0.0015  max mem: 349\n",
            "Train: Epoch[1/1]  [120/209]  eta: 0:00:08  Lr: 0.000047  Loss: 1.1361  Acc@1: 75.0000 (48.9325)  Acc@5: 95.8333 (86.0537)  time: 0.1077  data: 0.0020  max mem: 349\n",
            "Train: Epoch[1/1]  [130/209]  eta: 0:00:07  Lr: 0.000047  Loss: 1.2418  Acc@1: 75.0000 (50.8588)  Acc@5: 95.8333 (86.8639)  time: 0.1071  data: 0.0031  max mem: 349\n",
            "Train: Epoch[1/1]  [140/209]  eta: 0:00:06  Lr: 0.000047  Loss: 1.0444  Acc@1: 75.0000 (52.5709)  Acc@5: 95.8333 (87.6478)  time: 0.0980  data: 0.0026  max mem: 349\n",
            "Train: Epoch[1/1]  [150/209]  eta: 0:00:05  Lr: 0.000047  Loss: 1.0222  Acc@1: 75.0000 (53.8631)  Acc@5: 95.8333 (88.1898)  time: 0.0894  data: 0.0013  max mem: 349\n",
            "Train: Epoch[1/1]  [160/209]  eta: 0:00:04  Lr: 0.000047  Loss: 1.0426  Acc@1: 75.0000 (55.3313)  Acc@5: 95.8333 (88.7164)  time: 0.0888  data: 0.0008  max mem: 349\n",
            "Train: Epoch[1/1]  [170/209]  eta: 0:00:03  Lr: 0.000047  Loss: 0.9505  Acc@1: 79.1667 (56.9444)  Acc@5: 100.0000 (89.3031)  time: 0.0893  data: 0.0007  max mem: 349\n",
            "Train: Epoch[1/1]  [180/209]  eta: 0:00:02  Lr: 0.000047  Loss: 1.0492  Acc@1: 79.1667 (57.8269)  Acc@5: 100.0000 (89.6639)  time: 0.0891  data: 0.0006  max mem: 349\n",
            "Train: Epoch[1/1]  [190/209]  eta: 0:00:01  Lr: 0.000047  Loss: 0.8633  Acc@1: 75.0000 (58.7042)  Acc@5: 100.0000 (90.0960)  time: 0.0890  data: 0.0006  max mem: 349\n",
            "Train: Epoch[1/1]  [200/209]  eta: 0:00:00  Lr: 0.000047  Loss: 0.9251  Acc@1: 75.0000 (59.6808)  Acc@5: 100.0000 (90.5058)  time: 0.0895  data: 0.0007  max mem: 349\n",
            "Train: Epoch[1/1]  [208/209]  eta: 0:00:00  Lr: 0.000047  Loss: 0.9595  Acc@1: 79.1667 (60.4600)  Acc@5: 100.0000 (90.7800)  time: 0.0866  data: 0.0004  max mem: 349\n",
            "Train: Epoch[1/1] Total time: 0:00:20 (0.0959 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.9595  Acc@1: 79.1667 (60.4600)  Acc@5: 100.0000 (90.7800)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:16  Loss: 1.2398 (1.2398)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.3875  data: 0.3090  max mem: 349\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 1.3613 (1.3183)  Acc@1: 87.5000 (84.8485)  Acc@5: 100.0000 (98.8636)  time: 0.1169  data: 0.0288  max mem: 349\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 1.3613 (1.3262)  Acc@1: 87.5000 (86.5079)  Acc@5: 100.0000 (99.0079)  time: 0.0884  data: 0.0006  max mem: 349\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 1.2941 (1.2991)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (98.7903)  time: 0.0877  data: 0.0011  max mem: 349\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 1.2340 (1.2826)  Acc@1: 91.6667 (88.3130)  Acc@5: 100.0000 (98.6789)  time: 0.0885  data: 0.0016  max mem: 349\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 1.2280 (1.2766)  Acc@1: 91.6667 (88.4000)  Acc@5: 100.0000 (98.7000)  time: 0.0864  data: 0.0016  max mem: 349\n",
            "Test: [Task 1] Total time: 0:00:04 (0.0982 s / it)\n",
            "* Acc@1 88.400 Acc@5 98.700 loss 1.277\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:30  Loss: 1.6511 (1.6511)  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  time: 0.7278  data: 0.6259  max mem: 349\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 1.6132 (1.5794)  Acc@1: 79.1667 (79.9242)  Acc@5: 95.8333 (96.2121)  time: 0.1475  data: 0.0580  max mem: 349\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 1.5628 (1.5833)  Acc@1: 79.1667 (81.3492)  Acc@5: 95.8333 (96.0317)  time: 0.0918  data: 0.0025  max mem: 349\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 1.5358 (1.5715)  Acc@1: 83.3333 (81.7204)  Acc@5: 95.8333 (96.2366)  time: 0.0916  data: 0.0028  max mem: 349\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 1.4664 (1.5652)  Acc@1: 83.3333 (82.0122)  Acc@5: 100.0000 (96.8496)  time: 0.0888  data: 0.0010  max mem: 349\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 1.4394 (1.5614)  Acc@1: 83.3333 (82.2000)  Acc@5: 100.0000 (96.9000)  time: 0.0875  data: 0.0010  max mem: 349\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1071 s / it)\n",
            "* Acc@1 82.200 Acc@5 96.900 loss 1.561\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:19  Loss: 2.4162 (2.4162)  Acc@1: 29.1667 (29.1667)  Acc@5: 91.6667 (91.6667)  time: 0.4548  data: 0.3740  max mem: 349\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 2.4986 (2.4993)  Acc@1: 41.6667 (41.2879)  Acc@5: 95.8333 (93.5606)  time: 0.1227  data: 0.0347  max mem: 349\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 2.4986 (2.4903)  Acc@1: 41.6667 (41.6667)  Acc@5: 95.8333 (93.8492)  time: 0.0881  data: 0.0006  max mem: 349\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 2.3901 (2.4578)  Acc@1: 45.8333 (43.6828)  Acc@5: 91.6667 (92.7419)  time: 0.0872  data: 0.0005  max mem: 349\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 2.4342 (2.4799)  Acc@1: 45.8333 (43.4959)  Acc@5: 91.6667 (92.5813)  time: 0.0887  data: 0.0003  max mem: 349\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 2.4514 (2.4856)  Acc@1: 45.8333 (43.3000)  Acc@5: 91.6667 (92.5000)  time: 0.0868  data: 0.0003  max mem: 349\n",
            "Test: [Task 3] Total time: 0:00:04 (0.0988 s / it)\n",
            "* Acc@1 43.300 Acc@5 92.500 loss 2.486\n",
            "[Average accuracy till task3]\tAcc@1: 71.3000\tAcc@5: 96.0333\tLoss: 1.7745\tForgetting: 1.2000\tBackward: 3.3500\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([7200, 384])\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 1.0341  Acc@1: 79.1667 (78.9583)  Acc@5: 100.0000 (99.3750)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000467  Loss: 0.9859  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000464  Loss: 0.7696  Acc@1: 87.5000 (86.8750)  Acc@5: 100.0000 (99.3750)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 0.9725  Acc@1: 87.5000 (89.1667)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000448  Loss: 0.8570  Acc@1: 87.5000 (87.7083)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000437  Loss: 0.7881  Acc@1: 91.6667 (90.8333)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 0.6913  Acc@1: 91.6667 (92.2917)  Acc@5: 100.0000 (99.3750)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000409  Loss: 0.5425  Acc@1: 91.6667 (92.0833)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000391  Loss: 0.7954  Acc@1: 91.6667 (89.1667)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 0.5717  Acc@1: 87.5000 (88.1250)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000352  Loss: 0.6395  Acc@1: 91.6667 (91.0417)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000330  Loss: 0.4382  Acc@1: 91.6667 (92.5000)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 0.7727  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000283  Loss: 0.7808  Acc@1: 91.6667 (91.4583)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000259  Loss: 0.6616  Acc@1: 91.6667 (92.5000)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 0.5149  Acc@1: 91.6667 (92.2917)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000210  Loss: 0.4512  Acc@1: 95.8333 (93.1250)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000186  Loss: 0.5455  Acc@1: 95.8333 (94.1667)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.6788  Acc@1: 91.6667 (92.5000)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000139  Loss: 0.6679  Acc@1: 95.8333 (93.7500)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000117  Loss: 0.3511  Acc@1: 95.8333 (95.4167)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 0.5542  Acc@1: 95.8333 (94.7917)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000078  Loss: 0.5259  Acc@1: 95.8333 (94.1667)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000060  Loss: 0.5668  Acc@1: 95.8333 (94.3750)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 0.3810  Acc@1: 91.6667 (92.9167)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000031  Loss: 0.4964  Acc@1: 91.6667 (91.0417)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000020  Loss: 0.5130  Acc@1: 91.6667 (92.0833)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.6178  Acc@1: 91.6667 (92.9167)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000005  Loss: 0.6321  Acc@1: 91.6667 (92.2917)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000001  Loss: 0.3763  Acc@1: 95.8333 (94.3750)  Acc@5: 100.0000 (99.7917)\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:19  Loss: 0.6776 (0.6776)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.4717  data: 0.3922  max mem: 355\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 0.9206 (0.8952)  Acc@1: 79.1667 (78.4091)  Acc@5: 100.0000 (98.4848)  time: 0.1230  data: 0.0365  max mem: 355\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.9206 (0.8889)  Acc@1: 79.1667 (80.5556)  Acc@5: 100.0000 (98.6111)  time: 0.0867  data: 0.0008  max mem: 355\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.8510 (0.8661)  Acc@1: 87.5000 (82.9301)  Acc@5: 100.0000 (98.6559)  time: 0.0869  data: 0.0004  max mem: 355\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.8298 (0.8471)  Acc@1: 91.6667 (84.1463)  Acc@5: 100.0000 (98.8821)  time: 0.0886  data: 0.0003  max mem: 355\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.7839 (0.8393)  Acc@1: 91.6667 (84.3000)  Acc@5: 100.0000 (98.9000)  time: 0.0865  data: 0.0002  max mem: 355\n",
            "Test: [Task 1] Total time: 0:00:04 (0.0984 s / it)\n",
            "* Acc@1 84.300 Acc@5 98.900 loss 0.839\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:18  Loss: 1.0929 (1.0929)  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  time: 0.4487  data: 0.3725  max mem: 355\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:03  Loss: 1.0590 (1.0050)  Acc@1: 83.3333 (82.9545)  Acc@5: 95.8333 (96.9697)  time: 0.1218  data: 0.0345  max mem: 355\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 1.0154 (1.0157)  Acc@1: 83.3333 (83.5317)  Acc@5: 100.0000 (96.8254)  time: 0.0880  data: 0.0006  max mem: 355\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.9462 (1.0027)  Acc@1: 83.3333 (84.2742)  Acc@5: 100.0000 (96.9086)  time: 0.0876  data: 0.0006  max mem: 355\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.8763 (0.9912)  Acc@1: 83.3333 (84.3496)  Acc@5: 100.0000 (97.4594)  time: 0.0887  data: 0.0004  max mem: 355\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.8703 (0.9873)  Acc@1: 87.5000 (84.6000)  Acc@5: 100.0000 (97.5000)  time: 0.0870  data: 0.0003  max mem: 355\n",
            "Test: [Task 2] Total time: 0:00:04 (0.0995 s / it)\n",
            "* Acc@1 84.600 Acc@5 97.500 loss 0.987\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:27  Loss: 0.9335 (0.9335)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.6587  data: 0.5414  max mem: 355\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:04  Loss: 1.0499 (1.0765)  Acc@1: 79.1667 (79.9242)  Acc@5: 100.0000 (97.7273)  time: 0.1433  data: 0.0506  max mem: 355\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 1.0392 (1.0368)  Acc@1: 83.3333 (81.3492)  Acc@5: 100.0000 (98.2143)  time: 0.0899  data: 0.0011  max mem: 355\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.9464 (1.0112)  Acc@1: 83.3333 (82.1237)  Acc@5: 100.0000 (98.1183)  time: 0.0888  data: 0.0005  max mem: 355\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 1.0080 (1.0248)  Acc@1: 83.3333 (82.3171)  Acc@5: 95.8333 (97.9675)  time: 0.0898  data: 0.0004  max mem: 355\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 1.0080 (1.0285)  Acc@1: 83.3333 (82.2000)  Acc@5: 95.8333 (98.0000)  time: 0.0879  data: 0.0004  max mem: 355\n",
            "Test: [Task 3] Total time: 0:00:04 (0.1049 s / it)\n",
            "* Acc@1 82.200 Acc@5 98.000 loss 1.029\n",
            "[Average accuracy till task3]\tAcc@1: 83.7000\tAcc@5: 98.1333\tLoss: 0.9517\tForgetting: 3.6500\tBackward: -3.6500\n",
            "Train: Epoch[1/1]  [  0/209]  eta: 0:01:32  Lr: 0.000047  Loss: 2.4680  Acc@1: 12.5000 (12.5000)  Acc@5: 37.5000 (37.5000)  time: 0.4404  data: 0.3369  max mem: 355\n",
            "Train: Epoch[1/1]  [ 10/209]  eta: 0:00:25  Lr: 0.000047  Loss: 2.3176  Acc@1: 8.3333 (8.3333)  Acc@5: 41.6667 (44.6970)  time: 0.1292  data: 0.0358  max mem: 355\n",
            "Train: Epoch[1/1]  [ 20/209]  eta: 0:00:21  Lr: 0.000047  Loss: 2.1705  Acc@1: 8.3333 (11.9048)  Acc@5: 50.0000 (55.7540)  time: 0.0949  data: 0.0035  max mem: 355\n",
            "Train: Epoch[1/1]  [ 30/209]  eta: 0:00:19  Lr: 0.000047  Loss: 2.0793  Acc@1: 16.6667 (15.5914)  Acc@5: 70.8333 (60.6183)  time: 0.0939  data: 0.0011  max mem: 355\n",
            "Train: Epoch[1/1]  [ 40/209]  eta: 0:00:17  Lr: 0.000047  Loss: 2.0291  Acc@1: 20.8333 (17.9878)  Acc@5: 75.0000 (64.6341)  time: 0.0939  data: 0.0011  max mem: 355\n",
            "Train: Epoch[1/1]  [ 50/209]  eta: 0:00:15  Lr: 0.000047  Loss: 1.9904  Acc@1: 29.1667 (22.3039)  Acc@5: 79.1667 (68.1373)  time: 0.0910  data: 0.0007  max mem: 355\n",
            "Train: Epoch[1/1]  [ 60/209]  eta: 0:00:14  Lr: 0.000047  Loss: 1.7744  Acc@1: 41.6667 (27.3907)  Acc@5: 87.5000 (71.9262)  time: 0.0907  data: 0.0006  max mem: 355\n",
            "Train: Epoch[1/1]  [ 70/209]  eta: 0:00:13  Lr: 0.000047  Loss: 1.7855  Acc@1: 45.8333 (30.3991)  Acc@5: 91.6667 (74.4718)  time: 0.0913  data: 0.0008  max mem: 355\n",
            "Train: Epoch[1/1]  [ 80/209]  eta: 0:00:12  Lr: 0.000047  Loss: 1.5952  Acc@1: 50.0000 (33.6934)  Acc@5: 91.6667 (76.8519)  time: 0.0938  data: 0.0012  max mem: 355\n",
            "Train: Epoch[1/1]  [ 90/209]  eta: 0:00:11  Lr: 0.000047  Loss: 1.6051  Acc@1: 58.3333 (36.4011)  Acc@5: 91.6667 (78.4799)  time: 0.1043  data: 0.0023  max mem: 355\n",
            "Train: Epoch[1/1]  [100/209]  eta: 0:00:11  Lr: 0.000047  Loss: 1.6790  Acc@1: 58.3333 (38.4901)  Acc@5: 91.6667 (80.0743)  time: 0.1182  data: 0.0024  max mem: 355\n",
            "Train: Epoch[1/1]  [110/209]  eta: 0:00:10  Lr: 0.000047  Loss: 1.5307  Acc@1: 62.5000 (40.7282)  Acc@5: 95.8333 (81.3438)  time: 0.1167  data: 0.0023  max mem: 355\n",
            "Train: Epoch[1/1]  [120/209]  eta: 0:00:09  Lr: 0.000047  Loss: 1.3009  Acc@1: 66.6667 (43.0785)  Acc@5: 95.8333 (82.6791)  time: 0.1040  data: 0.0018  max mem: 355\n",
            "Train: Epoch[1/1]  [130/209]  eta: 0:00:07  Lr: 0.000047  Loss: 1.1906  Acc@1: 70.8333 (45.1654)  Acc@5: 95.8333 (83.7150)  time: 0.0948  data: 0.0007  max mem: 355\n",
            "Train: Epoch[1/1]  [140/209]  eta: 0:00:06  Lr: 0.000047  Loss: 1.2142  Acc@1: 70.8333 (47.1040)  Acc@5: 95.8333 (84.6631)  time: 0.0911  data: 0.0006  max mem: 355\n",
            "Train: Epoch[1/1]  [150/209]  eta: 0:00:05  Lr: 0.000047  Loss: 1.2259  Acc@1: 70.8333 (48.4823)  Acc@5: 95.8333 (85.3201)  time: 0.0910  data: 0.0005  max mem: 355\n",
            "Train: Epoch[1/1]  [160/209]  eta: 0:00:04  Lr: 0.000047  Loss: 1.0634  Acc@1: 66.6667 (49.7412)  Acc@5: 95.8333 (86.0766)  time: 0.0915  data: 0.0003  max mem: 355\n",
            "Train: Epoch[1/1]  [170/209]  eta: 0:00:03  Lr: 0.000047  Loss: 1.0748  Acc@1: 66.6667 (51.0721)  Acc@5: 95.8333 (86.7446)  time: 0.0914  data: 0.0003  max mem: 355\n",
            "Train: Epoch[1/1]  [180/209]  eta: 0:00:02  Lr: 0.000047  Loss: 1.1158  Acc@1: 75.0000 (52.5092)  Acc@5: 95.8333 (87.2928)  time: 0.0912  data: 0.0004  max mem: 355\n",
            "Train: Epoch[1/1]  [190/209]  eta: 0:00:01  Lr: 0.000047  Loss: 1.2016  Acc@1: 75.0000 (53.7086)  Acc@5: 95.8333 (87.8054)  time: 0.0915  data: 0.0007  max mem: 355\n",
            "Train: Epoch[1/1]  [200/209]  eta: 0:00:00  Lr: 0.000047  Loss: 1.0919  Acc@1: 75.0000 (55.0166)  Acc@5: 95.8333 (88.3292)  time: 0.0914  data: 0.0007  max mem: 355\n",
            "Train: Epoch[1/1]  [208/209]  eta: 0:00:00  Lr: 0.000047  Loss: 0.7438  Acc@1: 75.0000 (55.7200)  Acc@5: 100.0000 (88.6600)  time: 0.0885  data: 0.0003  max mem: 355\n",
            "Train: Epoch[1/1] Total time: 0:00:20 (0.0978 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.7438  Acc@1: 75.0000 (55.7200)  Acc@5: 100.0000 (88.6600)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:18  Loss: 0.8267 (0.8267)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.4499  data: 0.3762  max mem: 355\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 0.9792 (0.9860)  Acc@1: 83.3333 (79.1667)  Acc@5: 100.0000 (97.7273)  time: 0.1233  data: 0.0350  max mem: 355\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.9792 (0.9949)  Acc@1: 83.3333 (80.5556)  Acc@5: 100.0000 (97.6190)  time: 0.0899  data: 0.0006  max mem: 355\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.9467 (0.9808)  Acc@1: 83.3333 (82.1237)  Acc@5: 95.8333 (97.5806)  time: 0.0902  data: 0.0015  max mem: 355\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.9197 (0.9634)  Acc@1: 87.5000 (83.5366)  Acc@5: 100.0000 (97.9675)  time: 0.0912  data: 0.0017  max mem: 355\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.8993 (0.9543)  Acc@1: 87.5000 (83.7000)  Acc@5: 100.0000 (98.0000)  time: 0.0892  data: 0.0017  max mem: 355\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1014 s / it)\n",
            "* Acc@1 83.700 Acc@5 98.000 loss 0.954\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:31  Loss: 1.1575 (1.1575)  Acc@1: 87.5000 (87.5000)  Acc@5: 91.6667 (91.6667)  time: 0.7529  data: 0.6246  max mem: 355\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 1.1575 (1.1066)  Acc@1: 83.3333 (84.8485)  Acc@5: 95.8333 (96.5909)  time: 0.1509  data: 0.0577  max mem: 355\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 1.1223 (1.1163)  Acc@1: 83.3333 (85.1191)  Acc@5: 95.8333 (96.2302)  time: 0.0897  data: 0.0008  max mem: 355\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 1.0591 (1.1025)  Acc@1: 87.5000 (85.6183)  Acc@5: 95.8333 (96.5054)  time: 0.0889  data: 0.0004  max mem: 355\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 1.0215 (1.0949)  Acc@1: 87.5000 (85.1626)  Acc@5: 100.0000 (96.9512)  time: 0.0898  data: 0.0002  max mem: 355\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.9841 (1.0896)  Acc@1: 87.5000 (85.4000)  Acc@5: 100.0000 (97.0000)  time: 0.0880  data: 0.0002  max mem: 355\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1070 s / it)\n",
            "* Acc@1 85.400 Acc@5 97.000 loss 1.090\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:18  Loss: 1.0978 (1.0978)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.4361  data: 0.3472  max mem: 355\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 1.2250 (1.2448)  Acc@1: 75.0000 (75.7576)  Acc@5: 95.8333 (96.5909)  time: 0.1207  data: 0.0322  max mem: 355\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 1.2099 (1.2052)  Acc@1: 75.0000 (77.1825)  Acc@5: 95.8333 (97.2222)  time: 0.0889  data: 0.0005  max mem: 355\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 1.0993 (1.1651)  Acc@1: 79.1667 (78.2258)  Acc@5: 100.0000 (97.4462)  time: 0.0887  data: 0.0003  max mem: 355\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 1.1313 (1.1796)  Acc@1: 79.1667 (78.7602)  Acc@5: 95.8333 (96.8496)  time: 0.0891  data: 0.0002  max mem: 355\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 1.1503 (1.1857)  Acc@1: 79.1667 (78.6000)  Acc@5: 95.8333 (96.9000)  time: 0.0875  data: 0.0002  max mem: 355\n",
            "Test: [Task 3] Total time: 0:00:04 (0.0992 s / it)\n",
            "* Acc@1 78.600 Acc@5 96.900 loss 1.186\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:21  Loss: 3.1948 (3.1948)  Acc@1: 12.5000 (12.5000)  Acc@5: 62.5000 (62.5000)  time: 0.5210  data: 0.4358  max mem: 355\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:04  Loss: 2.9570 (2.9796)  Acc@1: 16.6667 (18.5606)  Acc@5: 79.1667 (78.4091)  time: 0.1283  data: 0.0407  max mem: 355\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 2.9738 (3.0003)  Acc@1: 20.8333 (19.0476)  Acc@5: 79.1667 (77.7778)  time: 0.0882  data: 0.0008  max mem: 355\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 3.0139 (2.9763)  Acc@1: 20.8333 (19.7581)  Acc@5: 79.1667 (79.1667)  time: 0.0885  data: 0.0003  max mem: 355\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 2.9915 (2.9847)  Acc@1: 20.8333 (20.3252)  Acc@5: 79.1667 (78.6585)  time: 0.0894  data: 0.0003  max mem: 355\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 2.9768 (2.9764)  Acc@1: 25.0000 (20.6000)  Acc@5: 79.1667 (78.7000)  time: 0.0876  data: 0.0003  max mem: 355\n",
            "Test: [Task 4] Total time: 0:00:04 (0.1010 s / it)\n",
            "* Acc@1 20.600 Acc@5 78.700 loss 2.976\n",
            "[Average accuracy till task4]\tAcc@1: 67.0750\tAcc@5: 92.6500\tLoss: 1.5515\tForgetting: 2.3667\tBackward: 13.5000\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([9600, 384])\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 0.8753  Acc@1: 79.1667 (75.6944)  Acc@5: 100.0000 (97.5000)\n",
            "torch.Size([9600, 384])\n",
            "Averaged stats: Lr: 0.000467  Loss: 0.6818  Acc@1: 79.1667 (80.6944)  Acc@5: 100.0000 (98.4722)\n",
            "torch.Size([9600, 384])\n",
            "Averaged stats: Lr: 0.000464  Loss: 0.8187  Acc@1: 83.3333 (85.5556)  Acc@5: 100.0000 (99.0278)\n",
            "torch.Size([9600, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 0.9693  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([9600, 384])\n",
            "Averaged stats: Lr: 0.000448  Loss: 0.6013  Acc@1: 87.5000 (90.0000)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([9600, 384])\n",
            "Averaged stats: Lr: 0.000437  Loss: 0.6047  Acc@1: 91.6667 (90.2778)  Acc@5: 100.0000 (99.8611)\n",
            "torch.Size([9600, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 0.5057  Acc@1: 91.6667 (90.6944)  Acc@5: 100.0000 (99.8611)\n",
            "torch.Size([9600, 384])\n",
            "Averaged stats: Lr: 0.000409  Loss: 0.6019  Acc@1: 91.6667 (90.0000)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([9600, 384])\n",
            "Averaged stats: Lr: 0.000391  Loss: 0.9392  Acc@1: 91.6667 (92.3611)  Acc@5: 100.0000 (99.4444)\n",
            "torch.Size([9600, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 0.6844  Acc@1: 91.6667 (91.3889)  Acc@5: 100.0000 (99.7222)\n",
            "torch.Size([9600, 384])\n",
            "Averaged stats: Lr: 0.000352  Loss: 0.7409  Acc@1: 91.6667 (90.1389)  Acc@5: 100.0000 (99.8611)\n",
            "torch.Size([9600, 384])\n",
            "Averaged stats: Lr: 0.000330  Loss: 0.5040  Acc@1: 91.6667 (92.0833)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([9600, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 0.5895  Acc@1: 91.6667 (91.9444)  Acc@5: 100.0000 (99.8611)\n",
            "torch.Size([9600, 384])\n",
            "Averaged stats: Lr: 0.000283  Loss: 0.5078  Acc@1: 91.6667 (90.8333)  Acc@5: 100.0000 (99.3056)\n",
            "torch.Size([9600, 384])\n",
            "Averaged stats: Lr: 0.000259  Loss: 0.6616  Acc@1: 91.6667 (92.3611)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([9600, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 0.5318  Acc@1: 95.8333 (93.6111)  Acc@5: 100.0000 (99.8611)\n",
            "torch.Size([9600, 384])\n",
            "Averaged stats: Lr: 0.000210  Loss: 0.3980  Acc@1: 95.8333 (93.4722)  Acc@5: 100.0000 (99.8611)\n",
            "torch.Size([9600, 384])\n",
            "Averaged stats: Lr: 0.000186  Loss: 0.6233  Acc@1: 95.8333 (93.1944)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([9600, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.5473  Acc@1: 91.6667 (92.0833)  Acc@5: 100.0000 (99.7222)\n",
            "torch.Size([9600, 384])\n",
            "Averaged stats: Lr: 0.000139  Loss: 0.5505  Acc@1: 95.8333 (94.3056)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([9600, 384])\n",
            "Averaged stats: Lr: 0.000117  Loss: 0.3662  Acc@1: 91.6667 (93.8889)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([9600, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 0.6444  Acc@1: 95.8333 (94.0278)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([9600, 384])\n",
            "Averaged stats: Lr: 0.000078  Loss: 0.5113  Acc@1: 95.8333 (93.4722)  Acc@5: 100.0000 (99.8611)\n",
            "torch.Size([9600, 384])\n",
            "Averaged stats: Lr: 0.000060  Loss: 0.3649  Acc@1: 91.6667 (94.4444)  Acc@5: 100.0000 (99.8611)\n",
            "torch.Size([9600, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 0.5159  Acc@1: 91.6667 (92.3611)  Acc@5: 100.0000 (99.8611)\n",
            "torch.Size([9600, 384])\n",
            "Averaged stats: Lr: 0.000031  Loss: 0.4882  Acc@1: 91.6667 (93.0556)  Acc@5: 100.0000 (99.8611)\n",
            "torch.Size([9600, 384])\n",
            "Averaged stats: Lr: 0.000020  Loss: 0.4923  Acc@1: 91.6667 (93.1944)  Acc@5: 100.0000 (99.8611)\n",
            "torch.Size([9600, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.4006  Acc@1: 91.6667 (92.6389)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([9600, 384])\n",
            "Averaged stats: Lr: 0.000005  Loss: 0.3559  Acc@1: 91.6667 (93.3333)  Acc@5: 100.0000 (99.7222)\n",
            "torch.Size([9600, 384])\n",
            "Averaged stats: Lr: 0.000001  Loss: 0.3707  Acc@1: 95.8333 (94.1667)  Acc@5: 100.0000 (99.7222)\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:16  Loss: 0.5368 (0.5368)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.3912  data: 0.3120  max mem: 356\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 0.7233 (0.7533)  Acc@1: 83.3333 (80.3030)  Acc@5: 100.0000 (98.8636)  time: 0.1174  data: 0.0327  max mem: 356\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.7233 (0.7420)  Acc@1: 83.3333 (82.5397)  Acc@5: 100.0000 (98.6111)  time: 0.0885  data: 0.0025  max mem: 356\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.6848 (0.7233)  Acc@1: 87.5000 (83.7366)  Acc@5: 100.0000 (98.5215)  time: 0.0876  data: 0.0003  max mem: 356\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6628 (0.7055)  Acc@1: 87.5000 (84.2480)  Acc@5: 100.0000 (98.5772)  time: 0.0883  data: 0.0003  max mem: 356\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6628 (0.6970)  Acc@1: 87.5000 (84.4000)  Acc@5: 100.0000 (98.6000)  time: 0.0864  data: 0.0002  max mem: 356\n",
            "Test: [Task 1] Total time: 0:00:04 (0.0969 s / it)\n",
            "* Acc@1 84.400 Acc@5 98.600 loss 0.697\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:19  Loss: 1.0541 (1.0541)  Acc@1: 79.1667 (79.1667)  Acc@5: 87.5000 (87.5000)  time: 0.4698  data: 0.3985  max mem: 356\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:03  Loss: 0.9124 (0.9160)  Acc@1: 83.3333 (80.6818)  Acc@5: 95.8333 (95.8333)  time: 0.1237  data: 0.0367  max mem: 356\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 0.9124 (0.9378)  Acc@1: 83.3333 (80.9524)  Acc@5: 95.8333 (95.8333)  time: 0.0884  data: 0.0004  max mem: 356\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.8963 (0.9207)  Acc@1: 83.3333 (81.3172)  Acc@5: 95.8333 (96.1022)  time: 0.0886  data: 0.0003  max mem: 356\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.7977 (0.9040)  Acc@1: 83.3333 (81.5041)  Acc@5: 100.0000 (96.6463)  time: 0.0898  data: 0.0003  max mem: 356\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.7924 (0.8981)  Acc@1: 83.3333 (81.6000)  Acc@5: 100.0000 (96.7000)  time: 0.0879  data: 0.0003  max mem: 356\n",
            "Test: [Task 2] Total time: 0:00:04 (0.0995 s / it)\n",
            "* Acc@1 81.600 Acc@5 96.700 loss 0.898\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:18  Loss: 0.6741 (0.6741)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.4445  data: 0.3306  max mem: 356\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 0.8455 (0.8810)  Acc@1: 83.3333 (82.1970)  Acc@5: 95.8333 (96.5909)  time: 0.1244  data: 0.0321  max mem: 356\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 0.8220 (0.8316)  Acc@1: 83.3333 (82.9365)  Acc@5: 95.8333 (97.2222)  time: 0.0920  data: 0.0018  max mem: 356\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.7541 (0.8020)  Acc@1: 83.3333 (83.1989)  Acc@5: 95.8333 (97.3118)  time: 0.0914  data: 0.0010  max mem: 356\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.7871 (0.8114)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (97.1545)  time: 0.0910  data: 0.0005  max mem: 356\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.7871 (0.8162)  Acc@1: 83.3333 (83.2000)  Acc@5: 95.8333 (97.2000)  time: 0.0895  data: 0.0004  max mem: 356\n",
            "Test: [Task 3] Total time: 0:00:04 (0.1028 s / it)\n",
            "* Acc@1 83.200 Acc@5 97.200 loss 0.816\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:30  Loss: 1.2958 (1.2958)  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.8333)  time: 0.7154  data: 0.6160  max mem: 356\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:04  Loss: 0.8757 (0.9353)  Acc@1: 83.3333 (82.9545)  Acc@5: 100.0000 (98.8636)  time: 0.1491  data: 0.0564  max mem: 356\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 0.9061 (0.9567)  Acc@1: 83.3333 (81.5476)  Acc@5: 100.0000 (98.4127)  time: 0.0920  data: 0.0005  max mem: 356\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 0.9061 (0.9286)  Acc@1: 79.1667 (81.7204)  Acc@5: 100.0000 (98.1183)  time: 0.0921  data: 0.0004  max mem: 356\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.8860 (0.9447)  Acc@1: 79.1667 (80.8943)  Acc@5: 100.0000 (97.8659)  time: 0.0927  data: 0.0003  max mem: 356\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.8811 (0.9374)  Acc@1: 79.1667 (81.0000)  Acc@5: 100.0000 (97.9000)  time: 0.0913  data: 0.0003  max mem: 356\n",
            "Test: [Task 4] Total time: 0:00:04 (0.1086 s / it)\n",
            "* Acc@1 81.000 Acc@5 97.900 loss 0.937\n",
            "[Average accuracy till task4]\tAcc@1: 82.5500\tAcc@5: 97.6000\tLoss: 0.8372\tForgetting: 3.4000\tBackward: -3.0667\n",
            "Train: Epoch[1/1]  [  0/209]  eta: 0:01:31  Lr: 0.000047  Loss: 2.2615  Acc@1: 16.6667 (16.6667)  Acc@5: 45.8333 (45.8333)  time: 0.4393  data: 0.3160  max mem: 356\n",
            "Train: Epoch[1/1]  [ 10/209]  eta: 0:00:26  Lr: 0.000047  Loss: 2.2726  Acc@1: 16.6667 (17.4242)  Acc@5: 58.3333 (57.9545)  time: 0.1341  data: 0.0373  max mem: 356\n",
            "Train: Epoch[1/1]  [ 20/209]  eta: 0:00:21  Lr: 0.000047  Loss: 2.1839  Acc@1: 20.8333 (20.8333)  Acc@5: 66.6667 (65.2778)  time: 0.0987  data: 0.0051  max mem: 356\n",
            "Train: Epoch[1/1]  [ 30/209]  eta: 0:00:19  Lr: 0.000047  Loss: 2.0333  Acc@1: 25.0000 (23.9247)  Acc@5: 75.0000 (69.7581)  time: 0.0942  data: 0.0008  max mem: 356\n",
            "Train: Epoch[1/1]  [ 40/209]  eta: 0:00:17  Lr: 0.000047  Loss: 1.8660  Acc@1: 33.3333 (27.6423)  Acc@5: 83.3333 (73.5772)  time: 0.0945  data: 0.0004  max mem: 356\n",
            "Train: Epoch[1/1]  [ 50/209]  eta: 0:00:16  Lr: 0.000047  Loss: 1.8201  Acc@1: 41.6667 (32.1895)  Acc@5: 91.6667 (77.3693)  time: 0.0947  data: 0.0003  max mem: 356\n",
            "Train: Epoch[1/1]  [ 60/209]  eta: 0:00:15  Lr: 0.000047  Loss: 1.6113  Acc@1: 54.1667 (35.9973)  Acc@5: 91.6667 (79.8497)  time: 0.0955  data: 0.0004  max mem: 356\n",
            "Train: Epoch[1/1]  [ 70/209]  eta: 0:00:14  Lr: 0.000047  Loss: 1.4422  Acc@1: 54.1667 (38.9671)  Acc@5: 91.6667 (81.4554)  time: 0.0987  data: 0.0014  max mem: 356\n",
            "Train: Epoch[1/1]  [ 80/209]  eta: 0:00:13  Lr: 0.000047  Loss: 1.5830  Acc@1: 58.3333 (41.6667)  Acc@5: 95.8333 (82.9218)  time: 0.1029  data: 0.0022  max mem: 356\n",
            "Train: Epoch[1/1]  [ 90/209]  eta: 0:00:12  Lr: 0.000047  Loss: 1.3635  Acc@1: 62.5000 (44.0018)  Acc@5: 95.8333 (84.0659)  time: 0.1058  data: 0.0016  max mem: 356\n",
            "Train: Epoch[1/1]  [100/209]  eta: 0:00:11  Lr: 0.000047  Loss: 1.3200  Acc@1: 62.5000 (45.7921)  Acc@5: 95.8333 (84.9422)  time: 0.1080  data: 0.0009  max mem: 356\n",
            "Train: Epoch[1/1]  [110/209]  eta: 0:00:10  Lr: 0.000047  Loss: 1.2721  Acc@1: 62.5000 (47.4099)  Acc@5: 95.8333 (85.8484)  time: 0.1043  data: 0.0015  max mem: 356\n",
            "Train: Epoch[1/1]  [120/209]  eta: 0:00:09  Lr: 0.000047  Loss: 1.2832  Acc@1: 66.6667 (49.0014)  Acc@5: 95.8333 (86.7769)  time: 0.0968  data: 0.0017  max mem: 356\n",
            "Train: Epoch[1/1]  [130/209]  eta: 0:00:08  Lr: 0.000047  Loss: 1.3689  Acc@1: 70.8333 (50.5725)  Acc@5: 100.0000 (87.6272)  time: 0.0937  data: 0.0007  max mem: 356\n",
            "Train: Epoch[1/1]  [140/209]  eta: 0:00:06  Lr: 0.000047  Loss: 1.1509  Acc@1: 66.6667 (51.7731)  Acc@5: 100.0000 (88.4161)  time: 0.0931  data: 0.0006  max mem: 356\n",
            "Train: Epoch[1/1]  [150/209]  eta: 0:00:05  Lr: 0.000047  Loss: 0.9797  Acc@1: 70.8333 (52.8422)  Acc@5: 100.0000 (88.9625)  time: 0.0924  data: 0.0009  max mem: 356\n",
            "Train: Epoch[1/1]  [160/209]  eta: 0:00:04  Lr: 0.000047  Loss: 1.3356  Acc@1: 75.0000 (54.3478)  Acc@5: 95.8333 (89.4928)  time: 0.0926  data: 0.0013  max mem: 356\n",
            "Train: Epoch[1/1]  [170/209]  eta: 0:00:03  Lr: 0.000047  Loss: 0.9582  Acc@1: 75.0000 (55.5799)  Acc@5: 100.0000 (90.0585)  time: 0.0925  data: 0.0013  max mem: 356\n",
            "Train: Epoch[1/1]  [180/209]  eta: 0:00:02  Lr: 0.000047  Loss: 0.9887  Acc@1: 70.8333 (56.4917)  Acc@5: 100.0000 (90.4466)  time: 0.0922  data: 0.0009  max mem: 356\n",
            "Train: Epoch[1/1]  [190/209]  eta: 0:00:01  Lr: 0.000047  Loss: 1.0905  Acc@1: 70.8333 (57.2644)  Acc@5: 95.8333 (90.7504)  time: 0.0918  data: 0.0007  max mem: 356\n",
            "Train: Epoch[1/1]  [200/209]  eta: 0:00:00  Lr: 0.000047  Loss: 0.7677  Acc@1: 75.0000 (58.3748)  Acc@5: 95.8333 (91.1070)  time: 0.0912  data: 0.0006  max mem: 356\n",
            "Train: Epoch[1/1]  [208/209]  eta: 0:00:00  Lr: 0.000047  Loss: 0.8219  Acc@1: 75.0000 (58.9600)  Acc@5: 100.0000 (91.3800)  time: 0.0888  data: 0.0004  max mem: 356\n",
            "Train: Epoch[1/1] Total time: 0:00:20 (0.0986 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.8219  Acc@1: 75.0000 (58.9600)  Acc@5: 100.0000 (91.3800)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:29  Loss: 0.6837 (0.6837)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.6955  data: 0.5882  max mem: 356\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 0.7964 (0.8020)  Acc@1: 79.1667 (79.5455)  Acc@5: 100.0000 (99.2424)  time: 0.1470  data: 0.0553  max mem: 356\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.7964 (0.8034)  Acc@1: 83.3333 (82.3413)  Acc@5: 100.0000 (98.4127)  time: 0.0908  data: 0.0015  max mem: 356\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.7689 (0.7900)  Acc@1: 83.3333 (82.9301)  Acc@5: 100.0000 (98.5215)  time: 0.0902  data: 0.0015  max mem: 356\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.7136 (0.7717)  Acc@1: 87.5000 (84.2480)  Acc@5: 100.0000 (98.7805)  time: 0.0904  data: 0.0014  max mem: 356\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.7136 (0.7628)  Acc@1: 87.5000 (84.4000)  Acc@5: 100.0000 (98.8000)  time: 0.0889  data: 0.0010  max mem: 356\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1066 s / it)\n",
            "* Acc@1 84.400 Acc@5 98.800 loss 0.763\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:14  Loss: 1.1214 (1.1214)  Acc@1: 79.1667 (79.1667)  Acc@5: 87.5000 (87.5000)  time: 0.3440  data: 0.2639  max mem: 356\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 0.9694 (0.9647)  Acc@1: 83.3333 (80.3030)  Acc@5: 95.8333 (96.5909)  time: 0.1388  data: 0.0510  max mem: 356\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 0.9694 (0.9905)  Acc@1: 83.3333 (80.5556)  Acc@5: 95.8333 (95.8333)  time: 0.1033  data: 0.0150  max mem: 356\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.9679 (0.9746)  Acc@1: 79.1667 (80.3763)  Acc@5: 95.8333 (95.9677)  time: 0.0889  data: 0.0003  max mem: 356\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.8534 (0.9597)  Acc@1: 79.1667 (80.5894)  Acc@5: 95.8333 (96.5447)  time: 0.0898  data: 0.0002  max mem: 356\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.8428 (0.9536)  Acc@1: 79.1667 (80.7000)  Acc@5: 100.0000 (96.6000)  time: 0.0876  data: 0.0002  max mem: 356\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1035 s / it)\n",
            "* Acc@1 80.700 Acc@5 96.600 loss 0.954\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:19  Loss: 0.7217 (0.7217)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  time: 0.4620  data: 0.3722  max mem: 356\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 0.8918 (0.9358)  Acc@1: 79.1667 (82.5758)  Acc@5: 95.8333 (96.5909)  time: 0.1240  data: 0.0357  max mem: 356\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 0.8843 (0.8808)  Acc@1: 79.1667 (82.3413)  Acc@5: 95.8333 (97.2222)  time: 0.0890  data: 0.0012  max mem: 356\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.7989 (0.8510)  Acc@1: 83.3333 (83.0645)  Acc@5: 100.0000 (97.7151)  time: 0.0880  data: 0.0004  max mem: 356\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.8084 (0.8578)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (97.1545)  time: 0.0884  data: 0.0003  max mem: 356\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.8244 (0.8653)  Acc@1: 83.3333 (83.1000)  Acc@5: 95.8333 (97.2000)  time: 0.0865  data: 0.0003  max mem: 356\n",
            "Test: [Task 3] Total time: 0:00:04 (0.0988 s / it)\n",
            "* Acc@1 83.100 Acc@5 97.200 loss 0.865\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:18  Loss: 1.5830 (1.5830)  Acc@1: 58.3333 (58.3333)  Acc@5: 83.3333 (83.3333)  time: 0.4404  data: 0.3699  max mem: 356\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:03  Loss: 1.0176 (1.0725)  Acc@1: 79.1667 (76.8939)  Acc@5: 95.8333 (95.4545)  time: 0.1218  data: 0.0346  max mem: 356\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 1.0771 (1.1068)  Acc@1: 75.0000 (75.7937)  Acc@5: 95.8333 (95.6349)  time: 0.0887  data: 0.0015  max mem: 356\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 1.0771 (1.0813)  Acc@1: 79.1667 (77.5538)  Acc@5: 95.8333 (95.9677)  time: 0.0881  data: 0.0018  max mem: 356\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 1.0558 (1.0934)  Acc@1: 79.1667 (77.3374)  Acc@5: 95.8333 (95.8333)  time: 0.0887  data: 0.0010  max mem: 356\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 1.0464 (1.0864)  Acc@1: 79.1667 (77.5000)  Acc@5: 95.8333 (95.8000)  time: 0.0870  data: 0.0009  max mem: 356\n",
            "Test: [Task 4] Total time: 0:00:04 (0.0992 s / it)\n",
            "* Acc@1 77.500 Acc@5 95.800 loss 1.086\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:26  Loss: 3.0814 (3.0814)  Acc@1: 0.0000 (0.0000)  Acc@5: 70.8333 (70.8333)  time: 0.6255  data: 0.4462  max mem: 356\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:04  Loss: 3.1274 (3.1250)  Acc@1: 4.1667 (4.1667)  Acc@5: 70.8333 (67.4242)  time: 0.1378  data: 0.0419  max mem: 356\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 3.1355 (3.1438)  Acc@1: 4.1667 (3.3730)  Acc@5: 66.6667 (67.8571)  time: 0.0881  data: 0.0009  max mem: 356\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 3.2113 (3.1668)  Acc@1: 0.0000 (3.2258)  Acc@5: 66.6667 (66.5323)  time: 0.0871  data: 0.0004  max mem: 356\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 3.2286 (3.2048)  Acc@1: 0.0000 (2.9472)  Acc@5: 58.3333 (64.7358)  time: 0.0876  data: 0.0003  max mem: 356\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 3.2759 (3.2121)  Acc@1: 0.0000 (2.9000)  Acc@5: 58.3333 (64.5000)  time: 0.0857  data: 0.0003  max mem: 356\n",
            "Test: [Task 5] Total time: 0:00:04 (0.1023 s / it)\n",
            "* Acc@1 2.900 Acc@5 64.500 loss 3.212\n",
            "[Average accuracy till task5]\tAcc@1: 65.7200\tAcc@5: 90.5800\tLoss: 1.3761\tForgetting: 2.7750\tBackward: 24.4750\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([11976, 384])\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 1.0461  Acc@1: 79.1667 (77.1875)  Acc@5: 100.0000 (96.0417)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000467  Loss: 0.7713  Acc@1: 79.1667 (79.0625)  Acc@5: 100.0000 (97.3958)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000464  Loss: 0.5223  Acc@1: 83.3333 (85.1042)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 0.9314  Acc@1: 91.6667 (87.8125)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000448  Loss: 0.4208  Acc@1: 91.6667 (87.9167)  Acc@5: 100.0000 (99.4792)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000437  Loss: 0.6648  Acc@1: 91.6667 (90.4167)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 0.8259  Acc@1: 87.5000 (90.2083)  Acc@5: 100.0000 (99.6875)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000409  Loss: 0.6204  Acc@1: 91.6667 (91.1458)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000391  Loss: 0.5613  Acc@1: 87.5000 (90.0000)  Acc@5: 100.0000 (99.6875)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 0.6482  Acc@1: 87.5000 (88.5417)  Acc@5: 100.0000 (99.4792)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000352  Loss: 0.6865  Acc@1: 87.5000 (90.7292)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000330  Loss: 0.4439  Acc@1: 95.8333 (92.9167)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 0.5117  Acc@1: 87.5000 (91.2500)  Acc@5: 100.0000 (99.6875)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000283  Loss: 0.7619  Acc@1: 91.6667 (90.9375)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000259  Loss: 0.5731  Acc@1: 91.6667 (91.5625)  Acc@5: 100.0000 (99.8958)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 0.6106  Acc@1: 95.8333 (93.3333)  Acc@5: 100.0000 (99.8958)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000210  Loss: 0.4847  Acc@1: 95.8333 (92.9167)  Acc@5: 100.0000 (99.8958)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000186  Loss: 0.4254  Acc@1: 95.8333 (93.3333)  Acc@5: 100.0000 (99.6875)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.5500  Acc@1: 91.6667 (93.3333)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000139  Loss: 0.4175  Acc@1: 95.8333 (93.4375)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000117  Loss: 0.3941  Acc@1: 95.8333 (93.3333)  Acc@5: 100.0000 (99.8958)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 0.5283  Acc@1: 95.8333 (92.2917)  Acc@5: 100.0000 (99.8958)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000078  Loss: 0.3168  Acc@1: 91.6667 (92.0833)  Acc@5: 100.0000 (99.8958)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000060  Loss: 0.4346  Acc@1: 91.6667 (94.2708)  Acc@5: 100.0000 (99.8958)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 0.5219  Acc@1: 91.6667 (93.5417)  Acc@5: 100.0000 (99.6875)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000031  Loss: 0.4958  Acc@1: 91.6667 (92.9167)  Acc@5: 100.0000 (99.8958)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000020  Loss: 0.4686  Acc@1: 95.8333 (93.2292)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.5016  Acc@1: 91.6667 (91.9792)  Acc@5: 100.0000 (99.8958)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000005  Loss: 0.6027  Acc@1: 91.6667 (93.5417)  Acc@5: 100.0000 (99.8958)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000001  Loss: 0.4379  Acc@1: 91.6667 (91.4583)  Acc@5: 100.0000 (99.5833)\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:30  Loss: 0.6048 (0.6048)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.7182  data: 0.5480  max mem: 356\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 0.6552 (0.7071)  Acc@1: 79.1667 (79.5455)  Acc@5: 100.0000 (98.4848)  time: 0.1446  data: 0.0515  max mem: 356\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.6552 (0.7042)  Acc@1: 83.3333 (82.1429)  Acc@5: 100.0000 (98.2143)  time: 0.0887  data: 0.0015  max mem: 356\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.6418 (0.6864)  Acc@1: 87.5000 (82.7957)  Acc@5: 100.0000 (98.3871)  time: 0.0904  data: 0.0007  max mem: 356\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6017 (0.6654)  Acc@1: 87.5000 (83.6382)  Acc@5: 100.0000 (98.6789)  time: 0.0909  data: 0.0002  max mem: 356\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6017 (0.6563)  Acc@1: 87.5000 (83.8000)  Acc@5: 100.0000 (98.7000)  time: 0.0894  data: 0.0002  max mem: 356\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1060 s / it)\n",
            "* Acc@1 83.800 Acc@5 98.700 loss 0.656\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:14  Loss: 1.0347 (1.0347)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  time: 0.3448  data: 0.2640  max mem: 356\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:03  Loss: 0.8706 (0.8503)  Acc@1: 79.1667 (78.7879)  Acc@5: 95.8333 (96.2121)  time: 0.1221  data: 0.0361  max mem: 356\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 0.8743 (0.8828)  Acc@1: 79.1667 (78.7698)  Acc@5: 95.8333 (95.8333)  time: 0.0956  data: 0.0068  max mem: 356\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.8743 (0.8665)  Acc@1: 83.3333 (79.4355)  Acc@5: 95.8333 (95.9677)  time: 0.0919  data: 0.0004  max mem: 356\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.7598 (0.8461)  Acc@1: 79.1667 (79.7764)  Acc@5: 100.0000 (96.4431)  time: 0.0929  data: 0.0003  max mem: 356\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.7494 (0.8403)  Acc@1: 79.1667 (79.9000)  Acc@5: 100.0000 (96.5000)  time: 0.0909  data: 0.0003  max mem: 356\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1014 s / it)\n",
            "* Acc@1 79.900 Acc@5 96.500 loss 0.840\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:20  Loss: 0.6423 (0.6423)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.4976  data: 0.4035  max mem: 356\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:04  Loss: 0.8089 (0.8421)  Acc@1: 79.1667 (78.7879)  Acc@5: 95.8333 (96.5909)  time: 0.1298  data: 0.0375  max mem: 356\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 0.7784 (0.7944)  Acc@1: 79.1667 (80.5556)  Acc@5: 95.8333 (96.8254)  time: 0.0928  data: 0.0006  max mem: 356\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.7066 (0.7685)  Acc@1: 79.1667 (80.7796)  Acc@5: 95.8333 (97.1774)  time: 0.0933  data: 0.0003  max mem: 356\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.7723 (0.7755)  Acc@1: 83.3333 (81.5041)  Acc@5: 95.8333 (96.7480)  time: 0.0939  data: 0.0003  max mem: 356\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.7723 (0.7800)  Acc@1: 83.3333 (81.4000)  Acc@5: 95.8333 (96.8000)  time: 0.0922  data: 0.0003  max mem: 356\n",
            "Test: [Task 3] Total time: 0:00:04 (0.1050 s / it)\n",
            "* Acc@1 81.400 Acc@5 96.800 loss 0.780\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:27  Loss: 1.1250 (1.1250)  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.8333)  time: 0.6448  data: 0.5210  max mem: 356\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:04  Loss: 0.7882 (0.8085)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (98.1061)  time: 0.1453  data: 0.0484  max mem: 356\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 0.7995 (0.8251)  Acc@1: 83.3333 (81.1508)  Acc@5: 100.0000 (97.8175)  time: 0.0950  data: 0.0008  max mem: 356\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 0.7995 (0.7862)  Acc@1: 79.1667 (82.1237)  Acc@5: 100.0000 (97.7151)  time: 0.0949  data: 0.0007  max mem: 356\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7077 (0.8022)  Acc@1: 79.1667 (80.9959)  Acc@5: 95.8333 (97.4594)  time: 0.0952  data: 0.0005  max mem: 356\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.7056 (0.7947)  Acc@1: 79.1667 (81.2000)  Acc@5: 95.8333 (97.5000)  time: 0.0939  data: 0.0004  max mem: 356\n",
            "Test: [Task 4] Total time: 0:00:04 (0.1097 s / it)\n",
            "* Acc@1 81.200 Acc@5 97.500 loss 0.795\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:14  Loss: 0.8157 (0.8157)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.3561  data: 0.2764  max mem: 356\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:03  Loss: 0.9227 (0.9172)  Acc@1: 79.1667 (78.7879)  Acc@5: 100.0000 (98.1061)  time: 0.1204  data: 0.0285  max mem: 356\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 0.9278 (0.9537)  Acc@1: 79.1667 (77.1825)  Acc@5: 95.8333 (97.0238)  time: 0.0962  data: 0.0020  max mem: 356\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 0.9950 (0.9661)  Acc@1: 79.1667 (77.0161)  Acc@5: 95.8333 (96.7742)  time: 0.0952  data: 0.0004  max mem: 356\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 1.0353 (1.0099)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.9350)  time: 0.0947  data: 0.0003  max mem: 356\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 1.0575 (1.0237)  Acc@1: 75.0000 (74.7000)  Acc@5: 95.8333 (95.8000)  time: 0.0933  data: 0.0003  max mem: 356\n",
            "Test: [Task 5] Total time: 0:00:04 (0.1030 s / it)\n",
            "* Acc@1 74.700 Acc@5 95.800 loss 1.024\n",
            "[Average accuracy till task5]\tAcc@1: 80.2000\tAcc@5: 97.0600\tLoss: 0.8190\tForgetting: 3.5750\tBackward: -3.2750\n",
            "Train: Epoch[1/1]  [  0/209]  eta: 0:01:46  Lr: 0.000047  Loss: 2.2555  Acc@1: 16.6667 (16.6667)  Acc@5: 62.5000 (62.5000)  time: 0.5082  data: 0.3812  max mem: 356\n",
            "Train: Epoch[1/1]  [ 10/209]  eta: 0:00:28  Lr: 0.000047  Loss: 2.2619  Acc@1: 12.5000 (10.9848)  Acc@5: 54.1667 (56.0606)  time: 0.1421  data: 0.0452  max mem: 356\n",
            "Train: Epoch[1/1]  [ 20/209]  eta: 0:00:22  Lr: 0.000047  Loss: 2.1081  Acc@1: 16.6667 (14.2857)  Acc@5: 66.6667 (63.6905)  time: 0.1001  data: 0.0061  max mem: 356\n",
            "Train: Epoch[1/1]  [ 30/209]  eta: 0:00:20  Lr: 0.000047  Loss: 1.8921  Acc@1: 20.8333 (17.3387)  Acc@5: 75.0000 (68.4140)  time: 0.0958  data: 0.0007  max mem: 356\n",
            "Train: Epoch[1/1]  [ 40/209]  eta: 0:00:18  Lr: 0.000047  Loss: 1.8183  Acc@1: 25.0000 (22.1545)  Acc@5: 83.3333 (73.1707)  time: 0.1012  data: 0.0012  max mem: 356\n",
            "Train: Epoch[1/1]  [ 50/209]  eta: 0:00:17  Lr: 0.000047  Loss: 1.6835  Acc@1: 37.5000 (25.8170)  Acc@5: 87.5000 (75.8170)  time: 0.1050  data: 0.0020  max mem: 356\n",
            "Train: Epoch[1/1]  [ 60/209]  eta: 0:00:16  Lr: 0.000047  Loss: 1.7285  Acc@1: 45.8333 (30.1913)  Acc@5: 91.6667 (79.0301)  time: 0.1052  data: 0.0012  max mem: 356\n",
            "Train: Epoch[1/1]  [ 70/209]  eta: 0:00:15  Lr: 0.000047  Loss: 1.8149  Acc@1: 50.0000 (33.3333)  Acc@5: 95.8333 (81.1033)  time: 0.1067  data: 0.0022  max mem: 356\n",
            "Train: Epoch[1/1]  [ 80/209]  eta: 0:00:14  Lr: 0.000047  Loss: 1.3874  Acc@1: 54.1667 (36.5741)  Acc@5: 95.8333 (82.6646)  time: 0.1095  data: 0.0023  max mem: 356\n",
            "Train: Epoch[1/1]  [ 90/209]  eta: 0:00:12  Lr: 0.000047  Loss: 1.6881  Acc@1: 58.3333 (38.9194)  Acc@5: 91.6667 (83.9286)  time: 0.1038  data: 0.0009  max mem: 356\n",
            "Train: Epoch[1/1]  [100/209]  eta: 0:00:11  Lr: 0.000047  Loss: 1.4239  Acc@1: 54.1667 (40.9653)  Acc@5: 95.8333 (85.0660)  time: 0.0961  data: 0.0009  max mem: 356\n",
            "Train: Epoch[1/1]  [110/209]  eta: 0:00:10  Lr: 0.000047  Loss: 1.2680  Acc@1: 58.3333 (43.1682)  Acc@5: 95.8333 (85.8859)  time: 0.0973  data: 0.0008  max mem: 356\n",
            "Train: Epoch[1/1]  [120/209]  eta: 0:00:09  Lr: 0.000047  Loss: 1.1972  Acc@1: 62.5000 (44.6281)  Acc@5: 95.8333 (86.7424)  time: 0.0952  data: 0.0011  max mem: 356\n",
            "Train: Epoch[1/1]  [130/209]  eta: 0:00:08  Lr: 0.000047  Loss: 1.0557  Acc@1: 66.6667 (46.5967)  Acc@5: 95.8333 (87.4682)  time: 0.0914  data: 0.0009  max mem: 356\n",
            "Train: Epoch[1/1]  [140/209]  eta: 0:00:07  Lr: 0.000047  Loss: 1.0599  Acc@1: 70.8333 (48.3156)  Acc@5: 100.0000 (88.1501)  time: 0.0909  data: 0.0006  max mem: 356\n",
            "Train: Epoch[1/1]  [150/209]  eta: 0:00:06  Lr: 0.000047  Loss: 1.2551  Acc@1: 66.6667 (49.5309)  Acc@5: 100.0000 (88.7417)  time: 0.0909  data: 0.0006  max mem: 356\n",
            "Train: Epoch[1/1]  [160/209]  eta: 0:00:04  Lr: 0.000047  Loss: 1.0946  Acc@1: 66.6667 (50.4141)  Acc@5: 100.0000 (89.3375)  time: 0.0909  data: 0.0005  max mem: 356\n",
            "Train: Epoch[1/1]  [170/209]  eta: 0:00:03  Lr: 0.000047  Loss: 1.0168  Acc@1: 66.6667 (51.3645)  Acc@5: 100.0000 (89.8392)  time: 0.0908  data: 0.0005  max mem: 356\n",
            "Train: Epoch[1/1]  [180/209]  eta: 0:00:02  Lr: 0.000047  Loss: 1.1706  Acc@1: 66.6667 (52.3020)  Acc@5: 100.0000 (90.2855)  time: 0.0904  data: 0.0005  max mem: 356\n",
            "Train: Epoch[1/1]  [190/209]  eta: 0:00:01  Lr: 0.000047  Loss: 0.9452  Acc@1: 66.6667 (53.0977)  Acc@5: 100.0000 (90.7068)  time: 0.0901  data: 0.0007  max mem: 356\n",
            "Train: Epoch[1/1]  [200/209]  eta: 0:00:00  Lr: 0.000047  Loss: 1.1464  Acc@1: 70.8333 (54.0630)  Acc@5: 100.0000 (91.0655)  time: 0.0942  data: 0.0012  max mem: 356\n",
            "Train: Epoch[1/1]  [208/209]  eta: 0:00:00  Lr: 0.000047  Loss: 1.5283  Acc@1: 70.8333 (54.6400)  Acc@5: 100.0000 (91.3200)  time: 0.0905  data: 0.0008  max mem: 356\n",
            "Train: Epoch[1/1] Total time: 0:00:20 (0.0994 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.5283  Acc@1: 70.8333 (54.6400)  Acc@5: 100.0000 (91.3200)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:30  Loss: 0.7114 (0.7114)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.7323  data: 0.6337  max mem: 356\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 0.7588 (0.7773)  Acc@1: 79.1667 (78.4091)  Acc@5: 100.0000 (98.1061)  time: 0.1532  data: 0.0679  max mem: 356\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.7588 (0.7805)  Acc@1: 79.1667 (80.9524)  Acc@5: 100.0000 (97.6190)  time: 0.0920  data: 0.0062  max mem: 356\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.6915 (0.7622)  Acc@1: 83.3333 (81.5860)  Acc@5: 100.0000 (97.5806)  time: 0.0888  data: 0.0009  max mem: 356\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6644 (0.7402)  Acc@1: 83.3333 (82.8252)  Acc@5: 100.0000 (97.9675)  time: 0.0887  data: 0.0003  max mem: 356\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6600 (0.7305)  Acc@1: 83.3333 (83.0000)  Acc@5: 100.0000 (98.0000)  time: 0.0873  data: 0.0003  max mem: 356\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1072 s / it)\n",
            "* Acc@1 83.000 Acc@5 98.000 loss 0.730\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:14  Loss: 1.0576 (1.0576)  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  time: 0.3460  data: 0.2703  max mem: 356\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:03  Loss: 0.9633 (0.9054)  Acc@1: 83.3333 (78.0303)  Acc@5: 95.8333 (95.8333)  time: 0.1210  data: 0.0372  max mem: 356\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 0.9610 (0.9429)  Acc@1: 79.1667 (78.3730)  Acc@5: 95.8333 (95.2381)  time: 0.0930  data: 0.0072  max mem: 356\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.9391 (0.9304)  Acc@1: 75.0000 (77.8226)  Acc@5: 95.8333 (95.1613)  time: 0.0881  data: 0.0005  max mem: 356\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.8519 (0.9127)  Acc@1: 79.1667 (78.2520)  Acc@5: 95.8333 (95.7317)  time: 0.0884  data: 0.0004  max mem: 356\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.8261 (0.9066)  Acc@1: 79.1667 (78.4000)  Acc@5: 95.8333 (95.8000)  time: 0.0861  data: 0.0004  max mem: 356\n",
            "Test: [Task 2] Total time: 0:00:04 (0.0979 s / it)\n",
            "* Acc@1 78.400 Acc@5 95.800 loss 0.907\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:20  Loss: 0.7178 (0.7178)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.4894  data: 0.3887  max mem: 356\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 0.8850 (0.9295)  Acc@1: 79.1667 (78.0303)  Acc@5: 95.8333 (96.5909)  time: 0.1233  data: 0.0362  max mem: 356\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 0.8824 (0.8907)  Acc@1: 79.1667 (79.3651)  Acc@5: 95.8333 (96.4286)  time: 0.0861  data: 0.0007  max mem: 356\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.8265 (0.8623)  Acc@1: 79.1667 (79.3011)  Acc@5: 95.8333 (97.0430)  time: 0.0861  data: 0.0004  max mem: 356\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.8509 (0.8727)  Acc@1: 75.0000 (78.8618)  Acc@5: 95.8333 (96.7480)  time: 0.0870  data: 0.0004  max mem: 356\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.8509 (0.8791)  Acc@1: 75.0000 (78.5000)  Acc@5: 95.8333 (96.8000)  time: 0.0849  data: 0.0004  max mem: 356\n",
            "Test: [Task 3] Total time: 0:00:04 (0.0984 s / it)\n",
            "* Acc@1 78.500 Acc@5 96.800 loss 0.879\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:34  Loss: 1.3097 (1.3097)  Acc@1: 66.6667 (66.6667)  Acc@5: 91.6667 (91.6667)  time: 0.8186  data: 0.7080  max mem: 356\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:04  Loss: 0.8644 (0.8990)  Acc@1: 79.1667 (78.0303)  Acc@5: 100.0000 (97.7273)  time: 0.1557  data: 0.0667  max mem: 356\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 0.9035 (0.9117)  Acc@1: 79.1667 (77.3810)  Acc@5: 95.8333 (97.0238)  time: 0.0883  data: 0.0014  max mem: 356\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 0.8181 (0.8766)  Acc@1: 79.1667 (78.4946)  Acc@5: 95.8333 (97.0430)  time: 0.0878  data: 0.0021  max mem: 356\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.8140 (0.8934)  Acc@1: 79.1667 (78.3537)  Acc@5: 95.8333 (96.9512)  time: 0.0876  data: 0.0028  max mem: 356\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.8118 (0.8864)  Acc@1: 79.1667 (78.6000)  Acc@5: 95.8333 (97.0000)  time: 0.0863  data: 0.0027  max mem: 356\n",
            "Test: [Task 4] Total time: 0:00:04 (0.1069 s / it)\n",
            "* Acc@1 78.600 Acc@5 97.000 loss 0.886\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:16  Loss: 0.8474 (0.8474)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.4018  data: 0.3230  max mem: 356\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:03  Loss: 0.9621 (0.9722)  Acc@1: 75.0000 (77.6515)  Acc@5: 100.0000 (97.7273)  time: 0.1170  data: 0.0328  max mem: 356\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 1.0097 (0.9980)  Acc@1: 75.0000 (75.7937)  Acc@5: 95.8333 (96.6270)  time: 0.0868  data: 0.0020  max mem: 356\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 1.0381 (1.0088)  Acc@1: 75.0000 (75.2688)  Acc@5: 95.8333 (96.5054)  time: 0.0858  data: 0.0004  max mem: 356\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 1.0743 (1.0543)  Acc@1: 70.8333 (74.1870)  Acc@5: 95.8333 (95.7317)  time: 0.0869  data: 0.0004  max mem: 356\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 1.1404 (1.0686)  Acc@1: 70.8333 (74.0000)  Acc@5: 93.7500 (95.7000)  time: 0.0850  data: 0.0004  max mem: 356\n",
            "Test: [Task 5] Total time: 0:00:04 (0.0957 s / it)\n",
            "* Acc@1 74.000 Acc@5 95.700 loss 1.069\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:15  Loss: 3.6169 (3.6169)  Acc@1: 4.1667 (4.1667)  Acc@5: 45.8333 (45.8333)  time: 0.3719  data: 0.2858  max mem: 356\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:03  Loss: 3.8213 (3.8024)  Acc@1: 0.0000 (1.1364)  Acc@5: 37.5000 (39.3939)  time: 0.1142  data: 0.0292  max mem: 356\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 3.7961 (3.7905)  Acc@1: 0.0000 (1.1905)  Acc@5: 37.5000 (38.8889)  time: 0.0862  data: 0.0020  max mem: 356\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 3.7961 (3.8192)  Acc@1: 0.0000 (1.4785)  Acc@5: 37.5000 (38.3065)  time: 0.0857  data: 0.0005  max mem: 356\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 3.8312 (3.8199)  Acc@1: 0.0000 (1.2195)  Acc@5: 37.5000 (38.0081)  time: 0.0873  data: 0.0004  max mem: 356\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 3.8198 (3.8199)  Acc@1: 0.0000 (1.2000)  Acc@5: 37.5000 (37.8000)  time: 0.0852  data: 0.0004  max mem: 356\n",
            "Test: [Task 6] Total time: 0:00:03 (0.0948 s / it)\n",
            "* Acc@1 1.200 Acc@5 37.800 loss 3.820\n",
            "[Average accuracy till task6]\tAcc@1: 65.6167\tAcc@5: 86.8500\tLoss: 1.3818\tForgetting: 3.8800\tBackward: 32.3600\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([14376, 384])\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 0.6090  Acc@1: 79.1667 (80.3333)  Acc@5: 95.8333 (94.5000)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000467  Loss: 0.5654  Acc@1: 79.1667 (78.2500)  Acc@5: 100.0000 (96.9167)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000464  Loss: 0.8025  Acc@1: 83.3333 (82.6667)  Acc@5: 100.0000 (98.7500)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 0.9430  Acc@1: 83.3333 (87.5000)  Acc@5: 100.0000 (99.6667)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000448  Loss: 0.6992  Acc@1: 87.5000 (87.0000)  Acc@5: 100.0000 (99.0000)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000437  Loss: 0.5701  Acc@1: 91.6667 (89.4167)  Acc@5: 100.0000 (99.6667)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 0.6066  Acc@1: 87.5000 (88.6667)  Acc@5: 100.0000 (99.5000)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000409  Loss: 0.8872  Acc@1: 87.5000 (88.8333)  Acc@5: 100.0000 (99.6667)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000391  Loss: 0.6862  Acc@1: 91.6667 (91.8333)  Acc@5: 100.0000 (99.9167)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 0.4635  Acc@1: 91.6667 (90.4167)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000352  Loss: 0.6753  Acc@1: 91.6667 (90.7500)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000330  Loss: 0.4327  Acc@1: 91.6667 (91.0833)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 0.3934  Acc@1: 91.6667 (92.0833)  Acc@5: 100.0000 (99.4167)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000283  Loss: 0.4712  Acc@1: 95.8333 (91.7500)  Acc@5: 100.0000 (99.9167)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000259  Loss: 0.5198  Acc@1: 91.6667 (92.0000)  Acc@5: 100.0000 (99.8333)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 0.5276  Acc@1: 91.6667 (91.7500)  Acc@5: 100.0000 (99.7500)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000210  Loss: 0.5112  Acc@1: 91.6667 (92.1667)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000186  Loss: 0.5166  Acc@1: 91.6667 (92.0833)  Acc@5: 100.0000 (99.8333)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.4532  Acc@1: 91.6667 (92.4167)  Acc@5: 100.0000 (99.8333)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000139  Loss: 0.6438  Acc@1: 91.6667 (92.6667)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000117  Loss: 0.4931  Acc@1: 91.6667 (91.2500)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 0.3503  Acc@1: 91.6667 (91.8333)  Acc@5: 100.0000 (99.7500)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000078  Loss: 0.4564  Acc@1: 91.6667 (91.9167)  Acc@5: 100.0000 (99.8333)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000060  Loss: 0.4701  Acc@1: 91.6667 (92.8333)  Acc@5: 100.0000 (99.9167)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 0.4581  Acc@1: 91.6667 (92.3333)  Acc@5: 100.0000 (99.8333)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000031  Loss: 0.5129  Acc@1: 95.8333 (92.5833)  Acc@5: 100.0000 (99.9167)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000020  Loss: 0.3681  Acc@1: 91.6667 (93.0833)  Acc@5: 100.0000 (99.9167)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.5906  Acc@1: 91.6667 (91.0833)  Acc@5: 100.0000 (99.6667)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000005  Loss: 0.6599  Acc@1: 91.6667 (92.1667)  Acc@5: 100.0000 (99.7500)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000001  Loss: 0.4331  Acc@1: 95.8333 (92.0833)  Acc@5: 100.0000 (99.8333)\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:14  Loss: 0.7368 (0.7368)  Acc@1: 70.8333 (70.8333)  Acc@5: 100.0000 (100.0000)  time: 0.3473  data: 0.2721  max mem: 357\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 0.7352 (0.7198)  Acc@1: 75.0000 (78.4091)  Acc@5: 100.0000 (98.1061)  time: 0.1196  data: 0.0322  max mem: 357\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.7234 (0.7165)  Acc@1: 83.3333 (80.7540)  Acc@5: 100.0000 (97.6190)  time: 0.0935  data: 0.0046  max mem: 357\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.6505 (0.6919)  Acc@1: 83.3333 (81.4516)  Acc@5: 100.0000 (97.8495)  time: 0.0906  data: 0.0010  max mem: 357\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.5998 (0.6746)  Acc@1: 87.5000 (82.4187)  Acc@5: 100.0000 (97.9675)  time: 0.0912  data: 0.0007  max mem: 357\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.5998 (0.6652)  Acc@1: 87.5000 (82.6000)  Acc@5: 100.0000 (98.0000)  time: 0.0892  data: 0.0006  max mem: 357\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1014 s / it)\n",
            "* Acc@1 82.600 Acc@5 98.000 loss 0.665\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:32  Loss: 1.0084 (1.0084)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  time: 0.7775  data: 0.6551  max mem: 357\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 0.8874 (0.8431)  Acc@1: 79.1667 (78.4091)  Acc@5: 95.8333 (95.0758)  time: 0.1535  data: 0.0606  max mem: 357\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 0.8834 (0.8837)  Acc@1: 75.0000 (76.3889)  Acc@5: 95.8333 (94.4444)  time: 0.0910  data: 0.0008  max mem: 357\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.8706 (0.8710)  Acc@1: 75.0000 (77.1505)  Acc@5: 95.8333 (94.8925)  time: 0.0920  data: 0.0004  max mem: 357\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.7639 (0.8468)  Acc@1: 79.1667 (77.7439)  Acc@5: 95.8333 (95.5285)  time: 0.0927  data: 0.0003  max mem: 357\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.7272 (0.8410)  Acc@1: 79.1667 (77.9000)  Acc@5: 95.8333 (95.6000)  time: 0.0909  data: 0.0003  max mem: 357\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1097 s / it)\n",
            "* Acc@1 77.900 Acc@5 95.600 loss 0.841\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:15  Loss: 0.5564 (0.5564)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.3727  data: 0.2929  max mem: 357\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 0.7963 (0.8027)  Acc@1: 83.3333 (79.1667)  Acc@5: 95.8333 (96.2121)  time: 0.1204  data: 0.0285  max mem: 357\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 0.7413 (0.7482)  Acc@1: 83.3333 (80.9524)  Acc@5: 95.8333 (96.2302)  time: 0.0943  data: 0.0012  max mem: 357\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.6662 (0.7279)  Acc@1: 79.1667 (80.1075)  Acc@5: 95.8333 (96.9086)  time: 0.0932  data: 0.0004  max mem: 357\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.7287 (0.7345)  Acc@1: 79.1667 (80.8943)  Acc@5: 95.8333 (96.6463)  time: 0.0932  data: 0.0003  max mem: 357\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.7287 (0.7382)  Acc@1: 79.1667 (80.7000)  Acc@5: 95.8333 (96.7000)  time: 0.0916  data: 0.0003  max mem: 357\n",
            "Test: [Task 3] Total time: 0:00:04 (0.1017 s / it)\n",
            "* Acc@1 80.700 Acc@5 96.700 loss 0.738\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:14  Loss: 1.0882 (1.0882)  Acc@1: 66.6667 (66.6667)  Acc@5: 95.8333 (95.8333)  time: 0.3448  data: 0.2728  max mem: 357\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:03  Loss: 0.7749 (0.7978)  Acc@1: 79.1667 (80.3030)  Acc@5: 100.0000 (98.1061)  time: 0.1230  data: 0.0322  max mem: 357\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 0.7749 (0.7973)  Acc@1: 79.1667 (79.5635)  Acc@5: 100.0000 (97.6190)  time: 0.0966  data: 0.0042  max mem: 357\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 0.7374 (0.7529)  Acc@1: 79.1667 (80.7796)  Acc@5: 95.8333 (97.5806)  time: 0.0923  data: 0.0009  max mem: 357\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7033 (0.7696)  Acc@1: 79.1667 (79.9797)  Acc@5: 95.8333 (97.3577)  time: 0.0921  data: 0.0012  max mem: 357\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.6789 (0.7618)  Acc@1: 79.1667 (80.2000)  Acc@5: 100.0000 (97.4000)  time: 0.0905  data: 0.0012  max mem: 357\n",
            "Test: [Task 4] Total time: 0:00:04 (0.1032 s / it)\n",
            "* Acc@1 80.200 Acc@5 97.400 loss 0.762\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:26  Loss: 0.7840 (0.7840)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.6237  data: 0.5337  max mem: 357\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:04  Loss: 0.8717 (0.8496)  Acc@1: 75.0000 (76.8939)  Acc@5: 100.0000 (98.4848)  time: 0.1477  data: 0.0550  max mem: 357\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 0.8840 (0.8793)  Acc@1: 75.0000 (76.3889)  Acc@5: 95.8333 (96.8254)  time: 0.0955  data: 0.0042  max mem: 357\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 0.8840 (0.8825)  Acc@1: 75.0000 (76.2097)  Acc@5: 95.8333 (96.9086)  time: 0.0917  data: 0.0008  max mem: 357\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.9415 (0.9168)  Acc@1: 75.0000 (75.8130)  Acc@5: 95.8333 (96.6463)  time: 0.0924  data: 0.0004  max mem: 357\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.9830 (0.9303)  Acc@1: 70.8333 (75.7000)  Acc@5: 95.8333 (96.5000)  time: 0.0907  data: 0.0004  max mem: 357\n",
            "Test: [Task 5] Total time: 0:00:04 (0.1080 s / it)\n",
            "* Acc@1 75.700 Acc@5 96.500 loss 0.930\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:21  Loss: 0.8356 (0.8356)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  time: 0.5026  data: 0.4189  max mem: 357\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:04  Loss: 0.9743 (1.0055)  Acc@1: 70.8333 (71.9697)  Acc@5: 95.8333 (96.5909)  time: 0.1297  data: 0.0385  max mem: 357\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 1.0095 (1.0122)  Acc@1: 70.8333 (74.4048)  Acc@5: 95.8333 (96.4286)  time: 0.0912  data: 0.0004  max mem: 357\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 1.0530 (1.0311)  Acc@1: 70.8333 (73.1183)  Acc@5: 95.8333 (96.7742)  time: 0.0901  data: 0.0003  max mem: 357\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.0530 (1.0427)  Acc@1: 70.8333 (72.6626)  Acc@5: 95.8333 (96.6463)  time: 0.0907  data: 0.0003  max mem: 357\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.0530 (1.0431)  Acc@1: 70.8333 (72.5000)  Acc@5: 95.8333 (96.7000)  time: 0.0891  data: 0.0003  max mem: 357\n",
            "Test: [Task 6] Total time: 0:00:04 (0.1020 s / it)\n",
            "* Acc@1 72.500 Acc@5 96.700 loss 1.043\n",
            "[Average accuracy till task6]\tAcc@1: 78.2667\tAcc@5: 96.8167\tLoss: 0.8300\tForgetting: 3.8400\tBackward: -3.4000\n",
            "Train: Epoch[1/1]  [  0/209]  eta: 0:02:04  Lr: 0.000047  Loss: 2.6281  Acc@1: 4.1667 (4.1667)  Acc@5: 29.1667 (29.1667)  time: 0.5935  data: 0.4887  max mem: 357\n",
            "Train: Epoch[1/1]  [ 10/209]  eta: 0:00:27  Lr: 0.000047  Loss: 2.2733  Acc@1: 8.3333 (9.8485)  Acc@5: 50.0000 (50.0000)  time: 0.1381  data: 0.0456  max mem: 357\n",
            "Train: Epoch[1/1]  [ 20/209]  eta: 0:00:21  Lr: 0.000047  Loss: 2.2580  Acc@1: 12.5000 (14.4841)  Acc@5: 58.3333 (56.7460)  time: 0.0919  data: 0.0011  max mem: 357\n",
            "Train: Epoch[1/1]  [ 30/209]  eta: 0:00:19  Lr: 0.000047  Loss: 2.1915  Acc@1: 20.8333 (17.8763)  Acc@5: 66.6667 (61.4247)  time: 0.0920  data: 0.0009  max mem: 357\n",
            "Train: Epoch[1/1]  [ 40/209]  eta: 0:00:17  Lr: 0.000047  Loss: 1.9750  Acc@1: 29.1667 (22.4594)  Acc@5: 79.1667 (67.2764)  time: 0.0954  data: 0.0018  max mem: 357\n",
            "Train: Epoch[1/1]  [ 50/209]  eta: 0:00:16  Lr: 0.000047  Loss: 1.8616  Acc@1: 37.5000 (25.7353)  Acc@5: 83.3333 (70.5065)  time: 0.0999  data: 0.0015  max mem: 357\n",
            "Train: Epoch[1/1]  [ 60/209]  eta: 0:00:15  Lr: 0.000047  Loss: 1.6015  Acc@1: 41.6667 (30.4645)  Acc@5: 91.6667 (74.4536)  time: 0.1043  data: 0.0016  max mem: 357\n",
            "Train: Epoch[1/1]  [ 70/209]  eta: 0:00:14  Lr: 0.000047  Loss: 1.5611  Acc@1: 54.1667 (34.0376)  Acc@5: 95.8333 (77.1127)  time: 0.1081  data: 0.0030  max mem: 357\n",
            "Train: Epoch[1/1]  [ 80/209]  eta: 0:00:13  Lr: 0.000047  Loss: 1.6348  Acc@1: 50.0000 (35.9568)  Acc@5: 95.8333 (79.2695)  time: 0.1050  data: 0.0030  max mem: 357\n",
            "Train: Epoch[1/1]  [ 90/209]  eta: 0:00:12  Lr: 0.000047  Loss: 1.5116  Acc@1: 58.3333 (38.8736)  Acc@5: 95.8333 (81.0440)  time: 0.0959  data: 0.0022  max mem: 357\n",
            "Train: Epoch[1/1]  [100/209]  eta: 0:00:11  Lr: 0.000047  Loss: 1.4056  Acc@1: 62.5000 (40.9653)  Acc@5: 95.8333 (82.3432)  time: 0.0906  data: 0.0013  max mem: 357\n",
            "Train: Epoch[1/1]  [110/209]  eta: 0:00:10  Lr: 0.000047  Loss: 1.3991  Acc@1: 58.3333 (42.9805)  Acc@5: 95.8333 (83.3709)  time: 0.0919  data: 0.0014  max mem: 357\n",
            "Train: Epoch[1/1]  [120/209]  eta: 0:00:08  Lr: 0.000047  Loss: 1.4074  Acc@1: 62.5000 (44.5592)  Acc@5: 95.8333 (84.4353)  time: 0.0927  data: 0.0013  max mem: 357\n",
            "Train: Epoch[1/1]  [130/209]  eta: 0:00:07  Lr: 0.000047  Loss: 1.2589  Acc@1: 66.6667 (46.7875)  Acc@5: 95.8333 (85.5280)  time: 0.0911  data: 0.0009  max mem: 357\n",
            "Train: Epoch[1/1]  [140/209]  eta: 0:00:06  Lr: 0.000047  Loss: 1.0729  Acc@1: 66.6667 (48.2861)  Acc@5: 100.0000 (86.4362)  time: 0.0902  data: 0.0006  max mem: 357\n",
            "Train: Epoch[1/1]  [150/209]  eta: 0:00:05  Lr: 0.000047  Loss: 1.1257  Acc@1: 70.8333 (49.8896)  Acc@5: 95.8333 (87.0861)  time: 0.0900  data: 0.0008  max mem: 357\n",
            "Train: Epoch[1/1]  [160/209]  eta: 0:00:04  Lr: 0.000047  Loss: 0.9549  Acc@1: 70.8333 (51.3199)  Acc@5: 95.8333 (87.7588)  time: 0.0897  data: 0.0011  max mem: 357\n",
            "Train: Epoch[1/1]  [170/209]  eta: 0:00:03  Lr: 0.000047  Loss: 0.9809  Acc@1: 66.6667 (52.3148)  Acc@5: 95.8333 (88.3285)  time: 0.0892  data: 0.0010  max mem: 357\n",
            "Train: Epoch[1/1]  [180/209]  eta: 0:00:02  Lr: 0.000047  Loss: 0.7539  Acc@1: 70.8333 (53.4070)  Acc@5: 100.0000 (88.8582)  time: 0.0892  data: 0.0010  max mem: 357\n",
            "Train: Epoch[1/1]  [190/209]  eta: 0:00:01  Lr: 0.000047  Loss: 0.8347  Acc@1: 75.0000 (54.3630)  Acc@5: 100.0000 (89.3325)  time: 0.0908  data: 0.0010  max mem: 357\n",
            "Train: Epoch[1/1]  [200/209]  eta: 0:00:00  Lr: 0.000047  Loss: 0.9916  Acc@1: 75.0000 (55.4312)  Acc@5: 100.0000 (89.8425)  time: 0.0988  data: 0.0014  max mem: 357\n",
            "Train: Epoch[1/1]  [208/209]  eta: 0:00:00  Lr: 0.000047  Loss: 0.9219  Acc@1: 79.1667 (56.1600)  Acc@5: 100.0000 (90.2000)  time: 0.0952  data: 0.0011  max mem: 357\n",
            "Train: Epoch[1/1] Total time: 0:00:20 (0.0975 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.9219  Acc@1: 79.1667 (56.1600)  Acc@5: 100.0000 (90.2000)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:28  Loss: 0.8262 (0.8262)  Acc@1: 70.8333 (70.8333)  Acc@5: 100.0000 (100.0000)  time: 0.6866  data: 0.5964  max mem: 357\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 0.8142 (0.7847)  Acc@1: 75.0000 (76.8939)  Acc@5: 100.0000 (98.4848)  time: 0.1434  data: 0.0557  max mem: 357\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.7749 (0.7834)  Acc@1: 79.1667 (79.5635)  Acc@5: 100.0000 (97.6190)  time: 0.0886  data: 0.0021  max mem: 357\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.7090 (0.7608)  Acc@1: 79.1667 (79.4355)  Acc@5: 100.0000 (97.5806)  time: 0.0882  data: 0.0015  max mem: 357\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6910 (0.7408)  Acc@1: 83.3333 (80.3862)  Acc@5: 100.0000 (97.8659)  time: 0.0883  data: 0.0003  max mem: 357\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6265 (0.7307)  Acc@1: 83.3333 (80.6000)  Acc@5: 100.0000 (97.9000)  time: 0.0867  data: 0.0003  max mem: 357\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1041 s / it)\n",
            "* Acc@1 80.600 Acc@5 97.900 loss 0.731\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:20  Loss: 1.0711 (1.0711)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  time: 0.4894  data: 0.4162  max mem: 357\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 0.9334 (0.9218)  Acc@1: 79.1667 (75.7576)  Acc@5: 95.8333 (95.4545)  time: 0.1252  data: 0.0388  max mem: 357\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 0.9478 (0.9638)  Acc@1: 75.0000 (75.1984)  Acc@5: 95.8333 (94.0476)  time: 0.0872  data: 0.0008  max mem: 357\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.9311 (0.9528)  Acc@1: 75.0000 (75.5376)  Acc@5: 95.8333 (94.3548)  time: 0.0864  data: 0.0004  max mem: 357\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.8554 (0.9332)  Acc@1: 75.0000 (75.6098)  Acc@5: 95.8333 (94.8171)  time: 0.0875  data: 0.0003  max mem: 357\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.8418 (0.9269)  Acc@1: 75.0000 (75.8000)  Acc@5: 95.8333 (94.9000)  time: 0.0853  data: 0.0003  max mem: 357\n",
            "Test: [Task 2] Total time: 0:00:04 (0.0981 s / it)\n",
            "* Acc@1 75.800 Acc@5 94.900 loss 0.927\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:15  Loss: 0.5948 (0.5948)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.3590  data: 0.2813  max mem: 357\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 0.8561 (0.8366)  Acc@1: 75.0000 (77.2727)  Acc@5: 95.8333 (96.2121)  time: 0.1182  data: 0.0333  max mem: 357\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 0.8053 (0.8052)  Acc@1: 79.1667 (80.5556)  Acc@5: 95.8333 (95.8333)  time: 0.0908  data: 0.0048  max mem: 357\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.7755 (0.7823)  Acc@1: 83.3333 (80.2419)  Acc@5: 95.8333 (96.2366)  time: 0.0876  data: 0.0008  max mem: 357\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.7802 (0.7876)  Acc@1: 79.1667 (80.2846)  Acc@5: 95.8333 (95.7317)  time: 0.0876  data: 0.0005  max mem: 357\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.7802 (0.7879)  Acc@1: 79.1667 (80.1000)  Acc@5: 95.8333 (95.8000)  time: 0.0859  data: 0.0005  max mem: 357\n",
            "Test: [Task 3] Total time: 0:00:04 (0.0983 s / it)\n",
            "* Acc@1 80.100 Acc@5 95.800 loss 0.788\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:27  Loss: 1.2254 (1.2254)  Acc@1: 66.6667 (66.6667)  Acc@5: 91.6667 (91.6667)  time: 0.6650  data: 0.5541  max mem: 357\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:04  Loss: 0.7950 (0.8288)  Acc@1: 79.1667 (78.7879)  Acc@5: 95.8333 (96.9697)  time: 0.1406  data: 0.0512  max mem: 357\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 0.8169 (0.8339)  Acc@1: 79.1667 (78.1746)  Acc@5: 95.8333 (96.8254)  time: 0.0879  data: 0.0015  max mem: 357\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 0.7730 (0.7947)  Acc@1: 79.1667 (79.3011)  Acc@5: 95.8333 (97.1774)  time: 0.0880  data: 0.0014  max mem: 357\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7730 (0.8126)  Acc@1: 79.1667 (79.0650)  Acc@5: 95.8333 (96.8496)  time: 0.0902  data: 0.0038  max mem: 357\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.7132 (0.8057)  Acc@1: 79.1667 (79.3000)  Acc@5: 100.0000 (96.9000)  time: 0.0885  data: 0.0038  max mem: 357\n",
            "Test: [Task 4] Total time: 0:00:04 (0.1045 s / it)\n",
            "* Acc@1 79.300 Acc@5 96.900 loss 0.806\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:19  Loss: 0.8385 (0.8385)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  time: 0.4651  data: 0.3954  max mem: 357\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:03  Loss: 0.8919 (0.8738)  Acc@1: 75.0000 (79.1667)  Acc@5: 100.0000 (98.1061)  time: 0.1220  data: 0.0363  max mem: 357\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 0.8994 (0.9091)  Acc@1: 75.0000 (76.3889)  Acc@5: 95.8333 (97.2222)  time: 0.0865  data: 0.0004  max mem: 357\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 0.8994 (0.9096)  Acc@1: 75.0000 (75.9409)  Acc@5: 95.8333 (97.3118)  time: 0.0860  data: 0.0003  max mem: 357\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.9779 (0.9381)  Acc@1: 70.8333 (75.5081)  Acc@5: 95.8333 (96.8496)  time: 0.0875  data: 0.0002  max mem: 357\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.9953 (0.9498)  Acc@1: 70.8333 (75.4000)  Acc@5: 95.8333 (96.7000)  time: 0.0851  data: 0.0002  max mem: 357\n",
            "Test: [Task 5] Total time: 0:00:04 (0.0973 s / it)\n",
            "* Acc@1 75.400 Acc@5 96.700 loss 0.950\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:20  Loss: 0.9224 (0.9224)  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.8333)  time: 0.4886  data: 0.4060  max mem: 357\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:03  Loss: 1.1655 (1.1842)  Acc@1: 66.6667 (66.2879)  Acc@5: 91.6667 (93.1818)  time: 0.1239  data: 0.0375  max mem: 357\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 1.2170 (1.1929)  Acc@1: 66.6667 (67.0635)  Acc@5: 91.6667 (93.8492)  time: 0.0862  data: 0.0005  max mem: 357\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 1.2490 (1.2194)  Acc@1: 66.6667 (65.9946)  Acc@5: 91.6667 (93.9516)  time: 0.0863  data: 0.0003  max mem: 357\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.2787 (1.2240)  Acc@1: 66.6667 (65.8537)  Acc@5: 95.8333 (94.5122)  time: 0.0877  data: 0.0004  max mem: 357\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.2490 (1.2245)  Acc@1: 66.6667 (65.9000)  Acc@5: 95.8333 (94.5000)  time: 0.0857  data: 0.0003  max mem: 357\n",
            "Test: [Task 6] Total time: 0:00:04 (0.0978 s / it)\n",
            "* Acc@1 65.900 Acc@5 94.500 loss 1.224\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:14  Loss: 4.0691 (4.0691)  Acc@1: 0.0000 (0.0000)  Acc@5: 41.6667 (41.6667)  time: 0.3541  data: 0.2820  max mem: 357\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:03  Loss: 3.9792 (3.9727)  Acc@1: 0.0000 (0.7576)  Acc@5: 25.0000 (27.2727)  time: 0.1176  data: 0.0330  max mem: 357\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:02  Loss: 3.8822 (3.9428)  Acc@1: 0.0000 (0.5952)  Acc@5: 25.0000 (27.1825)  time: 0.0907  data: 0.0050  max mem: 357\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:01  Loss: 3.9020 (3.9385)  Acc@1: 0.0000 (0.8065)  Acc@5: 29.1667 (27.6882)  time: 0.0881  data: 0.0014  max mem: 357\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 3.9159 (3.9438)  Acc@1: 0.0000 (0.7114)  Acc@5: 29.1667 (27.6423)  time: 0.0878  data: 0.0008  max mem: 357\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 3.9115 (3.9309)  Acc@1: 0.0000 (0.9000)  Acc@5: 29.1667 (27.8000)  time: 0.0859  data: 0.0008  max mem: 357\n",
            "Test: [Task 7] Total time: 0:00:04 (0.0983 s / it)\n",
            "* Acc@1 0.900 Acc@5 27.800 loss 3.931\n",
            "[Average accuracy till task7]\tAcc@1: 65.4286\tAcc@5: 86.3571\tLoss: 1.3366\tForgetting: 3.8000\tBackward: 37.5333\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([16776, 384])\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 0.9547  Acc@1: 83.3333 (79.6528)  Acc@5: 91.6667 (92.7083)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000467  Loss: 0.8718  Acc@1: 87.5000 (82.2222)  Acc@5: 95.8333 (96.6667)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000464  Loss: 0.8176  Acc@1: 83.3333 (84.7917)  Acc@5: 100.0000 (98.6806)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 0.6431  Acc@1: 87.5000 (86.4583)  Acc@5: 100.0000 (99.2361)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000448  Loss: 0.8198  Acc@1: 87.5000 (88.2639)  Acc@5: 100.0000 (99.5139)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000437  Loss: 0.5949  Acc@1: 87.5000 (89.8611)  Acc@5: 100.0000 (99.4444)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 0.4393  Acc@1: 87.5000 (90.7639)  Acc@5: 100.0000 (99.6528)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000409  Loss: 0.8040  Acc@1: 91.6667 (90.3472)  Acc@5: 100.0000 (99.7222)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000391  Loss: 0.5760  Acc@1: 87.5000 (90.4167)  Acc@5: 100.0000 (99.4444)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 0.7742  Acc@1: 91.6667 (90.6250)  Acc@5: 100.0000 (99.5139)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000352  Loss: 0.5362  Acc@1: 87.5000 (90.1389)  Acc@5: 100.0000 (99.6528)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000330  Loss: 0.3988  Acc@1: 91.6667 (91.3194)  Acc@5: 100.0000 (99.8611)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 0.3083  Acc@1: 91.6667 (91.9444)  Acc@5: 100.0000 (99.7222)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000283  Loss: 0.4727  Acc@1: 91.6667 (91.5972)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000259  Loss: 0.4893  Acc@1: 91.6667 (90.9028)  Acc@5: 100.0000 (99.3750)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 0.4351  Acc@1: 91.6667 (92.0139)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000210  Loss: 0.5565  Acc@1: 91.6667 (91.2500)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000186  Loss: 0.6614  Acc@1: 91.6667 (92.2222)  Acc@5: 100.0000 (99.7222)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.3300  Acc@1: 91.6667 (91.0417)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000139  Loss: 0.4892  Acc@1: 91.6667 (92.7778)  Acc@5: 100.0000 (99.4444)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000117  Loss: 0.4241  Acc@1: 91.6667 (92.5000)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 0.3709  Acc@1: 91.6667 (93.3333)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000078  Loss: 0.5427  Acc@1: 91.6667 (93.0556)  Acc@5: 100.0000 (99.7222)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000060  Loss: 0.6432  Acc@1: 91.6667 (92.7083)  Acc@5: 100.0000 (99.5139)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 0.3479  Acc@1: 91.6667 (91.5972)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000031  Loss: 0.4398  Acc@1: 91.6667 (92.2222)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000020  Loss: 0.3578  Acc@1: 95.8333 (93.4028)  Acc@5: 100.0000 (99.9306)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.4629  Acc@1: 95.8333 (92.6389)  Acc@5: 100.0000 (99.6528)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000005  Loss: 0.3463  Acc@1: 91.6667 (92.4306)  Acc@5: 100.0000 (99.7222)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000001  Loss: 0.4352  Acc@1: 91.6667 (92.1528)  Acc@5: 100.0000 (99.7222)\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:13  Loss: 0.7510 (0.7510)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  time: 0.3279  data: 0.2526  max mem: 358\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 0.7449 (0.7262)  Acc@1: 79.1667 (77.2727)  Acc@5: 100.0000 (97.3485)  time: 0.1179  data: 0.0340  max mem: 358\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.7298 (0.7230)  Acc@1: 79.1667 (80.1587)  Acc@5: 95.8333 (96.6270)  time: 0.0937  data: 0.0062  max mem: 358\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.6789 (0.6966)  Acc@1: 83.3333 (80.1075)  Acc@5: 95.8333 (96.6398)  time: 0.0908  data: 0.0004  max mem: 358\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.5990 (0.6759)  Acc@1: 83.3333 (80.7927)  Acc@5: 100.0000 (97.0528)  time: 0.0913  data: 0.0004  max mem: 358\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.5530 (0.6657)  Acc@1: 83.3333 (81.0000)  Acc@5: 100.0000 (97.1000)  time: 0.0892  data: 0.0004  max mem: 358\n",
            "Test: [Task 1] Total time: 0:00:04 (0.0998 s / it)\n",
            "* Acc@1 81.000 Acc@5 97.100 loss 0.666\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:29  Loss: 1.0146 (1.0146)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  time: 0.7105  data: 0.6175  max mem: 358\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 0.8893 (0.8567)  Acc@1: 79.1667 (78.0303)  Acc@5: 95.8333 (95.8333)  time: 0.1493  data: 0.0585  max mem: 358\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 0.8831 (0.8939)  Acc@1: 75.0000 (76.7857)  Acc@5: 95.8333 (94.6429)  time: 0.0938  data: 0.0047  max mem: 358\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.8817 (0.8852)  Acc@1: 75.0000 (76.3441)  Acc@5: 95.8333 (94.8925)  time: 0.0932  data: 0.0038  max mem: 358\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.7927 (0.8628)  Acc@1: 75.0000 (76.7276)  Acc@5: 95.8333 (95.2236)  time: 0.0923  data: 0.0005  max mem: 358\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.7762 (0.8576)  Acc@1: 75.0000 (76.8000)  Acc@5: 95.8333 (95.3000)  time: 0.0906  data: 0.0003  max mem: 358\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1089 s / it)\n",
            "* Acc@1 76.800 Acc@5 95.300 loss 0.858\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:14  Loss: 0.5688 (0.5688)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.3439  data: 0.2547  max mem: 358\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:04  Loss: 0.7505 (0.7868)  Acc@1: 79.1667 (77.6515)  Acc@5: 95.8333 (96.5909)  time: 0.1338  data: 0.0443  max mem: 358\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 0.7329 (0.7328)  Acc@1: 79.1667 (80.1587)  Acc@5: 95.8333 (96.4286)  time: 0.1024  data: 0.0118  max mem: 358\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.6630 (0.7121)  Acc@1: 79.1667 (79.4355)  Acc@5: 95.8333 (96.6398)  time: 0.0924  data: 0.0004  max mem: 358\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.6928 (0.7220)  Acc@1: 79.1667 (80.1829)  Acc@5: 95.8333 (96.2398)  time: 0.0930  data: 0.0003  max mem: 358\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.6928 (0.7224)  Acc@1: 79.1667 (80.0000)  Acc@5: 95.8333 (96.3000)  time: 0.0911  data: 0.0002  max mem: 358\n",
            "Test: [Task 3] Total time: 0:00:04 (0.1050 s / it)\n",
            "* Acc@1 80.000 Acc@5 96.300 loss 0.722\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:22  Loss: 1.1168 (1.1168)  Acc@1: 66.6667 (66.6667)  Acc@5: 87.5000 (87.5000)  time: 0.5305  data: 0.4557  max mem: 358\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:04  Loss: 0.7695 (0.8117)  Acc@1: 79.1667 (77.6515)  Acc@5: 95.8333 (96.2121)  time: 0.1330  data: 0.0421  max mem: 358\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 0.7866 (0.8141)  Acc@1: 75.0000 (76.9841)  Acc@5: 95.8333 (96.2302)  time: 0.0928  data: 0.0005  max mem: 358\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 0.7641 (0.7614)  Acc@1: 79.1667 (78.6290)  Acc@5: 95.8333 (96.7742)  time: 0.0928  data: 0.0003  max mem: 358\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7344 (0.7741)  Acc@1: 79.1667 (78.2520)  Acc@5: 95.8333 (96.5447)  time: 0.0932  data: 0.0003  max mem: 358\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.6924 (0.7659)  Acc@1: 79.1667 (78.5000)  Acc@5: 95.8333 (96.6000)  time: 0.0915  data: 0.0003  max mem: 358\n",
            "Test: [Task 4] Total time: 0:00:04 (0.1058 s / it)\n",
            "* Acc@1 78.500 Acc@5 96.600 loss 0.766\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:24  Loss: 0.7148 (0.7148)  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  time: 0.5785  data: 0.4853  max mem: 358\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:04  Loss: 0.8246 (0.7862)  Acc@1: 75.0000 (78.0303)  Acc@5: 100.0000 (97.7273)  time: 0.1427  data: 0.0501  max mem: 358\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 0.8246 (0.8129)  Acc@1: 79.1667 (77.1825)  Acc@5: 95.8333 (97.0238)  time: 0.0954  data: 0.0040  max mem: 358\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 0.8149 (0.8116)  Acc@1: 79.1667 (77.1505)  Acc@5: 95.8333 (96.9086)  time: 0.0918  data: 0.0020  max mem: 358\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.8675 (0.8452)  Acc@1: 75.0000 (76.5244)  Acc@5: 95.8333 (96.7480)  time: 0.0923  data: 0.0015  max mem: 358\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.8684 (0.8578)  Acc@1: 75.0000 (76.4000)  Acc@5: 95.8333 (96.6000)  time: 0.0910  data: 0.0015  max mem: 358\n",
            "Test: [Task 5] Total time: 0:00:04 (0.1071 s / it)\n",
            "* Acc@1 76.400 Acc@5 96.600 loss 0.858\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:19  Loss: 0.7750 (0.7750)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  time: 0.4627  data: 0.3800  max mem: 358\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:04  Loss: 0.8552 (0.9017)  Acc@1: 79.1667 (76.8939)  Acc@5: 95.8333 (95.0758)  time: 0.1262  data: 0.0349  max mem: 358\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 0.9071 (0.9027)  Acc@1: 75.0000 (76.1905)  Acc@5: 95.8333 (95.4365)  time: 0.0919  data: 0.0004  max mem: 358\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 0.9125 (0.9250)  Acc@1: 75.0000 (75.8065)  Acc@5: 95.8333 (95.5645)  time: 0.0910  data: 0.0005  max mem: 358\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 0.9222 (0.9379)  Acc@1: 75.0000 (75.5081)  Acc@5: 95.8333 (95.6301)  time: 0.0913  data: 0.0004  max mem: 358\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 0.9222 (0.9403)  Acc@1: 75.0000 (75.3000)  Acc@5: 95.8333 (95.7000)  time: 0.0896  data: 0.0003  max mem: 358\n",
            "Test: [Task 6] Total time: 0:00:04 (0.1017 s / it)\n",
            "* Acc@1 75.300 Acc@5 95.700 loss 0.940\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:14  Loss: 1.1483 (1.1483)  Acc@1: 75.0000 (75.0000)  Acc@5: 87.5000 (87.5000)  time: 0.3353  data: 0.2577  max mem: 358\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:03  Loss: 1.0467 (1.0862)  Acc@1: 66.6667 (68.5606)  Acc@5: 95.8333 (94.6970)  time: 0.1206  data: 0.0321  max mem: 358\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:02  Loss: 1.0086 (1.0997)  Acc@1: 70.8333 (68.8492)  Acc@5: 95.8333 (94.8413)  time: 0.0944  data: 0.0052  max mem: 358\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:01  Loss: 1.0301 (1.1213)  Acc@1: 70.8333 (68.5484)  Acc@5: 95.8333 (93.6828)  time: 0.0900  data: 0.0005  max mem: 358\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 1.0493 (1.1271)  Acc@1: 66.6667 (68.4959)  Acc@5: 95.8333 (93.4959)  time: 0.0907  data: 0.0002  max mem: 358\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 1.0445 (1.1234)  Acc@1: 66.6667 (68.6000)  Acc@5: 95.8333 (93.5000)  time: 0.0888  data: 0.0002  max mem: 358\n",
            "Test: [Task 7] Total time: 0:00:04 (0.0996 s / it)\n",
            "* Acc@1 68.600 Acc@5 93.500 loss 1.123\n",
            "[Average accuracy till task7]\tAcc@1: 76.6571\tAcc@5: 95.8714\tLoss: 0.8476\tForgetting: 4.0500\tBackward: -3.1000\n",
            "Train: Epoch[1/1]  [  0/209]  eta: 0:02:07  Lr: 0.000047  Loss: 2.3635  Acc@1: 12.5000 (12.5000)  Acc@5: 54.1667 (54.1667)  time: 0.6115  data: 0.4238  max mem: 358\n",
            "Train: Epoch[1/1]  [ 10/209]  eta: 0:00:30  Lr: 0.000047  Loss: 2.2009  Acc@1: 25.0000 (25.0000)  Acc@5: 62.5000 (62.1212)  time: 0.1517  data: 0.0411  max mem: 358\n",
            "Train: Epoch[1/1]  [ 20/209]  eta: 0:00:24  Lr: 0.000047  Loss: 2.1084  Acc@1: 25.0000 (26.5873)  Acc@5: 70.8333 (68.8492)  time: 0.1038  data: 0.0022  max mem: 358\n",
            "Train: Epoch[1/1]  [ 30/209]  eta: 0:00:21  Lr: 0.000047  Loss: 1.9170  Acc@1: 37.5000 (30.6452)  Acc@5: 79.1667 (73.7903)  time: 0.1037  data: 0.0027  max mem: 358\n",
            "Train: Epoch[1/1]  [ 40/209]  eta: 0:00:19  Lr: 0.000047  Loss: 1.9060  Acc@1: 41.6667 (32.8252)  Acc@5: 87.5000 (77.7439)  time: 0.1055  data: 0.0032  max mem: 358\n",
            "Train: Epoch[1/1]  [ 50/209]  eta: 0:00:17  Lr: 0.000047  Loss: 1.7525  Acc@1: 45.8333 (36.6013)  Acc@5: 91.6667 (80.2288)  time: 0.1002  data: 0.0019  max mem: 358\n",
            "Train: Epoch[1/1]  [ 60/209]  eta: 0:00:16  Lr: 0.000047  Loss: 1.5026  Acc@1: 54.1667 (40.9153)  Acc@5: 91.6667 (82.3771)  time: 0.0928  data: 0.0008  max mem: 358\n",
            "Train: Epoch[1/1]  [ 70/209]  eta: 0:00:14  Lr: 0.000047  Loss: 1.4435  Acc@1: 62.5000 (44.0141)  Acc@5: 95.8333 (84.3310)  time: 0.0905  data: 0.0006  max mem: 358\n",
            "Train: Epoch[1/1]  [ 80/209]  eta: 0:00:13  Lr: 0.000047  Loss: 1.4712  Acc@1: 66.6667 (47.0165)  Acc@5: 95.8333 (85.4938)  time: 0.0903  data: 0.0006  max mem: 358\n",
            "Train: Epoch[1/1]  [ 90/209]  eta: 0:00:12  Lr: 0.000047  Loss: 1.3731  Acc@1: 70.8333 (49.5879)  Acc@5: 95.8333 (86.4927)  time: 0.0905  data: 0.0006  max mem: 358\n",
            "Train: Epoch[1/1]  [100/209]  eta: 0:00:11  Lr: 0.000047  Loss: 1.3683  Acc@1: 75.0000 (51.8152)  Acc@5: 95.8333 (87.4587)  time: 0.0963  data: 0.0015  max mem: 358\n",
            "Train: Epoch[1/1]  [110/209]  eta: 0:00:10  Lr: 0.000047  Loss: 1.2257  Acc@1: 70.8333 (53.2658)  Acc@5: 95.8333 (88.2508)  time: 0.0987  data: 0.0017  max mem: 358\n",
            "Train: Epoch[1/1]  [120/209]  eta: 0:00:09  Lr: 0.000047  Loss: 1.2825  Acc@1: 66.6667 (54.7176)  Acc@5: 95.8333 (88.9463)  time: 0.0936  data: 0.0012  max mem: 358\n",
            "Train: Epoch[1/1]  [130/209]  eta: 0:00:07  Lr: 0.000047  Loss: 1.0800  Acc@1: 70.8333 (56.2659)  Acc@5: 95.8333 (89.5674)  time: 0.0906  data: 0.0010  max mem: 358\n",
            "Train: Epoch[1/1]  [140/209]  eta: 0:00:06  Lr: 0.000047  Loss: 0.9994  Acc@1: 79.1667 (57.5355)  Acc@5: 95.8333 (90.0709)  time: 0.0899  data: 0.0009  max mem: 358\n",
            "Train: Epoch[1/1]  [150/209]  eta: 0:00:05  Lr: 0.000047  Loss: 0.9009  Acc@1: 75.0000 (58.4161)  Acc@5: 95.8333 (90.5353)  time: 0.0894  data: 0.0008  max mem: 358\n",
            "Train: Epoch[1/1]  [160/209]  eta: 0:00:04  Lr: 0.000047  Loss: 1.0676  Acc@1: 75.0000 (59.4979)  Acc@5: 95.8333 (90.9938)  time: 0.0919  data: 0.0008  max mem: 358\n",
            "Train: Epoch[1/1]  [170/209]  eta: 0:00:03  Lr: 0.000047  Loss: 1.2494  Acc@1: 79.1667 (60.6969)  Acc@5: 95.8333 (91.3255)  time: 0.0980  data: 0.0016  max mem: 358\n",
            "Train: Epoch[1/1]  [180/209]  eta: 0:00:02  Lr: 0.000047  Loss: 0.9100  Acc@1: 79.1667 (61.4871)  Acc@5: 95.8333 (91.6436)  time: 0.1027  data: 0.0018  max mem: 358\n",
            "Train: Epoch[1/1]  [190/209]  eta: 0:00:01  Lr: 0.000047  Loss: 0.9022  Acc@1: 75.0000 (62.2164)  Acc@5: 100.0000 (91.9503)  time: 0.1052  data: 0.0016  max mem: 358\n",
            "Train: Epoch[1/1]  [200/209]  eta: 0:00:00  Lr: 0.000047  Loss: 1.1293  Acc@1: 79.1667 (62.7902)  Acc@5: 100.0000 (92.2678)  time: 0.1020  data: 0.0014  max mem: 358\n",
            "Train: Epoch[1/1]  [208/209]  eta: 0:00:00  Lr: 0.000047  Loss: 0.6103  Acc@1: 79.1667 (63.2800)  Acc@5: 100.0000 (92.4400)  time: 0.0920  data: 0.0005  max mem: 358\n",
            "Train: Epoch[1/1] Total time: 0:00:20 (0.0996 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.6103  Acc@1: 79.1667 (63.2800)  Acc@5: 100.0000 (92.4400)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:20  Loss: 0.7953 (0.7953)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  time: 0.4970  data: 0.4127  max mem: 358\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 0.7953 (0.7513)  Acc@1: 75.0000 (77.2727)  Acc@5: 100.0000 (97.7273)  time: 0.1241  data: 0.0383  max mem: 358\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.7393 (0.7418)  Acc@1: 79.1667 (80.9524)  Acc@5: 100.0000 (97.2222)  time: 0.0863  data: 0.0006  max mem: 358\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.6789 (0.7185)  Acc@1: 83.3333 (81.3172)  Acc@5: 95.8333 (97.0430)  time: 0.0865  data: 0.0003  max mem: 358\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6426 (0.6965)  Acc@1: 83.3333 (82.2154)  Acc@5: 100.0000 (97.4594)  time: 0.0878  data: 0.0002  max mem: 358\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6412 (0.6867)  Acc@1: 83.3333 (82.4000)  Acc@5: 100.0000 (97.5000)  time: 0.0858  data: 0.0002  max mem: 358\n",
            "Test: [Task 1] Total time: 0:00:04 (0.0982 s / it)\n",
            "* Acc@1 82.400 Acc@5 97.500 loss 0.687\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:19  Loss: 1.0147 (1.0147)  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.8333)  time: 0.4568  data: 0.3854  max mem: 358\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:03  Loss: 0.9589 (0.9458)  Acc@1: 70.8333 (73.8636)  Acc@5: 95.8333 (95.8333)  time: 0.1213  data: 0.0360  max mem: 358\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 0.9477 (0.9740)  Acc@1: 70.8333 (74.2064)  Acc@5: 95.8333 (94.6429)  time: 0.0860  data: 0.0007  max mem: 358\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.9477 (0.9781)  Acc@1: 70.8333 (73.3871)  Acc@5: 95.8333 (94.4892)  time: 0.0859  data: 0.0004  max mem: 358\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.9151 (0.9617)  Acc@1: 70.8333 (73.3740)  Acc@5: 95.8333 (94.7154)  time: 0.0877  data: 0.0004  max mem: 358\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.9120 (0.9562)  Acc@1: 70.8333 (73.5000)  Acc@5: 95.8333 (94.8000)  time: 0.0855  data: 0.0004  max mem: 358\n",
            "Test: [Task 2] Total time: 0:00:04 (0.0968 s / it)\n",
            "* Acc@1 73.500 Acc@5 94.800 loss 0.956\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:20  Loss: 0.5813 (0.5813)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.4807  data: 0.4088  max mem: 358\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:04  Loss: 0.7810 (0.7977)  Acc@1: 79.1667 (78.4091)  Acc@5: 95.8333 (96.5909)  time: 0.1250  data: 0.0385  max mem: 358\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 0.7295 (0.7618)  Acc@1: 83.3333 (79.9603)  Acc@5: 95.8333 (95.6349)  time: 0.0886  data: 0.0020  max mem: 358\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.7006 (0.7400)  Acc@1: 83.3333 (80.2419)  Acc@5: 95.8333 (96.1022)  time: 0.0878  data: 0.0022  max mem: 358\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.7048 (0.7523)  Acc@1: 83.3333 (80.3862)  Acc@5: 95.8333 (95.6301)  time: 0.0874  data: 0.0012  max mem: 358\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.7048 (0.7519)  Acc@1: 83.3333 (80.2000)  Acc@5: 95.8333 (95.7000)  time: 0.0859  data: 0.0012  max mem: 358\n",
            "Test: [Task 3] Total time: 0:00:04 (0.1001 s / it)\n",
            "* Acc@1 80.200 Acc@5 95.700 loss 0.752\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:23  Loss: 1.0852 (1.0852)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  time: 0.5501  data: 0.4627  max mem: 358\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:04  Loss: 0.7354 (0.7998)  Acc@1: 79.1667 (78.7879)  Acc@5: 95.8333 (97.3485)  time: 0.1303  data: 0.0458  max mem: 358\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 0.7937 (0.8120)  Acc@1: 79.1667 (78.5714)  Acc@5: 95.8333 (97.0238)  time: 0.0869  data: 0.0023  max mem: 358\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 0.7482 (0.7694)  Acc@1: 79.1667 (79.9731)  Acc@5: 95.8333 (97.1774)  time: 0.0863  data: 0.0004  max mem: 358\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7378 (0.7814)  Acc@1: 79.1667 (79.8781)  Acc@5: 95.8333 (97.0528)  time: 0.0875  data: 0.0004  max mem: 358\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.6988 (0.7729)  Acc@1: 79.1667 (80.2000)  Acc@5: 95.8333 (97.1000)  time: 0.0857  data: 0.0004  max mem: 358\n",
            "Test: [Task 4] Total time: 0:00:04 (0.0995 s / it)\n",
            "* Acc@1 80.200 Acc@5 97.100 loss 0.773\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:14  Loss: 0.7841 (0.7841)  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  time: 0.3476  data: 0.2710  max mem: 358\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:03  Loss: 0.8499 (0.8294)  Acc@1: 75.0000 (77.2727)  Acc@5: 100.0000 (97.7273)  time: 0.1125  data: 0.0294  max mem: 358\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 0.8499 (0.8538)  Acc@1: 70.8333 (75.5952)  Acc@5: 95.8333 (97.0238)  time: 0.0877  data: 0.0030  max mem: 358\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 0.8296 (0.8500)  Acc@1: 75.0000 (76.0753)  Acc@5: 95.8333 (96.7742)  time: 0.0868  data: 0.0005  max mem: 358\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.9128 (0.8853)  Acc@1: 70.8333 (75.1016)  Acc@5: 95.8333 (96.5447)  time: 0.0876  data: 0.0002  max mem: 358\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.9239 (0.8981)  Acc@1: 70.8333 (75.0000)  Acc@5: 95.8333 (96.4000)  time: 0.0856  data: 0.0002  max mem: 358\n",
            "Test: [Task 5] Total time: 0:00:03 (0.0950 s / it)\n",
            "* Acc@1 75.000 Acc@5 96.400 loss 0.898\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:14  Loss: 0.7896 (0.7896)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  time: 0.3469  data: 0.2655  max mem: 358\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:03  Loss: 0.9270 (0.9691)  Acc@1: 70.8333 (73.8636)  Acc@5: 95.8333 (95.4545)  time: 0.1152  data: 0.0319  max mem: 358\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 1.0092 (0.9798)  Acc@1: 70.8333 (74.0079)  Acc@5: 95.8333 (95.2381)  time: 0.0899  data: 0.0045  max mem: 358\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 1.0253 (1.0136)  Acc@1: 75.0000 (72.9839)  Acc@5: 95.8333 (95.1613)  time: 0.0879  data: 0.0008  max mem: 358\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.0374 (1.0166)  Acc@1: 75.0000 (73.3740)  Acc@5: 95.8333 (95.4268)  time: 0.0877  data: 0.0007  max mem: 358\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.0374 (1.0186)  Acc@1: 75.0000 (73.2000)  Acc@5: 95.8333 (95.4000)  time: 0.0859  data: 0.0007  max mem: 358\n",
            "Test: [Task 6] Total time: 0:00:04 (0.0980 s / it)\n",
            "* Acc@1 73.200 Acc@5 95.400 loss 1.019\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:34  Loss: 1.3279 (1.3279)  Acc@1: 75.0000 (75.0000)  Acc@5: 87.5000 (87.5000)  time: 0.8106  data: 0.7013  max mem: 358\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:04  Loss: 1.1820 (1.2586)  Acc@1: 66.6667 (64.7727)  Acc@5: 95.8333 (93.5606)  time: 0.1545  data: 0.0651  max mem: 358\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:02  Loss: 1.1820 (1.2573)  Acc@1: 62.5000 (63.2937)  Acc@5: 91.6667 (92.8571)  time: 0.0878  data: 0.0010  max mem: 358\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:01  Loss: 1.1781 (1.2796)  Acc@1: 62.5000 (63.7097)  Acc@5: 91.6667 (91.9355)  time: 0.0871  data: 0.0004  max mem: 358\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 1.1966 (1.2871)  Acc@1: 66.6667 (64.4309)  Acc@5: 91.6667 (91.7683)  time: 0.0878  data: 0.0002  max mem: 358\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 1.1966 (1.2840)  Acc@1: 66.6667 (64.5000)  Acc@5: 91.6667 (91.7000)  time: 0.0862  data: 0.0002  max mem: 358\n",
            "Test: [Task 7] Total time: 0:00:04 (0.1064 s / it)\n",
            "* Acc@1 64.500 Acc@5 91.700 loss 1.284\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:15  Loss: 4.1451 (4.1451)  Acc@1: 0.0000 (0.0000)  Acc@5: 33.3333 (33.3333)  time: 0.3572  data: 0.2835  max mem: 358\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:03  Loss: 4.1795 (4.1706)  Acc@1: 0.0000 (0.7576)  Acc@5: 20.8333 (23.1061)  time: 0.1150  data: 0.0298  max mem: 358\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:02  Loss: 4.1645 (4.1292)  Acc@1: 0.0000 (1.3889)  Acc@5: 20.8333 (24.6032)  time: 0.0890  data: 0.0024  max mem: 358\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:01  Loss: 4.0772 (4.1220)  Acc@1: 0.0000 (1.2097)  Acc@5: 20.8333 (23.5215)  time: 0.0874  data: 0.0005  max mem: 358\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 4.0731 (4.1228)  Acc@1: 0.0000 (1.2195)  Acc@5: 20.8333 (23.9837)  time: 0.0884  data: 0.0003  max mem: 358\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 4.0731 (4.1196)  Acc@1: 0.0000 (1.2000)  Acc@5: 20.8333 (23.9000)  time: 0.0864  data: 0.0003  max mem: 358\n",
            "Test: [Task 8] Total time: 0:00:04 (0.0964 s / it)\n",
            "* Acc@1 1.200 Acc@5 23.900 loss 4.120\n",
            "[Average accuracy till task8]\tAcc@1: 66.2750\tAcc@5: 86.5625\tLoss: 1.3110\tForgetting: 3.3714\tBackward: 42.3143\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([19176, 384])\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 0.8137  Acc@1: 83.3333 (81.3095)  Acc@5: 95.8333 (92.9762)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000467  Loss: 0.5329  Acc@1: 83.3333 (83.2738)  Acc@5: 100.0000 (97.0833)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000464  Loss: 0.8806  Acc@1: 83.3333 (84.7619)  Acc@5: 100.0000 (98.6310)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 0.3617  Acc@1: 87.5000 (87.2619)  Acc@5: 95.8333 (98.5119)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000448  Loss: 0.5344  Acc@1: 87.5000 (87.0833)  Acc@5: 100.0000 (99.2857)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000437  Loss: 0.6567  Acc@1: 87.5000 (88.2738)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 0.4988  Acc@1: 87.5000 (90.8333)  Acc@5: 100.0000 (99.6429)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000409  Loss: 0.4639  Acc@1: 87.5000 (89.2857)  Acc@5: 100.0000 (99.6429)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000391  Loss: 0.5804  Acc@1: 91.6667 (89.2857)  Acc@5: 100.0000 (99.6429)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 0.4316  Acc@1: 91.6667 (90.0595)  Acc@5: 100.0000 (99.2262)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000352  Loss: 0.4881  Acc@1: 91.6667 (91.7262)  Acc@5: 100.0000 (99.5238)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000330  Loss: 0.4863  Acc@1: 91.6667 (91.3095)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 0.4995  Acc@1: 91.6667 (91.1310)  Acc@5: 100.0000 (99.8214)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000283  Loss: 0.4176  Acc@1: 91.6667 (90.9524)  Acc@5: 100.0000 (99.7024)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000259  Loss: 0.6280  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (99.7619)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 0.3300  Acc@1: 91.6667 (91.1310)  Acc@5: 100.0000 (99.7024)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000210  Loss: 0.4128  Acc@1: 91.6667 (91.2500)  Acc@5: 100.0000 (99.4643)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000186  Loss: 0.4201  Acc@1: 91.6667 (91.7857)  Acc@5: 100.0000 (99.8214)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.6085  Acc@1: 91.6667 (93.0952)  Acc@5: 100.0000 (99.7024)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000139  Loss: 0.4640  Acc@1: 91.6667 (91.3095)  Acc@5: 100.0000 (99.7024)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000117  Loss: 0.5496  Acc@1: 91.6667 (93.2143)  Acc@5: 100.0000 (99.8214)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 0.5681  Acc@1: 91.6667 (91.3095)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000078  Loss: 0.5603  Acc@1: 91.6667 (91.5476)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000060  Loss: 0.4857  Acc@1: 95.8333 (93.0357)  Acc@5: 100.0000 (99.8214)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 0.5924  Acc@1: 91.6667 (92.6190)  Acc@5: 100.0000 (99.7024)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000031  Loss: 0.4896  Acc@1: 91.6667 (91.6071)  Acc@5: 100.0000 (99.8214)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000020  Loss: 0.3716  Acc@1: 95.8333 (92.5000)  Acc@5: 100.0000 (99.8810)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.4184  Acc@1: 95.8333 (92.1429)  Acc@5: 100.0000 (99.7024)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000005  Loss: 0.4817  Acc@1: 91.6667 (92.3214)  Acc@5: 100.0000 (99.7024)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000001  Loss: 0.4457  Acc@1: 91.6667 (91.7262)  Acc@5: 100.0000 (99.7024)\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:21  Loss: 0.6901 (0.6901)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  time: 0.5127  data: 0.4320  max mem: 358\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 0.7257 (0.7453)  Acc@1: 75.0000 (76.1364)  Acc@5: 100.0000 (96.9697)  time: 0.1293  data: 0.0395  max mem: 358\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.7257 (0.7333)  Acc@1: 75.0000 (78.3730)  Acc@5: 95.8333 (96.6270)  time: 0.0899  data: 0.0003  max mem: 358\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.6631 (0.7046)  Acc@1: 83.3333 (79.1667)  Acc@5: 95.8333 (96.6398)  time: 0.0894  data: 0.0004  max mem: 358\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.5872 (0.6801)  Acc@1: 83.3333 (79.9797)  Acc@5: 100.0000 (97.0528)  time: 0.0902  data: 0.0003  max mem: 358\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.5736 (0.6697)  Acc@1: 83.3333 (80.2000)  Acc@5: 100.0000 (97.1000)  time: 0.0883  data: 0.0003  max mem: 358\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1030 s / it)\n",
            "* Acc@1 80.200 Acc@5 97.100 loss 0.670\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:28  Loss: 1.0310 (1.0310)  Acc@1: 66.6667 (66.6667)  Acc@5: 91.6667 (91.6667)  time: 0.6860  data: 0.6039  max mem: 358\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 0.8763 (0.8535)  Acc@1: 75.0000 (76.5152)  Acc@5: 95.8333 (95.0758)  time: 0.1463  data: 0.0556  max mem: 358\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 0.8518 (0.8874)  Acc@1: 75.0000 (75.5952)  Acc@5: 95.8333 (94.0476)  time: 0.0926  data: 0.0014  max mem: 358\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.8518 (0.8781)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (94.3548)  time: 0.0925  data: 0.0028  max mem: 358\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.8073 (0.8592)  Acc@1: 75.0000 (75.2033)  Acc@5: 95.8333 (94.8171)  time: 0.0917  data: 0.0021  max mem: 358\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.8007 (0.8542)  Acc@1: 75.0000 (75.3000)  Acc@5: 95.8333 (94.9000)  time: 0.0906  data: 0.0020  max mem: 358\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1079 s / it)\n",
            "* Acc@1 75.300 Acc@5 94.900 loss 0.854\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:15  Loss: 0.5412 (0.5412)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.3788  data: 0.2972  max mem: 358\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 0.7580 (0.7903)  Acc@1: 79.1667 (78.7879)  Acc@5: 95.8333 (95.8333)  time: 0.1237  data: 0.0331  max mem: 358\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 0.7455 (0.7365)  Acc@1: 75.0000 (79.7619)  Acc@5: 95.8333 (95.6349)  time: 0.0950  data: 0.0037  max mem: 358\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.6783 (0.7207)  Acc@1: 75.0000 (78.8979)  Acc@5: 95.8333 (95.9677)  time: 0.0921  data: 0.0005  max mem: 358\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.7012 (0.7342)  Acc@1: 79.1667 (79.5732)  Acc@5: 95.8333 (95.4268)  time: 0.0927  data: 0.0003  max mem: 358\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.7012 (0.7335)  Acc@1: 79.1667 (79.4000)  Acc@5: 95.8333 (95.5000)  time: 0.0913  data: 0.0003  max mem: 358\n",
            "Test: [Task 3] Total time: 0:00:04 (0.1019 s / it)\n",
            "* Acc@1 79.400 Acc@5 95.500 loss 0.734\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:14  Loss: 1.0548 (1.0548)  Acc@1: 66.6667 (66.6667)  Acc@5: 91.6667 (91.6667)  time: 0.3416  data: 0.2642  max mem: 358\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:03  Loss: 0.8039 (0.8193)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (96.9697)  time: 0.1208  data: 0.0309  max mem: 358\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 0.8039 (0.8169)  Acc@1: 75.0000 (74.8016)  Acc@5: 95.8333 (96.2302)  time: 0.0953  data: 0.0040  max mem: 358\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 0.7499 (0.7589)  Acc@1: 79.1667 (77.1505)  Acc@5: 95.8333 (96.6398)  time: 0.0923  data: 0.0004  max mem: 358\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7181 (0.7669)  Acc@1: 79.1667 (77.1341)  Acc@5: 95.8333 (96.2398)  time: 0.0928  data: 0.0003  max mem: 358\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.7159 (0.7576)  Acc@1: 79.1667 (77.4000)  Acc@5: 95.8333 (96.3000)  time: 0.0911  data: 0.0003  max mem: 358\n",
            "Test: [Task 4] Total time: 0:00:04 (0.1013 s / it)\n",
            "* Acc@1 77.400 Acc@5 96.300 loss 0.758\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:16  Loss: 0.6625 (0.6625)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.3953  data: 0.2975  max mem: 358\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:03  Loss: 0.7629 (0.7532)  Acc@1: 79.1667 (80.3030)  Acc@5: 100.0000 (97.7273)  time: 0.1218  data: 0.0320  max mem: 358\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 0.7884 (0.7787)  Acc@1: 75.0000 (78.3730)  Acc@5: 95.8333 (96.6270)  time: 0.0941  data: 0.0042  max mem: 358\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 0.7884 (0.7763)  Acc@1: 79.1667 (78.4946)  Acc@5: 95.8333 (96.2366)  time: 0.0934  data: 0.0023  max mem: 358\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.7960 (0.8120)  Acc@1: 75.0000 (77.4390)  Acc@5: 95.8333 (96.2398)  time: 0.0927  data: 0.0012  max mem: 358\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.8182 (0.8256)  Acc@1: 70.8333 (77.3000)  Acc@5: 95.8333 (96.1000)  time: 0.0913  data: 0.0011  max mem: 358\n",
            "Test: [Task 5] Total time: 0:00:04 (0.1030 s / it)\n",
            "* Acc@1 77.300 Acc@5 96.100 loss 0.826\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:16  Loss: 0.7662 (0.7662)  Acc@1: 83.3333 (83.3333)  Acc@5: 91.6667 (91.6667)  time: 0.3934  data: 0.3090  max mem: 358\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:03  Loss: 0.8457 (0.8760)  Acc@1: 79.1667 (77.6515)  Acc@5: 95.8333 (95.4545)  time: 0.1204  data: 0.0288  max mem: 358\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 0.8759 (0.8817)  Acc@1: 79.1667 (76.7857)  Acc@5: 95.8333 (96.0317)  time: 0.0924  data: 0.0007  max mem: 358\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 0.9127 (0.9025)  Acc@1: 75.0000 (75.6720)  Acc@5: 95.8333 (95.9677)  time: 0.0915  data: 0.0004  max mem: 358\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 0.9134 (0.9155)  Acc@1: 75.0000 (75.2033)  Acc@5: 95.8333 (95.8333)  time: 0.0915  data: 0.0003  max mem: 358\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 0.9134 (0.9177)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8000)  time: 0.0902  data: 0.0003  max mem: 358\n",
            "Test: [Task 6] Total time: 0:00:04 (0.1005 s / it)\n",
            "* Acc@1 75.000 Acc@5 95.800 loss 0.918\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:20  Loss: 1.0561 (1.0561)  Acc@1: 79.1667 (79.1667)  Acc@5: 87.5000 (87.5000)  time: 0.4922  data: 0.4143  max mem: 358\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:04  Loss: 0.9295 (0.9695)  Acc@1: 75.0000 (73.4849)  Acc@5: 95.8333 (95.0758)  time: 0.1280  data: 0.0384  max mem: 358\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:02  Loss: 0.9249 (0.9927)  Acc@1: 75.0000 (73.0159)  Acc@5: 95.8333 (94.4444)  time: 0.0914  data: 0.0006  max mem: 358\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:01  Loss: 0.9724 (1.0255)  Acc@1: 70.8333 (71.5054)  Acc@5: 91.6667 (93.5484)  time: 0.0906  data: 0.0003  max mem: 358\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 0.9825 (1.0320)  Acc@1: 66.6667 (71.2398)  Acc@5: 91.6667 (93.2927)  time: 0.0905  data: 0.0002  max mem: 358\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 0.9825 (1.0272)  Acc@1: 70.8333 (71.6000)  Acc@5: 93.7500 (93.3000)  time: 0.0888  data: 0.0002  max mem: 358\n",
            "Test: [Task 7] Total time: 0:00:04 (0.1018 s / it)\n",
            "* Acc@1 71.600 Acc@5 93.300 loss 1.027\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:15  Loss: 1.0900 (1.0900)  Acc@1: 66.6667 (66.6667)  Acc@5: 95.8333 (95.8333)  time: 0.3762  data: 0.3004  max mem: 358\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:03  Loss: 1.2186 (1.1899)  Acc@1: 66.6667 (67.4242)  Acc@5: 91.6667 (92.0455)  time: 0.1189  data: 0.0307  max mem: 358\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:02  Loss: 1.1715 (1.1591)  Acc@1: 66.6667 (67.8571)  Acc@5: 95.8333 (93.2540)  time: 0.0914  data: 0.0024  max mem: 358\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:01  Loss: 1.1259 (1.1411)  Acc@1: 66.6667 (67.8763)  Acc@5: 95.8333 (94.0860)  time: 0.0894  data: 0.0010  max mem: 358\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 1.1556 (1.1627)  Acc@1: 66.6667 (66.8699)  Acc@5: 95.8333 (93.2927)  time: 0.0893  data: 0.0006  max mem: 358\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 1.1556 (1.1585)  Acc@1: 66.6667 (66.9000)  Acc@5: 93.7500 (93.3000)  time: 0.0878  data: 0.0005  max mem: 358\n",
            "Test: [Task 8] Total time: 0:00:04 (0.0995 s / it)\n",
            "* Acc@1 66.900 Acc@5 93.300 loss 1.159\n",
            "[Average accuracy till task8]\tAcc@1: 75.3875\tAcc@5: 95.2875\tLoss: 0.8680\tForgetting: 4.0857\tBackward: -2.7143\n",
            "Train: Epoch[1/1]  [  0/209]  eta: 0:03:13  Lr: 0.000047  Loss: 2.2903  Acc@1: 12.5000 (12.5000)  Acc@5: 58.3333 (58.3333)  time: 0.9276  data: 0.7869  max mem: 358\n",
            "Train: Epoch[1/1]  [ 10/209]  eta: 0:00:34  Lr: 0.000047  Loss: 2.2863  Acc@1: 16.6667 (15.9091)  Acc@5: 58.3333 (56.0606)  time: 0.1709  data: 0.0726  max mem: 358\n",
            "Train: Epoch[1/1]  [ 20/209]  eta: 0:00:25  Lr: 0.000047  Loss: 2.0504  Acc@1: 16.6667 (18.6508)  Acc@5: 62.5000 (62.5000)  time: 0.0927  data: 0.0011  max mem: 358\n",
            "Train: Epoch[1/1]  [ 30/209]  eta: 0:00:21  Lr: 0.000047  Loss: 1.9713  Acc@1: 25.0000 (22.1774)  Acc@5: 75.0000 (68.0108)  time: 0.0903  data: 0.0009  max mem: 358\n",
            "Train: Epoch[1/1]  [ 40/209]  eta: 0:00:18  Lr: 0.000047  Loss: 1.9179  Acc@1: 33.3333 (27.7439)  Acc@5: 83.3333 (73.2724)  time: 0.0899  data: 0.0008  max mem: 358\n",
            "Train: Epoch[1/1]  [ 50/209]  eta: 0:00:17  Lr: 0.000047  Loss: 1.8432  Acc@1: 41.6667 (30.1471)  Acc@5: 91.6667 (76.7157)  time: 0.0895  data: 0.0007  max mem: 358\n",
            "Train: Epoch[1/1]  [ 60/209]  eta: 0:00:15  Lr: 0.000047  Loss: 1.7148  Acc@1: 45.8333 (34.0847)  Acc@5: 91.6667 (79.4399)  time: 0.0898  data: 0.0005  max mem: 358\n",
            "Train: Epoch[1/1]  [ 70/209]  eta: 0:00:14  Lr: 0.000047  Loss: 1.6890  Acc@1: 58.3333 (37.7934)  Acc@5: 95.8333 (81.4554)  time: 0.0934  data: 0.0016  max mem: 358\n",
            "Train: Epoch[1/1]  [ 80/209]  eta: 0:00:13  Lr: 0.000047  Loss: 1.3876  Acc@1: 62.5000 (41.3580)  Acc@5: 95.8333 (83.0761)  time: 0.0944  data: 0.0024  max mem: 358\n",
            "Train: Epoch[1/1]  [ 90/209]  eta: 0:00:11  Lr: 0.000047  Loss: 1.2279  Acc@1: 62.5000 (43.4524)  Acc@5: 95.8333 (84.2949)  time: 0.0908  data: 0.0015  max mem: 358\n",
            "Train: Epoch[1/1]  [100/209]  eta: 0:00:10  Lr: 0.000047  Loss: 1.1915  Acc@1: 62.5000 (45.5033)  Acc@5: 95.8333 (85.2310)  time: 0.0899  data: 0.0009  max mem: 358\n",
            "Train: Epoch[1/1]  [110/209]  eta: 0:00:09  Lr: 0.000047  Loss: 1.3971  Acc@1: 62.5000 (47.7477)  Acc@5: 95.8333 (86.4114)  time: 0.0926  data: 0.0010  max mem: 358\n",
            "Train: Epoch[1/1]  [120/209]  eta: 0:00:08  Lr: 0.000047  Loss: 1.2379  Acc@1: 66.6667 (49.3457)  Acc@5: 100.0000 (87.2934)  time: 0.0988  data: 0.0009  max mem: 358\n",
            "Train: Epoch[1/1]  [130/209]  eta: 0:00:07  Lr: 0.000047  Loss: 1.3560  Acc@1: 66.6667 (51.0178)  Acc@5: 100.0000 (88.1043)  time: 0.1016  data: 0.0011  max mem: 358\n",
            "Train: Epoch[1/1]  [140/209]  eta: 0:00:06  Lr: 0.000047  Loss: 1.3017  Acc@1: 75.0000 (52.6300)  Acc@5: 95.8333 (88.6820)  time: 0.1004  data: 0.0011  max mem: 358\n",
            "Train: Epoch[1/1]  [150/209]  eta: 0:00:05  Lr: 0.000047  Loss: 0.9688  Acc@1: 75.0000 (54.2494)  Acc@5: 95.8333 (89.2384)  time: 0.1037  data: 0.0010  max mem: 358\n",
            "Train: Epoch[1/1]  [160/209]  eta: 0:00:04  Lr: 0.000047  Loss: 0.9878  Acc@1: 75.0000 (55.5642)  Acc@5: 95.8333 (89.7774)  time: 0.1080  data: 0.0016  max mem: 358\n",
            "Train: Epoch[1/1]  [170/209]  eta: 0:00:03  Lr: 0.000047  Loss: 1.0297  Acc@1: 75.0000 (56.8470)  Acc@5: 100.0000 (90.2290)  time: 0.0995  data: 0.0019  max mem: 358\n",
            "Train: Epoch[1/1]  [180/209]  eta: 0:00:02  Lr: 0.000047  Loss: 0.6998  Acc@1: 75.0000 (58.1031)  Acc@5: 95.8333 (90.6077)  time: 0.0893  data: 0.0014  max mem: 358\n",
            "Train: Epoch[1/1]  [190/209]  eta: 0:00:01  Lr: 0.000047  Loss: 0.7733  Acc@1: 83.3333 (59.2278)  Acc@5: 100.0000 (90.9686)  time: 0.0885  data: 0.0011  max mem: 358\n",
            "Train: Epoch[1/1]  [200/209]  eta: 0:00:00  Lr: 0.000047  Loss: 1.0102  Acc@1: 75.0000 (59.9295)  Acc@5: 95.8333 (91.2313)  time: 0.0887  data: 0.0010  max mem: 358\n",
            "Train: Epoch[1/1]  [208/209]  eta: 0:00:00  Lr: 0.000047  Loss: 0.9207  Acc@1: 75.0000 (60.5200)  Acc@5: 100.0000 (91.4600)  time: 0.0859  data: 0.0007  max mem: 358\n",
            "Train: Epoch[1/1] Total time: 0:00:20 (0.0984 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.9207  Acc@1: 75.0000 (60.5200)  Acc@5: 100.0000 (91.4600)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:13  Loss: 0.6665 (0.6665)  Acc@1: 87.5000 (87.5000)  Acc@5: 95.8333 (95.8333)  time: 0.3263  data: 0.2477  max mem: 358\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 0.8339 (0.7885)  Acc@1: 79.1667 (76.1364)  Acc@5: 95.8333 (94.6970)  time: 0.1166  data: 0.0346  max mem: 358\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.7837 (0.7696)  Acc@1: 79.1667 (77.7778)  Acc@5: 95.8333 (94.8413)  time: 0.0916  data: 0.0070  max mem: 358\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.7045 (0.7404)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.5645)  time: 0.0878  data: 0.0004  max mem: 358\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6433 (0.7157)  Acc@1: 83.3333 (80.2846)  Acc@5: 100.0000 (96.1382)  time: 0.0881  data: 0.0002  max mem: 358\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6397 (0.7048)  Acc@1: 83.3333 (80.5000)  Acc@5: 100.0000 (96.2000)  time: 0.0867  data: 0.0002  max mem: 358\n",
            "Test: [Task 1] Total time: 0:00:04 (0.0968 s / it)\n",
            "* Acc@1 80.500 Acc@5 96.200 loss 0.705\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:17  Loss: 1.1415 (1.1415)  Acc@1: 62.5000 (62.5000)  Acc@5: 91.6667 (91.6667)  time: 0.4180  data: 0.3343  max mem: 358\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:03  Loss: 0.9795 (1.0019)  Acc@1: 70.8333 (71.2121)  Acc@5: 95.8333 (94.3182)  time: 0.1178  data: 0.0316  max mem: 358\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 0.9584 (1.0260)  Acc@1: 70.8333 (72.8175)  Acc@5: 95.8333 (93.0556)  time: 0.0867  data: 0.0011  max mem: 358\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.9584 (1.0180)  Acc@1: 70.8333 (72.7151)  Acc@5: 91.6667 (93.0108)  time: 0.0862  data: 0.0010  max mem: 358\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.9322 (1.0013)  Acc@1: 70.8333 (72.4594)  Acc@5: 95.8333 (93.3943)  time: 0.0870  data: 0.0008  max mem: 358\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.9035 (0.9949)  Acc@1: 70.8333 (72.5000)  Acc@5: 95.8333 (93.5000)  time: 0.0855  data: 0.0008  max mem: 358\n",
            "Test: [Task 2] Total time: 0:00:04 (0.0980 s / it)\n",
            "* Acc@1 72.500 Acc@5 93.500 loss 0.995\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:32  Loss: 0.6144 (0.6144)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.7712  data: 0.6922  max mem: 358\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:04  Loss: 0.8601 (0.8506)  Acc@1: 75.0000 (77.2727)  Acc@5: 95.8333 (94.6970)  time: 0.1506  data: 0.0640  max mem: 358\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 0.8084 (0.8001)  Acc@1: 79.1667 (79.5635)  Acc@5: 95.8333 (95.0397)  time: 0.0871  data: 0.0010  max mem: 358\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.7115 (0.7850)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.4301)  time: 0.0862  data: 0.0005  max mem: 358\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.8131 (0.7969)  Acc@1: 79.1667 (79.3699)  Acc@5: 95.8333 (95.0203)  time: 0.0872  data: 0.0003  max mem: 358\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.7733 (0.7963)  Acc@1: 79.1667 (79.2000)  Acc@5: 95.8333 (95.1000)  time: 0.0854  data: 0.0002  max mem: 358\n",
            "Test: [Task 3] Total time: 0:00:04 (0.1048 s / it)\n",
            "* Acc@1 79.200 Acc@5 95.100 loss 0.796\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:19  Loss: 1.0021 (1.0021)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  time: 0.4577  data: 0.3764  max mem: 358\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:03  Loss: 0.8141 (0.8047)  Acc@1: 79.1667 (76.8939)  Acc@5: 95.8333 (96.9697)  time: 0.1226  data: 0.0350  max mem: 358\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 0.7908 (0.8058)  Acc@1: 75.0000 (76.5873)  Acc@5: 95.8333 (96.4286)  time: 0.0871  data: 0.0006  max mem: 358\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 0.7495 (0.7543)  Acc@1: 79.1667 (78.8979)  Acc@5: 95.8333 (96.9086)  time: 0.0861  data: 0.0003  max mem: 358\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.6918 (0.7637)  Acc@1: 79.1667 (78.7602)  Acc@5: 95.8333 (96.8496)  time: 0.0875  data: 0.0002  max mem: 358\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.6864 (0.7545)  Acc@1: 79.1667 (79.0000)  Acc@5: 95.8333 (96.9000)  time: 0.0853  data: 0.0002  max mem: 358\n",
            "Test: [Task 4] Total time: 0:00:04 (0.0973 s / it)\n",
            "* Acc@1 79.000 Acc@5 96.900 loss 0.754\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:16  Loss: 0.7177 (0.7177)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.3833  data: 0.2983  max mem: 358\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:04  Loss: 0.8763 (0.8265)  Acc@1: 70.8333 (76.5152)  Acc@5: 95.8333 (97.3485)  time: 0.1266  data: 0.0433  max mem: 358\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 0.8504 (0.8441)  Acc@1: 70.8333 (75.9921)  Acc@5: 95.8333 (96.4286)  time: 0.0940  data: 0.0093  max mem: 358\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 0.8152 (0.8392)  Acc@1: 75.0000 (76.3441)  Acc@5: 95.8333 (96.1022)  time: 0.0873  data: 0.0005  max mem: 358\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.8471 (0.8783)  Acc@1: 70.8333 (74.6951)  Acc@5: 95.8333 (95.8333)  time: 0.0879  data: 0.0004  max mem: 358\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.9482 (0.8914)  Acc@1: 70.8333 (74.6000)  Acc@5: 95.8333 (95.6000)  time: 0.0861  data: 0.0004  max mem: 358\n",
            "Test: [Task 5] Total time: 0:00:04 (0.0999 s / it)\n",
            "* Acc@1 74.600 Acc@5 95.600 loss 0.891\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:30  Loss: 0.7564 (0.7564)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  time: 0.7316  data: 0.6292  max mem: 358\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:04  Loss: 0.7855 (0.8427)  Acc@1: 79.1667 (77.2727)  Acc@5: 95.8333 (97.3485)  time: 0.1471  data: 0.0586  max mem: 358\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 0.8333 (0.8494)  Acc@1: 75.0000 (76.7857)  Acc@5: 100.0000 (97.4206)  time: 0.0883  data: 0.0016  max mem: 358\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 0.8521 (0.8720)  Acc@1: 75.0000 (74.7312)  Acc@5: 100.0000 (97.1774)  time: 0.0879  data: 0.0013  max mem: 358\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 0.8893 (0.8925)  Acc@1: 75.0000 (74.5935)  Acc@5: 95.8333 (96.7480)  time: 0.0878  data: 0.0005  max mem: 358\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 0.8893 (0.8942)  Acc@1: 75.0000 (74.5000)  Acc@5: 95.8333 (96.7000)  time: 0.0863  data: 0.0005  max mem: 358\n",
            "Test: [Task 6] Total time: 0:00:04 (0.1048 s / it)\n",
            "* Acc@1 74.500 Acc@5 96.700 loss 0.894\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:15  Loss: 1.0417 (1.0417)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  time: 0.3611  data: 0.2845  max mem: 358\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:03  Loss: 0.9622 (0.9834)  Acc@1: 75.0000 (74.2424)  Acc@5: 95.8333 (95.4545)  time: 0.1156  data: 0.0314  max mem: 358\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:02  Loss: 0.9546 (1.0070)  Acc@1: 75.0000 (73.6111)  Acc@5: 95.8333 (95.8333)  time: 0.0892  data: 0.0033  max mem: 358\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:01  Loss: 1.0211 (1.0364)  Acc@1: 70.8333 (71.6398)  Acc@5: 95.8333 (94.4892)  time: 0.0878  data: 0.0004  max mem: 358\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 1.0266 (1.0388)  Acc@1: 70.8333 (71.9512)  Acc@5: 91.6667 (94.3089)  time: 0.0883  data: 0.0002  max mem: 358\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 1.0266 (1.0349)  Acc@1: 70.8333 (72.2000)  Acc@5: 93.7500 (94.3000)  time: 0.0863  data: 0.0002  max mem: 358\n",
            "Test: [Task 7] Total time: 0:00:04 (0.0964 s / it)\n",
            "* Acc@1 72.200 Acc@5 94.300 loss 1.035\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:15  Loss: 1.2249 (1.2249)  Acc@1: 66.6667 (66.6667)  Acc@5: 91.6667 (91.6667)  time: 0.3608  data: 0.2787  max mem: 358\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:03  Loss: 1.2582 (1.2774)  Acc@1: 62.5000 (63.2576)  Acc@5: 91.6667 (92.8030)  time: 0.1211  data: 0.0368  max mem: 358\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:02  Loss: 1.2470 (1.2544)  Acc@1: 62.5000 (64.2857)  Acc@5: 91.6667 (92.8571)  time: 0.0922  data: 0.0064  max mem: 358\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:01  Loss: 1.1752 (1.2359)  Acc@1: 66.6667 (64.3817)  Acc@5: 95.8333 (93.6828)  time: 0.0885  data: 0.0003  max mem: 358\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 1.1917 (1.2515)  Acc@1: 62.5000 (63.7195)  Acc@5: 95.8333 (92.8862)  time: 0.0890  data: 0.0003  max mem: 358\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 1.1917 (1.2471)  Acc@1: 62.5000 (63.8000)  Acc@5: 93.7500 (92.9000)  time: 0.0870  data: 0.0003  max mem: 358\n",
            "Test: [Task 8] Total time: 0:00:04 (0.0984 s / it)\n",
            "* Acc@1 63.800 Acc@5 92.900 loss 1.247\n",
            "Test: [Task 9]  [ 0/42]  eta: 0:00:14  Loss: 4.8284 (4.8284)  Acc@1: 0.0000 (0.0000)  Acc@5: 8.3333 (8.3333)  time: 0.3342  data: 0.2584  max mem: 358\n",
            "Test: [Task 9]  [10/42]  eta: 0:00:04  Loss: 4.6708 (4.6286)  Acc@1: 0.0000 (0.0000)  Acc@5: 8.3333 (9.8485)  time: 0.1267  data: 0.0404  max mem: 358\n",
            "Test: [Task 9]  [20/42]  eta: 0:00:02  Loss: 4.6019 (4.6086)  Acc@1: 0.0000 (0.0000)  Acc@5: 8.3333 (10.9127)  time: 0.0990  data: 0.0114  max mem: 358\n",
            "Test: [Task 9]  [30/42]  eta: 0:00:01  Loss: 4.6019 (4.6352)  Acc@1: 0.0000 (0.0000)  Acc@5: 8.3333 (9.5430)  time: 0.0910  data: 0.0030  max mem: 358\n",
            "Test: [Task 9]  [40/42]  eta: 0:00:00  Loss: 4.5645 (4.6133)  Acc@1: 0.0000 (0.0000)  Acc@5: 8.3333 (10.5691)  time: 0.0894  data: 0.0014  max mem: 358\n",
            "Test: [Task 9]  [41/42]  eta: 0:00:00  Loss: 4.5645 (4.6042)  Acc@1: 0.0000 (0.0000)  Acc@5: 12.5000 (10.9000)  time: 0.0876  data: 0.0013  max mem: 358\n",
            "Test: [Task 9] Total time: 0:00:04 (0.1021 s / it)\n",
            "* Acc@1 0.000 Acc@5 10.900 loss 4.604\n",
            "[Average accuracy till task9]\tAcc@1: 66.2556\tAcc@5: 85.7889\tLoss: 1.3247\tForgetting: 3.6375\tBackward: 45.2875\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([21576, 384])\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 1.1357  Acc@1: 79.1667 (81.8229)  Acc@5: 95.8333 (92.3958)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000467  Loss: 0.6857  Acc@1: 79.1667 (83.5417)  Acc@5: 100.0000 (97.7604)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000464  Loss: 0.5421  Acc@1: 87.5000 (86.1458)  Acc@5: 100.0000 (99.4271)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 0.4429  Acc@1: 91.6667 (88.1771)  Acc@5: 100.0000 (99.3229)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000448  Loss: 0.5952  Acc@1: 87.5000 (87.8125)  Acc@5: 100.0000 (99.4271)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000437  Loss: 0.3503  Acc@1: 87.5000 (89.9479)  Acc@5: 100.0000 (99.6354)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 0.3685  Acc@1: 91.6667 (90.5208)  Acc@5: 100.0000 (99.6875)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000409  Loss: 0.6113  Acc@1: 91.6667 (90.2083)  Acc@5: 100.0000 (99.3229)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000391  Loss: 0.3993  Acc@1: 95.8333 (91.8229)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 0.3676  Acc@1: 91.6667 (91.4063)  Acc@5: 100.0000 (99.6354)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000352  Loss: 0.5101  Acc@1: 91.6667 (90.8854)  Acc@5: 100.0000 (99.7396)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000330  Loss: 0.6054  Acc@1: 91.6667 (92.7604)  Acc@5: 100.0000 (99.7396)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 0.5657  Acc@1: 91.6667 (92.9688)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000283  Loss: 0.3750  Acc@1: 91.6667 (92.8125)  Acc@5: 100.0000 (99.6875)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000259  Loss: 0.5284  Acc@1: 91.6667 (91.8750)  Acc@5: 100.0000 (99.6875)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 0.4602  Acc@1: 91.6667 (92.6563)  Acc@5: 100.0000 (99.8958)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000210  Loss: 0.2683  Acc@1: 91.6667 (92.2917)  Acc@5: 100.0000 (99.7396)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000186  Loss: 0.3387  Acc@1: 95.8333 (93.1250)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.5009  Acc@1: 95.8333 (93.2813)  Acc@5: 100.0000 (99.8958)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000139  Loss: 0.5079  Acc@1: 91.6667 (93.3333)  Acc@5: 100.0000 (99.7396)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000117  Loss: 0.4655  Acc@1: 91.6667 (92.5000)  Acc@5: 100.0000 (99.6354)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 0.2822  Acc@1: 95.8333 (93.1250)  Acc@5: 100.0000 (99.9479)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000078  Loss: 0.3971  Acc@1: 95.8333 (93.4375)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000060  Loss: 0.5986  Acc@1: 91.6667 (93.0208)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 0.4400  Acc@1: 91.6667 (91.9271)  Acc@5: 100.0000 (99.7396)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000031  Loss: 0.4615  Acc@1: 91.6667 (92.7083)  Acc@5: 100.0000 (99.9479)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000020  Loss: 0.3055  Acc@1: 95.8333 (93.4896)  Acc@5: 100.0000 (99.7396)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.4467  Acc@1: 91.6667 (92.0313)  Acc@5: 100.0000 (99.7396)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000005  Loss: 0.4275  Acc@1: 91.6667 (92.6563)  Acc@5: 100.0000 (99.6875)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000001  Loss: 0.3641  Acc@1: 91.6667 (93.3333)  Acc@5: 100.0000 (99.7917)\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:15  Loss: 0.7547 (0.7547)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  time: 0.3695  data: 0.2900  max mem: 359\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 0.7547 (0.7760)  Acc@1: 79.1667 (75.0000)  Acc@5: 95.8333 (95.4545)  time: 0.1249  data: 0.0396  max mem: 359\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.7477 (0.7567)  Acc@1: 79.1667 (77.7778)  Acc@5: 95.8333 (95.0397)  time: 0.0966  data: 0.0075  max mem: 359\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.6982 (0.7243)  Acc@1: 79.1667 (78.6290)  Acc@5: 95.8333 (95.5645)  time: 0.0927  data: 0.0003  max mem: 359\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6158 (0.6965)  Acc@1: 83.3333 (79.2683)  Acc@5: 100.0000 (96.3415)  time: 0.0928  data: 0.0004  max mem: 359\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.5729 (0.6850)  Acc@1: 83.3333 (79.5000)  Acc@5: 100.0000 (96.4000)  time: 0.0909  data: 0.0004  max mem: 359\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1040 s / it)\n",
            "* Acc@1 79.500 Acc@5 96.400 loss 0.685\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:30  Loss: 1.0503 (1.0503)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  time: 0.7192  data: 0.6166  max mem: 359\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 0.8718 (0.8611)  Acc@1: 75.0000 (78.0303)  Acc@5: 95.8333 (94.3182)  time: 0.1506  data: 0.0570  max mem: 359\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 0.8800 (0.9161)  Acc@1: 75.0000 (75.3968)  Acc@5: 95.8333 (94.0476)  time: 0.0938  data: 0.0013  max mem: 359\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.8983 (0.9076)  Acc@1: 75.0000 (75.1344)  Acc@5: 95.8333 (94.2204)  time: 0.0934  data: 0.0011  max mem: 359\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.8436 (0.8904)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (94.4106)  time: 0.0930  data: 0.0005  max mem: 359\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.8306 (0.8865)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (94.4000)  time: 0.0916  data: 0.0005  max mem: 359\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1100 s / it)\n",
            "* Acc@1 75.000 Acc@5 94.400 loss 0.886\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:15  Loss: 0.5503 (0.5503)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.3639  data: 0.2861  max mem: 359\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:04  Loss: 0.7693 (0.8021)  Acc@1: 79.1667 (79.9242)  Acc@5: 95.8333 (95.4545)  time: 0.1299  data: 0.0395  max mem: 359\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 0.7558 (0.7469)  Acc@1: 75.0000 (80.1587)  Acc@5: 95.8333 (95.6349)  time: 0.0995  data: 0.0076  max mem: 359\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.6857 (0.7318)  Acc@1: 75.0000 (79.5699)  Acc@5: 95.8333 (96.1022)  time: 0.0926  data: 0.0003  max mem: 359\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.6926 (0.7435)  Acc@1: 79.1667 (79.7764)  Acc@5: 95.8333 (95.7317)  time: 0.0936  data: 0.0002  max mem: 359\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.6879 (0.7406)  Acc@1: 79.1667 (79.6000)  Acc@5: 95.8333 (95.8000)  time: 0.0917  data: 0.0002  max mem: 359\n",
            "Test: [Task 3] Total time: 0:00:04 (0.1043 s / it)\n",
            "* Acc@1 79.600 Acc@5 95.800 loss 0.741\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:18  Loss: 1.1259 (1.1259)  Acc@1: 66.6667 (66.6667)  Acc@5: 87.5000 (87.5000)  time: 0.4383  data: 0.3525  max mem: 359\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:03  Loss: 0.8772 (0.8580)  Acc@1: 70.8333 (72.3485)  Acc@5: 95.8333 (95.8333)  time: 0.1241  data: 0.0327  max mem: 359\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 0.8215 (0.8523)  Acc@1: 70.8333 (73.6111)  Acc@5: 95.8333 (95.4365)  time: 0.0920  data: 0.0005  max mem: 359\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 0.7827 (0.7920)  Acc@1: 75.0000 (75.9409)  Acc@5: 95.8333 (96.1022)  time: 0.0922  data: 0.0004  max mem: 359\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7189 (0.8006)  Acc@1: 75.0000 (75.7114)  Acc@5: 95.8333 (95.6301)  time: 0.0932  data: 0.0003  max mem: 359\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.6909 (0.7896)  Acc@1: 75.0000 (76.0000)  Acc@5: 95.8333 (95.7000)  time: 0.0915  data: 0.0003  max mem: 359\n",
            "Test: [Task 4] Total time: 0:00:04 (0.1035 s / it)\n",
            "* Acc@1 76.000 Acc@5 95.700 loss 0.790\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:30  Loss: 0.6668 (0.6668)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.7277  data: 0.6189  max mem: 359\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:04  Loss: 0.7477 (0.7536)  Acc@1: 75.0000 (79.5455)  Acc@5: 100.0000 (98.1061)  time: 0.1512  data: 0.0566  max mem: 359\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 0.8008 (0.7823)  Acc@1: 75.0000 (77.5794)  Acc@5: 95.8333 (97.0238)  time: 0.0923  data: 0.0005  max mem: 359\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 0.7616 (0.7810)  Acc@1: 75.0000 (77.8226)  Acc@5: 95.8333 (96.2366)  time: 0.0915  data: 0.0009  max mem: 359\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.8031 (0.8202)  Acc@1: 75.0000 (76.0163)  Acc@5: 95.8333 (96.1382)  time: 0.0916  data: 0.0009  max mem: 359\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.8038 (0.8349)  Acc@1: 75.0000 (75.9000)  Acc@5: 95.8333 (96.0000)  time: 0.0901  data: 0.0009  max mem: 359\n",
            "Test: [Task 5] Total time: 0:00:04 (0.1089 s / it)\n",
            "* Acc@1 75.900 Acc@5 96.000 loss 0.835\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:18  Loss: 0.7712 (0.7712)  Acc@1: 83.3333 (83.3333)  Acc@5: 91.6667 (91.6667)  time: 0.4398  data: 0.3645  max mem: 359\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:03  Loss: 0.8115 (0.8488)  Acc@1: 79.1667 (76.8939)  Acc@5: 95.8333 (95.8333)  time: 0.1237  data: 0.0339  max mem: 359\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 0.8521 (0.8681)  Acc@1: 75.0000 (75.7937)  Acc@5: 95.8333 (96.0317)  time: 0.0911  data: 0.0006  max mem: 359\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 0.8831 (0.8818)  Acc@1: 75.0000 (75.4032)  Acc@5: 95.8333 (95.6989)  time: 0.0903  data: 0.0003  max mem: 359\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 0.8925 (0.9001)  Acc@1: 75.0000 (74.5935)  Acc@5: 95.8333 (95.3252)  time: 0.0907  data: 0.0003  max mem: 359\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 0.8925 (0.9022)  Acc@1: 75.0000 (74.6000)  Acc@5: 95.8333 (95.3000)  time: 0.0890  data: 0.0003  max mem: 359\n",
            "Test: [Task 6] Total time: 0:00:04 (0.1007 s / it)\n",
            "* Acc@1 74.600 Acc@5 95.300 loss 0.902\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:19  Loss: 1.0144 (1.0144)  Acc@1: 75.0000 (75.0000)  Acc@5: 87.5000 (87.5000)  time: 0.4693  data: 0.3949  max mem: 359\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:04  Loss: 0.9193 (0.9162)  Acc@1: 75.0000 (75.3788)  Acc@5: 95.8333 (95.0758)  time: 0.1268  data: 0.0365  max mem: 359\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:02  Loss: 0.8920 (0.9439)  Acc@1: 75.0000 (73.8095)  Acc@5: 95.8333 (94.4444)  time: 0.0909  data: 0.0010  max mem: 359\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:01  Loss: 0.9051 (0.9781)  Acc@1: 70.8333 (72.4462)  Acc@5: 91.6667 (93.1452)  time: 0.0898  data: 0.0009  max mem: 359\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 0.9342 (0.9817)  Acc@1: 70.8333 (72.6626)  Acc@5: 91.6667 (93.1911)  time: 0.0906  data: 0.0004  max mem: 359\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 0.9342 (0.9775)  Acc@1: 70.8333 (72.9000)  Acc@5: 91.6667 (93.2000)  time: 0.0889  data: 0.0004  max mem: 359\n",
            "Test: [Task 7] Total time: 0:00:04 (0.1013 s / it)\n",
            "* Acc@1 72.900 Acc@5 93.200 loss 0.977\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:23  Loss: 0.9586 (0.9586)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  time: 0.5687  data: 0.4467  max mem: 359\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:04  Loss: 1.1092 (1.0562)  Acc@1: 70.8333 (72.3485)  Acc@5: 91.6667 (91.6667)  time: 0.1345  data: 0.0411  max mem: 359\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:02  Loss: 1.0040 (1.0164)  Acc@1: 66.6667 (71.4286)  Acc@5: 91.6667 (93.6508)  time: 0.0903  data: 0.0010  max mem: 359\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:01  Loss: 0.9907 (0.9989)  Acc@1: 66.6667 (71.1022)  Acc@5: 95.8333 (94.0860)  time: 0.0896  data: 0.0010  max mem: 359\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 1.0421 (1.0216)  Acc@1: 66.6667 (70.1220)  Acc@5: 91.6667 (93.1911)  time: 0.0894  data: 0.0004  max mem: 359\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 1.0421 (1.0184)  Acc@1: 66.6667 (70.2000)  Acc@5: 91.6667 (93.2000)  time: 0.0874  data: 0.0004  max mem: 359\n",
            "Test: [Task 8] Total time: 0:00:04 (0.1037 s / it)\n",
            "* Acc@1 70.200 Acc@5 93.200 loss 1.018\n",
            "Test: [Task 9]  [ 0/42]  eta: 0:00:21  Loss: 1.0219 (1.0219)  Acc@1: 62.5000 (62.5000)  Acc@5: 95.8333 (95.8333)  time: 0.5178  data: 0.4452  max mem: 359\n",
            "Test: [Task 9]  [10/42]  eta: 0:00:04  Loss: 1.0219 (1.0523)  Acc@1: 70.8333 (69.3182)  Acc@5: 95.8333 (93.1818)  time: 0.1288  data: 0.0411  max mem: 359\n",
            "Test: [Task 9]  [20/42]  eta: 0:00:02  Loss: 1.0583 (1.0456)  Acc@1: 70.8333 (69.6429)  Acc@5: 95.8333 (94.0476)  time: 0.0889  data: 0.0005  max mem: 359\n",
            "Test: [Task 9]  [30/42]  eta: 0:00:01  Loss: 1.1018 (1.0885)  Acc@1: 66.6667 (68.2796)  Acc@5: 95.8333 (94.0860)  time: 0.0879  data: 0.0003  max mem: 359\n",
            "Test: [Task 9]  [40/42]  eta: 0:00:00  Loss: 0.9611 (1.0456)  Acc@1: 70.8333 (69.6138)  Acc@5: 95.8333 (94.6138)  time: 0.0886  data: 0.0003  max mem: 359\n",
            "Test: [Task 9]  [41/42]  eta: 0:00:00  Loss: 0.9399 (1.0349)  Acc@1: 70.8333 (70.0000)  Acc@5: 95.8333 (94.7000)  time: 0.0868  data: 0.0003  max mem: 359\n",
            "Test: [Task 9] Total time: 0:00:04 (0.1005 s / it)\n",
            "* Acc@1 70.000 Acc@5 94.700 loss 1.035\n",
            "[Average accuracy till task9]\tAcc@1: 74.8556\tAcc@5: 94.9667\tLoss: 0.8744\tForgetting: 4.0750\tBackward: -2.3000\n",
            "Train: Epoch[1/1]  [  0/209]  eta: 0:01:42  Lr: 0.000047  Loss: 2.4192  Acc@1: 12.5000 (12.5000)  Acc@5: 50.0000 (50.0000)  time: 0.4887  data: 0.3746  max mem: 359\n",
            "Train: Epoch[1/1]  [ 10/209]  eta: 0:00:27  Lr: 0.000047  Loss: 2.3050  Acc@1: 12.5000 (12.5000)  Acc@5: 50.0000 (50.0000)  time: 0.1375  data: 0.0451  max mem: 359\n",
            "Train: Epoch[1/1]  [ 20/209]  eta: 0:00:21  Lr: 0.000047  Loss: 1.9632  Acc@1: 16.6667 (19.0476)  Acc@5: 58.3333 (59.9206)  time: 0.0956  data: 0.0063  max mem: 359\n",
            "Train: Epoch[1/1]  [ 30/209]  eta: 0:00:19  Lr: 0.000047  Loss: 1.7979  Acc@1: 33.3333 (25.2688)  Acc@5: 79.1667 (67.4731)  time: 0.0895  data: 0.0004  max mem: 359\n",
            "Train: Epoch[1/1]  [ 40/209]  eta: 0:00:17  Lr: 0.000047  Loss: 1.8573  Acc@1: 37.5000 (30.2846)  Acc@5: 87.5000 (72.4594)  time: 0.0893  data: 0.0008  max mem: 359\n",
            "Train: Epoch[1/1]  [ 50/209]  eta: 0:00:15  Lr: 0.000047  Loss: 1.6083  Acc@1: 54.1667 (34.9673)  Acc@5: 91.6667 (76.3072)  time: 0.0886  data: 0.0007  max mem: 359\n",
            "Train: Epoch[1/1]  [ 60/209]  eta: 0:00:14  Lr: 0.000047  Loss: 1.4827  Acc@1: 58.3333 (40.2322)  Acc@5: 91.6667 (79.1667)  time: 0.0916  data: 0.0008  max mem: 359\n",
            "Train: Epoch[1/1]  [ 70/209]  eta: 0:00:14  Lr: 0.000047  Loss: 1.3472  Acc@1: 66.6667 (44.2488)  Acc@5: 95.8333 (81.6315)  time: 0.1072  data: 0.0015  max mem: 359\n",
            "Train: Epoch[1/1]  [ 80/209]  eta: 0:00:13  Lr: 0.000047  Loss: 1.3400  Acc@1: 70.8333 (47.8395)  Acc@5: 95.8333 (83.3333)  time: 0.1214  data: 0.0022  max mem: 359\n",
            "Train: Epoch[1/1]  [ 90/209]  eta: 0:00:12  Lr: 0.000047  Loss: 1.1810  Acc@1: 75.0000 (51.0531)  Acc@5: 95.8333 (84.8901)  time: 0.1134  data: 0.0017  max mem: 359\n",
            "Train: Epoch[1/1]  [100/209]  eta: 0:00:11  Lr: 0.000047  Loss: 1.1774  Acc@1: 79.1667 (54.0017)  Acc@5: 95.8333 (86.0561)  time: 0.1033  data: 0.0011  max mem: 359\n",
            "Train: Epoch[1/1]  [110/209]  eta: 0:00:10  Lr: 0.000047  Loss: 1.0736  Acc@1: 79.1667 (56.0060)  Acc@5: 95.8333 (87.0120)  time: 0.0983  data: 0.0015  max mem: 359\n",
            "Train: Epoch[1/1]  [120/209]  eta: 0:00:09  Lr: 0.000047  Loss: 1.0193  Acc@1: 79.1667 (57.9545)  Acc@5: 100.0000 (87.9821)  time: 0.0918  data: 0.0013  max mem: 359\n",
            "Train: Epoch[1/1]  [130/209]  eta: 0:00:07  Lr: 0.000047  Loss: 0.7251  Acc@1: 79.1667 (59.8282)  Acc@5: 100.0000 (88.7723)  time: 0.0891  data: 0.0009  max mem: 359\n",
            "Train: Epoch[1/1]  [140/209]  eta: 0:00:06  Lr: 0.000047  Loss: 0.8264  Acc@1: 83.3333 (61.6430)  Acc@5: 100.0000 (89.4208)  time: 0.0882  data: 0.0006  max mem: 359\n",
            "Train: Epoch[1/1]  [150/209]  eta: 0:00:05  Lr: 0.000047  Loss: 1.0973  Acc@1: 79.1667 (62.7483)  Acc@5: 100.0000 (89.8731)  time: 0.0883  data: 0.0005  max mem: 359\n",
            "Train: Epoch[1/1]  [160/209]  eta: 0:00:04  Lr: 0.000047  Loss: 0.8895  Acc@1: 79.1667 (63.8458)  Acc@5: 95.8333 (90.3727)  time: 0.0885  data: 0.0005  max mem: 359\n",
            "Train: Epoch[1/1]  [170/209]  eta: 0:00:03  Lr: 0.000047  Loss: 0.7914  Acc@1: 79.1667 (64.7661)  Acc@5: 95.8333 (90.7895)  time: 0.0880  data: 0.0005  max mem: 359\n",
            "Train: Epoch[1/1]  [180/209]  eta: 0:00:02  Lr: 0.000047  Loss: 0.7559  Acc@1: 79.1667 (65.6538)  Acc@5: 100.0000 (91.2063)  time: 0.0881  data: 0.0006  max mem: 359\n",
            "Train: Epoch[1/1]  [190/209]  eta: 0:00:01  Lr: 0.000047  Loss: 0.7428  Acc@1: 79.1667 (66.4267)  Acc@5: 100.0000 (91.4921)  time: 0.0884  data: 0.0006  max mem: 359\n",
            "Train: Epoch[1/1]  [200/209]  eta: 0:00:00  Lr: 0.000047  Loss: 0.5605  Acc@1: 83.3333 (67.2056)  Acc@5: 100.0000 (91.7910)  time: 0.0926  data: 0.0010  max mem: 359\n",
            "Train: Epoch[1/1]  [208/209]  eta: 0:00:00  Lr: 0.000047  Loss: 0.5442  Acc@1: 83.3333 (67.9000)  Acc@5: 100.0000 (92.0600)  time: 0.0898  data: 0.0008  max mem: 359\n",
            "Train: Epoch[1/1] Total time: 0:00:20 (0.0971 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.5442  Acc@1: 83.3333 (67.9000)  Acc@5: 100.0000 (92.0600)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:31  Loss: 0.7709 (0.7709)  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.8333)  time: 0.7602  data: 0.6664  max mem: 359\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 0.7709 (0.7778)  Acc@1: 75.0000 (75.3788)  Acc@5: 95.8333 (95.8333)  time: 0.1509  data: 0.0651  max mem: 359\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.7504 (0.7717)  Acc@1: 75.0000 (77.3810)  Acc@5: 95.8333 (95.2381)  time: 0.0888  data: 0.0029  max mem: 359\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.6947 (0.7392)  Acc@1: 79.1667 (78.2258)  Acc@5: 95.8333 (95.5645)  time: 0.0874  data: 0.0010  max mem: 359\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.5950 (0.7099)  Acc@1: 83.3333 (79.2683)  Acc@5: 95.8333 (96.2398)  time: 0.0875  data: 0.0007  max mem: 359\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.5884 (0.6988)  Acc@1: 83.3333 (79.5000)  Acc@5: 100.0000 (96.3000)  time: 0.0860  data: 0.0007  max mem: 359\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1056 s / it)\n",
            "* Acc@1 79.500 Acc@5 96.300 loss 0.699\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:22  Loss: 1.1022 (1.1022)  Acc@1: 66.6667 (66.6667)  Acc@5: 91.6667 (91.6667)  time: 0.5298  data: 0.4626  max mem: 359\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 0.8899 (0.9408)  Acc@1: 75.0000 (73.4849)  Acc@5: 95.8333 (93.9394)  time: 0.1286  data: 0.0427  max mem: 359\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 0.8905 (0.9781)  Acc@1: 75.0000 (74.2064)  Acc@5: 95.8333 (93.8492)  time: 0.0870  data: 0.0005  max mem: 359\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.9588 (0.9776)  Acc@1: 70.8333 (73.9247)  Acc@5: 91.6667 (93.4140)  time: 0.0863  data: 0.0004  max mem: 359\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.9310 (0.9675)  Acc@1: 70.8333 (73.7805)  Acc@5: 91.6667 (93.6992)  time: 0.0876  data: 0.0004  max mem: 359\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.9267 (0.9645)  Acc@1: 70.8333 (73.8000)  Acc@5: 91.6667 (93.7000)  time: 0.0854  data: 0.0003  max mem: 359\n",
            "Test: [Task 2] Total time: 0:00:04 (0.0992 s / it)\n",
            "* Acc@1 73.800 Acc@5 93.700 loss 0.965\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:15  Loss: 0.5819 (0.5819)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.3786  data: 0.3028  max mem: 359\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 0.8214 (0.8138)  Acc@1: 79.1667 (77.6515)  Acc@5: 95.8333 (95.4545)  time: 0.1158  data: 0.0321  max mem: 359\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 0.7885 (0.7730)  Acc@1: 75.0000 (79.1667)  Acc@5: 95.8333 (95.6349)  time: 0.0881  data: 0.0027  max mem: 359\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.7221 (0.7627)  Acc@1: 75.0000 (78.3602)  Acc@5: 95.8333 (96.1022)  time: 0.0874  data: 0.0004  max mem: 359\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.7871 (0.7741)  Acc@1: 75.0000 (78.7602)  Acc@5: 95.8333 (95.6301)  time: 0.0880  data: 0.0003  max mem: 359\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.7221 (0.7718)  Acc@1: 75.0000 (78.7000)  Acc@5: 95.8333 (95.7000)  time: 0.0862  data: 0.0003  max mem: 359\n",
            "Test: [Task 3] Total time: 0:00:04 (0.0965 s / it)\n",
            "* Acc@1 78.700 Acc@5 95.700 loss 0.772\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:16  Loss: 1.1628 (1.1628)  Acc@1: 70.8333 (70.8333)  Acc@5: 83.3333 (83.3333)  time: 0.3936  data: 0.3168  max mem: 359\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:03  Loss: 0.8416 (0.8601)  Acc@1: 70.8333 (74.2424)  Acc@5: 95.8333 (94.6970)  time: 0.1163  data: 0.0309  max mem: 359\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 0.8416 (0.8492)  Acc@1: 70.8333 (74.8016)  Acc@5: 95.8333 (95.0397)  time: 0.0879  data: 0.0019  max mem: 359\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 0.7607 (0.7959)  Acc@1: 75.0000 (76.6129)  Acc@5: 95.8333 (95.6989)  time: 0.0871  data: 0.0011  max mem: 359\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7607 (0.8065)  Acc@1: 75.0000 (76.5244)  Acc@5: 95.8333 (95.7317)  time: 0.0876  data: 0.0009  max mem: 359\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.7291 (0.7953)  Acc@1: 79.1667 (76.8000)  Acc@5: 95.8333 (95.8000)  time: 0.0861  data: 0.0007  max mem: 359\n",
            "Test: [Task 4] Total time: 0:00:04 (0.0985 s / it)\n",
            "* Acc@1 76.800 Acc@5 95.800 loss 0.795\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:32  Loss: 0.5965 (0.5965)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.7687  data: 0.6668  max mem: 359\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:04  Loss: 0.7640 (0.7481)  Acc@1: 79.1667 (78.4091)  Acc@5: 100.0000 (98.4848)  time: 0.1520  data: 0.0621  max mem: 359\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 0.7664 (0.7726)  Acc@1: 75.0000 (77.5794)  Acc@5: 95.8333 (97.4206)  time: 0.0888  data: 0.0012  max mem: 359\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 0.7475 (0.7692)  Acc@1: 79.1667 (78.6290)  Acc@5: 95.8333 (96.7742)  time: 0.0875  data: 0.0006  max mem: 359\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.7946 (0.8078)  Acc@1: 79.1667 (77.7439)  Acc@5: 95.8333 (96.3415)  time: 0.0880  data: 0.0004  max mem: 359\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.8047 (0.8234)  Acc@1: 79.1667 (77.6000)  Acc@5: 95.8333 (96.2000)  time: 0.0863  data: 0.0004  max mem: 359\n",
            "Test: [Task 5] Total time: 0:00:04 (0.1064 s / it)\n",
            "* Acc@1 77.600 Acc@5 96.200 loss 0.823\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:22  Loss: 0.8023 (0.8023)  Acc@1: 83.3333 (83.3333)  Acc@5: 91.6667 (91.6667)  time: 0.5341  data: 0.4631  max mem: 359\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:04  Loss: 0.8757 (0.9159)  Acc@1: 79.1667 (73.8636)  Acc@5: 95.8333 (94.3182)  time: 0.1303  data: 0.0429  max mem: 359\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 0.8912 (0.9344)  Acc@1: 75.0000 (73.6111)  Acc@5: 95.8333 (94.4444)  time: 0.0884  data: 0.0006  max mem: 359\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 0.9402 (0.9563)  Acc@1: 70.8333 (72.7151)  Acc@5: 95.8333 (94.6237)  time: 0.0871  data: 0.0003  max mem: 359\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 0.9440 (0.9692)  Acc@1: 70.8333 (72.5610)  Acc@5: 95.8333 (94.5122)  time: 0.0884  data: 0.0003  max mem: 359\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 0.9440 (0.9704)  Acc@1: 70.8333 (72.5000)  Acc@5: 95.8333 (94.5000)  time: 0.0865  data: 0.0003  max mem: 359\n",
            "Test: [Task 6] Total time: 0:00:04 (0.1007 s / it)\n",
            "* Acc@1 72.500 Acc@5 94.500 loss 0.970\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:16  Loss: 1.0765 (1.0765)  Acc@1: 75.0000 (75.0000)  Acc@5: 87.5000 (87.5000)  time: 0.3827  data: 0.3060  max mem: 359\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:03  Loss: 0.9389 (0.9407)  Acc@1: 75.0000 (75.7576)  Acc@5: 91.6667 (94.3182)  time: 0.1213  data: 0.0359  max mem: 359\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:02  Loss: 0.9141 (0.9667)  Acc@1: 75.0000 (73.2143)  Acc@5: 95.8333 (94.2460)  time: 0.0918  data: 0.0048  max mem: 359\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:01  Loss: 0.9632 (0.9947)  Acc@1: 66.6667 (72.5806)  Acc@5: 95.8333 (93.6828)  time: 0.0888  data: 0.0004  max mem: 359\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 0.9591 (0.9969)  Acc@1: 75.0000 (73.0691)  Acc@5: 91.6667 (93.5976)  time: 0.0895  data: 0.0004  max mem: 359\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 0.9123 (0.9929)  Acc@1: 75.0000 (73.3000)  Acc@5: 91.6667 (93.6000)  time: 0.0876  data: 0.0004  max mem: 359\n",
            "Test: [Task 7] Total time: 0:00:04 (0.1000 s / it)\n",
            "* Acc@1 73.300 Acc@5 93.600 loss 0.993\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:29  Loss: 1.0469 (1.0469)  Acc@1: 66.6667 (66.6667)  Acc@5: 91.6667 (91.6667)  time: 0.7090  data: 0.6022  max mem: 359\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:04  Loss: 1.2255 (1.1753)  Acc@1: 66.6667 (67.0455)  Acc@5: 91.6667 (91.6667)  time: 0.1459  data: 0.0550  max mem: 359\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:02  Loss: 1.1721 (1.1285)  Acc@1: 66.6667 (68.2540)  Acc@5: 91.6667 (92.4603)  time: 0.0897  data: 0.0004  max mem: 359\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:01  Loss: 1.0868 (1.1133)  Acc@1: 66.6667 (67.3387)  Acc@5: 91.6667 (93.0108)  time: 0.0898  data: 0.0010  max mem: 359\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 1.1397 (1.1397)  Acc@1: 62.5000 (67.1748)  Acc@5: 91.6667 (92.0732)  time: 0.0896  data: 0.0011  max mem: 359\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 1.1397 (1.1385)  Acc@1: 62.5000 (67.2000)  Acc@5: 91.6667 (92.1000)  time: 0.0884  data: 0.0010  max mem: 359\n",
            "Test: [Task 8] Total time: 0:00:04 (0.1067 s / it)\n",
            "* Acc@1 67.200 Acc@5 92.100 loss 1.139\n",
            "Test: [Task 9]  [ 0/42]  eta: 0:00:18  Loss: 1.0918 (1.0918)  Acc@1: 54.1667 (54.1667)  Acc@5: 95.8333 (95.8333)  time: 0.4484  data: 0.3773  max mem: 359\n",
            "Test: [Task 9]  [10/42]  eta: 0:00:03  Loss: 1.1556 (1.1865)  Acc@1: 66.6667 (63.6364)  Acc@5: 91.6667 (92.8030)  time: 0.1230  data: 0.0358  max mem: 359\n",
            "Test: [Task 9]  [20/42]  eta: 0:00:02  Loss: 1.1846 (1.1931)  Acc@1: 66.6667 (63.8889)  Acc@5: 91.6667 (93.6508)  time: 0.0894  data: 0.0010  max mem: 359\n",
            "Test: [Task 9]  [30/42]  eta: 0:00:01  Loss: 1.2245 (1.2397)  Acc@1: 58.3333 (62.6344)  Acc@5: 95.8333 (93.8172)  time: 0.0891  data: 0.0003  max mem: 359\n",
            "Test: [Task 9]  [40/42]  eta: 0:00:00  Loss: 1.1270 (1.1920)  Acc@1: 62.5000 (63.4146)  Acc@5: 95.8333 (94.4106)  time: 0.0895  data: 0.0004  max mem: 359\n",
            "Test: [Task 9]  [41/42]  eta: 0:00:00  Loss: 1.1169 (1.1815)  Acc@1: 62.5000 (63.8000)  Acc@5: 95.8333 (94.5000)  time: 0.0877  data: 0.0004  max mem: 359\n",
            "Test: [Task 9] Total time: 0:00:04 (0.0997 s / it)\n",
            "* Acc@1 63.800 Acc@5 94.500 loss 1.182\n",
            "Test: [Task 10]  [ 0/42]  eta: 0:00:19  Loss: 4.5780 (4.5780)  Acc@1: 0.0000 (0.0000)  Acc@5: 8.3333 (8.3333)  time: 0.4753  data: 0.3902  max mem: 359\n",
            "Test: [Task 10]  [10/42]  eta: 0:00:04  Loss: 4.6235 (4.6705)  Acc@1: 0.0000 (0.0000)  Acc@5: 8.3333 (10.6061)  time: 0.1262  data: 0.0368  max mem: 359\n",
            "Test: [Task 10]  [20/42]  eta: 0:00:02  Loss: 4.6578 (4.6383)  Acc@1: 0.0000 (0.0000)  Acc@5: 12.5000 (11.7063)  time: 0.0895  data: 0.0009  max mem: 359\n",
            "Test: [Task 10]  [30/42]  eta: 0:00:01  Loss: 4.6997 (4.6613)  Acc@1: 0.0000 (0.0000)  Acc@5: 12.5000 (11.9624)  time: 0.0886  data: 0.0004  max mem: 359\n",
            "Test: [Task 10]  [40/42]  eta: 0:00:00  Loss: 4.7157 (4.6692)  Acc@1: 0.0000 (0.0000)  Acc@5: 12.5000 (11.8902)  time: 0.0899  data: 0.0003  max mem: 359\n",
            "Test: [Task 10]  [41/42]  eta: 0:00:00  Loss: 4.7156 (4.6604)  Acc@1: 0.0000 (0.0000)  Acc@5: 12.5000 (12.0000)  time: 0.0881  data: 0.0002  max mem: 359\n",
            "Test: [Task 10] Total time: 0:00:04 (0.1007 s / it)\n",
            "* Acc@1 0.000 Acc@5 12.000 loss 4.660\n",
            "[Average accuracy till task10]\tAcc@1: 66.3200\tAcc@5: 86.4400\tLoss: 1.2998\tForgetting: 3.6333\tBackward: 47.6889\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([23976, 384])\n",
            "/usr/local/lib/python3.11/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 0.7977  Acc@1: 83.3333 (81.8056)  Acc@5: 95.8333 (92.9167)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000467  Loss: 0.4433  Acc@1: 87.5000 (84.6759)  Acc@5: 100.0000 (97.7778)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000464  Loss: 0.4744  Acc@1: 83.3333 (85.3241)  Acc@5: 100.0000 (99.2593)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 0.6234  Acc@1: 87.5000 (87.1296)  Acc@5: 100.0000 (99.0741)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000448  Loss: 0.6920  Acc@1: 87.5000 (88.5185)  Acc@5: 100.0000 (99.3056)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000437  Loss: 0.4521  Acc@1: 87.5000 (90.0463)  Acc@5: 100.0000 (99.3519)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 0.4354  Acc@1: 91.6667 (90.7407)  Acc@5: 100.0000 (99.7222)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000409  Loss: 0.4332  Acc@1: 91.6667 (90.3241)  Acc@5: 100.0000 (99.7685)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000391  Loss: 0.5523  Acc@1: 91.6667 (90.1389)  Acc@5: 100.0000 (99.6759)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 0.2710  Acc@1: 91.6667 (91.0648)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000352  Loss: 0.4980  Acc@1: 91.6667 (91.1574)  Acc@5: 100.0000 (99.8611)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000330  Loss: 0.4342  Acc@1: 91.6667 (90.9259)  Acc@5: 100.0000 (99.3981)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 0.5260  Acc@1: 91.6667 (91.8056)  Acc@5: 100.0000 (99.9074)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000283  Loss: 0.4647  Acc@1: 91.6667 (91.2963)  Acc@5: 100.0000 (99.7685)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000259  Loss: 0.2777  Acc@1: 95.8333 (92.7778)  Acc@5: 100.0000 (99.6759)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 0.4427  Acc@1: 87.5000 (90.8796)  Acc@5: 100.0000 (99.7685)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000210  Loss: 0.4693  Acc@1: 91.6667 (92.8241)  Acc@5: 100.0000 (99.6296)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000186  Loss: 0.3613  Acc@1: 91.6667 (93.0556)  Acc@5: 100.0000 (99.8148)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.6001  Acc@1: 91.6667 (92.0370)  Acc@5: 100.0000 (99.6759)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000139  Loss: 0.4085  Acc@1: 91.6667 (92.2222)  Acc@5: 100.0000 (99.7685)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000117  Loss: 0.3016  Acc@1: 91.6667 (92.0370)  Acc@5: 100.0000 (99.8148)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 0.3218  Acc@1: 91.6667 (91.8982)  Acc@5: 100.0000 (99.8148)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000078  Loss: 0.4728  Acc@1: 91.6667 (92.3148)  Acc@5: 100.0000 (99.4907)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000060  Loss: 0.4677  Acc@1: 87.5000 (92.1296)  Acc@5: 100.0000 (99.6759)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 0.6317  Acc@1: 91.6667 (93.0556)  Acc@5: 100.0000 (99.8611)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000031  Loss: 0.2748  Acc@1: 95.8333 (92.3148)  Acc@5: 100.0000 (99.9074)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000020  Loss: 0.4229  Acc@1: 95.8333 (91.1111)  Acc@5: 100.0000 (99.8611)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.2704  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000005  Loss: 0.4593  Acc@1: 91.6667 (91.8056)  Acc@5: 100.0000 (99.4444)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000001  Loss: 0.3976  Acc@1: 91.6667 (91.7593)  Acc@5: 100.0000 (99.7685)\n",
            "/usr/local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:21  Loss: 0.7515 (0.7515)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  time: 0.5200  data: 0.4440  max mem: 359\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 0.7557 (0.7845)  Acc@1: 75.0000 (75.3788)  Acc@5: 95.8333 (94.6970)  time: 0.1321  data: 0.0410  max mem: 359\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.7557 (0.7616)  Acc@1: 79.1667 (77.7778)  Acc@5: 95.8333 (94.8413)  time: 0.0930  data: 0.0006  max mem: 359\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.6992 (0.7277)  Acc@1: 79.1667 (78.3602)  Acc@5: 95.8333 (95.2957)  time: 0.0932  data: 0.0007  max mem: 359\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6212 (0.7011)  Acc@1: 79.1667 (78.8618)  Acc@5: 95.8333 (95.8333)  time: 0.0932  data: 0.0007  max mem: 359\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.5598 (0.6893)  Acc@1: 83.3333 (79.1000)  Acc@5: 100.0000 (95.9000)  time: 0.0919  data: 0.0007  max mem: 359\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1067 s / it)\n",
            "* Acc@1 79.100 Acc@5 95.900 loss 0.689\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:31  Loss: 1.0837 (1.0837)  Acc@1: 70.8333 (70.8333)  Acc@5: 87.5000 (87.5000)  time: 0.7465  data: 0.6427  max mem: 359\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 0.8505 (0.8882)  Acc@1: 75.0000 (76.1364)  Acc@5: 91.6667 (93.1818)  time: 0.1523  data: 0.0594  max mem: 359\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 0.8644 (0.9468)  Acc@1: 75.0000 (74.8016)  Acc@5: 91.6667 (93.2540)  time: 0.0932  data: 0.0010  max mem: 359\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.9186 (0.9384)  Acc@1: 70.8333 (74.3280)  Acc@5: 95.8333 (93.4140)  time: 0.0937  data: 0.0006  max mem: 359\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.8877 (0.9214)  Acc@1: 70.8333 (73.9837)  Acc@5: 95.8333 (93.8008)  time: 0.0943  data: 0.0003  max mem: 359\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.8630 (0.9183)  Acc@1: 70.8333 (74.0000)  Acc@5: 95.8333 (93.8000)  time: 0.0926  data: 0.0003  max mem: 359\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1109 s / it)\n",
            "* Acc@1 74.000 Acc@5 93.800 loss 0.918\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:16  Loss: 0.5085 (0.5085)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.3946  data: 0.3158  max mem: 359\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:04  Loss: 0.7868 (0.7860)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.0758)  time: 0.1254  data: 0.0338  max mem: 359\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 0.7540 (0.7360)  Acc@1: 79.1667 (79.7619)  Acc@5: 95.8333 (95.4365)  time: 0.0964  data: 0.0031  max mem: 359\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.6942 (0.7309)  Acc@1: 75.0000 (78.3602)  Acc@5: 95.8333 (95.8333)  time: 0.0941  data: 0.0006  max mem: 359\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.7261 (0.7423)  Acc@1: 75.0000 (78.8618)  Acc@5: 95.8333 (95.5285)  time: 0.0939  data: 0.0005  max mem: 359\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.6942 (0.7395)  Acc@1: 75.0000 (78.8000)  Acc@5: 95.8333 (95.6000)  time: 0.0923  data: 0.0005  max mem: 359\n",
            "Test: [Task 3] Total time: 0:00:04 (0.1037 s / it)\n",
            "* Acc@1 78.800 Acc@5 95.600 loss 0.740\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:16  Loss: 1.2504 (1.2504)  Acc@1: 70.8333 (70.8333)  Acc@5: 87.5000 (87.5000)  time: 0.3927  data: 0.3090  max mem: 359\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:03  Loss: 0.9454 (0.9536)  Acc@1: 70.8333 (69.6970)  Acc@5: 95.8333 (94.6970)  time: 0.1200  data: 0.0299  max mem: 359\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 0.9147 (0.9311)  Acc@1: 70.8333 (71.4286)  Acc@5: 95.8333 (93.8492)  time: 0.0927  data: 0.0012  max mem: 359\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 0.8388 (0.8692)  Acc@1: 75.0000 (72.7151)  Acc@5: 95.8333 (94.7581)  time: 0.0931  data: 0.0003  max mem: 359\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.8222 (0.8809)  Acc@1: 75.0000 (73.0691)  Acc@5: 95.8333 (94.5122)  time: 0.0933  data: 0.0003  max mem: 359\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.8149 (0.8699)  Acc@1: 75.0000 (73.4000)  Acc@5: 95.8333 (94.6000)  time: 0.0918  data: 0.0003  max mem: 359\n",
            "Test: [Task 4] Total time: 0:00:04 (0.1023 s / it)\n",
            "* Acc@1 73.400 Acc@5 94.600 loss 0.870\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:31  Loss: 0.7136 (0.7136)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.7390  data: 0.6311  max mem: 359\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:04  Loss: 0.7570 (0.7714)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (97.7273)  time: 0.1517  data: 0.0592  max mem: 359\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 0.7994 (0.8096)  Acc@1: 75.0000 (77.1825)  Acc@5: 95.8333 (96.4286)  time: 0.0922  data: 0.0015  max mem: 359\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 0.7834 (0.7980)  Acc@1: 75.0000 (77.6882)  Acc@5: 95.8333 (95.8333)  time: 0.0914  data: 0.0015  max mem: 359\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.8267 (0.8392)  Acc@1: 75.0000 (76.3211)  Acc@5: 95.8333 (95.7317)  time: 0.0921  data: 0.0011  max mem: 359\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.8296 (0.8559)  Acc@1: 70.8333 (76.1000)  Acc@5: 95.8333 (95.6000)  time: 0.0906  data: 0.0011  max mem: 359\n",
            "Test: [Task 5] Total time: 0:00:04 (0.1094 s / it)\n",
            "* Acc@1 76.100 Acc@5 95.600 loss 0.856\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:18  Loss: 0.7695 (0.7695)  Acc@1: 83.3333 (83.3333)  Acc@5: 91.6667 (91.6667)  time: 0.4340  data: 0.3453  max mem: 359\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:03  Loss: 0.8689 (0.8535)  Acc@1: 79.1667 (76.8939)  Acc@5: 95.8333 (95.0758)  time: 0.1220  data: 0.0319  max mem: 359\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 0.8853 (0.8784)  Acc@1: 75.0000 (75.3968)  Acc@5: 95.8333 (95.0397)  time: 0.0909  data: 0.0005  max mem: 359\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 0.9261 (0.8998)  Acc@1: 70.8333 (74.7312)  Acc@5: 95.8333 (95.0269)  time: 0.0910  data: 0.0003  max mem: 359\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 0.9261 (0.9127)  Acc@1: 75.0000 (74.2886)  Acc@5: 95.8333 (94.9187)  time: 0.0911  data: 0.0003  max mem: 359\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 0.9261 (0.9137)  Acc@1: 75.0000 (74.3000)  Acc@5: 95.8333 (94.9000)  time: 0.0893  data: 0.0002  max mem: 359\n",
            "Test: [Task 6] Total time: 0:00:04 (0.1007 s / it)\n",
            "* Acc@1 74.300 Acc@5 94.900 loss 0.914\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:15  Loss: 1.1125 (1.1125)  Acc@1: 66.6667 (66.6667)  Acc@5: 91.6667 (91.6667)  time: 0.3784  data: 0.2856  max mem: 359\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:03  Loss: 0.9130 (0.9097)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (93.9394)  time: 0.1212  data: 0.0362  max mem: 359\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:02  Loss: 0.8963 (0.9428)  Acc@1: 75.0000 (73.4127)  Acc@5: 91.6667 (93.6508)  time: 0.0933  data: 0.0058  max mem: 359\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:01  Loss: 0.9175 (0.9809)  Acc@1: 70.8333 (71.9086)  Acc@5: 95.8333 (92.8763)  time: 0.0912  data: 0.0004  max mem: 359\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 0.9538 (0.9837)  Acc@1: 66.6667 (72.0528)  Acc@5: 95.8333 (93.0894)  time: 0.0904  data: 0.0005  max mem: 359\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 0.9538 (0.9785)  Acc@1: 70.8333 (72.3000)  Acc@5: 95.8333 (93.1000)  time: 0.0884  data: 0.0005  max mem: 359\n",
            "Test: [Task 7] Total time: 0:00:04 (0.1013 s / it)\n",
            "* Acc@1 72.300 Acc@5 93.100 loss 0.978\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:26  Loss: 0.9297 (0.9297)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  time: 0.6243  data: 0.5447  max mem: 359\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:04  Loss: 1.0751 (1.0469)  Acc@1: 75.0000 (73.4849)  Acc@5: 91.6667 (92.8030)  time: 0.1395  data: 0.0513  max mem: 359\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:02  Loss: 0.9863 (0.9995)  Acc@1: 70.8333 (72.4206)  Acc@5: 91.6667 (94.0476)  time: 0.0904  data: 0.0024  max mem: 359\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:01  Loss: 0.9809 (0.9799)  Acc@1: 70.8333 (71.9086)  Acc@5: 95.8333 (94.2204)  time: 0.0899  data: 0.0032  max mem: 359\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 1.0309 (1.0016)  Acc@1: 70.8333 (71.6463)  Acc@5: 91.6667 (93.6992)  time: 0.0898  data: 0.0024  max mem: 359\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 1.0309 (0.9985)  Acc@1: 70.8333 (71.8000)  Acc@5: 91.6667 (93.7000)  time: 0.0882  data: 0.0021  max mem: 359\n",
            "Test: [Task 8] Total time: 0:00:04 (0.1049 s / it)\n",
            "* Acc@1 71.800 Acc@5 93.700 loss 0.999\n",
            "Test: [Task 9]  [ 0/42]  eta: 0:00:14  Loss: 0.8386 (0.8386)  Acc@1: 70.8333 (70.8333)  Acc@5: 100.0000 (100.0000)  time: 0.3562  data: 0.2680  max mem: 359\n",
            "Test: [Task 9]  [10/42]  eta: 0:00:03  Loss: 0.9280 (0.9155)  Acc@1: 70.8333 (74.2424)  Acc@5: 91.6667 (94.3182)  time: 0.1182  data: 0.0318  max mem: 359\n",
            "Test: [Task 9]  [20/42]  eta: 0:00:02  Loss: 0.9280 (0.8948)  Acc@1: 75.0000 (75.3968)  Acc@5: 91.6667 (94.4444)  time: 0.0913  data: 0.0042  max mem: 359\n",
            "Test: [Task 9]  [30/42]  eta: 0:00:01  Loss: 0.9501 (0.9389)  Acc@1: 70.8333 (73.5215)  Acc@5: 95.8333 (94.4892)  time: 0.0881  data: 0.0004  max mem: 359\n",
            "Test: [Task 9]  [40/42]  eta: 0:00:00  Loss: 0.8076 (0.8875)  Acc@1: 70.8333 (75.0000)  Acc@5: 95.8333 (95.0203)  time: 0.0885  data: 0.0003  max mem: 359\n",
            "Test: [Task 9]  [41/42]  eta: 0:00:00  Loss: 0.7847 (0.8756)  Acc@1: 70.8333 (75.4000)  Acc@5: 95.8333 (95.1000)  time: 0.0870  data: 0.0003  max mem: 359\n",
            "Test: [Task 9] Total time: 0:00:04 (0.0978 s / it)\n",
            "* Acc@1 75.400 Acc@5 95.100 loss 0.876\n",
            "Test: [Task 10]  [ 0/42]  eta: 0:00:22  Loss: 1.1000 (1.1000)  Acc@1: 58.3333 (58.3333)  Acc@5: 100.0000 (100.0000)  time: 0.5323  data: 0.4636  max mem: 359\n",
            "Test: [Task 10]  [10/42]  eta: 0:00:04  Loss: 1.1725 (1.2095)  Acc@1: 62.5000 (65.1515)  Acc@5: 95.8333 (94.6970)  time: 0.1293  data: 0.0427  max mem: 359\n",
            "Test: [Task 10]  [20/42]  eta: 0:00:02  Loss: 1.2004 (1.1855)  Acc@1: 62.5000 (63.8889)  Acc@5: 95.8333 (93.8492)  time: 0.0878  data: 0.0005  max mem: 359\n",
            "Test: [Task 10]  [30/42]  eta: 0:00:01  Loss: 1.2045 (1.1900)  Acc@1: 62.5000 (63.8441)  Acc@5: 91.6667 (93.6828)  time: 0.0866  data: 0.0003  max mem: 359\n",
            "Test: [Task 10]  [40/42]  eta: 0:00:00  Loss: 1.1979 (1.1935)  Acc@1: 66.6667 (63.5163)  Acc@5: 95.8333 (93.9024)  time: 0.0875  data: 0.0003  max mem: 359\n",
            "Test: [Task 10]  [41/42]  eta: 0:00:00  Loss: 1.1979 (1.1858)  Acc@1: 66.6667 (63.6000)  Acc@5: 95.8333 (93.9000)  time: 0.0854  data: 0.0003  max mem: 359\n",
            "Test: [Task 10] Total time: 0:00:04 (0.0997 s / it)\n",
            "* Acc@1 63.600 Acc@5 93.900 loss 1.186\n",
            "[Average accuracy till task10]\tAcc@1: 73.8800\tAcc@5: 94.6200\tLoss: 0.9025\tForgetting: 4.2333\tBackward: -1.8778\n",
            "Total training time: 0:20:05\n",
            "[rank0]:[W929 16:54:30.941581906 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
          ]
        }
      ],
      "source": [
        "!torchrun --nproc_per_node=1 main.py cifar100_hideprompt_5e --original_model vit_small_patch16_224.dino --model vit_small_patch16_224.dino --batch-size 24 --data-path ./datasets/ --output_dir ./output/cifar100_full_dino_1epoch --epochs 1 --sched constant --seed 20 --train_inference_task_only --lr 0.0005\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "902dd0fc",
      "metadata": {
        "id": "902dd0fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f0d66eb-24f3-4c55-a979-9c9a53a314f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(subparser_name='cifar100_hideprompt_5e', batch_size=24, epochs=1, original_model='vit_small_patch16_224.dino', model='vit_small_patch16_224.dino', input_size=224, pretrained=True, drop=0.0, drop_path=0.0, opt='adam', opt_eps=1e-08, opt_betas=(0.9, 0.999), clip_grad=1.0, momentum=0.9, weight_decay=0.0, reinit_optimizer=True, sched='step', lr=0.03, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=0, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, unscale_lr=True, color_jitter=None, aa=None, smoothing=0.1, train_interpolation='bicubic', reprob=0.0, remode='pixel', recount=1, data_path='./datasets/', dataset='Split-CIFAR100', shuffle=False, output_dir='./output/cifar100_full_dino_epoch_final', device='cuda', seed=20, eval=False, num_workers=4, pin_mem=True, world_size=1, dist_url='env://', num_tasks=10, train_mask=True, task_inc=False, use_g_prompt=False, g_prompt_length=5, g_prompt_layer_idx=[], use_prefix_tune_for_g_prompt=False, use_e_prompt=True, e_prompt_layer_idx=[0, 1, 2, 3, 4], use_prefix_tune_for_e_prompt=True, larger_prompt_lr=True, prompt_pool=True, size=10, length=5, top_k=1, initializer='uniform', prompt_key=False, prompt_key_init='uniform', use_prompt_mask=True, mask_first_epoch=False, shared_prompt_pool=True, shared_prompt_key=False, batchwise_prompt=False, embedding_key='cls', predefined_key='', pull_constraint=True, pull_constraint_coeff=1.0, same_key_value=False, global_pool='token', head_type='token', freeze=['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed'], crct_epochs=1, train_inference_task_only=False, original_model_mlp_structure=[2], ca_lr=0.005, milestones=[10], trained_original_model='./output/cifar100_full_dino_1epoch', prompt_momentum=0.1, reg=0.1, not_train_ca=False, ca_epochs=30, ca_storage_efficient_method='multi-centroid', n_centroids=10, print_freq=10, config='cifar100_hideprompt_5e')\n",
            "| distributed init (rank 0): env://\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "[rank0]:[W929 19:07:13.033086146 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/helpers.py:7: FutureWarning: Importing from timm.models.helpers is deprecated, please import via timm.models\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
            "2025-09-29 19:07:17.041854: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1759172837.061716    5483 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1759172837.067661    5483 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1759172837.083049    5483 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1759172837.083082    5483 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1759172837.083086    5483 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1759172837.083090    5483 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-09-29 19:07:17.087757: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[rank0]: Traceback (most recent call last):\n",
            "[rank0]:   File \"/content/drive/MyDrive/HiDePrompt/HiDePrompt/main.py\", line 132, in <module>\n",
            "[rank0]:     main(args)\n",
            "[rank0]:   File \"/content/drive/MyDrive/HiDePrompt/HiDePrompt/main.py\", line 114, in main\n",
            "[rank0]:     import trainers.hideprompt_trainer as hideprompt_trainer\n",
            "[rank0]:   File \"/content/drive/MyDrive/HiDePrompt/HiDePrompt/trainers/hideprompt_trainer.py\", line 8, in <module>\n",
            "[rank0]:     import vits.hide_prompt_vision_transformer as hide_prompt_vision_transformer\n",
            "[rank0]:   File \"/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py\", line 38, in <module>\n",
            "[rank0]:     from peft.prompt.hide_prompt import EPrompt\n",
            "[rank0]: ModuleNotFoundError: No module named 'peft.prompt'\n",
            "[rank0]:[W929 19:07:22.456143058 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
            "E0929 19:07:24.280000 5476 torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 5483) of binary: /usr/bin/python3\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/torchrun\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 357, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py\", line 901, in main\n",
            "    run(args)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py\", line 892, in run\n",
            "    elastic_launch(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py\", line 143, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py\", line 277, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "============================================================\n",
            "main.py FAILED\n",
            "------------------------------------------------------------\n",
            "Failures:\n",
            "  <NO_OTHER_FAILURES>\n",
            "------------------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2025-09-29_19:07:24\n",
            "  host      : 3f904f9c7ef5\n",
            "  rank      : 0 (local_rank: 0)\n",
            "  exitcode  : 1 (pid: 5483)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "!torchrun --nproc_per_node=1 main.py cifar100_hideprompt_5e --original_model vit_small_patch16_224.dino --model vit_small_patch16_224.dino --batch-size 24 --epochs 1 --seed 20 --ca_lr 0.005 --crct_epochs 1 --prompt_momentum 0.1 --reg 0.1 --length 5 --larger_prompt_lr --data-path ./datasets/ --trained_original_model ./output/cifar100_full_dino_1epoch --output_dir ./output/cifar100_full_dino_epoch_final\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}