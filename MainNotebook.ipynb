{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1c65321c",
      "metadata": {
        "id": "1c65321c",
        "outputId": "94a7b7fc-b9f5-458e-d920-34d02996df7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32512"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import os\n",
        "os.system('cmd command')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4842b918",
      "metadata": {
        "id": "4842b918",
        "outputId": "b977b391-af4f-469f-c9db-5ed3911f9329",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "31ecb10e",
      "metadata": {
        "id": "31ecb10e",
        "outputId": "004e0bf6-1cff-45e0-c283-af304cc53cf3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/HiDePrompt\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/HiDePrompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd565e2a",
      "metadata": {
        "id": "cd565e2a"
      },
      "outputs": [],
      "source": [
        "!pip install -q condacolab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a2473b5",
      "metadata": {
        "id": "6a2473b5",
        "outputId": "e435931a-a6cb-4f7a-f09c-fd79432ddd86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬ Downloading https://github.com/jaimergp/miniforge/releases/download/24.11.2-1_colab/Miniforge3-colab-24.11.2-1_colab-Linux-x86_64.sh...\n",
            "ğŸ“¦ Installing...\n",
            "ğŸ“Œ Adjusting configuration...\n",
            "ğŸ©¹ Patching environment...\n",
            "â² Done in 0:00:11\n",
            "ğŸ” Restarting kernel...\n"
          ]
        }
      ],
      "source": [
        "import condacolab\n",
        "condacolab.install()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/HiDePrompt/HiDePrompt"
      ],
      "metadata": {
        "id": "4aBTrmkLHGUx",
        "outputId": "444c2c2e-9e6b-4d89-dabc-91daa1677078",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "4aBTrmkLHGUx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59ddd81a",
      "metadata": {
        "id": "59ddd81a",
        "outputId": "2d958915-469d-49c8-90be-f3d2d67d4cfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm (from -r requirements.txt (line 1))\n",
            "  Downloading timm-1.0.20-py3-none-any.whl.metadata (61 kB)\n",
            "Collecting pillow (from -r requirements.txt (line 2))\n",
            "  Downloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting matplotlib (from -r requirements.txt (line 3))\n",
            "  Downloading matplotlib-3.10.6-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
            "Collecting torchprofile (from -r requirements.txt (line 4))\n",
            "  Downloading torchprofile-0.0.4-py3-none-any.whl.metadata (303 bytes)\n",
            "Collecting torch (from -r requirements.txt (line 5))\n",
            "  Downloading torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
            "Collecting torchvision (from -r requirements.txt (line 6))\n",
            "  Downloading torchvision-0.23.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (2.3.0)\n",
            "Collecting scipy (from -r requirements.txt (line 8))\n",
            "  Downloading scipy-1.16.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
            "Collecting scikit-learn (from -r requirements.txt (line 9))\n",
            "  Downloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
            "Collecting numpy (from -r requirements.txt (line 10))\n",
            "  Downloading numpy-2.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "Collecting pyyaml (from timm->-r requirements.txt (line 1))\n",
            "  Downloading pyyaml-6.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)\n",
            "Collecting huggingface_hub (from timm->-r requirements.txt (line 1))\n",
            "  Downloading huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting safetensors (from timm->-r requirements.txt (line 1))\n",
            "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib->-r requirements.txt (line 3))\n",
            "  Downloading contourpy-1.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 3))\n",
            "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib->-r requirements.txt (line 3))\n",
            "  Downloading fonttools-4.60.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (111 kB)\n",
            "Collecting kiwisolver>=1.3.1 (from matplotlib->-r requirements.txt (line 3))\n",
            "  Downloading kiwisolver-1.4.9-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 3)) (24.2)\n",
            "Collecting pyparsing>=2.3.1 (from matplotlib->-r requirements.txt (line 3))\n",
            "  Downloading pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting python-dateutil>=2.7 (from matplotlib->-r requirements.txt (line 3))\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting filelock (from torch->-r requirements.txt (line 5))\n",
            "  Downloading filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting typing-extensions>=4.10.0 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting sympy>=1.13.3 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting networkx (from torch->-r requirements.txt (line 5))\n",
            "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting jinja2 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting fsspec (from torch->-r requirements.txt (line 5))\n",
            "  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.9.90 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Collecting nvidia-nccl-cu12==2.27.3 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.8.90 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.4.0 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/site-packages (from triton==3.4.0->torch->-r requirements.txt (line 5)) (65.6.3)\n",
            "Collecting joblib>=1.2.0 (from scikit-learn->-r requirements.txt (line 9))\n",
            "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn->-r requirements.txt (line 9))\n",
            "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting six>=1.5 (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 3))\n",
            "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch->-r requirements.txt (line 5))\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/site-packages (from huggingface_hub->timm->-r requirements.txt (line 1)) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/site-packages (from huggingface_hub->timm->-r requirements.txt (line 1)) (4.67.1)\n",
            "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface_hub->timm->-r requirements.txt (line 1))\n",
            "  Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch->-r requirements.txt (line 5))\n",
            "  Downloading markupsafe-3.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 1)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 1)) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 1)) (2024.12.14)\n",
            "Downloading timm-1.0.20-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m141.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.10.6-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m179.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchprofile-0.0.4-py3-none-any.whl (7.7 kB)\n",
            "Downloading torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl (888.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m888.1/888.1 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m116.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "Downloading triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m155.5/155.5 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.23.0-cp311-cp311-manylinux_2_28_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m101.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.16.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m35.9/35.9 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m107.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m112.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading contourpy-1.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (355 kB)\n",
            "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading fonttools-4.60.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (5.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m96.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
            "Downloading kiwisolver-1.4.9-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
            "Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
            "Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\n",
            "Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
            "Downloading huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyyaml-6.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (806 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m806.6/806.6 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
            "Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markupsafe-3.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\n",
            "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: nvidia-cusparselt-cu12, mpmath, typing-extensions, triton, threadpoolctl, sympy, six, safetensors, pyyaml, pyparsing, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, kiwisolver, joblib, hf-xet, fsspec, fonttools, filelock, cycler, scipy, python-dateutil, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, jinja2, huggingface_hub, contourpy, scikit-learn, nvidia-cusolver-cu12, matplotlib, torch, torchvision, torchprofile, timm\n",
            "Successfully installed MarkupSafe-3.0.3 contourpy-1.3.3 cycler-0.12.1 filelock-3.19.1 fonttools-4.60.0 fsspec-2025.9.0 hf-xet-1.1.10 huggingface_hub-0.35.3 jinja2-3.1.6 joblib-1.5.2 kiwisolver-1.4.9 matplotlib-3.10.6 mpmath-1.3.0 networkx-3.5 numpy-2.3.3 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 pillow-11.3.0 pyparsing-3.2.5 python-dateutil-2.9.0.post0 pyyaml-6.0.3 safetensors-0.6.2 scikit-learn-1.7.2 scipy-1.16.2 six-1.17.0 sympy-1.14.0 threadpoolctl-3.6.0 timm-1.0.20 torch-2.8.0 torchprofile-0.0.4 torchvision-0.23.0 triton-3.4.0 typing-extensions-4.15.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "cycler",
                  "dateutil",
                  "kiwisolver",
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy",
                  "pyparsing",
                  "six"
                ]
              },
              "id": "5e9c917c5f81486e8bc6a6c737f1745b"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/HiDePrompt/HiDePrompt/"
      ],
      "metadata": {
        "id": "nTIVenS1Ih7Q",
        "outputId": "7a8a3d9a-5fdc-4cc1-e864-d35fc92ee4ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "nTIVenS1Ih7Q",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "383f6206",
      "metadata": {
        "id": "383f6206",
        "outputId": "887b9b8c-d03c-4914-e0cd-8eeb8ae0f9ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(subparser_name='cifar100_hideprompt_5e', pct=0.1, batch_size=24, epochs=10, original_model='vit_small_patch16_224.dino', model='vit_small_patch16_224.dino', input_size=224, pretrained=True, drop=0.0, drop_path=0.0, opt='adam', opt_eps=1e-08, opt_betas=(0.9, 0.999), clip_grad=1.0, momentum=0.9, weight_decay=0.0, reinit_optimizer=True, sched='constant', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=0, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, unscale_lr=True, color_jitter=None, aa=None, smoothing=0.1, train_interpolation='bicubic', reprob=0.0, remode='pixel', recount=1, data_path='./datasets/', dataset='Split-CIFAR100', shuffle=False, output_dir='./output/cifar100_full_dino_5epoch_10pct', device='cuda', seed=20, eval=False, num_workers=4, pin_mem=True, world_size=1, dist_url='env://', num_tasks=10, train_mask=True, task_inc=False, use_g_prompt=False, g_prompt_length=5, g_prompt_layer_idx=[], use_prefix_tune_for_g_prompt=False, use_e_prompt=True, e_prompt_layer_idx=[0, 1, 2, 3, 4], use_prefix_tune_for_e_prompt=True, larger_prompt_lr=False, prompt_pool=True, size=10, length=20, top_k=1, initializer='uniform', prompt_key=False, prompt_key_init='uniform', use_prompt_mask=True, mask_first_epoch=False, shared_prompt_pool=True, shared_prompt_key=False, batchwise_prompt=False, embedding_key='cls', predefined_key='', pull_constraint=True, pull_constraint_coeff=1.0, same_key_value=False, global_pool='token', head_type='token', freeze=['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed'], crct_epochs=10, train_inference_task_only=True, original_model_mlp_structure=[2], ca_lr=0.005, milestones=[10], trained_original_model='', prompt_momentum=0.01, reg=0.01, not_train_ca=False, ca_epochs=30, ca_storage_efficient_method='multi-centroid', n_centroids=10, print_freq=10, config='cifar100_hideprompt_5e')\n",
            "| distributed init (rank 0): env://\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "[rank0]:[W1008 14:06:38.521404037 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/helpers.py:7: FutureWarning: Importing from timm.models.helpers is deprecated, please import via timm.models\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:886: UserWarning: Overwriting vit_tiny_patch16_224 in registry with vits.hide_prompt_vision_transformer.vit_tiny_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:895: UserWarning: Overwriting vit_tiny_patch16_384 in registry with vits.hide_prompt_vision_transformer.vit_tiny_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:904: UserWarning: Overwriting vit_small_patch32_224 in registry with vits.hide_prompt_vision_transformer.vit_small_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:913: UserWarning: Overwriting vit_small_patch32_384 in registry with vits.hide_prompt_vision_transformer.vit_small_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:922: UserWarning: Overwriting vit_small_patch16_224 in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:932: UserWarning: Overwriting vit_small_patch16_384 in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:942: UserWarning: Overwriting vit_base_patch32_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:952: UserWarning: Overwriting vit_base_patch32_384 in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:962: UserWarning: Overwriting vit_base_patch16_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:972: UserWarning: Overwriting vit_base_patch16_384 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:982: UserWarning: Overwriting vit_base_patch8_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:992: UserWarning: Overwriting vit_large_patch32_224 in registry with vits.hide_prompt_vision_transformer.vit_large_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1001: UserWarning: Overwriting vit_large_patch32_384 in registry with vits.hide_prompt_vision_transformer.vit_large_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1011: UserWarning: Overwriting vit_large_patch16_224 in registry with vits.hide_prompt_vision_transformer.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1021: UserWarning: Overwriting vit_large_patch16_384 in registry with vits.hide_prompt_vision_transformer.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1031: UserWarning: Overwriting vit_large_patch14_224 in registry with vits.hide_prompt_vision_transformer.vit_large_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1040: UserWarning: Overwriting vit_huge_patch14_224 in registry with vits.hide_prompt_vision_transformer.vit_huge_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1049: UserWarning: Overwriting vit_giant_patch14_224 in registry with vits.hide_prompt_vision_transformer.vit_giant_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1058: UserWarning: Overwriting vit_gigantic_patch14_224 in registry with vits.hide_prompt_vision_transformer.vit_gigantic_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1067: UserWarning: Overwriting vit_tiny_patch16_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_tiny_patch16_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1078: UserWarning: Overwriting vit_small_patch32_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_small_patch32_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1089: UserWarning: Overwriting vit_small_patch16_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1100: UserWarning: Overwriting vit_base_patch32_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1111: UserWarning: Overwriting vit_base_patch16_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1122: UserWarning: Overwriting vit_base_patch8_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_base_patch8_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1133: UserWarning: Overwriting vit_large_patch32_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_large_patch32_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1144: UserWarning: Overwriting vit_large_patch16_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_large_patch16_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1155: UserWarning: Overwriting vit_huge_patch14_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_huge_patch14_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1166: UserWarning: Overwriting vit_base_patch16_224_sam in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_sam. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1175: UserWarning: Overwriting vit_base_patch32_224_sam in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_224_sam. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1184: UserWarning: Overwriting vit_small_patch16_224_dino in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_224_dino. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1193: UserWarning: Overwriting vit_small_patch8_224_dino in registry with vits.hide_prompt_vision_transformer.vit_small_patch8_224_dino. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1211: UserWarning: Overwriting vit_base_patch8_224_dino in registry with vits.hide_prompt_vision_transformer.vit_base_patch8_224_dino. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1220: UserWarning: Overwriting vit_base_patch16_224_miil_in21k in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_miil_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1230: UserWarning: Overwriting vit_base_patch16_224_miil in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_miil. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1242: UserWarning: Overwriting vit_base_patch32_plus_256 in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_plus_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1251: UserWarning: Overwriting vit_base_patch16_plus_240 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_plus_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1260: UserWarning: Overwriting vit_base_patch16_rpn_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_rpn_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1271: UserWarning: Overwriting vit_small_patch16_36x1_224 in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_36x1_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1282: UserWarning: Overwriting vit_small_patch16_18x2_224 in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1294: UserWarning: Overwriting vit_base_patch16_18x2_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1331: UserWarning: Overwriting vit_base_patch16_224_dino in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_dino. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "Original train size:  50000\n",
            "Sampled train size:  5000\n",
            "Original train size:  50000\n",
            "Sampled train size:  5000\n",
            "100\n",
            "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, 13, 14, 15, 16, 17, 18, 19], [20, 21, 22, 23, 24, 25, 26, 27, 28, 29], [30, 31, 32, 33, 34, 35, 36, 37, 38, 39], [40, 41, 42, 43, 44, 45, 46, 47, 48, 49], [50, 51, 52, 53, 54, 55, 56, 57, 58, 59], [60, 61, 62, 63, 64, 65, 66, 67, 68, 69], [70, 71, 72, 73, 74, 75, 76, 77, 78, 79], [80, 81, 82, 83, 84, 85, 86, 87, 88, 89], [90, 91, 92, 93, 94, 95, 96, 97, 98, 99]]\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Creating original model: vit_small_patch16_224.dino\n",
            "[Sequential(\n",
            "  (0): Linear(in_features=384, out_features=768, bias=True)\n",
            "  (1): GELU(approximate='none')\n",
            "  (2): Dropout(p=0.0, inplace=False)\n",
            "), Sequential(\n",
            "  (0): Linear(in_features=768, out_features=384, bias=True)\n",
            "  (1): Dropout(p=0.0, inplace=False)\n",
            ")]\n",
            "model.safetensors: 100% 86.7M/86.7M [00:01<00:00, 51.9MB/s]\n",
            "Namespace(subparser_name='cifar100_hideprompt_5e', pct=0.1, batch_size=24, epochs=10, original_model='vit_small_patch16_224.dino', model='vit_small_patch16_224.dino', input_size=224, pretrained=True, drop=0.0, drop_path=0.0, opt='adam', opt_eps=1e-08, opt_betas=(0.9, 0.999), clip_grad=1.0, momentum=0.9, weight_decay=0.0, reinit_optimizer=True, sched='constant', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=0, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, unscale_lr=True, color_jitter=None, aa=None, smoothing=0.1, train_interpolation='bicubic', reprob=0.0, remode='pixel', recount=1, data_path='./datasets/', dataset='Split-CIFAR100', shuffle=False, output_dir='./output/cifar100_full_dino_5epoch_10pct', device='cuda', seed=20, eval=False, num_workers=4, pin_mem=True, world_size=1, dist_url='env://', num_tasks=10, train_mask=True, task_inc=False, use_g_prompt=False, g_prompt_length=5, g_prompt_layer_idx=[], use_prefix_tune_for_g_prompt=False, use_e_prompt=True, e_prompt_layer_idx=[0, 1, 2, 3, 4], use_prefix_tune_for_e_prompt=True, larger_prompt_lr=False, prompt_pool=True, size=10, length=20, top_k=1, initializer='uniform', prompt_key=False, prompt_key_init='uniform', use_prompt_mask=True, mask_first_epoch=False, shared_prompt_pool=True, shared_prompt_key=False, batchwise_prompt=False, embedding_key='cls', predefined_key='', pull_constraint=True, pull_constraint_coeff=1.0, same_key_value=False, global_pool='token', head_type='token', freeze=['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed'], crct_epochs=10, train_inference_task_only=True, original_model_mlp_structure=[2], ca_lr=0.005, milestones=[10], trained_original_model='', prompt_momentum=0.01, reg=0.01, not_train_ca=False, ca_epochs=30, ca_storage_efficient_method='multi-centroid', n_centroids=10, print_freq=10, config='cifar100_hideprompt_5e', rank=0, gpu=0, distributed=True, dist_backend='nccl', nb_classes=100)\n",
            "number of params: 630244\n",
            "Start training for 10 epochs\n",
            "Train: Epoch[ 1/10]  [ 0/23]  eta: 0:01:09  Lr: 0.000047  Loss: 2.4446  Acc@1: 4.1667 (4.1667)  Acc@5: 45.8333 (45.8333)  time: 3.0238  data: 1.2893  max mem: 196\n",
            "Train: Epoch[ 1/10]  [10/23]  eta: 0:00:04  Lr: 0.000047  Loss: 2.2694  Acc@1: 12.5000 (9.4697)  Acc@5: 54.1667 (53.7879)  time: 0.3482  data: 0.1182  max mem: 217\n",
            "Train: Epoch[ 1/10]  [20/23]  eta: 0:00:00  Lr: 0.000047  Loss: 2.1486  Acc@1: 16.6667 (14.2857)  Acc@5: 62.5000 (60.7143)  time: 0.0808  data: 0.0008  max mem: 217\n",
            "Train: Epoch[ 1/10]  [22/23]  eta: 0:00:00  Lr: 0.000047  Loss: 2.2717  Acc@1: 16.6667 (14.5522)  Acc@5: 62.5000 (61.0075)  time: 0.0816  data: 0.0007  max mem: 217\n",
            "Train: Epoch[ 1/10] Total time: 0:00:04 (0.2116 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000047  Loss: 2.2717  Acc@1: 16.6667 (14.5522)  Acc@5: 62.5000 (61.0075)\n",
            "Train: Epoch[ 2/10]  [ 0/23]  eta: 0:00:08  Lr: 0.000047  Loss: 2.2233  Acc@1: 12.5000 (12.5000)  Acc@5: 62.5000 (62.5000)  time: 0.3491  data: 0.2695  max mem: 217\n",
            "Train: Epoch[ 2/10]  [10/23]  eta: 0:00:01  Lr: 0.000047  Loss: 2.1777  Acc@1: 29.1667 (26.5152)  Acc@5: 75.0000 (75.0000)  time: 0.1190  data: 0.0323  max mem: 218\n",
            "Train: Epoch[ 2/10]  [20/23]  eta: 0:00:00  Lr: 0.000047  Loss: 2.1104  Acc@1: 29.1667 (28.7698)  Acc@5: 79.1667 (77.7778)  time: 0.0912  data: 0.0045  max mem: 218\n",
            "Train: Epoch[ 2/10]  [22/23]  eta: 0:00:00  Lr: 0.000047  Loss: 2.1508  Acc@1: 29.1667 (28.5448)  Acc@5: 79.1667 (78.1716)  time: 0.0846  data: 0.0007  max mem: 218\n",
            "Train: Epoch[ 2/10] Total time: 0:00:02 (0.1050 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 2.1508  Acc@1: 29.1667 (28.5448)  Acc@5: 79.1667 (78.1716)\n",
            "Train: Epoch[ 3/10]  [ 0/23]  eta: 0:00:13  Lr: 0.000047  Loss: 2.0407  Acc@1: 37.5000 (37.5000)  Acc@5: 83.3333 (83.3333)  time: 0.6002  data: 0.5019  max mem: 218\n",
            "Train: Epoch[ 3/10]  [10/23]  eta: 0:00:01  Lr: 0.000047  Loss: 1.9859  Acc@1: 41.6667 (42.8030)  Acc@5: 83.3333 (85.6061)  time: 0.1479  data: 0.0556  max mem: 218\n",
            "Train: Epoch[ 3/10]  [20/23]  eta: 0:00:00  Lr: 0.000047  Loss: 1.9601  Acc@1: 45.8333 (46.4286)  Acc@5: 87.5000 (86.9048)  time: 0.1000  data: 0.0079  max mem: 218\n",
            "Train: Epoch[ 3/10]  [22/23]  eta: 0:00:00  Lr: 0.000047  Loss: 1.6656  Acc@1: 50.0000 (47.5746)  Acc@5: 91.6667 (87.5000)  time: 0.0961  data: 0.0077  max mem: 218\n",
            "Train: Epoch[ 3/10] Total time: 0:00:02 (0.1212 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.6656  Acc@1: 50.0000 (47.5746)  Acc@5: 91.6667 (87.5000)\n",
            "Train: Epoch[ 4/10]  [ 0/23]  eta: 0:00:09  Lr: 0.000047  Loss: 1.8733  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  time: 0.4282  data: 0.2861  max mem: 218\n",
            "Train: Epoch[ 4/10]  [10/23]  eta: 0:00:01  Lr: 0.000047  Loss: 1.9630  Acc@1: 54.1667 (54.9242)  Acc@5: 91.6667 (93.1818)  time: 0.1291  data: 0.0309  max mem: 218\n",
            "Train: Epoch[ 4/10]  [20/23]  eta: 0:00:00  Lr: 0.000047  Loss: 1.7245  Acc@1: 58.3333 (57.5397)  Acc@5: 91.6667 (90.2778)  time: 0.0897  data: 0.0033  max mem: 218\n",
            "Train: Epoch[ 4/10]  [22/23]  eta: 0:00:00  Lr: 0.000047  Loss: 1.7460  Acc@1: 58.3333 (57.6493)  Acc@5: 87.5000 (89.5522)  time: 0.0842  data: 0.0010  max mem: 218\n",
            "Train: Epoch[ 4/10] Total time: 0:00:02 (0.1049 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.7460  Acc@1: 58.3333 (57.6493)  Acc@5: 87.5000 (89.5522)\n",
            "Train: Epoch[ 5/10]  [ 0/23]  eta: 0:00:07  Lr: 0.000047  Loss: 1.7427  Acc@1: 58.3333 (58.3333)  Acc@5: 91.6667 (91.6667)  time: 0.3320  data: 0.2426  max mem: 218\n",
            "Train: Epoch[ 5/10]  [10/23]  eta: 0:00:01  Lr: 0.000047  Loss: 1.6558  Acc@1: 62.5000 (62.5000)  Acc@5: 91.6667 (92.8030)  time: 0.1222  data: 0.0370  max mem: 218\n",
            "Train: Epoch[ 5/10]  [20/23]  eta: 0:00:00  Lr: 0.000047  Loss: 1.3737  Acc@1: 66.6667 (65.6746)  Acc@5: 95.8333 (94.0476)  time: 0.0916  data: 0.0083  max mem: 218\n",
            "Train: Epoch[ 5/10]  [22/23]  eta: 0:00:00  Lr: 0.000047  Loss: 1.4343  Acc@1: 66.6667 (66.6045)  Acc@5: 95.8333 (94.0299)  time: 0.0813  data: 0.0006  max mem: 218\n",
            "Train: Epoch[ 5/10] Total time: 0:00:02 (0.1020 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.4343  Acc@1: 66.6667 (66.6045)  Acc@5: 95.8333 (94.0299)\n",
            "Train: Epoch[ 6/10]  [ 0/23]  eta: 0:00:08  Lr: 0.000047  Loss: 1.6509  Acc@1: 58.3333 (58.3333)  Acc@5: 100.0000 (100.0000)  time: 0.3609  data: 0.2645  max mem: 218\n",
            "Train: Epoch[ 6/10]  [10/23]  eta: 0:00:01  Lr: 0.000047  Loss: 1.3734  Acc@1: 70.8333 (70.0758)  Acc@5: 95.8333 (95.8333)  time: 0.1170  data: 0.0286  max mem: 218\n",
            "Train: Epoch[ 6/10]  [20/23]  eta: 0:00:00  Lr: 0.000047  Loss: 1.3177  Acc@1: 70.8333 (71.4286)  Acc@5: 95.8333 (95.2381)  time: 0.0869  data: 0.0027  max mem: 218\n",
            "Train: Epoch[ 6/10]  [22/23]  eta: 0:00:00  Lr: 0.000047  Loss: 1.4211  Acc@1: 70.8333 (71.2687)  Acc@5: 95.8333 (95.1493)  time: 0.0818  data: 0.0003  max mem: 218\n",
            "Train: Epoch[ 6/10] Total time: 0:00:02 (0.0993 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.4211  Acc@1: 70.8333 (71.2687)  Acc@5: 95.8333 (95.1493)\n",
            "Train: Epoch[ 7/10]  [ 0/23]  eta: 0:00:10  Lr: 0.000047  Loss: 1.2684  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.4544  data: 0.3643  max mem: 218\n",
            "Train: Epoch[ 7/10]  [10/23]  eta: 0:00:01  Lr: 0.000047  Loss: 1.1345  Acc@1: 75.0000 (76.1364)  Acc@5: 95.8333 (96.2121)  time: 0.1196  data: 0.0339  max mem: 218\n",
            "Train: Epoch[ 7/10]  [20/23]  eta: 0:00:00  Lr: 0.000047  Loss: 1.0185  Acc@1: 75.0000 (78.1746)  Acc@5: 95.8333 (96.8254)  time: 0.0836  data: 0.0005  max mem: 218\n",
            "Train: Epoch[ 7/10]  [22/23]  eta: 0:00:00  Lr: 0.000047  Loss: 1.0452  Acc@1: 75.0000 (77.6119)  Acc@5: 95.8333 (96.4552)  time: 0.0806  data: 0.0005  max mem: 218\n",
            "Train: Epoch[ 7/10] Total time: 0:00:02 (0.1024 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.0452  Acc@1: 75.0000 (77.6119)  Acc@5: 95.8333 (96.4552)\n",
            "Train: Epoch[ 8/10]  [ 0/23]  eta: 0:00:18  Lr: 0.000047  Loss: 1.3136  Acc@1: 62.5000 (62.5000)  Acc@5: 91.6667 (91.6667)  time: 0.7847  data: 0.6088  max mem: 218\n",
            "Train: Epoch[ 8/10]  [10/23]  eta: 0:00:02  Lr: 0.000047  Loss: 0.8666  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (96.2121)  time: 0.1775  data: 0.0606  max mem: 218\n",
            "Train: Epoch[ 8/10]  [20/23]  eta: 0:00:00  Lr: 0.000047  Loss: 0.9131  Acc@1: 79.1667 (76.9841)  Acc@5: 95.8333 (96.4286)  time: 0.1113  data: 0.0094  max mem: 218\n",
            "Train: Epoch[ 8/10]  [22/23]  eta: 0:00:00  Lr: 0.000047  Loss: 1.0799  Acc@1: 79.1667 (77.6119)  Acc@5: 95.8333 (96.4552)  time: 0.1066  data: 0.0094  max mem: 218\n",
            "Train: Epoch[ 8/10] Total time: 0:00:03 (0.1420 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.0799  Acc@1: 79.1667 (77.6119)  Acc@5: 95.8333 (96.4552)\n",
            "Train: Epoch[ 9/10]  [ 0/23]  eta: 0:00:20  Lr: 0.000047  Loss: 1.0929  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  time: 0.8704  data: 0.7360  max mem: 218\n",
            "Train: Epoch[ 9/10]  [10/23]  eta: 0:00:02  Lr: 0.000047  Loss: 0.6721  Acc@1: 79.1667 (78.0303)  Acc@5: 100.0000 (98.1061)  time: 0.1817  data: 0.0681  max mem: 218\n",
            "Train: Epoch[ 9/10]  [20/23]  eta: 0:00:00  Lr: 0.000047  Loss: 0.7630  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (98.4127)  time: 0.1078  data: 0.0020  max mem: 218\n",
            "Train: Epoch[ 9/10]  [22/23]  eta: 0:00:00  Lr: 0.000047  Loss: 1.2575  Acc@1: 79.1667 (78.9179)  Acc@5: 100.0000 (98.5075)  time: 0.1029  data: 0.0018  max mem: 218\n",
            "Train: Epoch[ 9/10] Total time: 0:00:03 (0.1414 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.2575  Acc@1: 79.1667 (78.9179)  Acc@5: 100.0000 (98.5075)\n",
            "Train: Epoch[10/10]  [ 0/23]  eta: 0:00:14  Lr: 0.000047  Loss: 0.9354  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  time: 0.6254  data: 0.5042  max mem: 218\n",
            "Train: Epoch[10/10]  [10/23]  eta: 0:00:01  Lr: 0.000047  Loss: 0.8938  Acc@1: 79.1667 (80.6818)  Acc@5: 100.0000 (98.4848)  time: 0.1375  data: 0.0473  max mem: 218\n",
            "Train: Epoch[10/10]  [20/23]  eta: 0:00:00  Lr: 0.000047  Loss: 0.8502  Acc@1: 83.3333 (82.5397)  Acc@5: 100.0000 (98.4127)  time: 0.0860  data: 0.0012  max mem: 218\n",
            "Train: Epoch[10/10]  [22/23]  eta: 0:00:00  Lr: 0.000047  Loss: 0.6239  Acc@1: 83.3333 (81.7164)  Acc@5: 100.0000 (98.5075)  time: 0.0816  data: 0.0009  max mem: 218\n",
            "Train: Epoch[10/10] Total time: 0:00:02 (0.1105 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.6239  Acc@1: 83.3333 (81.7164)  Acc@5: 100.0000 (98.5075)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:12  Loss: 1.9617 (1.9617)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  time: 0.3067  data: 0.2209  max mem: 218\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 2.0206 (2.0027)  Acc@1: 87.5000 (89.3939)  Acc@5: 100.0000 (98.4848)  time: 0.1075  data: 0.0280  max mem: 218\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 2.0206 (2.0216)  Acc@1: 87.5000 (88.8889)  Acc@5: 100.0000 (98.2143)  time: 0.0854  data: 0.0045  max mem: 218\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 1.9738 (1.9915)  Acc@1: 87.5000 (90.0538)  Acc@5: 95.8333 (97.8495)  time: 0.0839  data: 0.0005  max mem: 218\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 1.9190 (1.9833)  Acc@1: 91.6667 (90.1423)  Acc@5: 100.0000 (98.0691)  time: 0.0845  data: 0.0004  max mem: 218\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 1.9004 (1.9801)  Acc@1: 91.6667 (90.1000)  Acc@5: 100.0000 (98.1000)  time: 0.0878  data: 0.0004  max mem: 218\n",
            "Test: [Task 1] Total time: 0:00:03 (0.0935 s / it)\n",
            "* Acc@1 90.100 Acc@5 98.100 loss 1.980\n",
            "[Average accuracy till task1]\tAcc@1: 90.1000\tAcc@5: 98.1000\tLoss: 1.9801\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:16  Loss: 1.9617 (1.9617)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  time: 0.3874  data: 0.3001  max mem: 218\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 2.0206 (2.0027)  Acc@1: 87.5000 (89.3939)  Acc@5: 100.0000 (98.4848)  time: 0.1131  data: 0.0289  max mem: 218\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 2.0206 (2.0216)  Acc@1: 87.5000 (88.8889)  Acc@5: 100.0000 (98.2143)  time: 0.0851  data: 0.0011  max mem: 218\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 1.9738 (1.9915)  Acc@1: 87.5000 (90.0538)  Acc@5: 95.8333 (97.8495)  time: 0.0853  data: 0.0004  max mem: 218\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 1.9190 (1.9833)  Acc@1: 91.6667 (90.1423)  Acc@5: 100.0000 (98.0691)  time: 0.0865  data: 0.0004  max mem: 218\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 1.9004 (1.9801)  Acc@1: 91.6667 (90.1000)  Acc@5: 100.0000 (98.1000)  time: 0.0846  data: 0.0003  max mem: 218\n",
            "Test: [Task 1] Total time: 0:00:03 (0.0944 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@1 90.100 Acc@5 98.100 loss 1.980\n",
            "[Average accuracy till task1]\tAcc@1: 90.1000\tAcc@5: 98.1000\tLoss: 1.9801\n",
            "Train: Epoch[ 1/10]  [ 0/20]  eta: 0:00:08  Lr: 0.000047  Loss: 2.4646  Acc@1: 8.3333 (8.3333)  Acc@5: 33.3333 (33.3333)  time: 0.4121  data: 0.2962  max mem: 218\n",
            "Train: Epoch[ 1/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 2.3387  Acc@1: 16.6667 (15.1515)  Acc@5: 58.3333 (56.0606)  time: 0.1243  data: 0.0310  max mem: 223\n",
            "Train: Epoch[ 1/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 2.1848  Acc@1: 16.6667 (15.8664)  Acc@5: 62.5000 (62.0042)  time: 0.1072  data: 0.0172  max mem: 223\n",
            "Train: Epoch[ 1/10] Total time: 0:00:02 (0.1108 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 2.1848  Acc@1: 16.6667 (15.8664)  Acc@5: 62.5000 (62.0042)\n",
            "Train: Epoch[ 2/10]  [ 0/20]  eta: 0:00:10  Lr: 0.000047  Loss: 2.2291  Acc@1: 16.6667 (16.6667)  Acc@5: 58.3333 (58.3333)  time: 0.5004  data: 0.3968  max mem: 223\n",
            "Train: Epoch[ 2/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 1.9894  Acc@1: 25.0000 (25.0000)  Acc@5: 70.8333 (68.9394)  time: 0.1271  data: 0.0371  max mem: 223\n",
            "Train: Epoch[ 2/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 2.0952  Acc@1: 29.1667 (27.9749)  Acc@5: 75.0000 (72.4426)  time: 0.1094  data: 0.0205  max mem: 223\n",
            "Train: Epoch[ 2/10] Total time: 0:00:02 (0.1131 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 2.0952  Acc@1: 29.1667 (27.9749)  Acc@5: 75.0000 (72.4426)\n",
            "Train: Epoch[ 3/10]  [ 0/20]  eta: 0:00:09  Lr: 0.000047  Loss: 2.1092  Acc@1: 25.0000 (25.0000)  Acc@5: 66.6667 (66.6667)  time: 0.4727  data: 0.3687  max mem: 223\n",
            "Train: Epoch[ 3/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 2.0034  Acc@1: 41.6667 (40.5303)  Acc@5: 83.3333 (83.7121)  time: 0.1351  data: 0.0342  max mem: 223\n",
            "Train: Epoch[ 3/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 1.8779  Acc@1: 41.6667 (42.5887)  Acc@5: 87.5000 (86.2213)  time: 0.1147  data: 0.0190  max mem: 223\n",
            "Train: Epoch[ 3/10] Total time: 0:00:02 (0.1240 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.8779  Acc@1: 41.6667 (42.5887)  Acc@5: 87.5000 (86.2213)\n",
            "Train: Epoch[ 4/10]  [ 0/20]  eta: 0:00:13  Lr: 0.000047  Loss: 1.8656  Acc@1: 45.8333 (45.8333)  Acc@5: 91.6667 (91.6667)  time: 0.6783  data: 0.5581  max mem: 223\n",
            "Train: Epoch[ 4/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 1.7567  Acc@1: 50.0000 (50.0000)  Acc@5: 91.6667 (90.9091)  time: 0.1613  data: 0.0564  max mem: 223\n",
            "Train: Epoch[ 4/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 1.6614  Acc@1: 50.0000 (50.5219)  Acc@5: 91.6667 (90.8142)  time: 0.1315  data: 0.0312  max mem: 223\n",
            "Train: Epoch[ 4/10] Total time: 0:00:02 (0.1368 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.6614  Acc@1: 50.0000 (50.5219)  Acc@5: 91.6667 (90.8142)\n",
            "Train: Epoch[ 5/10]  [ 0/20]  eta: 0:00:08  Lr: 0.000047  Loss: 1.4961  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.8333)  time: 0.4291  data: 0.3226  max mem: 223\n",
            "Train: Epoch[ 5/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 1.4410  Acc@1: 54.1667 (57.9545)  Acc@5: 91.6667 (93.5606)  time: 0.1224  data: 0.0307  max mem: 223\n",
            "Train: Epoch[ 5/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 1.6391  Acc@1: 58.3333 (60.3340)  Acc@5: 95.8333 (95.8246)  time: 0.1079  data: 0.0170  max mem: 223\n",
            "Train: Epoch[ 5/10] Total time: 0:00:02 (0.1117 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.6391  Acc@1: 58.3333 (60.3340)  Acc@5: 95.8333 (95.8246)\n",
            "Train: Epoch[ 6/10]  [ 0/20]  eta: 0:00:07  Lr: 0.000047  Loss: 1.4564  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.8333)  time: 0.3515  data: 0.2546  max mem: 223\n",
            "Train: Epoch[ 6/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 1.7823  Acc@1: 66.6667 (65.1515)  Acc@5: 95.8333 (95.0758)  time: 0.1218  data: 0.0283  max mem: 223\n",
            "Train: Epoch[ 6/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 1.3874  Acc@1: 70.8333 (66.3883)  Acc@5: 95.8333 (95.4071)  time: 0.1072  data: 0.0156  max mem: 223\n",
            "Train: Epoch[ 6/10] Total time: 0:00:02 (0.1111 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.3874  Acc@1: 70.8333 (66.3883)  Acc@5: 95.8333 (95.4071)\n",
            "Train: Epoch[ 7/10]  [ 0/20]  eta: 0:00:09  Lr: 0.000047  Loss: 1.3684  Acc@1: 62.5000 (62.5000)  Acc@5: 95.8333 (95.8333)  time: 0.4545  data: 0.3460  max mem: 223\n",
            "Train: Epoch[ 7/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 1.0280  Acc@1: 66.6667 (67.4242)  Acc@5: 95.8333 (96.2121)  time: 0.1270  data: 0.0328  max mem: 223\n",
            "Train: Epoch[ 7/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 1.3645  Acc@1: 66.6667 (67.8497)  Acc@5: 95.8333 (96.0334)  time: 0.1110  data: 0.0182  max mem: 223\n",
            "Train: Epoch[ 7/10] Total time: 0:00:02 (0.1145 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.3645  Acc@1: 66.6667 (67.8497)  Acc@5: 95.8333 (96.0334)\n",
            "Train: Epoch[ 8/10]  [ 0/20]  eta: 0:00:09  Lr: 0.000047  Loss: 1.0192  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.4653  data: 0.3525  max mem: 223\n",
            "Train: Epoch[ 8/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 1.1572  Acc@1: 79.1667 (76.8939)  Acc@5: 100.0000 (97.3485)  time: 0.1282  data: 0.0339  max mem: 223\n",
            "Train: Epoch[ 8/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 1.1571  Acc@1: 73.9130 (74.1127)  Acc@5: 95.8333 (97.2860)  time: 0.1117  data: 0.0187  max mem: 223\n",
            "Train: Epoch[ 8/10] Total time: 0:00:02 (0.1155 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.1571  Acc@1: 73.9130 (74.1127)  Acc@5: 95.8333 (97.2860)\n",
            "Train: Epoch[ 9/10]  [ 0/20]  eta: 0:00:07  Lr: 0.000047  Loss: 1.1392  Acc@1: 83.3333 (83.3333)  Acc@5: 91.6667 (91.6667)  time: 0.3681  data: 0.2585  max mem: 223\n",
            "Train: Epoch[ 9/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 1.1026  Acc@1: 75.0000 (73.8636)  Acc@5: 95.8333 (96.5909)  time: 0.1335  data: 0.0355  max mem: 223\n",
            "Train: Epoch[ 9/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 0.9543  Acc@1: 75.0000 (74.3215)  Acc@5: 95.8333 (96.4509)  time: 0.1152  data: 0.0200  max mem: 223\n",
            "Train: Epoch[ 9/10] Total time: 0:00:02 (0.1204 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.9543  Acc@1: 75.0000 (74.3215)  Acc@5: 95.8333 (96.4509)\n",
            "Train: Epoch[10/10]  [ 0/20]  eta: 0:00:18  Lr: 0.000047  Loss: 1.0956  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  time: 0.9089  data: 0.7957  max mem: 223\n",
            "Train: Epoch[10/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 1.0666  Acc@1: 75.0000 (74.2424)  Acc@5: 95.8333 (97.3485)  time: 0.1802  data: 0.0740  max mem: 223\n",
            "Train: Epoch[10/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 0.8274  Acc@1: 75.0000 (75.3653)  Acc@5: 95.8333 (97.4948)  time: 0.1407  data: 0.0409  max mem: 223\n",
            "Train: Epoch[10/10] Total time: 0:00:02 (0.1449 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.8274  Acc@1: 75.0000 (75.3653)  Acc@5: 95.8333 (97.4948)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:13  Loss: 2.0127 (2.0127)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.3261  data: 0.2495  max mem: 223\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 2.0735 (2.0714)  Acc@1: 83.3333 (82.5758)  Acc@5: 100.0000 (98.1061)  time: 0.1185  data: 0.0264  max mem: 223\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 2.0735 (2.0872)  Acc@1: 79.1667 (81.7460)  Acc@5: 100.0000 (97.8175)  time: 0.0957  data: 0.0023  max mem: 223\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 2.0212 (2.0613)  Acc@1: 83.3333 (83.4677)  Acc@5: 95.8333 (97.5806)  time: 0.0938  data: 0.0006  max mem: 223\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 1.9980 (2.0497)  Acc@1: 83.3333 (84.0447)  Acc@5: 95.8333 (97.6626)  time: 0.0944  data: 0.0004  max mem: 223\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 1.9843 (2.0455)  Acc@1: 87.5000 (84.1000)  Acc@5: 95.8333 (97.6000)  time: 0.0931  data: 0.0004  max mem: 223\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1017 s / it)\n",
            "* Acc@1 84.100 Acc@5 97.600 loss 2.045\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:13  Loss: 2.6762 (2.6762)  Acc@1: 66.6667 (66.6667)  Acc@5: 91.6667 (91.6667)  time: 0.3128  data: 0.2359  max mem: 223\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:03  Loss: 2.7366 (2.7501)  Acc@1: 66.6667 (68.5606)  Acc@5: 91.6667 (92.4242)  time: 0.1178  data: 0.0274  max mem: 223\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 2.7204 (2.7219)  Acc@1: 70.8333 (69.2460)  Acc@5: 91.6667 (92.8571)  time: 0.0960  data: 0.0034  max mem: 223\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 2.6647 (2.7101)  Acc@1: 70.8333 (69.3548)  Acc@5: 95.8333 (93.0108)  time: 0.0940  data: 0.0003  max mem: 223\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 2.6763 (2.7108)  Acc@1: 70.8333 (69.0041)  Acc@5: 95.8333 (93.6992)  time: 0.0946  data: 0.0003  max mem: 223\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 2.6647 (2.7039)  Acc@1: 70.8333 (69.2000)  Acc@5: 95.8333 (93.8000)  time: 0.0931  data: 0.0003  max mem: 223\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1015 s / it)\n",
            "* Acc@1 69.200 Acc@5 93.800 loss 2.704\n",
            "[Average accuracy till task2]\tAcc@1: 76.6500\tAcc@5: 95.7000\tLoss: 2.3747\tForgetting: 6.0000\tBackward: -6.0000\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([4152, 384])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 1.2002  Acc@1: 83.3333 (81.2500)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([4152, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 1.1574  Acc@1: 79.1667 (80.8333)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([4152, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 1.1876  Acc@1: 87.5000 (85.4167)  Acc@5: 100.0000 (98.3333)\n",
            "torch.Size([4152, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 1.0846  Acc@1: 87.5000 (88.7500)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([4152, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 0.7919  Acc@1: 87.5000 (89.5833)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([4152, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 0.9053  Acc@1: 83.3333 (87.0833)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([4152, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.9937  Acc@1: 83.3333 (85.8333)  Acc@5: 100.0000 (98.7500)\n",
            "torch.Size([4152, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 0.7244  Acc@1: 87.5000 (87.9167)  Acc@5: 100.0000 (98.3333)\n",
            "torch.Size([4152, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 1.1298  Acc@1: 87.5000 (88.7500)  Acc@5: 100.0000 (98.7500)\n",
            "torch.Size([4152, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.9839  Acc@1: 87.5000 (88.7500)  Acc@5: 100.0000 (99.5833)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:19  Loss: 1.6020 (1.6020)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.4603  data: 0.3600  max mem: 224\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 1.7424 (1.7318)  Acc@1: 83.3333 (81.0606)  Acc@5: 100.0000 (98.4848)  time: 0.1248  data: 0.0336  max mem: 224\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 1.7424 (1.7342)  Acc@1: 83.3333 (81.5476)  Acc@5: 100.0000 (98.2143)  time: 0.0906  data: 0.0007  max mem: 224\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 1.6941 (1.7054)  Acc@1: 83.3333 (83.6022)  Acc@5: 95.8333 (98.1183)  time: 0.0901  data: 0.0005  max mem: 224\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 1.6257 (1.6865)  Acc@1: 87.5000 (84.1463)  Acc@5: 100.0000 (98.2724)  time: 0.0906  data: 0.0004  max mem: 224\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 1.6252 (1.6829)  Acc@1: 87.5000 (84.2000)  Acc@5: 100.0000 (98.3000)  time: 0.0888  data: 0.0004  max mem: 224\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1005 s / it)\n",
            "* Acc@1 84.200 Acc@5 98.300 loss 1.683\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:14  Loss: 2.1041 (2.1041)  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  time: 0.3516  data: 0.2609  max mem: 224\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:03  Loss: 2.1571 (2.1362)  Acc@1: 79.1667 (77.6515)  Acc@5: 95.8333 (96.5909)  time: 0.1224  data: 0.0314  max mem: 224\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 2.1337 (2.1148)  Acc@1: 79.1667 (79.9603)  Acc@5: 95.8333 (96.0317)  time: 0.0964  data: 0.0051  max mem: 224\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 2.0270 (2.1053)  Acc@1: 83.3333 (80.3763)  Acc@5: 95.8333 (95.9677)  time: 0.0924  data: 0.0015  max mem: 224\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 2.0365 (2.1014)  Acc@1: 83.3333 (80.7927)  Acc@5: 100.0000 (96.3415)  time: 0.0916  data: 0.0011  max mem: 224\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 2.0204 (2.0950)  Acc@1: 83.3333 (80.9000)  Acc@5: 100.0000 (96.4000)  time: 0.0900  data: 0.0010  max mem: 224\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1015 s / it)\n",
            "* Acc@1 80.900 Acc@5 96.400 loss 2.095\n",
            "[Average accuracy till task2]\tAcc@1: 82.5500\tAcc@5: 97.3500\tLoss: 1.8889\tForgetting: 5.9000\tBackward: -5.9000\n",
            "Train: Epoch[ 1/10]  [ 0/21]  eta: 0:00:08  Lr: 0.000047  Loss: 2.3818  Acc@1: 8.3333 (8.3333)  Acc@5: 37.5000 (37.5000)  time: 0.3915  data: 0.2870  max mem: 224\n",
            "Train: Epoch[ 1/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 2.3121  Acc@1: 12.5000 (12.5000)  Acc@5: 62.5000 (56.0606)  time: 0.1321  data: 0.0367  max mem: 229\n",
            "Train: Epoch[ 1/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 2.0049  Acc@1: 12.5000 (15.2918)  Acc@5: 62.5000 (59.7586)  time: 0.1026  data: 0.0059  max mem: 229\n",
            "Train: Epoch[ 1/10] Total time: 0:00:02 (0.1230 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 2.0049  Acc@1: 12.5000 (15.2918)  Acc@5: 62.5000 (59.7586)\n",
            "Train: Epoch[ 2/10]  [ 0/21]  eta: 0:00:17  Lr: 0.000047  Loss: 2.1931  Acc@1: 20.8333 (20.8333)  Acc@5: 62.5000 (62.5000)  time: 0.8452  data: 0.7030  max mem: 229\n",
            "Train: Epoch[ 2/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 2.1113  Acc@1: 29.1667 (28.4091)  Acc@5: 79.1667 (79.1667)  time: 0.1764  data: 0.0664  max mem: 229\n",
            "Train: Epoch[ 2/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 1.8010  Acc@1: 29.1667 (29.7787)  Acc@5: 79.1667 (79.4769)  time: 0.0990  data: 0.0015  max mem: 229\n",
            "Train: Epoch[ 2/10] Total time: 0:00:02 (0.1387 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.8010  Acc@1: 29.1667 (29.7787)  Acc@5: 79.1667 (79.4769)\n",
            "Train: Epoch[ 3/10]  [ 0/21]  eta: 0:00:10  Lr: 0.000047  Loss: 1.9434  Acc@1: 33.3333 (33.3333)  Acc@5: 87.5000 (87.5000)  time: 0.5074  data: 0.3814  max mem: 229\n",
            "Train: Epoch[ 3/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.7444  Acc@1: 50.0000 (46.9697)  Acc@5: 95.8333 (93.1818)  time: 0.1353  data: 0.0369  max mem: 229\n",
            "Train: Epoch[ 3/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 2.0863  Acc@1: 50.0000 (49.2958)  Acc@5: 91.6667 (91.7505)  time: 0.0923  data: 0.0013  max mem: 229\n",
            "Train: Epoch[ 3/10] Total time: 0:00:02 (0.1166 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 2.0863  Acc@1: 50.0000 (49.2958)  Acc@5: 91.6667 (91.7505)\n",
            "Train: Epoch[ 4/10]  [ 0/21]  eta: 0:00:09  Lr: 0.000047  Loss: 1.6885  Acc@1: 58.3333 (58.3333)  Acc@5: 91.6667 (91.6667)  time: 0.4458  data: 0.3253  max mem: 229\n",
            "Train: Epoch[ 4/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.5813  Acc@1: 54.1667 (54.9242)  Acc@5: 95.8333 (95.0758)  time: 0.1269  data: 0.0342  max mem: 229\n",
            "Train: Epoch[ 4/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 1.4324  Acc@1: 62.5000 (60.9658)  Acc@5: 95.8333 (95.5734)  time: 0.0912  data: 0.0028  max mem: 229\n",
            "Train: Epoch[ 4/10] Total time: 0:00:02 (0.1159 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.4324  Acc@1: 62.5000 (60.9658)  Acc@5: 95.8333 (95.5734)\n",
            "Train: Epoch[ 5/10]  [ 0/21]  eta: 0:00:21  Lr: 0.000047  Loss: 1.4022  Acc@1: 66.6667 (66.6667)  Acc@5: 100.0000 (100.0000)  time: 1.0404  data: 0.8715  max mem: 229\n",
            "Train: Epoch[ 5/10]  [10/21]  eta: 0:00:02  Lr: 0.000047  Loss: 1.2034  Acc@1: 66.6667 (67.0455)  Acc@5: 100.0000 (96.9697)  time: 0.1976  data: 0.0838  max mem: 229\n",
            "Train: Epoch[ 5/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 1.3227  Acc@1: 66.6667 (67.0020)  Acc@5: 95.8333 (96.5795)  time: 0.1014  data: 0.0026  max mem: 229\n",
            "Train: Epoch[ 5/10] Total time: 0:00:03 (0.1499 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.3227  Acc@1: 66.6667 (67.0020)  Acc@5: 95.8333 (96.5795)\n",
            "Train: Epoch[ 6/10]  [ 0/21]  eta: 0:00:10  Lr: 0.000047  Loss: 1.2931  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  time: 0.5035  data: 0.3867  max mem: 229\n",
            "Train: Epoch[ 6/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.1904  Acc@1: 75.0000 (72.7273)  Acc@5: 95.8333 (96.9697)  time: 0.1278  data: 0.0361  max mem: 229\n",
            "Train: Epoch[ 6/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 1.2570  Acc@1: 70.8333 (72.4346)  Acc@5: 95.8333 (96.9819)  time: 0.0886  data: 0.0006  max mem: 229\n",
            "Train: Epoch[ 6/10] Total time: 0:00:02 (0.1122 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.2570  Acc@1: 70.8333 (72.4346)  Acc@5: 95.8333 (96.9819)\n",
            "Train: Epoch[ 7/10]  [ 0/21]  eta: 0:00:10  Lr: 0.000047  Loss: 1.3821  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  time: 0.5000  data: 0.3936  max mem: 229\n",
            "Train: Epoch[ 7/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.0405  Acc@1: 75.0000 (77.2727)  Acc@5: 100.0000 (97.3485)  time: 0.1286  data: 0.0364  max mem: 229\n",
            "Train: Epoch[ 7/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 1.1173  Acc@1: 75.0000 (77.0624)  Acc@5: 100.0000 (97.5855)  time: 0.0888  data: 0.0005  max mem: 229\n",
            "Train: Epoch[ 7/10] Total time: 0:00:02 (0.1118 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.1173  Acc@1: 75.0000 (77.0624)  Acc@5: 100.0000 (97.5855)\n",
            "Train: Epoch[ 8/10]  [ 0/21]  eta: 0:00:08  Lr: 0.000047  Loss: 1.1415  Acc@1: 66.6667 (66.6667)  Acc@5: 100.0000 (100.0000)  time: 0.3909  data: 0.2835  max mem: 229\n",
            "Train: Epoch[ 8/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.0417  Acc@1: 75.0000 (78.4091)  Acc@5: 100.0000 (97.7273)  time: 0.1265  data: 0.0346  max mem: 229\n",
            "Train: Epoch[ 8/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 0.9007  Acc@1: 75.0000 (78.0684)  Acc@5: 100.0000 (97.1831)  time: 0.0927  data: 0.0050  max mem: 229\n",
            "Train: Epoch[ 8/10] Total time: 0:00:02 (0.1106 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.9007  Acc@1: 75.0000 (78.0684)  Acc@5: 100.0000 (97.1831)\n",
            "Train: Epoch[ 9/10]  [ 0/21]  eta: 0:00:07  Lr: 0.000047  Loss: 0.8909  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  time: 0.3487  data: 0.2620  max mem: 229\n",
            "Train: Epoch[ 9/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 0.8045  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (96.5909)  time: 0.1272  data: 0.0382  max mem: 229\n",
            "Train: Epoch[ 9/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 0.8086  Acc@1: 79.1667 (79.8793)  Acc@5: 100.0000 (96.7807)  time: 0.0956  data: 0.0080  max mem: 229\n",
            "Train: Epoch[ 9/10] Total time: 0:00:02 (0.1113 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.8086  Acc@1: 79.1667 (79.8793)  Acc@5: 100.0000 (96.7807)\n",
            "Train: Epoch[10/10]  [ 0/21]  eta: 0:00:10  Lr: 0.000047  Loss: 0.8771  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.5013  data: 0.3520  max mem: 229\n",
            "Train: Epoch[10/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 0.8118  Acc@1: 83.3333 (84.4697)  Acc@5: 100.0000 (98.8636)  time: 0.1480  data: 0.0459  max mem: 229\n",
            "Train: Epoch[10/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 1.1597  Acc@1: 79.1667 (81.4889)  Acc@5: 100.0000 (98.5916)  time: 0.1030  data: 0.0098  max mem: 229\n",
            "Train: Epoch[10/10] Total time: 0:00:02 (0.1289 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.1597  Acc@1: 79.1667 (81.4889)  Acc@5: 100.0000 (98.5916)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:27  Loss: 1.7046 (1.7046)  Acc@1: 87.5000 (87.5000)  Acc@5: 95.8333 (95.8333)  time: 0.6593  data: 0.5842  max mem: 229\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 1.8285 (1.8034)  Acc@1: 79.1667 (75.3788)  Acc@5: 100.0000 (98.1061)  time: 0.1418  data: 0.0539  max mem: 229\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 1.8285 (1.8112)  Acc@1: 79.1667 (76.3889)  Acc@5: 100.0000 (98.2143)  time: 0.0890  data: 0.0009  max mem: 229\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 1.7564 (1.7840)  Acc@1: 79.1667 (77.8226)  Acc@5: 100.0000 (97.9839)  time: 0.0883  data: 0.0006  max mem: 229\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 1.7064 (1.7670)  Acc@1: 83.3333 (79.6748)  Acc@5: 95.8333 (97.9675)  time: 0.0889  data: 0.0002  max mem: 229\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 1.6667 (1.7633)  Acc@1: 83.3333 (79.8000)  Acc@5: 95.8333 (97.9000)  time: 0.0869  data: 0.0002  max mem: 229\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1038 s / it)\n",
            "* Acc@1 79.800 Acc@5 97.900 loss 1.763\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:12  Loss: 2.2082 (2.2082)  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.8333)  time: 0.2914  data: 0.2179  max mem: 229\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:03  Loss: 2.2511 (2.2180)  Acc@1: 75.0000 (72.7273)  Acc@5: 95.8333 (94.6970)  time: 0.1186  data: 0.0335  max mem: 229\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 2.1686 (2.1929)  Acc@1: 70.8333 (73.2143)  Acc@5: 95.8333 (94.6429)  time: 0.0948  data: 0.0077  max mem: 229\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 2.0941 (2.1816)  Acc@1: 70.8333 (73.6559)  Acc@5: 95.8333 (95.0269)  time: 0.0893  data: 0.0004  max mem: 229\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 2.1074 (2.1761)  Acc@1: 75.0000 (73.9837)  Acc@5: 95.8333 (95.5285)  time: 0.0902  data: 0.0003  max mem: 229\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 2.1061 (2.1685)  Acc@1: 75.0000 (74.2000)  Acc@5: 100.0000 (95.6000)  time: 0.0877  data: 0.0003  max mem: 229\n",
            "Test: [Task 2] Total time: 0:00:04 (0.0982 s / it)\n",
            "* Acc@1 74.200 Acc@5 95.600 loss 2.168\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:14  Loss: 2.3298 (2.3298)  Acc@1: 50.0000 (50.0000)  Acc@5: 100.0000 (100.0000)  time: 0.3398  data: 0.2526  max mem: 229\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 2.4050 (2.3968)  Acc@1: 58.3333 (58.7121)  Acc@5: 95.8333 (95.4545)  time: 0.1134  data: 0.0270  max mem: 229\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 2.4043 (2.3923)  Acc@1: 58.3333 (60.1190)  Acc@5: 95.8333 (95.0397)  time: 0.0900  data: 0.0024  max mem: 229\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 2.3248 (2.3657)  Acc@1: 62.5000 (62.7688)  Acc@5: 95.8333 (94.6237)  time: 0.0895  data: 0.0003  max mem: 229\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 2.3511 (2.3891)  Acc@1: 66.6667 (62.6016)  Acc@5: 95.8333 (93.9024)  time: 0.0897  data: 0.0003  max mem: 229\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 2.3691 (2.3934)  Acc@1: 66.6667 (62.4000)  Acc@5: 95.8333 (93.8000)  time: 0.0879  data: 0.0003  max mem: 229\n",
            "Test: [Task 3] Total time: 0:00:04 (0.0987 s / it)\n",
            "* Acc@1 62.400 Acc@5 93.800 loss 2.393\n",
            "[Average accuracy till task3]\tAcc@1: 72.1333\tAcc@5: 95.7667\tLoss: 2.1084\tForgetting: 5.1500\tBackward: -2.6500\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([6312, 384])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 1.3075  Acc@1: 75.0000 (77.7083)  Acc@5: 100.0000 (98.1250)\n",
            "torch.Size([6312, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 0.9994  Acc@1: 83.3333 (81.0417)  Acc@5: 100.0000 (98.9583)\n",
            "torch.Size([6312, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 1.2063  Acc@1: 83.3333 (82.9167)  Acc@5: 100.0000 (98.5417)\n",
            "torch.Size([6312, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 1.2003  Acc@1: 87.5000 (84.7917)  Acc@5: 100.0000 (98.7500)\n",
            "torch.Size([6312, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 0.9329  Acc@1: 83.3333 (86.2500)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([6312, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 1.0929  Acc@1: 87.5000 (88.1250)  Acc@5: 100.0000 (98.9583)\n",
            "torch.Size([6312, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.9606  Acc@1: 83.3333 (84.5833)  Acc@5: 100.0000 (99.3750)\n",
            "torch.Size([6312, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 0.8918  Acc@1: 87.5000 (88.3333)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([6312, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 0.9278  Acc@1: 87.5000 (86.6667)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([6312, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.9084  Acc@1: 87.5000 (87.0833)  Acc@5: 100.0000 (99.7917)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:21  Loss: 1.2300 (1.2300)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.5232  data: 0.4459  max mem: 230\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 1.4302 (1.4075)  Acc@1: 70.8333 (74.2424)  Acc@5: 100.0000 (98.8636)  time: 0.1298  data: 0.0414  max mem: 230\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 1.3925 (1.4006)  Acc@1: 75.0000 (76.7857)  Acc@5: 100.0000 (98.8095)  time: 0.0892  data: 0.0009  max mem: 230\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 1.3777 (1.3789)  Acc@1: 79.1667 (78.0914)  Acc@5: 100.0000 (98.5215)  time: 0.0884  data: 0.0008  max mem: 230\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 1.3066 (1.3549)  Acc@1: 83.3333 (80.0813)  Acc@5: 100.0000 (98.7805)  time: 0.0895  data: 0.0007  max mem: 230\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 1.3032 (1.3500)  Acc@1: 83.3333 (80.2000)  Acc@5: 100.0000 (98.8000)  time: 0.0880  data: 0.0007  max mem: 230\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1027 s / it)\n",
            "* Acc@1 80.200 Acc@5 98.800 loss 1.350\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:26  Loss: 1.6811 (1.6811)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  time: 0.6386  data: 0.5481  max mem: 230\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 1.7308 (1.7275)  Acc@1: 75.0000 (75.7576)  Acc@5: 95.8333 (95.8333)  time: 0.1406  data: 0.0501  max mem: 230\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 1.7166 (1.7031)  Acc@1: 79.1667 (78.3730)  Acc@5: 95.8333 (95.6349)  time: 0.0899  data: 0.0003  max mem: 230\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 1.6011 (1.6914)  Acc@1: 79.1667 (78.7634)  Acc@5: 95.8333 (95.6989)  time: 0.0889  data: 0.0003  max mem: 230\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 1.6156 (1.6847)  Acc@1: 79.1667 (78.7602)  Acc@5: 95.8333 (96.2398)  time: 0.0893  data: 0.0002  max mem: 230\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 1.5965 (1.6772)  Acc@1: 79.1667 (78.9000)  Acc@5: 100.0000 (96.3000)  time: 0.0874  data: 0.0002  max mem: 230\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1044 s / it)\n",
            "* Acc@1 78.900 Acc@5 96.300 loss 1.677\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:19  Loss: 1.5094 (1.5094)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.4529  data: 0.3768  max mem: 230\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 1.6313 (1.6558)  Acc@1: 79.1667 (76.8939)  Acc@5: 100.0000 (97.3485)  time: 0.1242  data: 0.0350  max mem: 230\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 1.6313 (1.6364)  Acc@1: 75.0000 (76.1905)  Acc@5: 95.8333 (97.2222)  time: 0.0904  data: 0.0007  max mem: 230\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 1.5505 (1.6084)  Acc@1: 75.0000 (77.0161)  Acc@5: 95.8333 (97.1774)  time: 0.0896  data: 0.0004  max mem: 230\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 1.6034 (1.6294)  Acc@1: 75.0000 (77.1341)  Acc@5: 95.8333 (97.0528)  time: 0.0904  data: 0.0002  max mem: 230\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 1.6309 (1.6347)  Acc@1: 75.0000 (77.1000)  Acc@5: 95.8333 (97.1000)  time: 0.0888  data: 0.0002  max mem: 230\n",
            "Test: [Task 3] Total time: 0:00:04 (0.1006 s / it)\n",
            "* Acc@1 77.100 Acc@5 97.100 loss 1.635\n",
            "[Average accuracy till task3]\tAcc@1: 78.7333\tAcc@5: 97.4000\tLoss: 1.5540\tForgetting: 5.9500\tBackward: -5.9500\n",
            "Train: Epoch[ 1/10]  [ 0/20]  eta: 0:00:07  Lr: 0.000047  Loss: 2.3116  Acc@1: 8.3333 (8.3333)  Acc@5: 58.3333 (58.3333)  time: 0.3942  data: 0.2753  max mem: 230\n",
            "Train: Epoch[ 1/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 2.3757  Acc@1: 8.3333 (8.7121)  Acc@5: 41.6667 (46.2121)  time: 0.1288  data: 0.0346  max mem: 230\n",
            "Train: Epoch[ 1/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 2.1850  Acc@1: 8.3333 (9.3418)  Acc@5: 50.0000 (51.5924)  time: 0.1101  data: 0.0191  max mem: 230\n",
            "Train: Epoch[ 1/10] Total time: 0:00:02 (0.1154 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 2.1850  Acc@1: 8.3333 (9.3418)  Acc@5: 50.0000 (51.5924)\n",
            "Train: Epoch[ 2/10]  [ 0/20]  eta: 0:00:15  Lr: 0.000047  Loss: 2.1878  Acc@1: 16.6667 (16.6667)  Acc@5: 70.8333 (70.8333)  time: 0.7645  data: 0.6163  max mem: 230\n",
            "Train: Epoch[ 2/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 2.2032  Acc@1: 20.8333 (22.3485)  Acc@5: 75.0000 (74.6212)  time: 0.1666  data: 0.0569  max mem: 230\n",
            "Train: Epoch[ 2/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 2.2316  Acc@1: 25.0000 (24.4161)  Acc@5: 75.0000 (76.8578)  time: 0.1328  data: 0.0313  max mem: 230\n",
            "Train: Epoch[ 2/10] Total time: 0:00:02 (0.1418 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 2.2316  Acc@1: 25.0000 (24.4161)  Acc@5: 75.0000 (76.8578)\n",
            "Train: Epoch[ 3/10]  [ 0/20]  eta: 0:00:13  Lr: 0.000047  Loss: 1.8133  Acc@1: 45.8333 (45.8333)  Acc@5: 95.8333 (95.8333)  time: 0.6531  data: 0.5531  max mem: 230\n",
            "Train: Epoch[ 3/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 2.0103  Acc@1: 37.5000 (35.9848)  Acc@5: 91.6667 (88.6364)  time: 0.1437  data: 0.0514  max mem: 230\n",
            "Train: Epoch[ 3/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 1.8163  Acc@1: 37.5000 (40.9766)  Acc@5: 91.6667 (89.3843)  time: 0.1182  data: 0.0283  max mem: 230\n",
            "Train: Epoch[ 3/10] Total time: 0:00:02 (0.1219 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.8163  Acc@1: 37.5000 (40.9766)  Acc@5: 91.6667 (89.3843)\n",
            "Train: Epoch[ 4/10]  [ 0/20]  eta: 0:00:08  Lr: 0.000047  Loss: 1.8512  Acc@1: 45.8333 (45.8333)  Acc@5: 87.5000 (87.5000)  time: 0.4158  data: 0.3106  max mem: 230\n",
            "Train: Epoch[ 4/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 1.8275  Acc@1: 50.0000 (51.1364)  Acc@5: 91.6667 (90.5303)  time: 0.1260  data: 0.0326  max mem: 230\n",
            "Train: Epoch[ 4/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 1.4101  Acc@1: 54.1667 (54.7771)  Acc@5: 91.6667 (91.5074)  time: 0.1084  data: 0.0180  max mem: 230\n",
            "Train: Epoch[ 4/10] Total time: 0:00:02 (0.1129 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.4101  Acc@1: 54.1667 (54.7771)  Acc@5: 91.6667 (91.5074)\n",
            "Train: Epoch[ 5/10]  [ 0/20]  eta: 0:00:10  Lr: 0.000047  Loss: 1.6114  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  time: 0.5276  data: 0.4211  max mem: 230\n",
            "Train: Epoch[ 5/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 1.6945  Acc@1: 62.5000 (65.5303)  Acc@5: 91.6667 (92.4242)  time: 0.1360  data: 0.0392  max mem: 230\n",
            "Train: Epoch[ 5/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 1.3199  Acc@1: 66.6667 (66.4544)  Acc@5: 95.8333 (94.4798)  time: 0.1139  data: 0.0216  max mem: 230\n",
            "Train: Epoch[ 5/10] Total time: 0:00:02 (0.1179 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.3199  Acc@1: 66.6667 (66.4544)  Acc@5: 95.8333 (94.4798)\n",
            "Train: Epoch[ 6/10]  [ 0/20]  eta: 0:00:11  Lr: 0.000047  Loss: 1.4368  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  time: 0.5696  data: 0.4714  max mem: 230\n",
            "Train: Epoch[ 6/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 1.5001  Acc@1: 66.6667 (67.4242)  Acc@5: 95.8333 (95.4545)  time: 0.1343  data: 0.0440  max mem: 230\n",
            "Train: Epoch[ 6/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 1.3742  Acc@1: 66.6667 (67.0913)  Acc@5: 95.8333 (95.1168)  time: 0.1132  data: 0.0242  max mem: 230\n",
            "Train: Epoch[ 6/10] Total time: 0:00:02 (0.1171 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.3742  Acc@1: 66.6667 (67.0913)  Acc@5: 95.8333 (95.1168)\n",
            "Train: Epoch[ 7/10]  [ 0/20]  eta: 0:00:10  Lr: 0.000047  Loss: 1.2086  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  time: 0.5418  data: 0.4450  max mem: 230\n",
            "Train: Epoch[ 7/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 1.1789  Acc@1: 79.1667 (76.5152)  Acc@5: 100.0000 (97.7273)  time: 0.1352  data: 0.0416  max mem: 230\n",
            "Train: Epoch[ 7/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 1.0688  Acc@1: 75.0000 (74.3100)  Acc@5: 95.8333 (96.8153)  time: 0.1128  data: 0.0229  max mem: 230\n",
            "Train: Epoch[ 7/10] Total time: 0:00:02 (0.1200 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.0688  Acc@1: 75.0000 (74.3100)  Acc@5: 95.8333 (96.8153)\n",
            "Train: Epoch[ 8/10]  [ 0/20]  eta: 0:00:15  Lr: 0.000047  Loss: 1.2478  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.8333)  time: 0.7901  data: 0.6197  max mem: 230\n",
            "Train: Epoch[ 8/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 1.0571  Acc@1: 79.1667 (78.0303)  Acc@5: 95.8333 (97.7273)  time: 0.1666  data: 0.0566  max mem: 230\n",
            "Train: Epoch[ 8/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 1.1477  Acc@1: 79.1667 (77.2824)  Acc@5: 95.8333 (97.4522)  time: 0.1320  data: 0.0313  max mem: 230\n",
            "Train: Epoch[ 8/10] Total time: 0:00:02 (0.1363 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.1477  Acc@1: 79.1667 (77.2824)  Acc@5: 95.8333 (97.4522)\n",
            "Train: Epoch[ 9/10]  [ 0/20]  eta: 0:00:10  Lr: 0.000047  Loss: 1.1832  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.8333)  time: 0.5438  data: 0.4512  max mem: 230\n",
            "Train: Epoch[ 9/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 1.0588  Acc@1: 75.0000 (75.7576)  Acc@5: 100.0000 (97.7273)  time: 0.1321  data: 0.0418  max mem: 230\n",
            "Train: Epoch[ 9/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 0.8955  Acc@1: 75.0000 (77.7070)  Acc@5: 100.0000 (97.8769)  time: 0.1116  data: 0.0231  max mem: 230\n",
            "Train: Epoch[ 9/10] Total time: 0:00:02 (0.1155 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.8955  Acc@1: 75.0000 (77.7070)  Acc@5: 100.0000 (97.8769)\n",
            "Train: Epoch[10/10]  [ 0/20]  eta: 0:00:07  Lr: 0.000047  Loss: 0.9191  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.3807  data: 0.2862  max mem: 230\n",
            "Train: Epoch[10/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 1.1393  Acc@1: 79.1667 (79.5455)  Acc@5: 100.0000 (98.8636)  time: 0.1245  data: 0.0316  max mem: 230\n",
            "Train: Epoch[10/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 0.8535  Acc@1: 79.1667 (77.2824)  Acc@5: 100.0000 (98.5138)  time: 0.1074  data: 0.0176  max mem: 230\n",
            "Train: Epoch[10/10] Total time: 0:00:02 (0.1116 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.8535  Acc@1: 79.1667 (77.2824)  Acc@5: 100.0000 (98.5138)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:14  Loss: 1.4132 (1.4132)  Acc@1: 87.5000 (87.5000)  Acc@5: 91.6667 (91.6667)  time: 0.3528  data: 0.2760  max mem: 230\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 1.5100 (1.5152)  Acc@1: 79.1667 (74.6212)  Acc@5: 100.0000 (97.7273)  time: 0.1180  data: 0.0307  max mem: 230\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 1.5100 (1.5240)  Acc@1: 79.1667 (75.9921)  Acc@5: 100.0000 (97.6190)  time: 0.0925  data: 0.0033  max mem: 230\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 1.4666 (1.5113)  Acc@1: 79.1667 (76.8817)  Acc@5: 95.8333 (97.4462)  time: 0.0905  data: 0.0005  max mem: 230\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 1.4585 (1.4896)  Acc@1: 83.3333 (78.7602)  Acc@5: 95.8333 (97.6626)  time: 0.0908  data: 0.0004  max mem: 230\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 1.4223 (1.4841)  Acc@1: 83.3333 (78.9000)  Acc@5: 95.8333 (97.7000)  time: 0.0891  data: 0.0003  max mem: 230\n",
            "Test: [Task 1] Total time: 0:00:04 (0.0990 s / it)\n",
            "* Acc@1 78.900 Acc@5 97.700 loss 1.484\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:20  Loss: 1.8112 (1.8112)  Acc@1: 83.3333 (83.3333)  Acc@5: 91.6667 (91.6667)  time: 0.4969  data: 0.4180  max mem: 230\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 1.8978 (1.8693)  Acc@1: 75.0000 (76.8939)  Acc@5: 91.6667 (94.3182)  time: 0.1275  data: 0.0392  max mem: 230\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 1.8782 (1.8479)  Acc@1: 75.0000 (77.9762)  Acc@5: 91.6667 (94.2460)  time: 0.0901  data: 0.0013  max mem: 230\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 1.7486 (1.8372)  Acc@1: 79.1667 (78.4946)  Acc@5: 95.8333 (94.3548)  time: 0.0896  data: 0.0012  max mem: 230\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 1.7576 (1.8332)  Acc@1: 79.1667 (78.1504)  Acc@5: 95.8333 (94.9187)  time: 0.0901  data: 0.0008  max mem: 230\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 1.7521 (1.8244)  Acc@1: 79.1667 (78.3000)  Acc@5: 95.8333 (95.0000)  time: 0.0885  data: 0.0008  max mem: 230\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1020 s / it)\n",
            "* Acc@1 78.300 Acc@5 95.000 loss 1.824\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:20  Loss: 1.6693 (1.6693)  Acc@1: 70.8333 (70.8333)  Acc@5: 100.0000 (100.0000)  time: 0.4912  data: 0.4056  max mem: 230\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:04  Loss: 1.8201 (1.8294)  Acc@1: 70.8333 (69.6970)  Acc@5: 95.8333 (95.0758)  time: 0.1267  data: 0.0394  max mem: 230\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 1.8130 (1.8112)  Acc@1: 70.8333 (71.4286)  Acc@5: 95.8333 (95.4365)  time: 0.0902  data: 0.0015  max mem: 230\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 1.7050 (1.7702)  Acc@1: 75.0000 (72.7151)  Acc@5: 95.8333 (95.4301)  time: 0.0903  data: 0.0004  max mem: 230\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 1.7671 (1.7947)  Acc@1: 75.0000 (72.8659)  Acc@5: 95.8333 (94.7154)  time: 0.0906  data: 0.0003  max mem: 230\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 1.7752 (1.8034)  Acc@1: 75.0000 (72.8000)  Acc@5: 91.6667 (94.6000)  time: 0.0887  data: 0.0003  max mem: 230\n",
            "Test: [Task 3] Total time: 0:00:04 (0.1013 s / it)\n",
            "* Acc@1 72.800 Acc@5 94.600 loss 1.803\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:17  Loss: 3.0501 (3.0501)  Acc@1: 37.5000 (37.5000)  Acc@5: 66.6667 (66.6667)  time: 0.4055  data: 0.3289  max mem: 230\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:03  Loss: 2.7935 (2.8300)  Acc@1: 37.5000 (36.7424)  Acc@5: 83.3333 (83.3333)  time: 0.1204  data: 0.0309  max mem: 230\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 2.7935 (2.8366)  Acc@1: 37.5000 (34.9206)  Acc@5: 83.3333 (82.9365)  time: 0.0905  data: 0.0007  max mem: 230\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 2.7917 (2.8126)  Acc@1: 33.3333 (34.9462)  Acc@5: 87.5000 (84.1398)  time: 0.0891  data: 0.0004  max mem: 230\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 2.7971 (2.8246)  Acc@1: 33.3333 (34.7561)  Acc@5: 83.3333 (83.9431)  time: 0.0898  data: 0.0003  max mem: 230\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 2.7624 (2.8165)  Acc@1: 33.3333 (34.8000)  Acc@5: 83.3333 (84.1000)  time: 0.0882  data: 0.0002  max mem: 230\n",
            "Test: [Task 4] Total time: 0:00:04 (0.0989 s / it)\n",
            "* Acc@1 34.800 Acc@5 84.100 loss 2.816\n",
            "[Average accuracy till task4]\tAcc@1: 66.2000\tAcc@5: 92.8500\tLoss: 1.9821\tForgetting: 3.7333\tBackward: 2.7667\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([8424, 384])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 1.1824  Acc@1: 75.0000 (74.7222)  Acc@5: 95.8333 (96.1111)\n",
            "torch.Size([8424, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 1.3417  Acc@1: 83.3333 (81.9444)  Acc@5: 100.0000 (98.6111)\n",
            "torch.Size([8424, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 0.9428  Acc@1: 87.5000 (85.1389)  Acc@5: 100.0000 (98.6111)\n",
            "torch.Size([8424, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 0.8623  Acc@1: 87.5000 (86.6667)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([8424, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 0.6940  Acc@1: 87.5000 (87.3611)  Acc@5: 100.0000 (98.6111)\n",
            "torch.Size([8424, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 0.9722  Acc@1: 91.6667 (88.4722)  Acc@5: 100.0000 (99.7222)\n",
            "torch.Size([8424, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.9766  Acc@1: 87.5000 (88.8889)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([8424, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 1.0145  Acc@1: 87.5000 (87.2222)  Acc@5: 100.0000 (98.7500)\n",
            "torch.Size([8424, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 0.9692  Acc@1: 91.6667 (88.1944)  Acc@5: 100.0000 (98.7500)\n",
            "torch.Size([8424, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.8118  Acc@1: 91.6667 (90.9722)  Acc@5: 100.0000 (98.8889)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:19  Loss: 1.0389 (1.0389)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  time: 0.4717  data: 0.4048  max mem: 230\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 1.2084 (1.1979)  Acc@1: 79.1667 (78.0303)  Acc@5: 100.0000 (98.4848)  time: 0.1252  data: 0.0377  max mem: 230\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 1.2084 (1.1962)  Acc@1: 79.1667 (79.7619)  Acc@5: 100.0000 (98.4127)  time: 0.0885  data: 0.0006  max mem: 230\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 1.1264 (1.1778)  Acc@1: 83.3333 (80.7796)  Acc@5: 95.8333 (98.1183)  time: 0.0874  data: 0.0004  max mem: 230\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 1.1030 (1.1545)  Acc@1: 83.3333 (82.0122)  Acc@5: 100.0000 (98.2724)  time: 0.0888  data: 0.0008  max mem: 230\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 1.0927 (1.1489)  Acc@1: 87.5000 (82.2000)  Acc@5: 100.0000 (98.3000)  time: 0.0870  data: 0.0008  max mem: 230\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1008 s / it)\n",
            "* Acc@1 82.200 Acc@5 98.300 loss 1.149\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:25  Loss: 1.5008 (1.5008)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  time: 0.6114  data: 0.5166  max mem: 230\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 1.5126 (1.5011)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.0758)  time: 0.1384  data: 0.0510  max mem: 230\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 1.4244 (1.4948)  Acc@1: 79.1667 (76.9841)  Acc@5: 95.8333 (94.4444)  time: 0.0893  data: 0.0030  max mem: 230\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 1.3955 (1.4771)  Acc@1: 79.1667 (77.0161)  Acc@5: 95.8333 (94.7581)  time: 0.0879  data: 0.0010  max mem: 230\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 1.3976 (1.4656)  Acc@1: 79.1667 (77.7439)  Acc@5: 95.8333 (95.4268)  time: 0.0887  data: 0.0004  max mem: 230\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 1.3958 (1.4581)  Acc@1: 79.1667 (77.8000)  Acc@5: 95.8333 (95.5000)  time: 0.0870  data: 0.0004  max mem: 230\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1028 s / it)\n",
            "* Acc@1 77.800 Acc@5 95.500 loss 1.458\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:18  Loss: 1.1688 (1.1688)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.4301  data: 0.3422  max mem: 230\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 1.3640 (1.3646)  Acc@1: 75.0000 (75.7576)  Acc@5: 95.8333 (96.2121)  time: 0.1203  data: 0.0317  max mem: 230\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 1.3640 (1.3422)  Acc@1: 75.0000 (75.5952)  Acc@5: 95.8333 (96.8254)  time: 0.0886  data: 0.0007  max mem: 230\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 1.2423 (1.3076)  Acc@1: 75.0000 (76.4785)  Acc@5: 95.8333 (97.0430)  time: 0.0882  data: 0.0006  max mem: 230\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 1.3023 (1.3254)  Acc@1: 79.1667 (76.9309)  Acc@5: 95.8333 (96.7480)  time: 0.0892  data: 0.0004  max mem: 230\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 1.3302 (1.3326)  Acc@1: 79.1667 (76.9000)  Acc@5: 95.8333 (96.8000)  time: 0.0873  data: 0.0004  max mem: 230\n",
            "Test: [Task 3] Total time: 0:00:04 (0.0984 s / it)\n",
            "* Acc@1 76.900 Acc@5 96.800 loss 1.333\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:14  Loss: 2.0107 (2.0107)  Acc@1: 62.5000 (62.5000)  Acc@5: 87.5000 (87.5000)  time: 0.3457  data: 0.2550  max mem: 230\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:03  Loss: 1.5801 (1.6169)  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.0758)  time: 0.1163  data: 0.0260  max mem: 230\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 1.5841 (1.6363)  Acc@1: 70.8333 (71.2302)  Acc@5: 95.8333 (95.6349)  time: 0.0911  data: 0.0018  max mem: 230\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 1.5847 (1.6153)  Acc@1: 70.8333 (71.5054)  Acc@5: 95.8333 (95.6989)  time: 0.0890  data: 0.0004  max mem: 230\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 1.5890 (1.6299)  Acc@1: 70.8333 (71.4431)  Acc@5: 95.8333 (95.3252)  time: 0.0896  data: 0.0003  max mem: 230\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 1.5773 (1.6228)  Acc@1: 70.8333 (71.8000)  Acc@5: 95.8333 (95.4000)  time: 0.0879  data: 0.0003  max mem: 230\n",
            "Test: [Task 4] Total time: 0:00:04 (0.0991 s / it)\n",
            "* Acc@1 71.800 Acc@5 95.400 loss 1.623\n",
            "[Average accuracy till task4]\tAcc@1: 77.1750\tAcc@5: 96.5000\tLoss: 1.3906\tForgetting: 3.7333\tBackward: -3.7333\n",
            "Train: Epoch[ 1/10]  [ 0/21]  eta: 0:00:16  Lr: 0.000047  Loss: 2.2642  Acc@1: 12.5000 (12.5000)  Acc@5: 62.5000 (62.5000)  time: 0.7824  data: 0.6533  max mem: 230\n",
            "Train: Epoch[ 1/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 2.2100  Acc@1: 12.5000 (14.3939)  Acc@5: 62.5000 (60.2273)  time: 0.1677  data: 0.0647  max mem: 230\n",
            "Train: Epoch[ 1/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 2.0044  Acc@1: 20.8333 (19.7556)  Acc@5: 70.8333 (67.8208)  time: 0.0954  data: 0.0030  max mem: 230\n",
            "Train: Epoch[ 1/10] Total time: 0:00:02 (0.1338 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 2.0044  Acc@1: 20.8333 (19.7556)  Acc@5: 70.8333 (67.8208)\n",
            "Train: Epoch[ 2/10]  [ 0/21]  eta: 0:00:10  Lr: 0.000047  Loss: 2.0497  Acc@1: 25.0000 (25.0000)  Acc@5: 87.5000 (87.5000)  time: 0.5142  data: 0.3865  max mem: 230\n",
            "Train: Epoch[ 2/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.8460  Acc@1: 29.1667 (32.5758)  Acc@5: 87.5000 (84.0909)  time: 0.1294  data: 0.0359  max mem: 230\n",
            "Train: Epoch[ 2/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 1.8115  Acc@1: 33.3333 (35.6415)  Acc@5: 87.5000 (86.5580)  time: 0.0882  data: 0.0006  max mem: 230\n",
            "Train: Epoch[ 2/10] Total time: 0:00:02 (0.1120 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.8115  Acc@1: 33.3333 (35.6415)  Acc@5: 87.5000 (86.5580)\n",
            "Train: Epoch[ 3/10]  [ 0/21]  eta: 0:00:12  Lr: 0.000047  Loss: 1.8584  Acc@1: 41.6667 (41.6667)  Acc@5: 95.8333 (95.8333)  time: 0.5875  data: 0.4843  max mem: 230\n",
            "Train: Epoch[ 3/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.6654  Acc@1: 45.8333 (46.5909)  Acc@5: 91.6667 (90.9091)  time: 0.1364  data: 0.0447  max mem: 230\n",
            "Train: Epoch[ 3/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 1.5522  Acc@1: 54.1667 (54.9898)  Acc@5: 91.6667 (92.4644)  time: 0.0883  data: 0.0005  max mem: 230\n",
            "Train: Epoch[ 3/10] Total time: 0:00:02 (0.1159 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.5522  Acc@1: 54.1667 (54.9898)  Acc@5: 91.6667 (92.4644)\n",
            "Train: Epoch[ 4/10]  [ 0/21]  eta: 0:00:07  Lr: 0.000047  Loss: 1.5780  Acc@1: 62.5000 (62.5000)  Acc@5: 87.5000 (87.5000)  time: 0.3488  data: 0.2527  max mem: 230\n",
            "Train: Epoch[ 4/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.5335  Acc@1: 54.1667 (57.9545)  Acc@5: 95.8333 (95.0758)  time: 0.1251  data: 0.0345  max mem: 230\n",
            "Train: Epoch[ 4/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 1.4344  Acc@1: 58.3333 (59.8778)  Acc@5: 95.8333 (96.1303)  time: 0.0945  data: 0.0064  max mem: 230\n",
            "Train: Epoch[ 4/10] Total time: 0:00:02 (0.1108 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.4344  Acc@1: 58.3333 (59.8778)  Acc@5: 95.8333 (96.1303)\n",
            "Train: Epoch[ 5/10]  [ 0/21]  eta: 0:00:09  Lr: 0.000047  Loss: 1.1998  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.4542  data: 0.3607  max mem: 230\n",
            "Train: Epoch[ 5/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.4072  Acc@1: 70.8333 (68.1818)  Acc@5: 95.8333 (96.9697)  time: 0.1361  data: 0.0363  max mem: 230\n",
            "Train: Epoch[ 5/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 1.4687  Acc@1: 66.6667 (67.2098)  Acc@5: 100.0000 (97.7597)  time: 0.0952  data: 0.0022  max mem: 230\n",
            "Train: Epoch[ 5/10] Total time: 0:00:02 (0.1176 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.4687  Acc@1: 66.6667 (67.2098)  Acc@5: 100.0000 (97.7597)\n",
            "Train: Epoch[ 6/10]  [ 0/21]  eta: 0:00:17  Lr: 0.000047  Loss: 1.2599  Acc@1: 70.8333 (70.8333)  Acc@5: 100.0000 (100.0000)  time: 0.8144  data: 0.6957  max mem: 230\n",
            "Train: Epoch[ 6/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.3267  Acc@1: 70.8333 (70.0758)  Acc@5: 100.0000 (96.9697)  time: 0.1673  data: 0.0636  max mem: 230\n",
            "Train: Epoch[ 6/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 1.1144  Acc@1: 70.8333 (70.2648)  Acc@5: 95.8333 (96.3340)  time: 0.0963  data: 0.0008  max mem: 230\n",
            "Train: Epoch[ 6/10] Total time: 0:00:02 (0.1368 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.1144  Acc@1: 70.8333 (70.2648)  Acc@5: 95.8333 (96.3340)\n",
            "Train: Epoch[ 7/10]  [ 0/21]  eta: 0:00:14  Lr: 0.000047  Loss: 1.3578  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.8333)  time: 0.6745  data: 0.5603  max mem: 230\n",
            "Train: Epoch[ 7/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.1146  Acc@1: 75.0000 (74.2424)  Acc@5: 100.0000 (98.1061)  time: 0.1450  data: 0.0518  max mem: 230\n",
            "Train: Epoch[ 7/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 1.1625  Acc@1: 75.0000 (74.5418)  Acc@5: 100.0000 (97.3523)  time: 0.0885  data: 0.0006  max mem: 230\n",
            "Train: Epoch[ 7/10] Total time: 0:00:02 (0.1202 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.1625  Acc@1: 75.0000 (74.5418)  Acc@5: 100.0000 (97.3523)\n",
            "Train: Epoch[ 8/10]  [ 0/21]  eta: 0:00:09  Lr: 0.000047  Loss: 1.0284  Acc@1: 70.8333 (70.8333)  Acc@5: 100.0000 (100.0000)  time: 0.4454  data: 0.3343  max mem: 230\n",
            "Train: Epoch[ 8/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 0.8913  Acc@1: 79.1667 (78.4091)  Acc@5: 100.0000 (98.4848)  time: 0.1238  data: 0.0312  max mem: 230\n",
            "Train: Epoch[ 8/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 1.4076  Acc@1: 75.0000 (74.9491)  Acc@5: 100.0000 (97.7597)  time: 0.0888  data: 0.0006  max mem: 230\n",
            "Train: Epoch[ 8/10] Total time: 0:00:02 (0.1095 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.4076  Acc@1: 75.0000 (74.9491)  Acc@5: 100.0000 (97.7597)\n",
            "Train: Epoch[ 9/10]  [ 0/21]  eta: 0:00:10  Lr: 0.000047  Loss: 0.8995  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.5207  data: 0.4174  max mem: 230\n",
            "Train: Epoch[ 9/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 0.8691  Acc@1: 83.3333 (82.9545)  Acc@5: 100.0000 (98.4848)  time: 0.1303  data: 0.0387  max mem: 230\n",
            "Train: Epoch[ 9/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 0.8082  Acc@1: 79.1667 (80.4481)  Acc@5: 100.0000 (98.7780)  time: 0.0883  data: 0.0005  max mem: 230\n",
            "Train: Epoch[ 9/10] Total time: 0:00:02 (0.1126 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.8082  Acc@1: 79.1667 (80.4481)  Acc@5: 100.0000 (98.7780)\n",
            "Train: Epoch[10/10]  [ 0/21]  eta: 0:00:09  Lr: 0.000047  Loss: 0.8519  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  time: 0.4545  data: 0.3409  max mem: 230\n",
            "Train: Epoch[10/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.1327  Acc@1: 79.1667 (79.5455)  Acc@5: 100.0000 (97.7273)  time: 0.1253  data: 0.0323  max mem: 230\n",
            "Train: Epoch[10/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 0.7492  Acc@1: 81.8182 (81.4664)  Acc@5: 100.0000 (98.5743)  time: 0.0892  data: 0.0008  max mem: 230\n",
            "Train: Epoch[10/10] Total time: 0:00:02 (0.1106 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.7492  Acc@1: 81.8182 (81.4664)  Acc@5: 100.0000 (98.5743)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:19  Loss: 1.2911 (1.2911)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.4574  data: 0.3741  max mem: 230\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 1.3190 (1.3225)  Acc@1: 79.1667 (76.8939)  Acc@5: 100.0000 (98.1061)  time: 0.1301  data: 0.0429  max mem: 230\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 1.3297 (1.3335)  Acc@1: 79.1667 (77.7778)  Acc@5: 100.0000 (97.8175)  time: 0.0932  data: 0.0056  max mem: 230\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 1.2616 (1.3206)  Acc@1: 75.0000 (77.2849)  Acc@5: 95.8333 (97.7151)  time: 0.0898  data: 0.0015  max mem: 230\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 1.2343 (1.2959)  Acc@1: 83.3333 (79.2683)  Acc@5: 100.0000 (97.9675)  time: 0.0907  data: 0.0008  max mem: 230\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 1.2216 (1.2902)  Acc@1: 83.3333 (79.4000)  Acc@5: 100.0000 (98.0000)  time: 0.0886  data: 0.0007  max mem: 230\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1027 s / it)\n",
            "* Acc@1 79.400 Acc@5 98.000 loss 1.290\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:22  Loss: 1.5672 (1.5672)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  time: 0.5473  data: 0.4759  max mem: 230\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 1.5672 (1.5447)  Acc@1: 79.1667 (78.4091)  Acc@5: 95.8333 (95.8333)  time: 0.1331  data: 0.0445  max mem: 230\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 1.4832 (1.5446)  Acc@1: 75.0000 (79.5635)  Acc@5: 95.8333 (94.8413)  time: 0.0906  data: 0.0008  max mem: 230\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 1.4630 (1.5307)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.0269)  time: 0.0897  data: 0.0003  max mem: 230\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 1.4659 (1.5215)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.6301)  time: 0.0903  data: 0.0003  max mem: 230\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 1.4630 (1.5145)  Acc@1: 79.1667 (79.2000)  Acc@5: 95.8333 (95.7000)  time: 0.0887  data: 0.0003  max mem: 230\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1027 s / it)\n",
            "* Acc@1 79.200 Acc@5 95.700 loss 1.514\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:18  Loss: 1.2324 (1.2324)  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)  time: 0.4495  data: 0.3780  max mem: 230\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 1.4457 (1.4697)  Acc@1: 75.0000 (76.5152)  Acc@5: 95.8333 (95.0758)  time: 0.1237  data: 0.0346  max mem: 230\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 1.4457 (1.4374)  Acc@1: 75.0000 (76.1905)  Acc@5: 95.8333 (95.8333)  time: 0.0903  data: 0.0003  max mem: 230\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 1.3122 (1.4061)  Acc@1: 75.0000 (76.4785)  Acc@5: 95.8333 (96.1022)  time: 0.0895  data: 0.0003  max mem: 230\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 1.3846 (1.4236)  Acc@1: 79.1667 (77.0325)  Acc@5: 95.8333 (95.8333)  time: 0.0902  data: 0.0002  max mem: 230\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 1.4125 (1.4335)  Acc@1: 79.1667 (76.8000)  Acc@5: 95.8333 (95.8000)  time: 0.0884  data: 0.0002  max mem: 230\n",
            "Test: [Task 3] Total time: 0:00:04 (0.1001 s / it)\n",
            "* Acc@1 76.800 Acc@5 95.800 loss 1.434\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:20  Loss: 2.2613 (2.2613)  Acc@1: 58.3333 (58.3333)  Acc@5: 75.0000 (75.0000)  time: 0.4853  data: 0.4107  max mem: 230\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:04  Loss: 1.7555 (1.7988)  Acc@1: 70.8333 (67.8030)  Acc@5: 91.6667 (91.6667)  time: 0.1264  data: 0.0378  max mem: 230\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 1.7729 (1.8289)  Acc@1: 62.5000 (65.2778)  Acc@5: 91.6667 (89.2857)  time: 0.0900  data: 0.0004  max mem: 230\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 1.7729 (1.8040)  Acc@1: 62.5000 (65.7258)  Acc@5: 87.5000 (89.6505)  time: 0.0896  data: 0.0004  max mem: 230\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 1.7651 (1.8159)  Acc@1: 66.6667 (66.7683)  Acc@5: 87.5000 (89.8374)  time: 0.0903  data: 0.0004  max mem: 230\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 1.7644 (1.8085)  Acc@1: 66.6667 (67.0000)  Acc@5: 91.6667 (89.9000)  time: 0.0882  data: 0.0004  max mem: 230\n",
            "Test: [Task 4] Total time: 0:00:04 (0.1024 s / it)\n",
            "* Acc@1 67.000 Acc@5 89.900 loss 1.809\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:31  Loss: 2.6292 (2.6292)  Acc@1: 20.8333 (20.8333)  Acc@5: 87.5000 (87.5000)  time: 0.7606  data: 0.6651  max mem: 230\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:04  Loss: 2.6292 (2.6242)  Acc@1: 25.0000 (27.6515)  Acc@5: 87.5000 (86.7424)  time: 0.1509  data: 0.0620  max mem: 230\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 2.6057 (2.6356)  Acc@1: 25.0000 (27.3810)  Acc@5: 83.3333 (84.5238)  time: 0.0893  data: 0.0011  max mem: 230\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 2.6783 (2.6491)  Acc@1: 20.8333 (25.4032)  Acc@5: 83.3333 (84.1398)  time: 0.0895  data: 0.0004  max mem: 230\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 2.6937 (2.6856)  Acc@1: 20.8333 (24.1870)  Acc@5: 79.1667 (82.6220)  time: 0.0902  data: 0.0003  max mem: 230\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 2.7088 (2.6927)  Acc@1: 20.8333 (23.9000)  Acc@5: 79.1667 (82.4000)  time: 0.0882  data: 0.0003  max mem: 230\n",
            "Test: [Task 5] Total time: 0:00:04 (0.1069 s / it)\n",
            "* Acc@1 23.900 Acc@5 82.400 loss 2.693\n",
            "[Average accuracy till task5]\tAcc@1: 65.2600\tAcc@5: 92.3600\tLoss: 1.7479\tForgetting: 2.6750\tBackward: 11.4750\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([10512, 384])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 0.9686  Acc@1: 75.0000 (76.5625)  Acc@5: 95.8333 (96.3542)\n",
            "torch.Size([10512, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 1.2693  Acc@1: 79.1667 (80.2083)  Acc@5: 100.0000 (96.6667)\n",
            "torch.Size([10512, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 1.2310  Acc@1: 87.5000 (84.8958)  Acc@5: 100.0000 (98.5417)\n",
            "torch.Size([10512, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 0.8625  Acc@1: 87.5000 (86.3542)  Acc@5: 100.0000 (98.5417)\n",
            "torch.Size([10512, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 1.1324  Acc@1: 83.3333 (85.7292)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([10512, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 1.0889  Acc@1: 87.5000 (86.8750)  Acc@5: 100.0000 (99.3750)\n",
            "torch.Size([10512, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.9304  Acc@1: 87.5000 (88.7500)  Acc@5: 100.0000 (98.9583)\n",
            "torch.Size([10512, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 0.9285  Acc@1: 91.6667 (90.7292)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([10512, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 0.8682  Acc@1: 91.6667 (90.0000)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([10512, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.8122  Acc@1: 91.6667 (89.3750)  Acc@5: 100.0000 (99.3750)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:21  Loss: 1.0456 (1.0456)  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  time: 0.5027  data: 0.4268  max mem: 231\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 1.0969 (1.1194)  Acc@1: 79.1667 (78.0303)  Acc@5: 100.0000 (97.3485)  time: 0.1266  data: 0.0398  max mem: 231\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 1.0992 (1.1190)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (97.0238)  time: 0.0880  data: 0.0008  max mem: 231\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 1.0437 (1.0982)  Acc@1: 83.3333 (80.5108)  Acc@5: 95.8333 (97.3118)  time: 0.0874  data: 0.0004  max mem: 231\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.9960 (1.0735)  Acc@1: 83.3333 (81.6057)  Acc@5: 100.0000 (97.5610)  time: 0.0884  data: 0.0002  max mem: 231\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.9763 (1.0674)  Acc@1: 83.3333 (81.8000)  Acc@5: 100.0000 (97.6000)  time: 0.0865  data: 0.0002  max mem: 231\n",
            "Test: [Task 1] Total time: 0:00:04 (0.0993 s / it)\n",
            "* Acc@1 81.800 Acc@5 97.600 loss 1.067\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:13  Loss: 1.4227 (1.4227)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  time: 0.3262  data: 0.2530  max mem: 231\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:03  Loss: 1.4191 (1.3849)  Acc@1: 75.0000 (73.4849)  Acc@5: 91.6667 (94.6970)  time: 0.1156  data: 0.0311  max mem: 231\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 1.3337 (1.3897)  Acc@1: 75.0000 (74.0079)  Acc@5: 95.8333 (94.2460)  time: 0.0915  data: 0.0047  max mem: 231\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 1.2876 (1.3721)  Acc@1: 75.0000 (74.0591)  Acc@5: 95.8333 (94.6237)  time: 0.0890  data: 0.0005  max mem: 231\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 1.2876 (1.3579)  Acc@1: 75.0000 (74.3902)  Acc@5: 95.8333 (95.1220)  time: 0.0894  data: 0.0003  max mem: 231\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 1.2731 (1.3506)  Acc@1: 75.0000 (74.4000)  Acc@5: 95.8333 (95.2000)  time: 0.0877  data: 0.0003  max mem: 231\n",
            "Test: [Task 2] Total time: 0:00:04 (0.0985 s / it)\n",
            "* Acc@1 74.400 Acc@5 95.200 loss 1.351\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:26  Loss: 0.9913 (0.9913)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  time: 0.6265  data: 0.5461  max mem: 231\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:04  Loss: 1.2007 (1.2023)  Acc@1: 75.0000 (77.6515)  Acc@5: 95.8333 (95.8333)  time: 0.1394  data: 0.0504  max mem: 231\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 1.2007 (1.1780)  Acc@1: 75.0000 (77.9762)  Acc@5: 95.8333 (96.4286)  time: 0.0893  data: 0.0006  max mem: 231\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 1.0999 (1.1462)  Acc@1: 79.1667 (78.6290)  Acc@5: 95.8333 (96.2366)  time: 0.0886  data: 0.0004  max mem: 231\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 1.1524 (1.1610)  Acc@1: 75.0000 (78.7602)  Acc@5: 95.8333 (96.0366)  time: 0.0896  data: 0.0004  max mem: 231\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 1.1575 (1.1700)  Acc@1: 75.0000 (78.5000)  Acc@5: 95.8333 (96.0000)  time: 0.0879  data: 0.0004  max mem: 231\n",
            "Test: [Task 3] Total time: 0:00:04 (0.1037 s / it)\n",
            "* Acc@1 78.500 Acc@5 96.000 loss 1.170\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:20  Loss: 1.6555 (1.6555)  Acc@1: 62.5000 (62.5000)  Acc@5: 87.5000 (87.5000)  time: 0.4845  data: 0.4118  max mem: 231\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:04  Loss: 1.2736 (1.2940)  Acc@1: 75.0000 (74.6212)  Acc@5: 95.8333 (96.2121)  time: 0.1271  data: 0.0379  max mem: 231\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 1.2834 (1.3184)  Acc@1: 75.0000 (75.3968)  Acc@5: 95.8333 (96.2302)  time: 0.0898  data: 0.0004  max mem: 231\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 1.3020 (1.2925)  Acc@1: 75.0000 (76.2097)  Acc@5: 95.8333 (96.3710)  time: 0.0889  data: 0.0003  max mem: 231\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 1.2595 (1.3072)  Acc@1: 75.0000 (75.8130)  Acc@5: 95.8333 (96.0366)  time: 0.0895  data: 0.0002  max mem: 231\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 1.2144 (1.3007)  Acc@1: 75.0000 (76.1000)  Acc@5: 95.8333 (96.1000)  time: 0.0877  data: 0.0002  max mem: 231\n",
            "Test: [Task 4] Total time: 0:00:04 (0.1005 s / it)\n",
            "* Acc@1 76.100 Acc@5 96.100 loss 1.301\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:19  Loss: 1.4635 (1.4635)  Acc@1: 62.5000 (62.5000)  Acc@5: 100.0000 (100.0000)  time: 0.4754  data: 0.3893  max mem: 231\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:03  Loss: 1.4808 (1.4668)  Acc@1: 66.6667 (70.4545)  Acc@5: 95.8333 (96.5909)  time: 0.1248  data: 0.0365  max mem: 231\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 1.4930 (1.4971)  Acc@1: 66.6667 (69.0476)  Acc@5: 95.8333 (95.6349)  time: 0.0890  data: 0.0008  max mem: 231\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 1.5299 (1.5223)  Acc@1: 66.6667 (67.6075)  Acc@5: 95.8333 (95.0269)  time: 0.0890  data: 0.0004  max mem: 231\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 1.6562 (1.5734)  Acc@1: 62.5000 (64.9390)  Acc@5: 91.6667 (94.3089)  time: 0.0904  data: 0.0005  max mem: 231\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 1.6622 (1.5855)  Acc@1: 62.5000 (64.5000)  Acc@5: 91.6667 (94.2000)  time: 0.0887  data: 0.0005  max mem: 231\n",
            "Test: [Task 5] Total time: 0:00:04 (0.1020 s / it)\n",
            "* Acc@1 64.500 Acc@5 94.200 loss 1.585\n",
            "[Average accuracy till task5]\tAcc@1: 75.0600\tAcc@5: 95.8200\tLoss: 1.2948\tForgetting: 3.7000\tBackward: -2.2750\n",
            "Train: Epoch[ 1/10]  [ 0/20]  eta: 0:00:16  Lr: 0.000047  Loss: 2.3531  Acc@1: 12.5000 (12.5000)  Acc@5: 41.6667 (41.6667)  time: 0.8280  data: 0.6673  max mem: 231\n",
            "Train: Epoch[ 1/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 2.2028  Acc@1: 12.5000 (11.7424)  Acc@5: 58.3333 (52.2727)  time: 0.1580  data: 0.0616  max mem: 231\n",
            "Train: Epoch[ 1/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 2.0987  Acc@1: 12.5000 (12.5000)  Acc@5: 58.3333 (59.7917)  time: 0.1273  data: 0.0340  max mem: 231\n",
            "Train: Epoch[ 1/10] Total time: 0:00:02 (0.1315 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 2.0987  Acc@1: 12.5000 (12.5000)  Acc@5: 58.3333 (59.7917)\n",
            "Train: Epoch[ 2/10]  [ 0/20]  eta: 0:00:09  Lr: 0.000047  Loss: 2.1152  Acc@1: 33.3333 (33.3333)  Acc@5: 75.0000 (75.0000)  time: 0.4943  data: 0.3698  max mem: 231\n",
            "Train: Epoch[ 2/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 2.0999  Acc@1: 29.1667 (27.6515)  Acc@5: 79.1667 (79.9242)  time: 0.1349  data: 0.0371  max mem: 231\n",
            "Train: Epoch[ 2/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 2.0110  Acc@1: 33.3333 (30.8333)  Acc@5: 83.3333 (82.0833)  time: 0.1145  data: 0.0205  max mem: 231\n",
            "Train: Epoch[ 2/10] Total time: 0:00:02 (0.1203 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 2.0110  Acc@1: 33.3333 (30.8333)  Acc@5: 83.3333 (82.0833)\n",
            "Train: Epoch[ 3/10]  [ 0/20]  eta: 0:00:07  Lr: 0.000047  Loss: 1.9408  Acc@1: 37.5000 (37.5000)  Acc@5: 87.5000 (87.5000)  time: 0.3966  data: 0.2954  max mem: 231\n",
            "Train: Epoch[ 3/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 1.7074  Acc@1: 37.5000 (37.8788)  Acc@5: 87.5000 (88.6364)  time: 0.1308  data: 0.0382  max mem: 231\n",
            "Train: Epoch[ 3/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 1.6240  Acc@1: 41.6667 (40.6250)  Acc@5: 91.6667 (90.6250)  time: 0.1124  data: 0.0212  max mem: 231\n",
            "Train: Epoch[ 3/10] Total time: 0:00:02 (0.1163 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.6240  Acc@1: 41.6667 (40.6250)  Acc@5: 91.6667 (90.6250)\n",
            "Train: Epoch[ 4/10]  [ 0/20]  eta: 0:00:11  Lr: 0.000047  Loss: 1.5591  Acc@1: 62.5000 (62.5000)  Acc@5: 100.0000 (100.0000)  time: 0.5617  data: 0.4591  max mem: 231\n",
            "Train: Epoch[ 4/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 1.5237  Acc@1: 50.0000 (50.3788)  Acc@5: 95.8333 (96.2121)  time: 0.1345  data: 0.0434  max mem: 231\n",
            "Train: Epoch[ 4/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 1.5532  Acc@1: 50.0000 (52.5000)  Acc@5: 95.8333 (94.3750)  time: 0.1142  data: 0.0239  max mem: 231\n",
            "Train: Epoch[ 4/10] Total time: 0:00:02 (0.1184 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.5532  Acc@1: 50.0000 (52.5000)  Acc@5: 95.8333 (94.3750)\n",
            "Train: Epoch[ 5/10]  [ 0/20]  eta: 0:00:08  Lr: 0.000047  Loss: 1.3902  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  time: 0.4166  data: 0.3140  max mem: 231\n",
            "Train: Epoch[ 5/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 1.5742  Acc@1: 54.1667 (56.4394)  Acc@5: 95.8333 (96.9697)  time: 0.1410  data: 0.0371  max mem: 231\n",
            "Train: Epoch[ 5/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 1.4335  Acc@1: 58.3333 (59.7917)  Acc@5: 95.8333 (97.5000)  time: 0.1211  data: 0.0205  max mem: 231\n",
            "Train: Epoch[ 5/10] Total time: 0:00:02 (0.1282 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.4335  Acc@1: 58.3333 (59.7917)  Acc@5: 95.8333 (97.5000)\n",
            "Train: Epoch[ 6/10]  [ 0/20]  eta: 0:00:16  Lr: 0.000047  Loss: 1.3451  Acc@1: 70.8333 (70.8333)  Acc@5: 100.0000 (100.0000)  time: 0.8011  data: 0.6579  max mem: 231\n",
            "Train: Epoch[ 6/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 1.4441  Acc@1: 66.6667 (62.5000)  Acc@5: 95.8333 (96.5909)  time: 0.1738  data: 0.0699  max mem: 231\n",
            "Train: Epoch[ 6/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 1.3537  Acc@1: 66.6667 (64.1667)  Acc@5: 100.0000 (97.7083)  time: 0.1380  data: 0.0388  max mem: 231\n",
            "Train: Epoch[ 6/10] Total time: 0:00:02 (0.1426 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.3537  Acc@1: 66.6667 (64.1667)  Acc@5: 100.0000 (97.7083)\n",
            "Train: Epoch[ 7/10]  [ 0/20]  eta: 0:00:10  Lr: 0.000047  Loss: 1.2088  Acc@1: 54.1667 (54.1667)  Acc@5: 100.0000 (100.0000)  time: 0.5015  data: 0.3988  max mem: 231\n",
            "Train: Epoch[ 7/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 1.2094  Acc@1: 70.8333 (68.5606)  Acc@5: 100.0000 (98.8636)  time: 0.1294  data: 0.0375  max mem: 231\n",
            "Train: Epoch[ 7/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 1.0961  Acc@1: 70.8333 (67.9167)  Acc@5: 100.0000 (98.7500)  time: 0.1118  data: 0.0208  max mem: 231\n",
            "Train: Epoch[ 7/10] Total time: 0:00:02 (0.1159 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.0961  Acc@1: 70.8333 (67.9167)  Acc@5: 100.0000 (98.7500)\n",
            "Train: Epoch[ 8/10]  [ 0/20]  eta: 0:00:08  Lr: 0.000047  Loss: 1.1839  Acc@1: 66.6667 (66.6667)  Acc@5: 95.8333 (95.8333)  time: 0.4331  data: 0.3079  max mem: 231\n",
            "Train: Epoch[ 8/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 1.0853  Acc@1: 70.8333 (70.8333)  Acc@5: 100.0000 (98.1061)  time: 0.1280  data: 0.0334  max mem: 231\n",
            "Train: Epoch[ 8/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 1.1694  Acc@1: 70.8333 (71.2500)  Acc@5: 95.8333 (97.5000)  time: 0.1108  data: 0.0185  max mem: 231\n",
            "Train: Epoch[ 8/10] Total time: 0:00:02 (0.1148 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.1694  Acc@1: 70.8333 (71.2500)  Acc@5: 95.8333 (97.5000)\n",
            "Train: Epoch[ 9/10]  [ 0/20]  eta: 0:00:07  Lr: 0.000047  Loss: 1.0929  Acc@1: 70.8333 (70.8333)  Acc@5: 100.0000 (100.0000)  time: 0.3783  data: 0.2746  max mem: 231\n",
            "Train: Epoch[ 9/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 1.0788  Acc@1: 75.0000 (73.8636)  Acc@5: 100.0000 (97.3485)  time: 0.1282  data: 0.0349  max mem: 231\n",
            "Train: Epoch[ 9/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 0.9987  Acc@1: 75.0000 (73.1250)  Acc@5: 100.0000 (97.5000)  time: 0.1110  data: 0.0194  max mem: 231\n",
            "Train: Epoch[ 9/10] Total time: 0:00:02 (0.1158 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.9987  Acc@1: 75.0000 (73.1250)  Acc@5: 100.0000 (97.5000)\n",
            "Train: Epoch[10/10]  [ 0/20]  eta: 0:00:08  Lr: 0.000047  Loss: 1.0537  Acc@1: 70.8333 (70.8333)  Acc@5: 100.0000 (100.0000)  time: 0.4144  data: 0.3075  max mem: 231\n",
            "Train: Epoch[10/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 0.9317  Acc@1: 75.0000 (76.8939)  Acc@5: 100.0000 (98.4848)  time: 0.1248  data: 0.0295  max mem: 231\n",
            "Train: Epoch[10/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 0.9907  Acc@1: 75.0000 (74.7917)  Acc@5: 100.0000 (98.7500)  time: 0.1093  data: 0.0163  max mem: 231\n",
            "Train: Epoch[10/10] Total time: 0:00:02 (0.1134 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.9907  Acc@1: 75.0000 (74.7917)  Acc@5: 100.0000 (98.7500)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:21  Loss: 1.1776 (1.1776)  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  time: 0.5129  data: 0.3874  max mem: 231\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 1.2188 (1.2201)  Acc@1: 79.1667 (76.8939)  Acc@5: 95.8333 (96.2121)  time: 0.1300  data: 0.0362  max mem: 231\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 1.2188 (1.2229)  Acc@1: 79.1667 (78.1746)  Acc@5: 95.8333 (96.0317)  time: 0.0909  data: 0.0010  max mem: 231\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 1.1447 (1.2004)  Acc@1: 79.1667 (78.2258)  Acc@5: 95.8333 (96.3710)  time: 0.0903  data: 0.0021  max mem: 231\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 1.1238 (1.1761)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (96.9512)  time: 0.0907  data: 0.0019  max mem: 231\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 1.0920 (1.1701)  Acc@1: 79.1667 (79.4000)  Acc@5: 95.8333 (97.0000)  time: 0.0890  data: 0.0016  max mem: 231\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1020 s / it)\n",
            "* Acc@1 79.400 Acc@5 97.000 loss 1.170\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:19  Loss: 1.4690 (1.4690)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  time: 0.4645  data: 0.3813  max mem: 231\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 1.4690 (1.4666)  Acc@1: 70.8333 (72.7273)  Acc@5: 91.6667 (93.9394)  time: 0.1262  data: 0.0353  max mem: 231\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 1.4064 (1.4728)  Acc@1: 75.0000 (74.0079)  Acc@5: 91.6667 (93.4524)  time: 0.0912  data: 0.0005  max mem: 231\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 1.3538 (1.4568)  Acc@1: 75.0000 (73.3871)  Acc@5: 95.8333 (93.9516)  time: 0.0902  data: 0.0003  max mem: 231\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 1.3563 (1.4429)  Acc@1: 70.8333 (73.4756)  Acc@5: 95.8333 (94.7154)  time: 0.0905  data: 0.0002  max mem: 231\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 1.3518 (1.4352)  Acc@1: 70.8333 (73.5000)  Acc@5: 95.8333 (94.8000)  time: 0.0891  data: 0.0002  max mem: 231\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1011 s / it)\n",
            "* Acc@1 73.500 Acc@5 94.800 loss 1.435\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:20  Loss: 1.1057 (1.1057)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.4954  data: 0.3840  max mem: 231\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:04  Loss: 1.3247 (1.3291)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (94.3182)  time: 0.1280  data: 0.0360  max mem: 231\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 1.3111 (1.3066)  Acc@1: 75.0000 (74.4048)  Acc@5: 95.8333 (95.0397)  time: 0.0906  data: 0.0007  max mem: 231\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 1.2138 (1.2751)  Acc@1: 70.8333 (74.5968)  Acc@5: 95.8333 (95.0269)  time: 0.0899  data: 0.0003  max mem: 231\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 1.2617 (1.2929)  Acc@1: 75.0000 (74.2886)  Acc@5: 95.8333 (94.9187)  time: 0.0907  data: 0.0002  max mem: 231\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 1.2977 (1.3035)  Acc@1: 75.0000 (74.2000)  Acc@5: 95.8333 (94.8000)  time: 0.0890  data: 0.0002  max mem: 231\n",
            "Test: [Task 3] Total time: 0:00:04 (0.1015 s / it)\n",
            "* Acc@1 74.200 Acc@5 94.800 loss 1.303\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:14  Loss: 1.8892 (1.8892)  Acc@1: 50.0000 (50.0000)  Acc@5: 83.3333 (83.3333)  time: 0.3568  data: 0.2730  max mem: 231\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:03  Loss: 1.3904 (1.4332)  Acc@1: 75.0000 (70.4545)  Acc@5: 95.8333 (93.9394)  time: 0.1217  data: 0.0325  max mem: 231\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 1.4309 (1.4562)  Acc@1: 70.8333 (70.4365)  Acc@5: 95.8333 (92.8571)  time: 0.0937  data: 0.0049  max mem: 231\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 1.4375 (1.4319)  Acc@1: 70.8333 (70.6989)  Acc@5: 91.6667 (93.4140)  time: 0.0901  data: 0.0012  max mem: 231\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 1.4129 (1.4484)  Acc@1: 70.8333 (70.9350)  Acc@5: 91.6667 (92.9878)  time: 0.0912  data: 0.0008  max mem: 231\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 1.3589 (1.4423)  Acc@1: 70.8333 (71.3000)  Acc@5: 93.7500 (93.0000)  time: 0.0892  data: 0.0008  max mem: 231\n",
            "Test: [Task 4] Total time: 0:00:04 (0.1011 s / it)\n",
            "* Acc@1 71.300 Acc@5 93.000 loss 1.442\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:29  Loss: 1.5374 (1.5374)  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  time: 0.6995  data: 0.6246  max mem: 231\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:04  Loss: 1.5870 (1.5627)  Acc@1: 70.8333 (71.2121)  Acc@5: 95.8333 (96.2121)  time: 0.1468  data: 0.0571  max mem: 231\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 1.5857 (1.5814)  Acc@1: 70.8333 (69.0476)  Acc@5: 95.8333 (94.6429)  time: 0.0906  data: 0.0004  max mem: 231\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 1.5857 (1.6065)  Acc@1: 66.6667 (67.7419)  Acc@5: 91.6667 (94.3548)  time: 0.0897  data: 0.0005  max mem: 231\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 1.7705 (1.6608)  Acc@1: 58.3333 (65.3455)  Acc@5: 91.6667 (93.5976)  time: 0.0900  data: 0.0004  max mem: 231\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 1.7831 (1.6733)  Acc@1: 58.3333 (64.8000)  Acc@5: 91.6667 (93.4000)  time: 0.0883  data: 0.0004  max mem: 231\n",
            "Test: [Task 5] Total time: 0:00:04 (0.1074 s / it)\n",
            "* Acc@1 64.800 Acc@5 93.400 loss 1.673\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:30  Loss: 2.9788 (2.9788)  Acc@1: 16.6667 (16.6667)  Acc@5: 66.6667 (66.6667)  time: 0.7253  data: 0.6429  max mem: 231\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:04  Loss: 3.1866 (3.1530)  Acc@1: 4.1667 (6.0606)  Acc@5: 66.6667 (64.3939)  time: 0.1478  data: 0.0608  max mem: 231\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 3.1875 (3.1642)  Acc@1: 4.1667 (5.5556)  Acc@5: 62.5000 (62.6984)  time: 0.0898  data: 0.0021  max mem: 231\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 3.2092 (3.1940)  Acc@1: 4.1667 (5.2419)  Acc@5: 62.5000 (62.5000)  time: 0.0901  data: 0.0011  max mem: 231\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 3.2446 (3.2012)  Acc@1: 4.1667 (5.0813)  Acc@5: 62.5000 (61.3821)  time: 0.0906  data: 0.0004  max mem: 231\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 3.2298 (3.2012)  Acc@1: 4.1667 (5.0000)  Acc@5: 62.5000 (61.4000)  time: 0.0892  data: 0.0004  max mem: 231\n",
            "Test: [Task 6] Total time: 0:00:04 (0.1069 s / it)\n",
            "* Acc@1 5.000 Acc@5 61.400 loss 3.201\n",
            "[Average accuracy till task6]\tAcc@1: 61.3667\tAcc@5: 89.0667\tLoss: 1.7043\tForgetting: 3.8000\tBackward: 16.5600\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([12624, 384])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 1.3549  Acc@1: 79.1667 (75.6667)  Acc@5: 95.8333 (95.8333)\n",
            "torch.Size([12624, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 1.0872  Acc@1: 83.3333 (81.0833)  Acc@5: 100.0000 (98.1667)\n",
            "torch.Size([12624, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 0.9919  Acc@1: 87.5000 (84.1667)  Acc@5: 100.0000 (98.6667)\n",
            "torch.Size([12624, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 0.7325  Acc@1: 87.5000 (86.9167)  Acc@5: 100.0000 (99.4167)\n",
            "torch.Size([12624, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 1.1519  Acc@1: 87.5000 (88.0833)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([12624, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 0.6797  Acc@1: 87.5000 (88.9167)  Acc@5: 100.0000 (99.7500)\n",
            "torch.Size([12624, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.9651  Acc@1: 91.6667 (88.5000)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([12624, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 1.0284  Acc@1: 91.6667 (89.7500)  Acc@5: 100.0000 (99.5000)\n",
            "torch.Size([12624, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 0.8257  Acc@1: 87.5000 (89.8333)  Acc@5: 100.0000 (99.3333)\n",
            "torch.Size([12624, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.8240  Acc@1: 87.5000 (89.1667)  Acc@5: 100.0000 (99.5000)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:23  Loss: 1.1210 (1.1210)  Acc@1: 75.0000 (75.0000)  Acc@5: 87.5000 (87.5000)  time: 0.5521  data: 0.4725  max mem: 231\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 1.1210 (1.1096)  Acc@1: 75.0000 (76.8939)  Acc@5: 95.8333 (94.6970)  time: 0.1343  data: 0.0472  max mem: 231\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 1.1143 (1.1010)  Acc@1: 75.0000 (77.1825)  Acc@5: 95.8333 (95.2381)  time: 0.0900  data: 0.0025  max mem: 231\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 1.0189 (1.0783)  Acc@1: 75.0000 (77.5538)  Acc@5: 95.8333 (95.8333)  time: 0.0881  data: 0.0004  max mem: 231\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.9940 (1.0545)  Acc@1: 79.1667 (78.5569)  Acc@5: 95.8333 (96.4431)  time: 0.0890  data: 0.0002  max mem: 231\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.9844 (1.0482)  Acc@1: 79.1667 (78.8000)  Acc@5: 95.8333 (96.5000)  time: 0.0871  data: 0.0002  max mem: 231\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1018 s / it)\n",
            "* Acc@1 78.800 Acc@5 96.500 loss 1.048\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:18  Loss: 1.3432 (1.3432)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  time: 0.4468  data: 0.3580  max mem: 231\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:03  Loss: 1.3432 (1.3284)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (92.4242)  time: 0.1214  data: 0.0340  max mem: 231\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 1.3034 (1.3456)  Acc@1: 70.8333 (71.6270)  Acc@5: 91.6667 (92.4603)  time: 0.0884  data: 0.0014  max mem: 231\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 1.2391 (1.3299)  Acc@1: 70.8333 (71.7742)  Acc@5: 95.8333 (93.0108)  time: 0.0883  data: 0.0008  max mem: 231\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 1.2161 (1.3108)  Acc@1: 70.8333 (72.5610)  Acc@5: 95.8333 (93.8008)  time: 0.0892  data: 0.0004  max mem: 231\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 1.2076 (1.3047)  Acc@1: 75.0000 (72.6000)  Acc@5: 95.8333 (93.9000)  time: 0.0875  data: 0.0003  max mem: 231\n",
            "Test: [Task 2] Total time: 0:00:04 (0.0985 s / it)\n",
            "* Acc@1 72.600 Acc@5 93.900 loss 1.305\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:15  Loss: 0.8278 (0.8278)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.3576  data: 0.2828  max mem: 231\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 1.0751 (1.0875)  Acc@1: 75.0000 (75.3788)  Acc@5: 95.8333 (95.8333)  time: 0.1193  data: 0.0341  max mem: 231\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 1.0751 (1.0687)  Acc@1: 75.0000 (76.3889)  Acc@5: 95.8333 (95.8333)  time: 0.0921  data: 0.0048  max mem: 231\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 1.0534 (1.0441)  Acc@1: 79.1667 (77.0161)  Acc@5: 95.8333 (95.8333)  time: 0.0897  data: 0.0005  max mem: 231\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 1.0534 (1.0572)  Acc@1: 75.0000 (77.2358)  Acc@5: 95.8333 (95.6301)  time: 0.0904  data: 0.0005  max mem: 231\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 1.0534 (1.0656)  Acc@1: 75.0000 (77.1000)  Acc@5: 95.8333 (95.6000)  time: 0.0881  data: 0.0005  max mem: 231\n",
            "Test: [Task 3] Total time: 0:00:04 (0.0995 s / it)\n",
            "* Acc@1 77.100 Acc@5 95.600 loss 1.066\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:33  Loss: 1.6033 (1.6033)  Acc@1: 62.5000 (62.5000)  Acc@5: 87.5000 (87.5000)  time: 0.7877  data: 0.7010  max mem: 231\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:04  Loss: 1.1910 (1.2162)  Acc@1: 75.0000 (74.2424)  Acc@5: 95.8333 (95.0758)  time: 0.1539  data: 0.0646  max mem: 231\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 1.2221 (1.2386)  Acc@1: 75.0000 (75.1984)  Acc@5: 95.8333 (95.4365)  time: 0.0896  data: 0.0010  max mem: 231\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 1.2148 (1.2077)  Acc@1: 79.1667 (75.9409)  Acc@5: 95.8333 (95.6989)  time: 0.0889  data: 0.0006  max mem: 231\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 1.1630 (1.2227)  Acc@1: 75.0000 (75.5081)  Acc@5: 95.8333 (95.1220)  time: 0.0898  data: 0.0002  max mem: 231\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 1.1101 (1.2167)  Acc@1: 75.0000 (75.8000)  Acc@5: 95.8333 (95.1000)  time: 0.0882  data: 0.0002  max mem: 231\n",
            "Test: [Task 4] Total time: 0:00:04 (0.1083 s / it)\n",
            "* Acc@1 75.800 Acc@5 95.100 loss 1.217\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:20  Loss: 1.2185 (1.2185)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  time: 0.4980  data: 0.4270  max mem: 231\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:04  Loss: 1.2556 (1.2439)  Acc@1: 75.0000 (73.4849)  Acc@5: 95.8333 (96.9697)  time: 0.1289  data: 0.0399  max mem: 231\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 1.2694 (1.2765)  Acc@1: 70.8333 (72.6191)  Acc@5: 95.8333 (95.8333)  time: 0.0906  data: 0.0010  max mem: 231\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 1.3101 (1.2934)  Acc@1: 70.8333 (72.3118)  Acc@5: 95.8333 (95.2957)  time: 0.0895  data: 0.0005  max mem: 231\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 1.3936 (1.3426)  Acc@1: 66.6667 (69.9187)  Acc@5: 95.8333 (94.7154)  time: 0.0902  data: 0.0002  max mem: 231\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 1.3945 (1.3546)  Acc@1: 66.6667 (69.6000)  Acc@5: 91.6667 (94.6000)  time: 0.0888  data: 0.0002  max mem: 231\n",
            "Test: [Task 5] Total time: 0:00:04 (0.1015 s / it)\n",
            "* Acc@1 69.600 Acc@5 94.600 loss 1.355\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:12  Loss: 1.3071 (1.3071)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  time: 0.2935  data: 0.2237  max mem: 231\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:03  Loss: 1.5479 (1.5623)  Acc@1: 58.3333 (62.1212)  Acc@5: 95.8333 (94.3182)  time: 0.1225  data: 0.0362  max mem: 231\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 1.5480 (1.5848)  Acc@1: 58.3333 (59.3254)  Acc@5: 95.8333 (94.8413)  time: 0.0972  data: 0.0089  max mem: 231\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 1.6864 (1.6225)  Acc@1: 54.1667 (57.9301)  Acc@5: 95.8333 (94.4892)  time: 0.0901  data: 0.0003  max mem: 231\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.7053 (1.6364)  Acc@1: 54.1667 (57.9268)  Acc@5: 91.6667 (94.0041)  time: 0.0911  data: 0.0003  max mem: 231\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.7053 (1.6386)  Acc@1: 54.1667 (57.9000)  Acc@5: 93.7500 (94.0000)  time: 0.0888  data: 0.0003  max mem: 231\n",
            "Test: [Task 6] Total time: 0:00:04 (0.1012 s / it)\n",
            "* Acc@1 57.900 Acc@5 94.000 loss 1.639\n",
            "[Average accuracy till task6]\tAcc@1: 71.9667\tAcc@5: 94.9500\tLoss: 1.2714\tForgetting: 4.2600\tBackward: -2.1000\n",
            "Train: Epoch[ 1/10]  [ 0/23]  eta: 0:00:13  Lr: 0.000047  Loss: 2.4145  Acc@1: 16.6667 (16.6667)  Acc@5: 37.5000 (37.5000)  time: 0.5783  data: 0.4422  max mem: 231\n",
            "Train: Epoch[ 1/10]  [10/23]  eta: 0:00:02  Lr: 0.000047  Loss: 2.2214  Acc@1: 12.5000 (13.6364)  Acc@5: 54.1667 (53.7879)  time: 0.1832  data: 0.0777  max mem: 231\n",
            "Train: Epoch[ 1/10]  [20/23]  eta: 0:00:00  Lr: 0.000047  Loss: 1.9590  Acc@1: 12.5000 (17.0635)  Acc@5: 58.3333 (62.1032)  time: 0.1176  data: 0.0207  max mem: 231\n",
            "Train: Epoch[ 1/10]  [22/23]  eta: 0:00:00  Lr: 0.000047  Loss: 2.1494  Acc@1: 16.6667 (16.9091)  Acc@5: 62.5000 (62.3636)  time: 0.0993  data: 0.0029  max mem: 231\n",
            "Train: Epoch[ 1/10] Total time: 0:00:03 (0.1388 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 2.1494  Acc@1: 16.6667 (16.9091)  Acc@5: 62.5000 (62.3636)\n",
            "Train: Epoch[ 2/10]  [ 0/23]  eta: 0:00:10  Lr: 0.000047  Loss: 2.0759  Acc@1: 29.1667 (29.1667)  Acc@5: 66.6667 (66.6667)  time: 0.4483  data: 0.3393  max mem: 231\n",
            "Train: Epoch[ 2/10]  [10/23]  eta: 0:00:01  Lr: 0.000047  Loss: 1.8623  Acc@1: 33.3333 (33.3333)  Acc@5: 83.3333 (78.7879)  time: 0.1345  data: 0.0415  max mem: 231\n",
            "Train: Epoch[ 2/10]  [20/23]  eta: 0:00:00  Lr: 0.000047  Loss: 1.7236  Acc@1: 37.5000 (38.2937)  Acc@5: 87.5000 (81.5476)  time: 0.0967  data: 0.0059  max mem: 231\n",
            "Train: Epoch[ 2/10]  [22/23]  eta: 0:00:00  Lr: 0.000047  Loss: 1.8735  Acc@1: 37.5000 (38.3636)  Acc@5: 87.5000 (82.1818)  time: 0.0905  data: 0.0007  max mem: 231\n",
            "Train: Epoch[ 2/10] Total time: 0:00:02 (0.1147 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.8735  Acc@1: 37.5000 (38.3636)  Acc@5: 87.5000 (82.1818)\n",
            "Train: Epoch[ 3/10]  [ 0/23]  eta: 0:00:13  Lr: 0.000047  Loss: 1.6864  Acc@1: 62.5000 (62.5000)  Acc@5: 91.6667 (91.6667)  time: 0.5816  data: 0.4820  max mem: 231\n",
            "Train: Epoch[ 3/10]  [10/23]  eta: 0:00:01  Lr: 0.000047  Loss: 1.8266  Acc@1: 50.0000 (48.4848)  Acc@5: 87.5000 (90.1515)  time: 0.1351  data: 0.0444  max mem: 231\n",
            "Train: Epoch[ 3/10]  [20/23]  eta: 0:00:00  Lr: 0.000047  Loss: 1.6221  Acc@1: 50.0000 (50.9921)  Acc@5: 91.6667 (92.2619)  time: 0.0905  data: 0.0004  max mem: 231\n",
            "Train: Epoch[ 3/10]  [22/23]  eta: 0:00:00  Lr: 0.000047  Loss: 1.6474  Acc@1: 54.1667 (52.0000)  Acc@5: 95.4546 (92.3636)  time: 0.0903  data: 0.0004  max mem: 231\n",
            "Train: Epoch[ 3/10] Total time: 0:00:02 (0.1154 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.6474  Acc@1: 54.1667 (52.0000)  Acc@5: 95.4546 (92.3636)\n",
            "Train: Epoch[ 4/10]  [ 0/23]  eta: 0:00:09  Lr: 0.000047  Loss: 1.6018  Acc@1: 50.0000 (50.0000)  Acc@5: 91.6667 (91.6667)  time: 0.3928  data: 0.2932  max mem: 231\n",
            "Train: Epoch[ 4/10]  [10/23]  eta: 0:00:01  Lr: 0.000047  Loss: 1.5017  Acc@1: 62.5000 (59.0909)  Acc@5: 95.8333 (94.6970)  time: 0.1245  data: 0.0310  max mem: 231\n",
            "Train: Epoch[ 4/10]  [20/23]  eta: 0:00:00  Lr: 0.000047  Loss: 1.0960  Acc@1: 62.5000 (63.2937)  Acc@5: 95.8333 (95.6349)  time: 0.0944  data: 0.0025  max mem: 231\n",
            "Train: Epoch[ 4/10]  [22/23]  eta: 0:00:00  Lr: 0.000047  Loss: 1.5135  Acc@1: 62.5000 (63.2727)  Acc@5: 95.8333 (95.6364)  time: 0.0932  data: 0.0012  max mem: 231\n",
            "Train: Epoch[ 4/10] Total time: 0:00:02 (0.1110 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.5135  Acc@1: 62.5000 (63.2727)  Acc@5: 95.8333 (95.6364)\n",
            "Train: Epoch[ 5/10]  [ 0/23]  eta: 0:00:14  Lr: 0.000047  Loss: 1.2934  Acc@1: 66.6667 (66.6667)  Acc@5: 91.6667 (91.6667)  time: 0.6158  data: 0.5223  max mem: 231\n",
            "Train: Epoch[ 5/10]  [10/23]  eta: 0:00:01  Lr: 0.000047  Loss: 1.2354  Acc@1: 62.5000 (61.7424)  Acc@5: 95.8333 (95.8333)  time: 0.1411  data: 0.0495  max mem: 231\n",
            "Train: Epoch[ 5/10]  [20/23]  eta: 0:00:00  Lr: 0.000047  Loss: 1.6262  Acc@1: 62.5000 (62.8968)  Acc@5: 95.8333 (96.2302)  time: 0.0972  data: 0.0020  max mem: 231\n",
            "Train: Epoch[ 5/10]  [22/23]  eta: 0:00:00  Lr: 0.000047  Loss: 1.4325  Acc@1: 58.3333 (63.8182)  Acc@5: 95.8333 (96.1818)  time: 0.0965  data: 0.0019  max mem: 231\n",
            "Train: Epoch[ 5/10] Total time: 0:00:02 (0.1239 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.4325  Acc@1: 58.3333 (63.8182)  Acc@5: 95.8333 (96.1818)\n",
            "Train: Epoch[ 6/10]  [ 0/23]  eta: 0:00:17  Lr: 0.000047  Loss: 1.0855  Acc@1: 70.8333 (70.8333)  Acc@5: 100.0000 (100.0000)  time: 0.7693  data: 0.6406  max mem: 231\n",
            "Train: Epoch[ 6/10]  [10/23]  eta: 0:00:02  Lr: 0.000047  Loss: 1.3342  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (96.5909)  time: 0.1657  data: 0.0600  max mem: 231\n",
            "Train: Epoch[ 6/10]  [20/23]  eta: 0:00:00  Lr: 0.000047  Loss: 0.8832  Acc@1: 66.6667 (69.2460)  Acc@5: 95.8333 (96.8254)  time: 0.0993  data: 0.0012  max mem: 231\n",
            "Train: Epoch[ 6/10]  [22/23]  eta: 0:00:00  Lr: 0.000047  Loss: 1.0866  Acc@1: 70.8333 (69.6364)  Acc@5: 95.8333 (97.0909)  time: 0.0991  data: 0.0011  max mem: 231\n",
            "Train: Epoch[ 6/10] Total time: 0:00:03 (0.1314 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.0866  Acc@1: 70.8333 (69.6364)  Acc@5: 95.8333 (97.0909)\n",
            "Train: Epoch[ 7/10]  [ 0/23]  eta: 0:00:09  Lr: 0.000047  Loss: 1.2468  Acc@1: 66.6667 (66.6667)  Acc@5: 100.0000 (100.0000)  time: 0.4277  data: 0.3144  max mem: 231\n",
            "Train: Epoch[ 7/10]  [10/23]  eta: 0:00:01  Lr: 0.000047  Loss: 1.0955  Acc@1: 70.8333 (73.8636)  Acc@5: 100.0000 (98.4848)  time: 0.1278  data: 0.0353  max mem: 231\n",
            "Train: Epoch[ 7/10]  [20/23]  eta: 0:00:00  Lr: 0.000047  Loss: 1.0871  Acc@1: 70.8333 (73.8095)  Acc@5: 100.0000 (98.2143)  time: 0.0942  data: 0.0040  max mem: 231\n",
            "Train: Epoch[ 7/10]  [22/23]  eta: 0:00:00  Lr: 0.000047  Loss: 0.9176  Acc@1: 70.8333 (74.5455)  Acc@5: 100.0000 (98.0000)  time: 0.0905  data: 0.0007  max mem: 231\n",
            "Train: Epoch[ 7/10] Total time: 0:00:02 (0.1119 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.9176  Acc@1: 70.8333 (74.5455)  Acc@5: 100.0000 (98.0000)\n",
            "Train: Epoch[ 8/10]  [ 0/23]  eta: 0:00:10  Lr: 0.000047  Loss: 1.0819  Acc@1: 66.6667 (66.6667)  Acc@5: 100.0000 (100.0000)  time: 0.4427  data: 0.3198  max mem: 231\n",
            "Train: Epoch[ 8/10]  [10/23]  eta: 0:00:01  Lr: 0.000047  Loss: 0.6770  Acc@1: 79.1667 (77.2727)  Acc@5: 100.0000 (99.2424)  time: 0.1288  data: 0.0346  max mem: 231\n",
            "Train: Epoch[ 8/10]  [20/23]  eta: 0:00:00  Lr: 0.000047  Loss: 0.9327  Acc@1: 79.1667 (74.8016)  Acc@5: 100.0000 (98.6111)  time: 0.0935  data: 0.0031  max mem: 231\n",
            "Train: Epoch[ 8/10]  [22/23]  eta: 0:00:00  Lr: 0.000047  Loss: 0.9033  Acc@1: 75.0000 (74.1818)  Acc@5: 100.0000 (98.7273)  time: 0.0911  data: 0.0007  max mem: 231\n",
            "Train: Epoch[ 8/10] Total time: 0:00:02 (0.1121 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.9033  Acc@1: 75.0000 (74.1818)  Acc@5: 100.0000 (98.7273)\n",
            "Train: Epoch[ 9/10]  [ 0/23]  eta: 0:00:09  Lr: 0.000047  Loss: 1.0553  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.8333)  time: 0.4219  data: 0.3277  max mem: 231\n",
            "Train: Epoch[ 9/10]  [10/23]  eta: 0:00:01  Lr: 0.000047  Loss: 1.0775  Acc@1: 75.0000 (75.3788)  Acc@5: 95.8333 (97.3485)  time: 0.1240  data: 0.0315  max mem: 231\n",
            "Train: Epoch[ 9/10]  [20/23]  eta: 0:00:00  Lr: 0.000047  Loss: 0.9407  Acc@1: 75.0000 (74.6032)  Acc@5: 95.8333 (97.4206)  time: 0.0921  data: 0.0013  max mem: 231\n",
            "Train: Epoch[ 9/10]  [22/23]  eta: 0:00:00  Lr: 0.000047  Loss: 0.9156  Acc@1: 75.0000 (75.2727)  Acc@5: 95.8333 (97.4545)  time: 0.0910  data: 0.0013  max mem: 231\n",
            "Train: Epoch[ 9/10] Total time: 0:00:02 (0.1095 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.9156  Acc@1: 75.0000 (75.2727)  Acc@5: 95.8333 (97.4545)\n",
            "Train: Epoch[10/10]  [ 0/23]  eta: 0:00:13  Lr: 0.000047  Loss: 1.0037  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.5754  data: 0.4705  max mem: 231\n",
            "Train: Epoch[10/10]  [10/23]  eta: 0:00:01  Lr: 0.000047  Loss: 0.6735  Acc@1: 79.1667 (80.3030)  Acc@5: 100.0000 (97.7273)  time: 0.1368  data: 0.0436  max mem: 231\n",
            "Train: Epoch[10/10]  [20/23]  eta: 0:00:00  Lr: 0.000047  Loss: 0.7383  Acc@1: 79.1667 (80.7540)  Acc@5: 100.0000 (98.2143)  time: 0.0919  data: 0.0008  max mem: 231\n",
            "Train: Epoch[10/10]  [22/23]  eta: 0:00:00  Lr: 0.000047  Loss: 1.0589  Acc@1: 79.1667 (79.8182)  Acc@5: 100.0000 (97.8182)  time: 0.0915  data: 0.0008  max mem: 231\n",
            "Train: Epoch[10/10] Total time: 0:00:02 (0.1180 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.0589  Acc@1: 79.1667 (79.8182)  Acc@5: 100.0000 (97.8182)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:30  Loss: 1.2602 (1.2602)  Acc@1: 75.0000 (75.0000)  Acc@5: 87.5000 (87.5000)  time: 0.7227  data: 0.6278  max mem: 231\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 1.2354 (1.2195)  Acc@1: 75.0000 (72.7273)  Acc@5: 95.8333 (95.0758)  time: 0.1494  data: 0.0622  max mem: 231\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 1.2092 (1.2153)  Acc@1: 75.0000 (74.2064)  Acc@5: 95.8333 (95.4365)  time: 0.0909  data: 0.0035  max mem: 231\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 1.1191 (1.1983)  Acc@1: 75.0000 (74.4624)  Acc@5: 95.8333 (95.8333)  time: 0.0893  data: 0.0011  max mem: 231\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 1.0828 (1.1717)  Acc@1: 79.1667 (76.1179)  Acc@5: 95.8333 (96.2398)  time: 0.0892  data: 0.0007  max mem: 231\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 1.0627 (1.1650)  Acc@1: 79.1667 (76.4000)  Acc@5: 95.8333 (96.3000)  time: 0.0876  data: 0.0006  max mem: 231\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1075 s / it)\n",
            "* Acc@1 76.400 Acc@5 96.300 loss 1.165\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:28  Loss: 1.3980 (1.3980)  Acc@1: 66.6667 (66.6667)  Acc@5: 95.8333 (95.8333)  time: 0.6799  data: 0.5502  max mem: 231\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 1.3980 (1.3898)  Acc@1: 70.8333 (71.2121)  Acc@5: 95.8333 (94.6970)  time: 0.1443  data: 0.0520  max mem: 231\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 1.3661 (1.4048)  Acc@1: 75.0000 (72.6191)  Acc@5: 95.8333 (93.8492)  time: 0.0901  data: 0.0013  max mem: 231\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 1.2813 (1.3933)  Acc@1: 70.8333 (71.9086)  Acc@5: 95.8333 (93.8172)  time: 0.0897  data: 0.0007  max mem: 231\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 1.3059 (1.3797)  Acc@1: 70.8333 (72.1545)  Acc@5: 95.8333 (94.1057)  time: 0.0900  data: 0.0006  max mem: 231\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 1.2648 (1.3723)  Acc@1: 70.8333 (72.2000)  Acc@5: 95.8333 (94.2000)  time: 0.0884  data: 0.0005  max mem: 231\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1056 s / it)\n",
            "* Acc@1 72.200 Acc@5 94.200 loss 1.372\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:16  Loss: 0.8996 (0.8996)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.4029  data: 0.3157  max mem: 231\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 1.1945 (1.1674)  Acc@1: 79.1667 (76.1364)  Acc@5: 95.8333 (95.0758)  time: 0.1181  data: 0.0294  max mem: 231\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 1.1945 (1.1553)  Acc@1: 79.1667 (76.5873)  Acc@5: 95.8333 (94.4444)  time: 0.0891  data: 0.0005  max mem: 231\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 1.0942 (1.1274)  Acc@1: 79.1667 (77.0161)  Acc@5: 95.8333 (94.8925)  time: 0.0889  data: 0.0004  max mem: 231\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 1.0942 (1.1402)  Acc@1: 75.0000 (76.9309)  Acc@5: 95.8333 (94.6138)  time: 0.0897  data: 0.0004  max mem: 231\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 1.0942 (1.1474)  Acc@1: 75.0000 (76.8000)  Acc@5: 95.8333 (94.7000)  time: 0.0878  data: 0.0004  max mem: 231\n",
            "Test: [Task 3] Total time: 0:00:04 (0.0993 s / it)\n",
            "* Acc@1 76.800 Acc@5 94.700 loss 1.147\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:29  Loss: 1.8044 (1.8044)  Acc@1: 50.0000 (50.0000)  Acc@5: 83.3333 (83.3333)  time: 0.6977  data: 0.5850  max mem: 231\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:04  Loss: 1.2424 (1.2872)  Acc@1: 75.0000 (71.9697)  Acc@5: 95.8333 (93.5606)  time: 0.1479  data: 0.0587  max mem: 231\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 1.2991 (1.3102)  Acc@1: 75.0000 (72.0238)  Acc@5: 95.8333 (92.6587)  time: 0.0917  data: 0.0036  max mem: 231\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 1.3250 (1.2854)  Acc@1: 70.8333 (73.1183)  Acc@5: 95.8333 (93.1452)  time: 0.0900  data: 0.0009  max mem: 231\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 1.2378 (1.3008)  Acc@1: 75.0000 (73.3740)  Acc@5: 95.8333 (93.1911)  time: 0.0897  data: 0.0005  max mem: 231\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 1.2256 (1.2956)  Acc@1: 75.0000 (73.7000)  Acc@5: 95.8333 (93.2000)  time: 0.0882  data: 0.0003  max mem: 231\n",
            "Test: [Task 4] Total time: 0:00:04 (0.1065 s / it)\n",
            "* Acc@1 73.700 Acc@5 93.200 loss 1.296\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:15  Loss: 1.3668 (1.3668)  Acc@1: 62.5000 (62.5000)  Acc@5: 95.8333 (95.8333)  time: 0.3573  data: 0.2822  max mem: 231\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:03  Loss: 1.3582 (1.3226)  Acc@1: 70.8333 (72.7273)  Acc@5: 95.8333 (96.9697)  time: 0.1151  data: 0.0271  max mem: 231\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 1.3582 (1.3595)  Acc@1: 70.8333 (70.4365)  Acc@5: 95.8333 (95.4365)  time: 0.0899  data: 0.0009  max mem: 231\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 1.3963 (1.3778)  Acc@1: 66.6667 (70.2957)  Acc@5: 91.6667 (94.7581)  time: 0.0889  data: 0.0003  max mem: 231\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 1.4405 (1.4226)  Acc@1: 66.6667 (68.1911)  Acc@5: 91.6667 (94.2073)  time: 0.0896  data: 0.0003  max mem: 231\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 1.4702 (1.4340)  Acc@1: 66.6667 (68.1000)  Acc@5: 91.6667 (94.1000)  time: 0.0879  data: 0.0002  max mem: 231\n",
            "Test: [Task 5] Total time: 0:00:04 (0.0975 s / it)\n",
            "* Acc@1 68.100 Acc@5 94.100 loss 1.434\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:20  Loss: 1.4657 (1.4657)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  time: 0.4928  data: 0.4177  max mem: 231\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:04  Loss: 1.8290 (1.7966)  Acc@1: 50.0000 (52.2727)  Acc@5: 91.6667 (91.2879)  time: 0.1272  data: 0.0391  max mem: 231\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 1.8290 (1.8226)  Acc@1: 45.8333 (51.1905)  Acc@5: 91.6667 (91.0714)  time: 0.0897  data: 0.0010  max mem: 231\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 1.9470 (1.8659)  Acc@1: 45.8333 (50.0000)  Acc@5: 91.6667 (90.4570)  time: 0.0892  data: 0.0005  max mem: 231\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.9542 (1.8753)  Acc@1: 50.0000 (50.2033)  Acc@5: 87.5000 (90.3455)  time: 0.0899  data: 0.0003  max mem: 231\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.9470 (1.8765)  Acc@1: 50.0000 (50.2000)  Acc@5: 87.5000 (90.3000)  time: 0.0881  data: 0.0003  max mem: 231\n",
            "Test: [Task 6] Total time: 0:00:04 (0.1012 s / it)\n",
            "* Acc@1 50.200 Acc@5 90.300 loss 1.877\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:30  Loss: 3.1037 (3.1037)  Acc@1: 12.5000 (12.5000)  Acc@5: 66.6667 (66.6667)  time: 0.7373  data: 0.6682  max mem: 231\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:04  Loss: 3.0780 (3.0816)  Acc@1: 12.5000 (12.5000)  Acc@5: 66.6667 (61.7424)  time: 0.1511  data: 0.0649  max mem: 231\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:02  Loss: 3.0151 (3.0570)  Acc@1: 12.5000 (14.6825)  Acc@5: 62.5000 (61.3095)  time: 0.0912  data: 0.0031  max mem: 231\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:01  Loss: 3.0151 (3.0678)  Acc@1: 16.6667 (14.9194)  Acc@5: 62.5000 (60.0806)  time: 0.0896  data: 0.0015  max mem: 231\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 3.0030 (3.0639)  Acc@1: 12.5000 (15.1423)  Acc@5: 62.5000 (61.3821)  time: 0.0895  data: 0.0008  max mem: 231\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 3.0030 (3.0570)  Acc@1: 16.6667 (15.4000)  Acc@5: 62.5000 (61.4000)  time: 0.0883  data: 0.0007  max mem: 231\n",
            "Test: [Task 7] Total time: 0:00:04 (0.1071 s / it)\n",
            "* Acc@1 15.400 Acc@5 61.400 loss 3.057\n",
            "[Average accuracy till task7]\tAcc@1: 61.8286\tAcc@5: 89.1714\tLoss: 1.6211\tForgetting: 3.4500\tBackward: 22.0000\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([14736, 384])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 1.1855  Acc@1: 83.3333 (80.0000)  Acc@5: 95.8333 (95.9722)\n",
            "torch.Size([14736, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 1.2063  Acc@1: 83.3333 (83.8889)  Acc@5: 100.0000 (97.7778)\n",
            "torch.Size([14736, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 0.8950  Acc@1: 87.5000 (85.0000)  Acc@5: 100.0000 (98.5417)\n",
            "torch.Size([14736, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 0.7967  Acc@1: 87.5000 (88.9583)  Acc@5: 100.0000 (99.3056)\n",
            "torch.Size([14736, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 0.6150  Acc@1: 91.6667 (89.5833)  Acc@5: 100.0000 (99.2361)\n",
            "torch.Size([14736, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 0.8911  Acc@1: 91.6667 (90.7639)  Acc@5: 100.0000 (99.5139)\n",
            "torch.Size([14736, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.7627  Acc@1: 91.6667 (89.7917)  Acc@5: 100.0000 (99.3750)\n",
            "torch.Size([14736, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 0.6739  Acc@1: 91.6667 (91.7361)  Acc@5: 100.0000 (99.5139)\n",
            "torch.Size([14736, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 0.8436  Acc@1: 91.6667 (90.8333)  Acc@5: 100.0000 (99.4444)\n",
            "torch.Size([14736, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.7441  Acc@1: 87.5000 (90.7639)  Acc@5: 100.0000 (99.0972)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:14  Loss: 1.0764 (1.0764)  Acc@1: 75.0000 (75.0000)  Acc@5: 87.5000 (87.5000)  time: 0.3336  data: 0.2579  max mem: 231\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 1.0764 (1.0480)  Acc@1: 75.0000 (75.7576)  Acc@5: 95.8333 (94.6970)  time: 0.1157  data: 0.0295  max mem: 232\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 1.0627 (1.0358)  Acc@1: 79.1667 (77.7778)  Acc@5: 95.8333 (95.0397)  time: 0.0909  data: 0.0035  max mem: 232\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.9333 (1.0101)  Acc@1: 79.1667 (78.0914)  Acc@5: 95.8333 (95.4301)  time: 0.0883  data: 0.0003  max mem: 232\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.9038 (0.9856)  Acc@1: 79.1667 (78.9634)  Acc@5: 95.8333 (96.1382)  time: 0.0894  data: 0.0002  max mem: 232\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.8990 (0.9794)  Acc@1: 83.3333 (79.2000)  Acc@5: 95.8333 (96.2000)  time: 0.0873  data: 0.0002  max mem: 232\n",
            "Test: [Task 1] Total time: 0:00:04 (0.0980 s / it)\n",
            "* Acc@1 79.200 Acc@5 96.200 loss 0.979\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:28  Loss: 1.2891 (1.2891)  Acc@1: 62.5000 (62.5000)  Acc@5: 95.8333 (95.8333)  time: 0.6797  data: 0.5844  max mem: 232\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 1.2891 (1.2974)  Acc@1: 66.6667 (68.1818)  Acc@5: 91.6667 (92.4242)  time: 0.1465  data: 0.0598  max mem: 232\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 1.2613 (1.3147)  Acc@1: 66.6667 (69.0476)  Acc@5: 91.6667 (91.8651)  time: 0.0930  data: 0.0066  max mem: 232\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 1.1778 (1.3014)  Acc@1: 66.6667 (68.8172)  Acc@5: 95.8333 (92.6075)  time: 0.0912  data: 0.0034  max mem: 232\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 1.1857 (1.2816)  Acc@1: 70.8333 (70.0203)  Acc@5: 95.8333 (93.2927)  time: 0.0900  data: 0.0006  max mem: 232\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 1.1751 (1.2752)  Acc@1: 70.8333 (70.0000)  Acc@5: 95.8333 (93.4000)  time: 0.0887  data: 0.0006  max mem: 232\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1071 s / it)\n",
            "* Acc@1 70.000 Acc@5 93.400 loss 1.275\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:20  Loss: 0.8012 (0.8012)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.4972  data: 0.4163  max mem: 232\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:04  Loss: 1.0324 (1.0554)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.4545)  time: 0.1273  data: 0.0386  max mem: 232\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 1.0600 (1.0343)  Acc@1: 75.0000 (75.9921)  Acc@5: 95.8333 (95.2381)  time: 0.0895  data: 0.0006  max mem: 232\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.9941 (1.0103)  Acc@1: 75.0000 (76.0753)  Acc@5: 95.8333 (95.1613)  time: 0.0888  data: 0.0003  max mem: 232\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 1.0046 (1.0260)  Acc@1: 75.0000 (76.3211)  Acc@5: 95.8333 (94.9187)  time: 0.0895  data: 0.0002  max mem: 232\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 1.0046 (1.0338)  Acc@1: 75.0000 (76.2000)  Acc@5: 95.8333 (94.9000)  time: 0.0878  data: 0.0002  max mem: 232\n",
            "Test: [Task 3] Total time: 0:00:04 (0.1005 s / it)\n",
            "* Acc@1 76.200 Acc@5 94.900 loss 1.034\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:15  Loss: 1.5029 (1.5029)  Acc@1: 62.5000 (62.5000)  Acc@5: 83.3333 (83.3333)  time: 0.3626  data: 0.2849  max mem: 232\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:03  Loss: 1.1374 (1.1471)  Acc@1: 75.0000 (72.7273)  Acc@5: 91.6667 (92.8030)  time: 0.1175  data: 0.0294  max mem: 232\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 1.1630 (1.1679)  Acc@1: 75.0000 (73.6111)  Acc@5: 91.6667 (93.6508)  time: 0.0912  data: 0.0022  max mem: 232\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 1.1474 (1.1294)  Acc@1: 75.0000 (74.0591)  Acc@5: 95.8333 (94.2204)  time: 0.0898  data: 0.0005  max mem: 232\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 1.0881 (1.1482)  Acc@1: 70.8333 (73.7805)  Acc@5: 95.8333 (93.8008)  time: 0.0909  data: 0.0008  max mem: 232\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 1.0371 (1.1424)  Acc@1: 70.8333 (74.1000)  Acc@5: 95.8333 (93.8000)  time: 0.0891  data: 0.0008  max mem: 232\n",
            "Test: [Task 4] Total time: 0:00:04 (0.1000 s / it)\n",
            "* Acc@1 74.100 Acc@5 93.800 loss 1.142\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:28  Loss: 1.1637 (1.1637)  Acc@1: 66.6667 (66.6667)  Acc@5: 95.8333 (95.8333)  time: 0.6706  data: 0.5729  max mem: 232\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:04  Loss: 1.1637 (1.1678)  Acc@1: 70.8333 (72.7273)  Acc@5: 95.8333 (95.8333)  time: 0.1432  data: 0.0526  max mem: 232\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 1.1969 (1.2067)  Acc@1: 70.8333 (71.4286)  Acc@5: 91.6667 (94.8413)  time: 0.0904  data: 0.0005  max mem: 232\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 1.2223 (1.2248)  Acc@1: 66.6667 (71.6398)  Acc@5: 91.6667 (94.3548)  time: 0.0907  data: 0.0008  max mem: 232\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 1.2904 (1.2710)  Acc@1: 66.6667 (70.2236)  Acc@5: 91.6667 (93.9024)  time: 0.0911  data: 0.0007  max mem: 232\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 1.3120 (1.2820)  Acc@1: 66.6667 (69.9000)  Acc@5: 91.6667 (93.8000)  time: 0.0891  data: 0.0006  max mem: 232\n",
            "Test: [Task 5] Total time: 0:00:04 (0.1063 s / it)\n",
            "* Acc@1 69.900 Acc@5 93.800 loss 1.282\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:20  Loss: 1.0532 (1.0532)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  time: 0.4836  data: 0.3963  max mem: 232\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:04  Loss: 1.2856 (1.3210)  Acc@1: 70.8333 (68.9394)  Acc@5: 95.8333 (94.6970)  time: 0.1267  data: 0.0365  max mem: 232\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 1.3212 (1.3335)  Acc@1: 66.6667 (67.4603)  Acc@5: 95.8333 (95.0397)  time: 0.0905  data: 0.0004  max mem: 232\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 1.4290 (1.3710)  Acc@1: 66.6667 (66.6667)  Acc@5: 95.8333 (94.7581)  time: 0.0901  data: 0.0003  max mem: 232\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.4464 (1.3866)  Acc@1: 66.6667 (66.5650)  Acc@5: 95.8333 (94.8171)  time: 0.0907  data: 0.0003  max mem: 232\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.4464 (1.3889)  Acc@1: 66.6667 (66.6000)  Acc@5: 95.8333 (94.8000)  time: 0.0888  data: 0.0003  max mem: 232\n",
            "Test: [Task 6] Total time: 0:00:04 (0.1014 s / it)\n",
            "* Acc@1 66.600 Acc@5 94.800 loss 1.389\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:16  Loss: 1.6830 (1.6830)  Acc@1: 54.1667 (54.1667)  Acc@5: 87.5000 (87.5000)  time: 0.3918  data: 0.3082  max mem: 232\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:03  Loss: 1.6127 (1.6441)  Acc@1: 58.3333 (59.4697)  Acc@5: 87.5000 (89.7727)  time: 0.1192  data: 0.0299  max mem: 232\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:02  Loss: 1.5397 (1.6365)  Acc@1: 62.5000 (59.1270)  Acc@5: 91.6667 (89.8810)  time: 0.0913  data: 0.0012  max mem: 232\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:01  Loss: 1.6048 (1.6574)  Acc@1: 62.5000 (59.5430)  Acc@5: 91.6667 (89.5161)  time: 0.0905  data: 0.0003  max mem: 232\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 1.5872 (1.6537)  Acc@1: 62.5000 (60.6707)  Acc@5: 87.5000 (89.6341)  time: 0.0907  data: 0.0003  max mem: 232\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 1.5872 (1.6505)  Acc@1: 62.5000 (60.9000)  Acc@5: 87.5000 (89.6000)  time: 0.0890  data: 0.0002  max mem: 232\n",
            "Test: [Task 7] Total time: 0:00:04 (0.0997 s / it)\n",
            "* Acc@1 60.900 Acc@5 89.600 loss 1.650\n",
            "[Average accuracy till task7]\tAcc@1: 70.9857\tAcc@5: 93.7857\tLoss: 1.2503\tForgetting: 4.3500\tBackward: -1.0500\n",
            "Train: Epoch[ 1/10]  [ 0/22]  eta: 0:00:21  Lr: 0.000047  Loss: 2.4181  Acc@1: 12.5000 (12.5000)  Acc@5: 45.8333 (45.8333)  time: 0.9626  data: 0.8447  max mem: 232\n",
            "Train: Epoch[ 1/10]  [10/22]  eta: 0:00:02  Lr: 0.000047  Loss: 2.2072  Acc@1: 12.5000 (14.3939)  Acc@5: 66.6667 (65.5303)  time: 0.1805  data: 0.0784  max mem: 232\n",
            "Train: Epoch[ 1/10]  [20/22]  eta: 0:00:00  Lr: 0.000047  Loss: 2.0770  Acc@1: 20.8333 (20.0397)  Acc@5: 70.8333 (71.0317)  time: 0.0986  data: 0.0014  max mem: 232\n",
            "Train: Epoch[ 1/10]  [21/22]  eta: 0:00:00  Lr: 0.000047  Loss: 1.8362  Acc@1: 20.8333 (20.5479)  Acc@5: 75.0000 (71.4286)  time: 0.0952  data: 0.0011  max mem: 232\n",
            "Train: Epoch[ 1/10] Total time: 0:00:03 (0.1399 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.8362  Acc@1: 20.8333 (20.5479)  Acc@5: 75.0000 (71.4286)\n",
            "Train: Epoch[ 2/10]  [ 0/22]  eta: 0:00:13  Lr: 0.000047  Loss: 2.0916  Acc@1: 33.3333 (33.3333)  Acc@5: 70.8333 (70.8333)  time: 0.6004  data: 0.4612  max mem: 232\n",
            "Train: Epoch[ 2/10]  [10/22]  eta: 0:00:01  Lr: 0.000047  Loss: 1.8959  Acc@1: 37.5000 (39.0152)  Acc@5: 83.3333 (82.9545)  time: 0.1463  data: 0.0497  max mem: 232\n",
            "Train: Epoch[ 2/10]  [20/22]  eta: 0:00:00  Lr: 0.000047  Loss: 1.7590  Acc@1: 41.6667 (43.0556)  Acc@5: 87.5000 (85.9127)  time: 0.0959  data: 0.0044  max mem: 232\n",
            "Train: Epoch[ 2/10]  [21/22]  eta: 0:00:00  Lr: 0.000047  Loss: 1.9272  Acc@1: 42.8571 (43.0528)  Acc@5: 87.5000 (86.1057)  time: 0.0898  data: 0.0006  max mem: 232\n",
            "Train: Epoch[ 2/10] Total time: 0:00:02 (0.1198 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.9272  Acc@1: 42.8571 (43.0528)  Acc@5: 87.5000 (86.1057)\n",
            "Train: Epoch[ 3/10]  [ 0/22]  eta: 0:00:09  Lr: 0.000047  Loss: 1.8752  Acc@1: 45.8333 (45.8333)  Acc@5: 83.3333 (83.3333)  time: 0.4168  data: 0.3130  max mem: 232\n",
            "Train: Epoch[ 3/10]  [10/22]  eta: 0:00:01  Lr: 0.000047  Loss: 1.6455  Acc@1: 54.1667 (55.6818)  Acc@5: 87.5000 (87.8788)  time: 0.1269  data: 0.0332  max mem: 232\n",
            "Train: Epoch[ 3/10]  [20/22]  eta: 0:00:00  Lr: 0.000047  Loss: 1.4557  Acc@1: 54.1667 (57.1429)  Acc@5: 91.6667 (91.4683)  time: 0.0941  data: 0.0029  max mem: 232\n",
            "Train: Epoch[ 3/10]  [21/22]  eta: 0:00:00  Lr: 0.000047  Loss: 1.2487  Acc@1: 54.1667 (57.7299)  Acc@5: 91.6667 (91.5851)  time: 0.0895  data: 0.0009  max mem: 232\n",
            "Train: Epoch[ 3/10] Total time: 0:00:02 (0.1095 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.2487  Acc@1: 54.1667 (57.7299)  Acc@5: 91.6667 (91.5851)\n",
            "Train: Epoch[ 4/10]  [ 0/22]  eta: 0:00:08  Lr: 0.000047  Loss: 1.6112  Acc@1: 58.3333 (58.3333)  Acc@5: 95.8333 (95.8333)  time: 0.3864  data: 0.2985  max mem: 232\n",
            "Train: Epoch[ 4/10]  [10/22]  eta: 0:00:01  Lr: 0.000047  Loss: 1.4583  Acc@1: 62.5000 (62.8788)  Acc@5: 95.8333 (96.5909)  time: 0.1242  data: 0.0305  max mem: 232\n",
            "Train: Epoch[ 4/10]  [20/22]  eta: 0:00:00  Lr: 0.000047  Loss: 1.5030  Acc@1: 62.5000 (61.7064)  Acc@5: 95.8333 (96.0317)  time: 0.0946  data: 0.0021  max mem: 232\n",
            "Train: Epoch[ 4/10]  [21/22]  eta: 0:00:00  Lr: 0.000047  Loss: 1.6572  Acc@1: 62.5000 (61.6438)  Acc@5: 95.8333 (96.0861)  time: 0.0906  data: 0.0007  max mem: 232\n",
            "Train: Epoch[ 4/10] Total time: 0:00:02 (0.1088 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.6572  Acc@1: 62.5000 (61.6438)  Acc@5: 95.8333 (96.0861)\n",
            "Train: Epoch[ 5/10]  [ 0/22]  eta: 0:00:16  Lr: 0.000047  Loss: 1.4657  Acc@1: 62.5000 (62.5000)  Acc@5: 100.0000 (100.0000)  time: 0.7526  data: 0.6480  max mem: 232\n",
            "Train: Epoch[ 5/10]  [10/22]  eta: 0:00:01  Lr: 0.000047  Loss: 1.3005  Acc@1: 70.8333 (70.4545)  Acc@5: 95.8333 (94.3182)  time: 0.1533  data: 0.0598  max mem: 232\n",
            "Train: Epoch[ 5/10]  [20/22]  eta: 0:00:00  Lr: 0.000047  Loss: 1.2042  Acc@1: 70.8333 (70.6349)  Acc@5: 95.8333 (96.6270)  time: 0.0919  data: 0.0007  max mem: 232\n",
            "Train: Epoch[ 5/10]  [21/22]  eta: 0:00:00  Lr: 0.000047  Loss: 0.9508  Acc@1: 70.8333 (71.0372)  Acc@5: 100.0000 (96.6732)  time: 0.0889  data: 0.0007  max mem: 232\n",
            "Train: Epoch[ 5/10] Total time: 0:00:02 (0.1228 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.9508  Acc@1: 70.8333 (71.0372)  Acc@5: 100.0000 (96.6732)\n",
            "Train: Epoch[ 6/10]  [ 0/22]  eta: 0:00:09  Lr: 0.000047  Loss: 1.1646  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  time: 0.4193  data: 0.3072  max mem: 232\n",
            "Train: Epoch[ 6/10]  [10/22]  eta: 0:00:01  Lr: 0.000047  Loss: 1.3170  Acc@1: 75.0000 (73.8636)  Acc@5: 95.8333 (95.8333)  time: 0.1409  data: 0.0395  max mem: 232\n",
            "Train: Epoch[ 6/10]  [20/22]  eta: 0:00:00  Lr: 0.000047  Loss: 1.0333  Acc@1: 75.0000 (73.4127)  Acc@5: 100.0000 (96.8254)  time: 0.1092  data: 0.0119  max mem: 232\n",
            "Train: Epoch[ 6/10]  [21/22]  eta: 0:00:00  Lr: 0.000047  Loss: 1.1611  Acc@1: 75.0000 (73.3855)  Acc@5: 100.0000 (96.8689)  time: 0.1037  data: 0.0097  max mem: 232\n",
            "Train: Epoch[ 6/10] Total time: 0:00:02 (0.1255 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.1611  Acc@1: 75.0000 (73.3855)  Acc@5: 100.0000 (96.8689)\n",
            "Train: Epoch[ 7/10]  [ 0/22]  eta: 0:00:17  Lr: 0.000047  Loss: 1.1080  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  time: 0.7875  data: 0.6676  max mem: 232\n",
            "Train: Epoch[ 7/10]  [10/22]  eta: 0:00:01  Lr: 0.000047  Loss: 1.1476  Acc@1: 75.0000 (74.2424)  Acc@5: 95.8333 (96.5909)  time: 0.1627  data: 0.0622  max mem: 232\n",
            "Train: Epoch[ 7/10]  [20/22]  eta: 0:00:00  Lr: 0.000047  Loss: 1.1587  Acc@1: 75.0000 (74.8016)  Acc@5: 95.8333 (97.0238)  time: 0.0958  data: 0.0011  max mem: 232\n",
            "Train: Epoch[ 7/10]  [21/22]  eta: 0:00:00  Lr: 0.000047  Loss: 0.8263  Acc@1: 75.0000 (74.9511)  Acc@5: 95.8333 (97.0646)  time: 0.0929  data: 0.0011  max mem: 232\n",
            "Train: Epoch[ 7/10] Total time: 0:00:02 (0.1282 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.8263  Acc@1: 75.0000 (74.9511)  Acc@5: 95.8333 (97.0646)\n",
            "Train: Epoch[ 8/10]  [ 0/22]  eta: 0:00:10  Lr: 0.000047  Loss: 1.1671  Acc@1: 75.0000 (75.0000)  Acc@5: 83.3333 (83.3333)  time: 0.4921  data: 0.3832  max mem: 232\n",
            "Train: Epoch[ 8/10]  [10/22]  eta: 0:00:01  Lr: 0.000047  Loss: 0.9831  Acc@1: 79.1667 (79.5455)  Acc@5: 100.0000 (96.5909)  time: 0.1292  data: 0.0357  max mem: 232\n",
            "Train: Epoch[ 8/10]  [20/22]  eta: 0:00:00  Lr: 0.000047  Loss: 1.1186  Acc@1: 79.1667 (78.7698)  Acc@5: 100.0000 (97.4206)  time: 0.0917  data: 0.0007  max mem: 232\n",
            "Train: Epoch[ 8/10]  [21/22]  eta: 0:00:00  Lr: 0.000047  Loss: 1.0638  Acc@1: 75.0000 (78.6693)  Acc@5: 100.0000 (97.4560)  time: 0.0884  data: 0.0007  max mem: 232\n",
            "Train: Epoch[ 8/10] Total time: 0:00:02 (0.1111 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.0638  Acc@1: 75.0000 (78.6693)  Acc@5: 100.0000 (97.4560)\n",
            "Train: Epoch[ 9/10]  [ 0/22]  eta: 0:00:08  Lr: 0.000047  Loss: 0.7422  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.4035  data: 0.3045  max mem: 232\n",
            "Train: Epoch[ 9/10]  [10/22]  eta: 0:00:01  Lr: 0.000047  Loss: 0.9266  Acc@1: 79.1667 (80.6818)  Acc@5: 100.0000 (96.5909)  time: 0.1312  data: 0.0392  max mem: 232\n",
            "Train: Epoch[ 9/10]  [20/22]  eta: 0:00:00  Lr: 0.000047  Loss: 0.7535  Acc@1: 79.1667 (80.7540)  Acc@5: 95.8333 (97.2222)  time: 0.0966  data: 0.0065  max mem: 232\n",
            "Train: Epoch[ 9/10]  [21/22]  eta: 0:00:00  Lr: 0.000047  Loss: 0.6466  Acc@1: 83.3333 (80.8219)  Acc@5: 95.8333 (97.2603)  time: 0.0881  data: 0.0007  max mem: 232\n",
            "Train: Epoch[ 9/10] Total time: 0:00:02 (0.1114 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.6466  Acc@1: 83.3333 (80.8219)  Acc@5: 95.8333 (97.2603)\n",
            "Train: Epoch[10/10]  [ 0/22]  eta: 0:00:11  Lr: 0.000047  Loss: 0.7472  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.5351  data: 0.4376  max mem: 232\n",
            "Train: Epoch[10/10]  [10/22]  eta: 0:00:01  Lr: 0.000047  Loss: 0.7676  Acc@1: 79.1667 (80.6818)  Acc@5: 100.0000 (98.4848)  time: 0.1327  data: 0.0406  max mem: 232\n",
            "Train: Epoch[10/10]  [20/22]  eta: 0:00:00  Lr: 0.000047  Loss: 0.9630  Acc@1: 83.3333 (80.5556)  Acc@5: 100.0000 (98.2143)  time: 0.0912  data: 0.0005  max mem: 232\n",
            "Train: Epoch[10/10]  [21/22]  eta: 0:00:00  Lr: 0.000047  Loss: 0.7389  Acc@1: 83.3333 (80.6262)  Acc@5: 100.0000 (98.2387)  time: 0.0882  data: 0.0005  max mem: 232\n",
            "Train: Epoch[10/10] Total time: 0:00:02 (0.1122 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.7389  Acc@1: 83.3333 (80.6262)  Acc@5: 100.0000 (98.2387)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:14  Loss: 1.1823 (1.1823)  Acc@1: 75.0000 (75.0000)  Acc@5: 87.5000 (87.5000)  time: 0.3488  data: 0.2695  max mem: 232\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 1.1534 (1.1399)  Acc@1: 75.0000 (74.2424)  Acc@5: 95.8333 (95.8333)  time: 0.1161  data: 0.0320  max mem: 232\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 1.1198 (1.1255)  Acc@1: 79.1667 (76.5873)  Acc@5: 95.8333 (95.6349)  time: 0.0917  data: 0.0046  max mem: 232\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 1.0103 (1.1020)  Acc@1: 79.1667 (76.4785)  Acc@5: 95.8333 (95.5645)  time: 0.0908  data: 0.0016  max mem: 232\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 1.0022 (1.0779)  Acc@1: 79.1667 (78.1504)  Acc@5: 95.8333 (96.0366)  time: 0.0908  data: 0.0011  max mem: 232\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.9683 (1.0721)  Acc@1: 79.1667 (78.4000)  Acc@5: 95.8333 (96.1000)  time: 0.0892  data: 0.0010  max mem: 232\n",
            "Test: [Task 1] Total time: 0:00:04 (0.0999 s / it)\n",
            "* Acc@1 78.400 Acc@5 96.100 loss 1.072\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:29  Loss: 1.3281 (1.3281)  Acc@1: 66.6667 (66.6667)  Acc@5: 95.8333 (95.8333)  time: 0.7089  data: 0.6179  max mem: 232\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 1.3678 (1.3562)  Acc@1: 66.6667 (66.6667)  Acc@5: 91.6667 (92.8030)  time: 0.1457  data: 0.0569  max mem: 232\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 1.3678 (1.3728)  Acc@1: 70.8333 (68.4524)  Acc@5: 87.5000 (91.8651)  time: 0.0888  data: 0.0006  max mem: 232\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 1.2646 (1.3700)  Acc@1: 66.6667 (68.4140)  Acc@5: 95.8333 (92.3387)  time: 0.0884  data: 0.0003  max mem: 232\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 1.2729 (1.3567)  Acc@1: 66.6667 (68.9024)  Acc@5: 95.8333 (92.7846)  time: 0.0893  data: 0.0002  max mem: 232\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 1.2299 (1.3506)  Acc@1: 68.7500 (68.9000)  Acc@5: 95.8333 (92.9000)  time: 0.0875  data: 0.0002  max mem: 232\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1052 s / it)\n",
            "* Acc@1 68.900 Acc@5 92.900 loss 1.351\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:20  Loss: 0.8200 (0.8200)  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  time: 0.4779  data: 0.3942  max mem: 232\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 1.0366 (1.0832)  Acc@1: 79.1667 (76.1364)  Acc@5: 95.8333 (95.0758)  time: 0.1248  data: 0.0363  max mem: 232\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 1.0640 (1.0758)  Acc@1: 79.1667 (76.9841)  Acc@5: 95.8333 (93.8492)  time: 0.0889  data: 0.0004  max mem: 232\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.9773 (1.0476)  Acc@1: 79.1667 (77.4194)  Acc@5: 95.8333 (94.4892)  time: 0.0888  data: 0.0003  max mem: 232\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 1.0286 (1.0635)  Acc@1: 75.0000 (77.3374)  Acc@5: 95.8333 (94.3089)  time: 0.0898  data: 0.0002  max mem: 232\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 1.0286 (1.0702)  Acc@1: 75.0000 (77.3000)  Acc@5: 95.8333 (94.3000)  time: 0.0878  data: 0.0002  max mem: 232\n",
            "Test: [Task 3] Total time: 0:00:04 (0.0999 s / it)\n",
            "* Acc@1 77.300 Acc@5 94.300 loss 1.070\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:19  Loss: 1.5813 (1.5813)  Acc@1: 50.0000 (50.0000)  Acc@5: 83.3333 (83.3333)  time: 0.4625  data: 0.3867  max mem: 232\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:03  Loss: 1.1585 (1.1880)  Acc@1: 75.0000 (71.2121)  Acc@5: 95.8333 (94.3182)  time: 0.1239  data: 0.0360  max mem: 232\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 1.1776 (1.2179)  Acc@1: 75.0000 (71.8254)  Acc@5: 95.8333 (93.6508)  time: 0.0892  data: 0.0007  max mem: 232\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 1.2437 (1.1867)  Acc@1: 75.0000 (72.7151)  Acc@5: 95.8333 (94.0860)  time: 0.0890  data: 0.0006  max mem: 232\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 1.1341 (1.2033)  Acc@1: 70.8333 (72.8659)  Acc@5: 95.8333 (93.6992)  time: 0.0896  data: 0.0008  max mem: 232\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 1.1299 (1.1963)  Acc@1: 70.8333 (73.2000)  Acc@5: 95.8333 (93.7000)  time: 0.0877  data: 0.0008  max mem: 232\n",
            "Test: [Task 4] Total time: 0:00:04 (0.1006 s / it)\n",
            "* Acc@1 73.200 Acc@5 93.700 loss 1.196\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:30  Loss: 1.2700 (1.2700)  Acc@1: 58.3333 (58.3333)  Acc@5: 91.6667 (91.6667)  time: 0.7378  data: 0.6611  max mem: 232\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:04  Loss: 1.2062 (1.2221)  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (96.2121)  time: 0.1509  data: 0.0654  max mem: 232\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 1.2509 (1.2624)  Acc@1: 70.8333 (69.4444)  Acc@5: 95.8333 (94.6429)  time: 0.0907  data: 0.0036  max mem: 232\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 1.2829 (1.2796)  Acc@1: 70.8333 (69.8925)  Acc@5: 91.6667 (94.0860)  time: 0.0893  data: 0.0009  max mem: 232\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 1.3263 (1.3295)  Acc@1: 66.6667 (67.8862)  Acc@5: 91.6667 (93.4959)  time: 0.0896  data: 0.0003  max mem: 232\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 1.3359 (1.3415)  Acc@1: 62.5000 (67.6000)  Acc@5: 91.6667 (93.3000)  time: 0.0879  data: 0.0003  max mem: 232\n",
            "Test: [Task 5] Total time: 0:00:04 (0.1069 s / it)\n",
            "* Acc@1 67.600 Acc@5 93.300 loss 1.341\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:12  Loss: 1.1364 (1.1364)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  time: 0.2941  data: 0.2215  max mem: 232\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:03  Loss: 1.4341 (1.4392)  Acc@1: 66.6667 (64.7727)  Acc@5: 91.6667 (93.5606)  time: 0.1219  data: 0.0370  max mem: 232\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 1.4643 (1.4609)  Acc@1: 66.6667 (64.0873)  Acc@5: 91.6667 (93.6508)  time: 0.0966  data: 0.0094  max mem: 232\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 1.5846 (1.5074)  Acc@1: 58.3333 (62.9032)  Acc@5: 91.6667 (92.7419)  time: 0.0894  data: 0.0003  max mem: 232\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.5985 (1.5160)  Acc@1: 58.3333 (63.4146)  Acc@5: 91.6667 (92.6829)  time: 0.0903  data: 0.0002  max mem: 232\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.5846 (1.5174)  Acc@1: 58.3333 (63.4000)  Acc@5: 91.6667 (92.6000)  time: 0.0878  data: 0.0002  max mem: 232\n",
            "Test: [Task 6] Total time: 0:00:04 (0.0993 s / it)\n",
            "* Acc@1 63.400 Acc@5 92.600 loss 1.517\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:20  Loss: 1.9593 (1.9593)  Acc@1: 50.0000 (50.0000)  Acc@5: 83.3333 (83.3333)  time: 0.4926  data: 0.4149  max mem: 232\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:04  Loss: 1.8146 (1.8788)  Acc@1: 50.0000 (53.0303)  Acc@5: 87.5000 (86.3636)  time: 0.1273  data: 0.0384  max mem: 232\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:02  Loss: 1.7816 (1.8533)  Acc@1: 54.1667 (54.5635)  Acc@5: 87.5000 (86.1111)  time: 0.0892  data: 0.0005  max mem: 232\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:01  Loss: 1.8212 (1.8711)  Acc@1: 54.1667 (54.0323)  Acc@5: 83.3333 (84.9462)  time: 0.0884  data: 0.0004  max mem: 232\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 1.8212 (1.8706)  Acc@1: 58.3333 (55.1829)  Acc@5: 87.5000 (85.6707)  time: 0.0894  data: 0.0004  max mem: 232\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 1.8212 (1.8672)  Acc@1: 58.3333 (55.5000)  Acc@5: 83.3333 (85.6000)  time: 0.0878  data: 0.0004  max mem: 232\n",
            "Test: [Task 7] Total time: 0:00:04 (0.1018 s / it)\n",
            "* Acc@1 55.500 Acc@5 85.600 loss 1.867\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:30  Loss: 3.2393 (3.2393)  Acc@1: 0.0000 (0.0000)  Acc@5: 54.1667 (54.1667)  time: 0.7267  data: 0.6466  max mem: 232\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:04  Loss: 3.3120 (3.3098)  Acc@1: 8.3333 (6.8182)  Acc@5: 54.1667 (56.0606)  time: 0.1477  data: 0.0608  max mem: 232\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:02  Loss: 3.2884 (3.2777)  Acc@1: 8.3333 (7.7381)  Acc@5: 50.0000 (54.7619)  time: 0.0897  data: 0.0015  max mem: 232\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:01  Loss: 3.2311 (3.2826)  Acc@1: 8.3333 (8.0645)  Acc@5: 50.0000 (52.9570)  time: 0.0896  data: 0.0009  max mem: 232\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 3.2311 (3.2902)  Acc@1: 8.3333 (8.4350)  Acc@5: 50.0000 (52.4390)  time: 0.0899  data: 0.0007  max mem: 232\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 3.2311 (3.2885)  Acc@1: 8.3333 (8.6000)  Acc@5: 50.0000 (52.7000)  time: 0.0884  data: 0.0007  max mem: 232\n",
            "Test: [Task 8] Total time: 0:00:04 (0.1065 s / it)\n",
            "* Acc@1 8.600 Acc@5 52.700 loss 3.289\n",
            "[Average accuracy till task8]\tAcc@1: 61.6125\tAcc@5: 87.6500\tLoss: 1.5880\tForgetting: 3.2857\tBackward: 26.2143\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([16872, 384])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 0.8848  Acc@1: 79.1667 (79.1071)  Acc@5: 95.8333 (94.8810)\n",
            "torch.Size([16872, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 0.7595  Acc@1: 87.5000 (84.1667)  Acc@5: 100.0000 (97.9762)\n",
            "torch.Size([16872, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 1.0061  Acc@1: 87.5000 (85.8929)  Acc@5: 100.0000 (98.9286)\n",
            "torch.Size([16872, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 0.6499  Acc@1: 87.5000 (86.7857)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([16872, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 0.4500  Acc@1: 91.6667 (88.6310)  Acc@5: 100.0000 (99.2857)\n",
            "torch.Size([16872, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 0.8221  Acc@1: 91.6667 (90.3571)  Acc@5: 100.0000 (99.4643)\n",
            "torch.Size([16872, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.9663  Acc@1: 91.6667 (90.3571)  Acc@5: 100.0000 (99.5238)\n",
            "torch.Size([16872, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 0.8375  Acc@1: 87.5000 (90.4167)  Acc@5: 100.0000 (99.5238)\n",
            "torch.Size([16872, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 0.6791  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (99.5238)\n",
            "torch.Size([16872, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.8372  Acc@1: 87.5000 (90.8929)  Acc@5: 100.0000 (99.8214)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:17  Loss: 0.9882 (0.9882)  Acc@1: 75.0000 (75.0000)  Acc@5: 83.3333 (83.3333)  time: 0.4192  data: 0.3312  max mem: 232\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 1.0257 (1.0199)  Acc@1: 75.0000 (73.4849)  Acc@5: 95.8333 (94.6970)  time: 0.1354  data: 0.0510  max mem: 232\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 1.0024 (0.9992)  Acc@1: 75.0000 (76.1905)  Acc@5: 95.8333 (94.8413)  time: 0.0983  data: 0.0120  max mem: 232\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.8942 (0.9707)  Acc@1: 79.1667 (76.8817)  Acc@5: 95.8333 (95.2957)  time: 0.0891  data: 0.0021  max mem: 232\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.8790 (0.9470)  Acc@1: 79.1667 (78.4553)  Acc@5: 95.8333 (96.0366)  time: 0.0889  data: 0.0018  max mem: 232\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.8610 (0.9404)  Acc@1: 83.3333 (78.7000)  Acc@5: 95.8333 (96.1000)  time: 0.0878  data: 0.0017  max mem: 232\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1030 s / it)\n",
            "* Acc@1 78.700 Acc@5 96.100 loss 0.940\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:14  Loss: 1.2347 (1.2347)  Acc@1: 62.5000 (62.5000)  Acc@5: 95.8333 (95.8333)  time: 0.3398  data: 0.2640  max mem: 232\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:03  Loss: 1.2347 (1.2327)  Acc@1: 66.6667 (66.2879)  Acc@5: 91.6667 (93.1818)  time: 0.1194  data: 0.0351  max mem: 232\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 1.2174 (1.2474)  Acc@1: 70.8333 (68.4524)  Acc@5: 91.6667 (92.2619)  time: 0.0931  data: 0.0064  max mem: 232\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 1.1058 (1.2381)  Acc@1: 70.8333 (68.8172)  Acc@5: 95.8333 (92.4731)  time: 0.0898  data: 0.0004  max mem: 232\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 1.1305 (1.2228)  Acc@1: 70.8333 (69.4106)  Acc@5: 95.8333 (92.8862)  time: 0.0908  data: 0.0003  max mem: 232\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 1.0869 (1.2173)  Acc@1: 70.8333 (69.4000)  Acc@5: 95.8333 (93.0000)  time: 0.0887  data: 0.0003  max mem: 232\n",
            "Test: [Task 2] Total time: 0:00:04 (0.0991 s / it)\n",
            "* Acc@1 69.400 Acc@5 93.000 loss 1.217\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:17  Loss: 0.7433 (0.7433)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.4267  data: 0.3577  max mem: 232\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 0.9835 (1.0316)  Acc@1: 79.1667 (75.3788)  Acc@5: 95.8333 (95.4545)  time: 0.1242  data: 0.0371  max mem: 232\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 1.0462 (1.0206)  Acc@1: 75.0000 (75.9921)  Acc@5: 95.8333 (94.2460)  time: 0.0917  data: 0.0027  max mem: 232\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.9808 (0.9973)  Acc@1: 75.0000 (75.8065)  Acc@5: 95.8333 (94.4892)  time: 0.0901  data: 0.0004  max mem: 232\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 1.0001 (1.0134)  Acc@1: 75.0000 (76.0163)  Acc@5: 95.8333 (94.3089)  time: 0.0912  data: 0.0003  max mem: 232\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 1.0001 (1.0197)  Acc@1: 75.0000 (75.9000)  Acc@5: 95.8333 (94.3000)  time: 0.0893  data: 0.0003  max mem: 232\n",
            "Test: [Task 3] Total time: 0:00:04 (0.1006 s / it)\n",
            "* Acc@1 75.900 Acc@5 94.300 loss 1.020\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:22  Loss: 1.4204 (1.4204)  Acc@1: 66.6667 (66.6667)  Acc@5: 83.3333 (83.3333)  time: 0.5277  data: 0.4531  max mem: 232\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:04  Loss: 1.1183 (1.1254)  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (93.1818)  time: 0.1315  data: 0.0419  max mem: 232\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 1.1285 (1.1497)  Acc@1: 70.8333 (71.6270)  Acc@5: 95.8333 (93.6508)  time: 0.0913  data: 0.0011  max mem: 232\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 1.1148 (1.1100)  Acc@1: 70.8333 (72.1774)  Acc@5: 95.8333 (94.0860)  time: 0.0907  data: 0.0011  max mem: 232\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 1.1043 (1.1242)  Acc@1: 70.8333 (71.5447)  Acc@5: 95.8333 (93.5976)  time: 0.0910  data: 0.0005  max mem: 232\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 1.0452 (1.1176)  Acc@1: 70.8333 (71.9000)  Acc@5: 95.8333 (93.6000)  time: 0.0891  data: 0.0005  max mem: 232\n",
            "Test: [Task 4] Total time: 0:00:04 (0.1041 s / it)\n",
            "* Acc@1 71.900 Acc@5 93.600 loss 1.118\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:26  Loss: 1.0673 (1.0673)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  time: 0.6329  data: 0.5302  max mem: 232\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:04  Loss: 1.0536 (1.0729)  Acc@1: 75.0000 (74.6212)  Acc@5: 95.8333 (95.8333)  time: 0.1411  data: 0.0494  max mem: 232\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 1.1028 (1.1156)  Acc@1: 70.8333 (74.4048)  Acc@5: 91.6667 (94.4444)  time: 0.0918  data: 0.0009  max mem: 232\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 1.1557 (1.1336)  Acc@1: 70.8333 (74.0591)  Acc@5: 95.8333 (94.3548)  time: 0.0919  data: 0.0003  max mem: 232\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 1.1919 (1.1839)  Acc@1: 66.6667 (72.2561)  Acc@5: 91.6667 (93.6992)  time: 0.0922  data: 0.0002  max mem: 232\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 1.2000 (1.1969)  Acc@1: 66.6667 (71.9000)  Acc@5: 91.6667 (93.6000)  time: 0.0905  data: 0.0002  max mem: 232\n",
            "Test: [Task 5] Total time: 0:00:04 (0.1062 s / it)\n",
            "* Acc@1 71.900 Acc@5 93.600 loss 1.197\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:15  Loss: 1.0234 (1.0234)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  time: 0.3660  data: 0.2966  max mem: 232\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:03  Loss: 1.2580 (1.2901)  Acc@1: 62.5000 (63.6364)  Acc@5: 95.8333 (93.9394)  time: 0.1216  data: 0.0341  max mem: 232\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 1.2757 (1.2994)  Acc@1: 62.5000 (64.2857)  Acc@5: 95.8333 (94.4444)  time: 0.0945  data: 0.0041  max mem: 232\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 1.4089 (1.3365)  Acc@1: 58.3333 (63.4409)  Acc@5: 95.8333 (93.8172)  time: 0.0918  data: 0.0003  max mem: 232\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.4155 (1.3506)  Acc@1: 66.6667 (63.7195)  Acc@5: 91.6667 (93.8008)  time: 0.0919  data: 0.0003  max mem: 232\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.4163 (1.3536)  Acc@1: 62.5000 (63.7000)  Acc@5: 91.6667 (93.7000)  time: 0.0902  data: 0.0002  max mem: 232\n",
            "Test: [Task 6] Total time: 0:00:04 (0.1010 s / it)\n",
            "* Acc@1 63.700 Acc@5 93.700 loss 1.354\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:20  Loss: 1.5234 (1.5234)  Acc@1: 58.3333 (58.3333)  Acc@5: 87.5000 (87.5000)  time: 0.4789  data: 0.4040  max mem: 232\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:04  Loss: 1.4447 (1.4437)  Acc@1: 62.5000 (63.6364)  Acc@5: 95.8333 (93.1818)  time: 0.1281  data: 0.0377  max mem: 232\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:02  Loss: 1.3655 (1.4438)  Acc@1: 62.5000 (62.6984)  Acc@5: 95.8333 (92.8571)  time: 0.0921  data: 0.0014  max mem: 232\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:01  Loss: 1.3694 (1.4648)  Acc@1: 62.5000 (62.9032)  Acc@5: 91.6667 (91.8011)  time: 0.0915  data: 0.0019  max mem: 232\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 1.4134 (1.4634)  Acc@1: 66.6667 (63.6179)  Acc@5: 91.6667 (91.2602)  time: 0.0920  data: 0.0011  max mem: 232\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 1.4134 (1.4613)  Acc@1: 66.6667 (63.8000)  Acc@5: 91.6667 (91.2000)  time: 0.0902  data: 0.0010  max mem: 232\n",
            "Test: [Task 7] Total time: 0:00:04 (0.1036 s / it)\n",
            "* Acc@1 63.800 Acc@5 91.200 loss 1.461\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:29  Loss: 1.6067 (1.6067)  Acc@1: 37.5000 (37.5000)  Acc@5: 95.8333 (95.8333)  time: 0.6984  data: 0.6204  max mem: 232\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:04  Loss: 1.6442 (1.6953)  Acc@1: 54.1667 (54.5455)  Acc@5: 91.6667 (89.3939)  time: 0.1470  data: 0.0596  max mem: 232\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:02  Loss: 1.6076 (1.6486)  Acc@1: 54.1667 (56.3492)  Acc@5: 91.6667 (89.6825)  time: 0.0914  data: 0.0020  max mem: 232\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:01  Loss: 1.5865 (1.6309)  Acc@1: 58.3333 (56.1828)  Acc@5: 91.6667 (91.5323)  time: 0.0910  data: 0.0004  max mem: 232\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 1.6321 (1.6503)  Acc@1: 54.1667 (55.6911)  Acc@5: 91.6667 (90.2439)  time: 0.0911  data: 0.0003  max mem: 232\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 1.6321 (1.6450)  Acc@1: 54.1667 (55.9000)  Acc@5: 91.6667 (90.3000)  time: 0.0896  data: 0.0003  max mem: 232\n",
            "Test: [Task 8] Total time: 0:00:04 (0.1070 s / it)\n",
            "* Acc@1 55.900 Acc@5 90.300 loss 1.645\n",
            "[Average accuracy till task8]\tAcc@1: 68.9000\tAcc@5: 93.2250\tLoss: 1.2440\tForgetting: 4.6571\tBackward: -1.1286\n",
            "Train: Epoch[ 1/10]  [ 0/21]  eta: 0:00:10  Lr: 0.000047  Loss: 2.2897  Acc@1: 20.8333 (20.8333)  Acc@5: 54.1667 (54.1667)  time: 0.5077  data: 0.4048  max mem: 232\n",
            "Train: Epoch[ 1/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 2.1783  Acc@1: 16.6667 (16.6667)  Acc@5: 58.3333 (57.5758)  time: 0.1317  data: 0.0376  max mem: 232\n",
            "Train: Epoch[ 1/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 2.1361  Acc@1: 16.6667 (17.6353)  Acc@5: 63.1579 (62.3247)  time: 0.0919  data: 0.0006  max mem: 232\n",
            "Train: Epoch[ 1/10] Total time: 0:00:02 (0.1153 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 2.1361  Acc@1: 16.6667 (17.6353)  Acc@5: 63.1579 (62.3247)\n",
            "Train: Epoch[ 2/10]  [ 0/21]  eta: 0:00:11  Lr: 0.000047  Loss: 2.0544  Acc@1: 41.6667 (41.6667)  Acc@5: 75.0000 (75.0000)  time: 0.5492  data: 0.4250  max mem: 232\n",
            "Train: Epoch[ 2/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.9159  Acc@1: 33.3333 (33.3333)  Acc@5: 75.0000 (79.1667)  time: 0.1332  data: 0.0399  max mem: 232\n",
            "Train: Epoch[ 2/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 1.8553  Acc@1: 37.5000 (39.2786)  Acc@5: 83.3333 (82.5651)  time: 0.0902  data: 0.0007  max mem: 232\n",
            "Train: Epoch[ 2/10] Total time: 0:00:02 (0.1161 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.8553  Acc@1: 37.5000 (39.2786)  Acc@5: 83.3333 (82.5651)\n",
            "Train: Epoch[ 3/10]  [ 0/21]  eta: 0:00:12  Lr: 0.000047  Loss: 1.9887  Acc@1: 45.8333 (45.8333)  Acc@5: 83.3333 (83.3333)  time: 0.5997  data: 0.4863  max mem: 232\n",
            "Train: Epoch[ 3/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.7574  Acc@1: 50.0000 (51.1364)  Acc@5: 95.8333 (93.9394)  time: 0.1371  data: 0.0450  max mem: 232\n",
            "Train: Epoch[ 3/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 1.6106  Acc@1: 50.0000 (52.3046)  Acc@5: 95.8333 (94.5892)  time: 0.0900  data: 0.0005  max mem: 232\n",
            "Train: Epoch[ 3/10] Total time: 0:00:02 (0.1197 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.6106  Acc@1: 50.0000 (52.3046)  Acc@5: 95.8333 (94.5892)\n",
            "Train: Epoch[ 4/10]  [ 0/21]  eta: 0:00:17  Lr: 0.000047  Loss: 1.6162  Acc@1: 66.6667 (66.6667)  Acc@5: 91.6667 (91.6667)  time: 0.8441  data: 0.6868  max mem: 232\n",
            "Train: Epoch[ 4/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.4566  Acc@1: 66.6667 (62.8788)  Acc@5: 95.8333 (95.4545)  time: 0.1699  data: 0.0630  max mem: 232\n",
            "Train: Epoch[ 4/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 1.4335  Acc@1: 66.6667 (63.9279)  Acc@5: 95.8333 (96.1924)  time: 0.1006  data: 0.0036  max mem: 232\n",
            "Train: Epoch[ 4/10] Total time: 0:00:02 (0.1410 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.4335  Acc@1: 66.6667 (63.9279)  Acc@5: 95.8333 (96.1924)\n",
            "Train: Epoch[ 5/10]  [ 0/21]  eta: 0:00:10  Lr: 0.000047  Loss: 1.4525  Acc@1: 54.1667 (54.1667)  Acc@5: 95.8333 (95.8333)  time: 0.5059  data: 0.3872  max mem: 232\n",
            "Train: Epoch[ 5/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.2531  Acc@1: 62.5000 (64.7727)  Acc@5: 95.8333 (95.8333)  time: 0.1472  data: 0.0501  max mem: 232\n",
            "Train: Epoch[ 5/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 1.2833  Acc@1: 70.8333 (66.5331)  Acc@5: 95.8333 (96.5932)  time: 0.1001  data: 0.0084  max mem: 232\n",
            "Train: Epoch[ 5/10] Total time: 0:00:02 (0.1234 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.2833  Acc@1: 70.8333 (66.5331)  Acc@5: 95.8333 (96.5932)\n",
            "Train: Epoch[ 6/10]  [ 0/21]  eta: 0:00:09  Lr: 0.000047  Loss: 1.1710  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.4556  data: 0.3396  max mem: 232\n",
            "Train: Epoch[ 6/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.2794  Acc@1: 79.1667 (77.6515)  Acc@5: 95.8333 (95.8333)  time: 0.1274  data: 0.0322  max mem: 232\n",
            "Train: Epoch[ 6/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 1.1776  Acc@1: 78.9474 (74.9499)  Acc@5: 95.8333 (96.5932)  time: 0.0913  data: 0.0008  max mem: 232\n",
            "Train: Epoch[ 6/10] Total time: 0:00:02 (0.1126 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.1776  Acc@1: 78.9474 (74.9499)  Acc@5: 95.8333 (96.5932)\n",
            "Train: Epoch[ 7/10]  [ 0/21]  eta: 0:00:10  Lr: 0.000047  Loss: 1.2225  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  time: 0.5130  data: 0.4114  max mem: 232\n",
            "Train: Epoch[ 7/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.0506  Acc@1: 79.1667 (75.7576)  Acc@5: 95.8333 (96.9697)  time: 0.1303  data: 0.0394  max mem: 232\n",
            "Train: Epoch[ 7/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 0.8494  Acc@1: 79.1667 (77.1543)  Acc@5: 95.8333 (96.7936)  time: 0.0896  data: 0.0013  max mem: 232\n",
            "Train: Epoch[ 7/10] Total time: 0:00:02 (0.1146 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.8494  Acc@1: 79.1667 (77.1543)  Acc@5: 95.8333 (96.7936)\n",
            "Train: Epoch[ 8/10]  [ 0/21]  eta: 0:00:11  Lr: 0.000047  Loss: 1.1211  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  time: 0.5634  data: 0.4662  max mem: 232\n",
            "Train: Epoch[ 8/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.3106  Acc@1: 79.1667 (80.3030)  Acc@5: 95.8333 (96.5909)  time: 0.1343  data: 0.0432  max mem: 232\n",
            "Train: Epoch[ 8/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 0.8460  Acc@1: 83.3333 (80.9619)  Acc@5: 100.0000 (97.3948)  time: 0.0890  data: 0.0006  max mem: 232\n",
            "Train: Epoch[ 8/10] Total time: 0:00:02 (0.1177 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.8460  Acc@1: 83.3333 (80.9619)  Acc@5: 100.0000 (97.3948)\n",
            "Train: Epoch[ 9/10]  [ 0/21]  eta: 0:00:19  Lr: 0.000047  Loss: 0.8970  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.9318  data: 0.7461  max mem: 232\n",
            "Train: Epoch[ 9/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.0756  Acc@1: 83.3333 (82.5758)  Acc@5: 100.0000 (99.2424)  time: 0.1805  data: 0.0692  max mem: 232\n",
            "Train: Epoch[ 9/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 0.6587  Acc@1: 83.3333 (81.7635)  Acc@5: 100.0000 (98.3968)  time: 0.0994  data: 0.0018  max mem: 232\n",
            "Train: Epoch[ 9/10] Total time: 0:00:03 (0.1450 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.6587  Acc@1: 83.3333 (81.7635)  Acc@5: 100.0000 (98.3968)\n",
            "Train: Epoch[10/10]  [ 0/21]  eta: 0:00:15  Lr: 0.000047  Loss: 0.6978  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  time: 0.7412  data: 0.6408  max mem: 232\n",
            "Train: Epoch[10/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 0.9587  Acc@1: 79.1667 (81.8182)  Acc@5: 100.0000 (98.1061)  time: 0.1522  data: 0.0594  max mem: 232\n",
            "Train: Epoch[10/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 0.9490  Acc@1: 79.1667 (80.7615)  Acc@5: 100.0000 (97.9960)  time: 0.0905  data: 0.0009  max mem: 232\n",
            "Train: Epoch[10/10] Total time: 0:00:02 (0.1254 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.9490  Acc@1: 79.1667 (80.7615)  Acc@5: 100.0000 (97.9960)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:13  Loss: 1.0473 (1.0473)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  time: 0.3298  data: 0.2522  max mem: 232\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 1.1117 (1.0943)  Acc@1: 75.0000 (73.8636)  Acc@5: 95.8333 (92.8030)  time: 0.1149  data: 0.0315  max mem: 232\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 1.0009 (1.0638)  Acc@1: 75.0000 (76.5873)  Acc@5: 95.8333 (93.4524)  time: 0.0914  data: 0.0050  max mem: 232\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.9936 (1.0359)  Acc@1: 75.0000 (77.0161)  Acc@5: 95.8333 (94.3548)  time: 0.0896  data: 0.0004  max mem: 232\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.9334 (1.0122)  Acc@1: 79.1667 (78.1504)  Acc@5: 95.8333 (95.0203)  time: 0.0897  data: 0.0003  max mem: 232\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.8869 (1.0060)  Acc@1: 83.3333 (78.4000)  Acc@5: 95.8333 (95.1000)  time: 0.0882  data: 0.0003  max mem: 232\n",
            "Test: [Task 1] Total time: 0:00:04 (0.0974 s / it)\n",
            "* Acc@1 78.400 Acc@5 95.100 loss 1.006\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:21  Loss: 1.3801 (1.3801)  Acc@1: 58.3333 (58.3333)  Acc@5: 91.6667 (91.6667)  time: 0.5174  data: 0.4232  max mem: 232\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 1.4492 (1.3892)  Acc@1: 66.6667 (62.8788)  Acc@5: 91.6667 (91.2879)  time: 0.1295  data: 0.0391  max mem: 232\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 1.4492 (1.4030)  Acc@1: 66.6667 (64.8810)  Acc@5: 91.6667 (90.8730)  time: 0.0893  data: 0.0007  max mem: 232\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 1.3257 (1.3923)  Acc@1: 66.6667 (65.1882)  Acc@5: 91.6667 (91.2634)  time: 0.0882  data: 0.0006  max mem: 232\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 1.2586 (1.3766)  Acc@1: 66.6667 (65.3455)  Acc@5: 91.6667 (91.9715)  time: 0.0890  data: 0.0004  max mem: 232\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 1.2287 (1.3713)  Acc@1: 66.6667 (65.4000)  Acc@5: 91.6667 (92.0000)  time: 0.0872  data: 0.0004  max mem: 232\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1014 s / it)\n",
            "* Acc@1 65.400 Acc@5 92.000 loss 1.371\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:32  Loss: 0.8254 (0.8254)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.7703  data: 0.6878  max mem: 232\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:04  Loss: 1.0983 (1.1131)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (93.1818)  time: 0.1523  data: 0.0636  max mem: 232\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 1.1078 (1.0917)  Acc@1: 75.0000 (75.5952)  Acc@5: 95.8333 (93.6508)  time: 0.0900  data: 0.0023  max mem: 232\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 1.0431 (1.0673)  Acc@1: 75.0000 (75.9409)  Acc@5: 95.8333 (93.6828)  time: 0.0891  data: 0.0022  max mem: 232\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 1.0681 (1.0810)  Acc@1: 75.0000 (76.4228)  Acc@5: 91.6667 (93.5976)  time: 0.0890  data: 0.0006  max mem: 232\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 1.0681 (1.0885)  Acc@1: 75.0000 (76.1000)  Acc@5: 91.6667 (93.5000)  time: 0.0877  data: 0.0006  max mem: 232\n",
            "Test: [Task 3] Total time: 0:00:04 (0.1072 s / it)\n",
            "* Acc@1 76.100 Acc@5 93.500 loss 1.088\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:14  Loss: 1.3945 (1.3945)  Acc@1: 62.5000 (62.5000)  Acc@5: 83.3333 (83.3333)  time: 0.3546  data: 0.2846  max mem: 232\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:03  Loss: 1.0987 (1.1285)  Acc@1: 75.0000 (72.7273)  Acc@5: 95.8333 (93.5606)  time: 0.1208  data: 0.0365  max mem: 232\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 1.1437 (1.1563)  Acc@1: 75.0000 (73.0159)  Acc@5: 95.8333 (93.8492)  time: 0.0933  data: 0.0061  max mem: 232\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 1.1362 (1.1184)  Acc@1: 75.0000 (74.3280)  Acc@5: 95.8333 (94.3548)  time: 0.0898  data: 0.0004  max mem: 232\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 1.0731 (1.1319)  Acc@1: 75.0000 (74.2886)  Acc@5: 95.8333 (93.9024)  time: 0.0903  data: 0.0003  max mem: 232\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 1.0312 (1.1253)  Acc@1: 75.0000 (74.5000)  Acc@5: 95.8333 (94.0000)  time: 0.0883  data: 0.0003  max mem: 232\n",
            "Test: [Task 4] Total time: 0:00:04 (0.0993 s / it)\n",
            "* Acc@1 74.500 Acc@5 94.000 loss 1.125\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:20  Loss: 1.1358 (1.1358)  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.8333)  time: 0.4865  data: 0.4145  max mem: 232\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:04  Loss: 1.1560 (1.1453)  Acc@1: 70.8333 (73.4849)  Acc@5: 95.8333 (95.0758)  time: 0.1265  data: 0.0385  max mem: 232\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 1.1976 (1.1776)  Acc@1: 70.8333 (72.4206)  Acc@5: 91.6667 (94.0476)  time: 0.0892  data: 0.0006  max mem: 232\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 1.1668 (1.1940)  Acc@1: 70.8333 (72.0430)  Acc@5: 91.6667 (94.2204)  time: 0.0882  data: 0.0003  max mem: 232\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 1.2096 (1.2493)  Acc@1: 66.6667 (70.1220)  Acc@5: 95.8333 (93.5976)  time: 0.0893  data: 0.0003  max mem: 232\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 1.3051 (1.2627)  Acc@1: 66.6667 (69.8000)  Acc@5: 91.6667 (93.3000)  time: 0.0875  data: 0.0003  max mem: 232\n",
            "Test: [Task 5] Total time: 0:00:04 (0.0999 s / it)\n",
            "* Acc@1 69.800 Acc@5 93.300 loss 1.263\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:17  Loss: 1.0664 (1.0664)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  time: 0.4104  data: 0.3315  max mem: 232\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:04  Loss: 1.2318 (1.2722)  Acc@1: 66.6667 (68.1818)  Acc@5: 95.8333 (95.0758)  time: 0.1400  data: 0.0567  max mem: 232\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 1.2419 (1.2800)  Acc@1: 66.6667 (68.4524)  Acc@5: 95.8333 (96.0317)  time: 0.1017  data: 0.0150  max mem: 232\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 1.3869 (1.3171)  Acc@1: 66.6667 (66.5323)  Acc@5: 95.8333 (95.6989)  time: 0.0903  data: 0.0009  max mem: 232\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.3908 (1.3382)  Acc@1: 66.6667 (66.9715)  Acc@5: 95.8333 (95.0203)  time: 0.0895  data: 0.0006  max mem: 232\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.4027 (1.3401)  Acc@1: 62.5000 (66.9000)  Acc@5: 95.8333 (95.0000)  time: 0.0884  data: 0.0006  max mem: 232\n",
            "Test: [Task 6] Total time: 0:00:04 (0.1048 s / it)\n",
            "* Acc@1 66.900 Acc@5 95.000 loss 1.340\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:13  Loss: 1.5479 (1.5479)  Acc@1: 62.5000 (62.5000)  Acc@5: 87.5000 (87.5000)  time: 0.3252  data: 0.2479  max mem: 232\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:03  Loss: 1.5033 (1.5010)  Acc@1: 62.5000 (63.2576)  Acc@5: 95.8333 (93.1818)  time: 0.1219  data: 0.0352  max mem: 232\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:02  Loss: 1.4378 (1.4959)  Acc@1: 62.5000 (63.2937)  Acc@5: 91.6667 (93.2540)  time: 0.0948  data: 0.0071  max mem: 232\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:01  Loss: 1.4378 (1.5153)  Acc@1: 62.5000 (63.0376)  Acc@5: 91.6667 (91.9355)  time: 0.0893  data: 0.0003  max mem: 232\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 1.4775 (1.5099)  Acc@1: 66.6667 (63.8211)  Acc@5: 91.6667 (91.8699)  time: 0.0906  data: 0.0003  max mem: 232\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 1.4775 (1.5077)  Acc@1: 66.6667 (63.9000)  Acc@5: 91.6667 (91.8000)  time: 0.0880  data: 0.0003  max mem: 232\n",
            "Test: [Task 7] Total time: 0:00:04 (0.0995 s / it)\n",
            "* Acc@1 63.900 Acc@5 91.800 loss 1.508\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:20  Loss: 1.7911 (1.7911)  Acc@1: 37.5000 (37.5000)  Acc@5: 95.8333 (95.8333)  time: 0.4917  data: 0.4090  max mem: 232\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:04  Loss: 1.8361 (1.8855)  Acc@1: 45.8333 (46.5909)  Acc@5: 91.6667 (87.8788)  time: 0.1272  data: 0.0376  max mem: 232\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:02  Loss: 1.8147 (1.8398)  Acc@1: 45.8333 (48.8095)  Acc@5: 87.5000 (88.0952)  time: 0.0898  data: 0.0005  max mem: 232\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:01  Loss: 1.7563 (1.8181)  Acc@1: 50.0000 (49.0591)  Acc@5: 91.6667 (89.1129)  time: 0.0894  data: 0.0004  max mem: 232\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 1.7683 (1.8304)  Acc@1: 50.0000 (48.9837)  Acc@5: 87.5000 (87.7033)  time: 0.0902  data: 0.0002  max mem: 232\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 1.7683 (1.8246)  Acc@1: 50.0000 (49.3000)  Acc@5: 87.5000 (87.8000)  time: 0.0886  data: 0.0002  max mem: 232\n",
            "Test: [Task 8] Total time: 0:00:04 (0.1009 s / it)\n",
            "* Acc@1 49.300 Acc@5 87.800 loss 1.825\n",
            "Test: [Task 9]  [ 0/42]  eta: 0:00:15  Loss: 3.6040 (3.6040)  Acc@1: 0.0000 (0.0000)  Acc@5: 25.0000 (25.0000)  time: 0.3690  data: 0.2950  max mem: 232\n",
            "Test: [Task 9]  [10/42]  eta: 0:00:03  Loss: 3.5837 (3.5622)  Acc@1: 0.0000 (0.3788)  Acc@5: 37.5000 (37.8788)  time: 0.1193  data: 0.0335  max mem: 232\n",
            "Test: [Task 9]  [20/42]  eta: 0:00:02  Loss: 3.5687 (3.5529)  Acc@1: 0.0000 (0.9921)  Acc@5: 37.5000 (38.0952)  time: 0.0922  data: 0.0043  max mem: 232\n",
            "Test: [Task 9]  [30/42]  eta: 0:00:01  Loss: 3.5784 (3.5774)  Acc@1: 0.0000 (1.2097)  Acc@5: 37.5000 (36.8280)  time: 0.0904  data: 0.0011  max mem: 232\n",
            "Test: [Task 9]  [40/42]  eta: 0:00:00  Loss: 3.4872 (3.5517)  Acc@1: 0.0000 (1.1179)  Acc@5: 41.6667 (37.9065)  time: 0.0906  data: 0.0006  max mem: 232\n",
            "Test: [Task 9]  [41/42]  eta: 0:00:00  Loss: 3.4584 (3.5426)  Acc@1: 0.0000 (1.1000)  Acc@5: 41.6667 (38.3000)  time: 0.0888  data: 0.0006  max mem: 232\n",
            "Test: [Task 9] Total time: 0:00:04 (0.1001 s / it)\n",
            "* Acc@1 1.100 Acc@5 38.300 loss 3.543\n",
            "[Average accuracy till task9]\tAcc@1: 60.6000\tAcc@5: 86.7556\tLoss: 1.5632\tForgetting: 3.3375\tBackward: 29.3625\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([18936, 384])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 0.9279  Acc@1: 79.1667 (80.8333)  Acc@5: 95.8333 (95.3646)\n",
            "torch.Size([18936, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 0.8315  Acc@1: 83.3333 (84.2708)  Acc@5: 100.0000 (98.0729)\n",
            "torch.Size([18936, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 0.6492  Acc@1: 83.3333 (85.0000)  Acc@5: 100.0000 (98.7500)\n",
            "torch.Size([18936, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 0.7191  Acc@1: 87.5000 (88.1771)  Acc@5: 100.0000 (98.6979)\n",
            "torch.Size([18936, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 0.7061  Acc@1: 91.6667 (87.8125)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([18936, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 0.5955  Acc@1: 91.6667 (89.0625)  Acc@5: 100.0000 (99.2188)\n",
            "torch.Size([18936, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.7003  Acc@1: 91.6667 (91.0938)  Acc@5: 100.0000 (99.3229)\n",
            "torch.Size([18936, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 0.8349  Acc@1: 91.6667 (91.9271)  Acc@5: 100.0000 (99.4271)\n",
            "torch.Size([18936, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 0.7572  Acc@1: 91.6667 (91.3021)  Acc@5: 100.0000 (99.2188)\n",
            "torch.Size([18936, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.7535  Acc@1: 87.5000 (91.1458)  Acc@5: 100.0000 (99.3750)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:25  Loss: 1.0428 (1.0428)  Acc@1: 70.8333 (70.8333)  Acc@5: 83.3333 (83.3333)  time: 0.6024  data: 0.5246  max mem: 233\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 1.0678 (1.0747)  Acc@1: 70.8333 (71.9697)  Acc@5: 91.6667 (92.4242)  time: 0.1373  data: 0.0490  max mem: 233\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 1.0185 (1.0444)  Acc@1: 75.0000 (74.0079)  Acc@5: 95.8333 (93.0556)  time: 0.0893  data: 0.0016  max mem: 233\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.9249 (1.0136)  Acc@1: 79.1667 (75.4032)  Acc@5: 95.8333 (94.0860)  time: 0.0885  data: 0.0018  max mem: 233\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.8858 (0.9850)  Acc@1: 79.1667 (76.5244)  Acc@5: 95.8333 (94.8171)  time: 0.0895  data: 0.0013  max mem: 233\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.8819 (0.9774)  Acc@1: 79.1667 (76.8000)  Acc@5: 95.8333 (94.9000)  time: 0.0880  data: 0.0013  max mem: 233\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1032 s / it)\n",
            "* Acc@1 76.800 Acc@5 94.900 loss 0.977\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:14  Loss: 1.2378 (1.2378)  Acc@1: 62.5000 (62.5000)  Acc@5: 95.8333 (95.8333)  time: 0.3434  data: 0.2721  max mem: 233\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:03  Loss: 1.2378 (1.2277)  Acc@1: 66.6667 (66.2879)  Acc@5: 95.8333 (92.8030)  time: 0.1155  data: 0.0283  max mem: 233\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 1.2037 (1.2506)  Acc@1: 66.6667 (67.6587)  Acc@5: 91.6667 (91.8651)  time: 0.0910  data: 0.0021  max mem: 233\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 1.1785 (1.2358)  Acc@1: 66.6667 (69.3548)  Acc@5: 91.6667 (91.9355)  time: 0.0896  data: 0.0003  max mem: 233\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 1.1133 (1.2206)  Acc@1: 70.8333 (69.8171)  Acc@5: 95.8333 (92.6829)  time: 0.0900  data: 0.0002  max mem: 233\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 1.0685 (1.2169)  Acc@1: 70.8333 (69.9000)  Acc@5: 95.8333 (92.8000)  time: 0.0884  data: 0.0002  max mem: 233\n",
            "Test: [Task 2] Total time: 0:00:04 (0.0978 s / it)\n",
            "* Acc@1 69.900 Acc@5 92.800 loss 1.217\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:14  Loss: 0.7738 (0.7738)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.3498  data: 0.2735  max mem: 233\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 0.9999 (1.0546)  Acc@1: 75.0000 (75.3788)  Acc@5: 95.8333 (94.3182)  time: 0.1227  data: 0.0365  max mem: 233\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 1.0671 (1.0422)  Acc@1: 75.0000 (75.9921)  Acc@5: 95.8333 (93.8492)  time: 0.0950  data: 0.0065  max mem: 233\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.9916 (1.0161)  Acc@1: 75.0000 (75.8065)  Acc@5: 95.8333 (93.9516)  time: 0.0907  data: 0.0003  max mem: 233\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.9916 (1.0279)  Acc@1: 75.0000 (76.0163)  Acc@5: 91.6667 (93.5976)  time: 0.0916  data: 0.0003  max mem: 233\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.9916 (1.0327)  Acc@1: 75.0000 (75.9000)  Acc@5: 91.6667 (93.6000)  time: 0.0894  data: 0.0003  max mem: 233\n",
            "Test: [Task 3] Total time: 0:00:04 (0.1009 s / it)\n",
            "* Acc@1 75.900 Acc@5 93.600 loss 1.033\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:19  Loss: 1.4117 (1.4117)  Acc@1: 66.6667 (66.6667)  Acc@5: 83.3333 (83.3333)  time: 0.4683  data: 0.3849  max mem: 233\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:04  Loss: 1.0940 (1.1338)  Acc@1: 70.8333 (70.4545)  Acc@5: 95.8333 (92.8030)  time: 0.1250  data: 0.0358  max mem: 233\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 1.1361 (1.1636)  Acc@1: 70.8333 (70.6349)  Acc@5: 91.6667 (92.8571)  time: 0.0904  data: 0.0010  max mem: 233\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 1.1139 (1.1143)  Acc@1: 70.8333 (72.5806)  Acc@5: 95.8333 (93.6828)  time: 0.0905  data: 0.0014  max mem: 233\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 1.0705 (1.1282)  Acc@1: 70.8333 (72.0528)  Acc@5: 95.8333 (93.1911)  time: 0.0908  data: 0.0008  max mem: 233\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 1.0360 (1.1212)  Acc@1: 70.8333 (72.4000)  Acc@5: 95.8333 (93.3000)  time: 0.0892  data: 0.0008  max mem: 233\n",
            "Test: [Task 4] Total time: 0:00:04 (0.1028 s / it)\n",
            "* Acc@1 72.400 Acc@5 93.300 loss 1.121\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:31  Loss: 1.0264 (1.0264)  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.8333)  time: 0.7400  data: 0.6548  max mem: 233\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:04  Loss: 1.0521 (1.0487)  Acc@1: 70.8333 (73.8636)  Acc@5: 95.8333 (95.8333)  time: 0.1502  data: 0.0598  max mem: 233\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 1.0858 (1.0950)  Acc@1: 70.8333 (73.2143)  Acc@5: 91.6667 (94.6429)  time: 0.0907  data: 0.0004  max mem: 233\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 1.1056 (1.1104)  Acc@1: 70.8333 (72.9839)  Acc@5: 95.8333 (94.4893)  time: 0.0905  data: 0.0004  max mem: 233\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 1.1610 (1.1594)  Acc@1: 70.8333 (71.2398)  Acc@5: 95.8333 (93.8008)  time: 0.0911  data: 0.0003  max mem: 233\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 1.2224 (1.1728)  Acc@1: 70.8333 (70.9000)  Acc@5: 91.6667 (93.7000)  time: 0.0894  data: 0.0003  max mem: 233\n",
            "Test: [Task 5] Total time: 0:00:04 (0.1077 s / it)\n",
            "* Acc@1 70.900 Acc@5 93.700 loss 1.173\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:21  Loss: 0.9738 (0.9738)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  time: 0.5043  data: 0.4154  max mem: 233\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:04  Loss: 1.1555 (1.2043)  Acc@1: 66.6667 (67.0455)  Acc@5: 95.8333 (94.3182)  time: 0.1286  data: 0.0382  max mem: 233\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 1.1910 (1.2144)  Acc@1: 66.6667 (66.8651)  Acc@5: 95.8333 (94.4444)  time: 0.0907  data: 0.0004  max mem: 233\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 1.2876 (1.2462)  Acc@1: 62.5000 (66.3979)  Acc@5: 95.8333 (94.4892)  time: 0.0907  data: 0.0005  max mem: 233\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.3256 (1.2659)  Acc@1: 66.6667 (66.5650)  Acc@5: 91.6667 (93.8008)  time: 0.0912  data: 0.0007  max mem: 233\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.3365 (1.2692)  Acc@1: 62.5000 (66.5000)  Acc@5: 91.6667 (93.7000)  time: 0.0892  data: 0.0007  max mem: 233\n",
            "Test: [Task 6] Total time: 0:00:04 (0.1021 s / it)\n",
            "* Acc@1 66.500 Acc@5 93.700 loss 1.269\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:15  Loss: 1.4729 (1.4729)  Acc@1: 54.1667 (54.1667)  Acc@5: 87.5000 (87.5000)  time: 0.3717  data: 0.2865  max mem: 233\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:03  Loss: 1.3735 (1.3745)  Acc@1: 62.5000 (60.9849)  Acc@5: 91.6667 (93.1818)  time: 0.1185  data: 0.0300  max mem: 233\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:02  Loss: 1.3303 (1.3734)  Acc@1: 62.5000 (60.7143)  Acc@5: 95.8333 (93.4524)  time: 0.0922  data: 0.0023  max mem: 233\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:01  Loss: 1.3334 (1.3970)  Acc@1: 62.5000 (61.6935)  Acc@5: 91.6667 (92.2043)  time: 0.0913  data: 0.0006  max mem: 233\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 1.3464 (1.3929)  Acc@1: 62.5000 (61.7886)  Acc@5: 91.6667 (91.8699)  time: 0.0913  data: 0.0006  max mem: 233\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 1.3326 (1.3900)  Acc@1: 62.5000 (62.1000)  Acc@5: 91.6667 (91.8000)  time: 0.0895  data: 0.0006  max mem: 233\n",
            "Test: [Task 7] Total time: 0:00:04 (0.1010 s / it)\n",
            "* Acc@1 62.100 Acc@5 91.800 loss 1.390\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:30  Loss: 1.2627 (1.2627)  Acc@1: 58.3333 (58.3333)  Acc@5: 95.8333 (95.8333)  time: 0.7210  data: 0.6301  max mem: 233\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:04  Loss: 1.3754 (1.4104)  Acc@1: 62.5000 (64.0152)  Acc@5: 91.6667 (90.1515)  time: 0.1490  data: 0.0597  max mem: 233\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:02  Loss: 1.3307 (1.3695)  Acc@1: 62.5000 (65.2778)  Acc@5: 91.6667 (90.8730)  time: 0.0910  data: 0.0015  max mem: 233\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:01  Loss: 1.2915 (1.3465)  Acc@1: 66.6667 (66.6667)  Acc@5: 95.8333 (92.4731)  time: 0.0907  data: 0.0004  max mem: 233\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 1.3347 (1.3692)  Acc@1: 66.6667 (65.9553)  Acc@5: 95.8333 (91.8699)  time: 0.0911  data: 0.0003  max mem: 233\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 1.3347 (1.3651)  Acc@1: 66.6667 (66.0000)  Acc@5: 93.7500 (91.9000)  time: 0.0895  data: 0.0002  max mem: 233\n",
            "Test: [Task 8] Total time: 0:00:04 (0.1074 s / it)\n",
            "* Acc@1 66.000 Acc@5 91.900 loss 1.365\n",
            "Test: [Task 9]  [ 0/42]  eta: 0:00:13  Loss: 1.5371 (1.5371)  Acc@1: 50.0000 (50.0000)  Acc@5: 100.0000 (100.0000)  time: 0.3179  data: 0.2438  max mem: 233\n",
            "Test: [Task 9]  [10/42]  eta: 0:00:03  Loss: 1.5717 (1.6122)  Acc@1: 50.0000 (53.0303)  Acc@5: 95.8333 (94.3182)  time: 0.1174  data: 0.0296  max mem: 233\n",
            "Test: [Task 9]  [20/42]  eta: 0:00:02  Loss: 1.6031 (1.5880)  Acc@1: 54.1667 (56.1508)  Acc@5: 91.6667 (93.4524)  time: 0.0935  data: 0.0043  max mem: 233\n",
            "Test: [Task 9]  [30/42]  eta: 0:00:01  Loss: 1.6072 (1.6353)  Acc@1: 54.1667 (54.7043)  Acc@5: 91.6667 (91.9355)  time: 0.0903  data: 0.0003  max mem: 233\n",
            "Test: [Task 9]  [40/42]  eta: 0:00:00  Loss: 1.5157 (1.5958)  Acc@1: 54.1667 (56.3008)  Acc@5: 91.6667 (92.5813)  time: 0.0914  data: 0.0003  max mem: 233\n",
            "Test: [Task 9]  [41/42]  eta: 0:00:00  Loss: 1.5091 (1.5843)  Acc@1: 54.1667 (56.5000)  Acc@5: 91.6667 (92.7000)  time: 0.0895  data: 0.0003  max mem: 233\n",
            "Test: [Task 9] Total time: 0:00:04 (0.0992 s / it)\n",
            "* Acc@1 56.500 Acc@5 92.700 loss 1.584\n",
            "[Average accuracy till task9]\tAcc@1: 68.5556\tAcc@5: 93.1556\tLoss: 1.2366\tForgetting: 4.1750\tBackward: 0.1750\n",
            "Train: Epoch[ 1/10]  [ 0/21]  eta: 0:00:11  Lr: 0.000047  Loss: 2.4436  Acc@1: 0.0000 (0.0000)  Acc@5: 54.1667 (54.1667)  time: 0.5594  data: 0.4665  max mem: 233\n",
            "Train: Epoch[ 1/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 2.3209  Acc@1: 12.5000 (13.2576)  Acc@5: 58.3333 (59.4697)  time: 0.1339  data: 0.0437  max mem: 233\n",
            "Train: Epoch[ 1/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 2.1080  Acc@1: 20.8333 (19.3416)  Acc@5: 62.5000 (62.7572)  time: 0.0882  data: 0.0008  max mem: 233\n",
            "Train: Epoch[ 1/10] Total time: 0:00:02 (0.1144 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 2.1080  Acc@1: 20.8333 (19.3416)  Acc@5: 62.5000 (62.7572)\n",
            "Train: Epoch[ 2/10]  [ 0/21]  eta: 0:00:17  Lr: 0.000047  Loss: 2.0921  Acc@1: 29.1667 (29.1667)  Acc@5: 62.5000 (62.5000)  time: 0.8144  data: 0.6968  max mem: 233\n",
            "Train: Epoch[ 2/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.9355  Acc@1: 41.6667 (37.8788)  Acc@5: 83.3333 (78.4091)  time: 0.1712  data: 0.0641  max mem: 233\n",
            "Train: Epoch[ 2/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 1.9089  Acc@1: 41.6667 (43.8272)  Acc@5: 83.3333 (82.7161)  time: 0.0963  data: 0.0009  max mem: 233\n",
            "Train: Epoch[ 2/10] Total time: 0:00:02 (0.1363 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.9089  Acc@1: 41.6667 (43.8272)  Acc@5: 83.3333 (82.7161)\n",
            "Train: Epoch[ 3/10]  [ 0/21]  eta: 0:00:14  Lr: 0.000047  Loss: 1.7219  Acc@1: 45.8333 (45.8333)  Acc@5: 91.6667 (91.6667)  time: 0.7109  data: 0.5554  max mem: 233\n",
            "Train: Epoch[ 3/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.6606  Acc@1: 58.3333 (57.9545)  Acc@5: 91.6667 (92.0455)  time: 0.1598  data: 0.0517  max mem: 233\n",
            "Train: Epoch[ 3/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 1.5154  Acc@1: 58.3333 (59.2593)  Acc@5: 95.8333 (93.6214)  time: 0.0949  data: 0.0008  max mem: 233\n",
            "Train: Epoch[ 3/10] Total time: 0:00:02 (0.1288 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.5154  Acc@1: 58.3333 (59.2593)  Acc@5: 95.8333 (93.6214)\n",
            "Train: Epoch[ 4/10]  [ 0/21]  eta: 0:00:08  Lr: 0.000047  Loss: 1.4076  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.4136  data: 0.2978  max mem: 233\n",
            "Train: Epoch[ 4/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.3802  Acc@1: 62.5000 (68.5606)  Acc@5: 95.8333 (96.9697)  time: 0.1353  data: 0.0436  max mem: 233\n",
            "Train: Epoch[ 4/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 1.1378  Acc@1: 70.8333 (71.8107)  Acc@5: 95.8333 (96.9136)  time: 0.0951  data: 0.0093  max mem: 233\n",
            "Train: Epoch[ 4/10] Total time: 0:00:02 (0.1149 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.1378  Acc@1: 70.8333 (71.8107)  Acc@5: 95.8333 (96.9136)\n",
            "Train: Epoch[ 5/10]  [ 0/21]  eta: 0:00:13  Lr: 0.000047  Loss: 1.1999  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.6499  data: 0.4832  max mem: 233\n",
            "Train: Epoch[ 5/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.2798  Acc@1: 75.0000 (75.7576)  Acc@5: 100.0000 (97.7273)  time: 0.1485  data: 0.0458  max mem: 233\n",
            "Train: Epoch[ 5/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 1.2217  Acc@1: 75.0000 (75.7202)  Acc@5: 100.0000 (97.5309)  time: 0.0910  data: 0.0011  max mem: 233\n",
            "Train: Epoch[ 5/10] Total time: 0:00:02 (0.1214 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.2217  Acc@1: 75.0000 (75.7202)  Acc@5: 100.0000 (97.5309)\n",
            "Train: Epoch[ 6/10]  [ 0/21]  eta: 0:00:10  Lr: 0.000047  Loss: 1.0083  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.4897  data: 0.3560  max mem: 233\n",
            "Train: Epoch[ 6/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.1665  Acc@1: 83.3333 (81.0606)  Acc@5: 100.0000 (98.8636)  time: 0.1297  data: 0.0342  max mem: 233\n",
            "Train: Epoch[ 6/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 0.9253  Acc@1: 83.3333 (83.1276)  Acc@5: 100.0000 (98.3539)  time: 0.0886  data: 0.0011  max mem: 233\n",
            "Train: Epoch[ 6/10] Total time: 0:00:02 (0.1116 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.9253  Acc@1: 83.3333 (83.1276)  Acc@5: 100.0000 (98.3539)\n",
            "Train: Epoch[ 7/10]  [ 0/21]  eta: 0:00:09  Lr: 0.000047  Loss: 1.0803  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  time: 0.4671  data: 0.3692  max mem: 233\n",
            "Train: Epoch[ 7/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 0.8354  Acc@1: 83.3333 (85.2273)  Acc@5: 100.0000 (98.4848)  time: 0.1446  data: 0.0375  max mem: 233\n",
            "Train: Epoch[ 7/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 0.9252  Acc@1: 83.3333 (84.3621)  Acc@5: 100.0000 (98.5597)  time: 0.0987  data: 0.0025  max mem: 233\n",
            "Train: Epoch[ 7/10] Total time: 0:00:02 (0.1223 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.9252  Acc@1: 83.3333 (84.3621)  Acc@5: 100.0000 (98.5597)\n",
            "Train: Epoch[ 8/10]  [ 0/21]  eta: 0:00:14  Lr: 0.000047  Loss: 0.7414  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.6903  data: 0.5808  max mem: 233\n",
            "Train: Epoch[ 8/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.0613  Acc@1: 83.3333 (80.6818)  Acc@5: 100.0000 (98.4848)  time: 0.1668  data: 0.0587  max mem: 233\n",
            "Train: Epoch[ 8/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 0.7850  Acc@1: 83.3333 (82.9218)  Acc@5: 100.0000 (98.5597)  time: 0.1055  data: 0.0086  max mem: 233\n",
            "Train: Epoch[ 8/10] Total time: 0:00:02 (0.1377 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.7850  Acc@1: 83.3333 (82.9218)  Acc@5: 100.0000 (98.5597)\n",
            "Train: Epoch[ 9/10]  [ 0/21]  eta: 0:00:11  Lr: 0.000047  Loss: 0.8246  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.5331  data: 0.4333  max mem: 233\n",
            "Train: Epoch[ 9/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 0.9115  Acc@1: 83.3333 (82.9545)  Acc@5: 100.0000 (98.4848)  time: 0.1319  data: 0.0408  max mem: 233\n",
            "Train: Epoch[ 9/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 0.7499  Acc@1: 87.5000 (85.1852)  Acc@5: 100.0000 (98.9712)  time: 0.0877  data: 0.0009  max mem: 233\n",
            "Train: Epoch[ 9/10] Total time: 0:00:02 (0.1130 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.7499  Acc@1: 87.5000 (85.1852)  Acc@5: 100.0000 (98.9712)\n",
            "Train: Epoch[10/10]  [ 0/21]  eta: 0:00:13  Lr: 0.000047  Loss: 0.6229  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.6289  data: 0.4678  max mem: 233\n",
            "Train: Epoch[10/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 0.6348  Acc@1: 83.3333 (84.0909)  Acc@5: 100.0000 (98.8636)  time: 0.1474  data: 0.0432  max mem: 233\n",
            "Train: Epoch[10/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 0.9250  Acc@1: 83.3333 (84.1564)  Acc@5: 100.0000 (99.1770)  time: 0.0947  data: 0.0007  max mem: 233\n",
            "Train: Epoch[10/10] Total time: 0:00:02 (0.1281 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.9250  Acc@1: 83.3333 (84.1564)  Acc@5: 100.0000 (99.1770)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:34  Loss: 1.0937 (1.0937)  Acc@1: 75.0000 (75.0000)  Acc@5: 87.5000 (87.5000)  time: 0.8167  data: 0.7186  max mem: 233\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 1.0999 (1.1178)  Acc@1: 70.8333 (70.4545)  Acc@5: 95.8333 (92.4242)  time: 0.1550  data: 0.0660  max mem: 233\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 1.0507 (1.0879)  Acc@1: 75.0000 (74.0079)  Acc@5: 95.8333 (93.0556)  time: 0.0886  data: 0.0007  max mem: 233\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.9692 (1.0561)  Acc@1: 79.1667 (75.0000)  Acc@5: 91.6667 (93.6828)  time: 0.0887  data: 0.0006  max mem: 233\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.9124 (1.0279)  Acc@1: 79.1667 (76.4228)  Acc@5: 95.8333 (94.7154)  time: 0.0891  data: 0.0004  max mem: 233\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.8933 (1.0207)  Acc@1: 83.3333 (76.7000)  Acc@5: 95.8333 (94.8000)  time: 0.0874  data: 0.0004  max mem: 233\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1078 s / it)\n",
            "* Acc@1 76.700 Acc@5 94.800 loss 1.021\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:32  Loss: 1.2645 (1.2645)  Acc@1: 62.5000 (62.5000)  Acc@5: 95.8333 (95.8333)  time: 0.7736  data: 0.6577  max mem: 233\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 1.2645 (1.2724)  Acc@1: 70.8333 (66.2879)  Acc@5: 91.6667 (92.8030)  time: 0.1523  data: 0.0608  max mem: 233\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 1.2505 (1.2905)  Acc@1: 70.8333 (68.4524)  Acc@5: 91.6667 (91.8651)  time: 0.0893  data: 0.0012  max mem: 233\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 1.1620 (1.2840)  Acc@1: 66.6667 (68.6828)  Acc@5: 91.6667 (92.0699)  time: 0.0881  data: 0.0009  max mem: 233\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 1.1511 (1.2735)  Acc@1: 66.6667 (68.6992)  Acc@5: 91.6667 (92.2764)  time: 0.0887  data: 0.0008  max mem: 233\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 1.1511 (1.2712)  Acc@1: 66.6667 (68.7000)  Acc@5: 95.8333 (92.4000)  time: 0.0867  data: 0.0008  max mem: 233\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1069 s / it)\n",
            "* Acc@1 68.700 Acc@5 92.400 loss 1.271\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:18  Loss: 0.8170 (0.8170)  Acc@1: 70.8333 (70.8333)  Acc@5: 100.0000 (100.0000)  time: 0.4376  data: 0.3569  max mem: 233\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 1.0530 (1.1043)  Acc@1: 75.0000 (73.8636)  Acc@5: 95.8333 (93.1818)  time: 0.1225  data: 0.0331  max mem: 233\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 1.0791 (1.1017)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (93.4524)  time: 0.0894  data: 0.0006  max mem: 233\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.9968 (1.0748)  Acc@1: 75.0000 (74.4624)  Acc@5: 91.6667 (93.2796)  time: 0.0882  data: 0.0005  max mem: 233\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 1.0098 (1.0861)  Acc@1: 75.0000 (74.6951)  Acc@5: 91.6667 (93.2927)  time: 0.0891  data: 0.0003  max mem: 233\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 1.0098 (1.0932)  Acc@1: 75.0000 (74.6000)  Acc@5: 91.6667 (93.3000)  time: 0.0876  data: 0.0003  max mem: 233\n",
            "Test: [Task 3] Total time: 0:00:04 (0.0989 s / it)\n",
            "* Acc@1 74.600 Acc@5 93.300 loss 1.093\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:19  Loss: 1.5460 (1.5460)  Acc@1: 62.5000 (62.5000)  Acc@5: 83.3333 (83.3333)  time: 0.4711  data: 0.3941  max mem: 233\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:03  Loss: 1.1695 (1.1573)  Acc@1: 70.8333 (70.4545)  Acc@5: 91.6667 (92.4242)  time: 0.1249  data: 0.0362  max mem: 233\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 1.2037 (1.1753)  Acc@1: 70.8333 (70.4365)  Acc@5: 91.6667 (92.6587)  time: 0.0891  data: 0.0003  max mem: 233\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 1.0833 (1.1341)  Acc@1: 70.8333 (72.0430)  Acc@5: 95.8333 (93.4140)  time: 0.0879  data: 0.0004  max mem: 233\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 1.0833 (1.1489)  Acc@1: 70.8333 (71.5447)  Acc@5: 95.8333 (92.6829)  time: 0.0889  data: 0.0004  max mem: 233\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 1.0573 (1.1413)  Acc@1: 70.8333 (71.9000)  Acc@5: 95.8333 (92.8000)  time: 0.0875  data: 0.0003  max mem: 233\n",
            "Test: [Task 4] Total time: 0:00:04 (0.0995 s / it)\n",
            "* Acc@1 71.900 Acc@5 92.800 loss 1.141\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:21  Loss: 1.0115 (1.0115)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  time: 0.5163  data: 0.4393  max mem: 233\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:04  Loss: 1.0161 (1.0643)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (96.2121)  time: 0.1282  data: 0.0409  max mem: 233\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 1.1205 (1.1061)  Acc@1: 75.0000 (74.6032)  Acc@5: 95.8333 (94.6429)  time: 0.0889  data: 0.0011  max mem: 233\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 1.1177 (1.1216)  Acc@1: 75.0000 (74.4624)  Acc@5: 91.6667 (94.4892)  time: 0.0889  data: 0.0010  max mem: 233\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 1.1177 (1.1725)  Acc@1: 70.8333 (72.3577)  Acc@5: 91.6667 (93.6992)  time: 0.0895  data: 0.0006  max mem: 233\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 1.2509 (1.1858)  Acc@1: 70.8333 (71.9000)  Acc@5: 91.6667 (93.6000)  time: 0.0880  data: 0.0005  max mem: 233\n",
            "Test: [Task 5] Total time: 0:00:04 (0.1022 s / it)\n",
            "* Acc@1 71.900 Acc@5 93.600 loss 1.186\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:24  Loss: 1.0261 (1.0261)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  time: 0.5828  data: 0.4841  max mem: 233\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:04  Loss: 1.2170 (1.2553)  Acc@1: 66.6667 (67.4242)  Acc@5: 95.8333 (94.3182)  time: 0.1363  data: 0.0479  max mem: 233\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 1.2278 (1.2714)  Acc@1: 66.6667 (68.0556)  Acc@5: 95.8333 (93.8492)  time: 0.0907  data: 0.0024  max mem: 233\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 1.3544 (1.3088)  Acc@1: 66.6667 (66.3979)  Acc@5: 91.6667 (93.4140)  time: 0.0900  data: 0.0005  max mem: 233\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.4090 (1.3260)  Acc@1: 66.6667 (66.0569)  Acc@5: 91.6667 (93.0894)  time: 0.0900  data: 0.0003  max mem: 233\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.4279 (1.3284)  Acc@1: 62.5000 (66.0000)  Acc@5: 91.6667 (93.0000)  time: 0.0885  data: 0.0003  max mem: 233\n",
            "Test: [Task 6] Total time: 0:00:04 (0.1034 s / it)\n",
            "* Acc@1 66.000 Acc@5 93.000 loss 1.328\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:14  Loss: 1.5285 (1.5285)  Acc@1: 62.5000 (62.5000)  Acc@5: 87.5000 (87.5000)  time: 0.3428  data: 0.2718  max mem: 233\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:03  Loss: 1.4601 (1.4327)  Acc@1: 62.5000 (62.5000)  Acc@5: 91.6667 (92.4242)  time: 0.1228  data: 0.0383  max mem: 233\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:02  Loss: 1.3359 (1.4268)  Acc@1: 62.5000 (62.6984)  Acc@5: 91.6667 (92.0635)  time: 0.0950  data: 0.0077  max mem: 233\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:01  Loss: 1.3815 (1.4476)  Acc@1: 66.6667 (63.3065)  Acc@5: 91.6667 (91.2634)  time: 0.0896  data: 0.0004  max mem: 233\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 1.3815 (1.4438)  Acc@1: 66.6667 (64.0244)  Acc@5: 91.6667 (91.1585)  time: 0.0904  data: 0.0003  max mem: 233\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 1.3620 (1.4416)  Acc@1: 66.6667 (64.1000)  Acc@5: 91.6667 (91.1000)  time: 0.0880  data: 0.0003  max mem: 233\n",
            "Test: [Task 7] Total time: 0:00:04 (0.0999 s / it)\n",
            "* Acc@1 64.100 Acc@5 91.100 loss 1.442\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:20  Loss: 1.4007 (1.4007)  Acc@1: 58.3333 (58.3333)  Acc@5: 95.8333 (95.8333)  time: 0.4907  data: 0.4025  max mem: 233\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:04  Loss: 1.5375 (1.5434)  Acc@1: 58.3333 (60.6061)  Acc@5: 87.5000 (88.2576)  time: 0.1275  data: 0.0372  max mem: 233\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:02  Loss: 1.4897 (1.4952)  Acc@1: 58.3333 (62.3016)  Acc@5: 87.5000 (89.0873)  time: 0.0897  data: 0.0006  max mem: 233\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:01  Loss: 1.4355 (1.4751)  Acc@1: 58.3333 (63.1720)  Acc@5: 91.6667 (90.4570)  time: 0.0887  data: 0.0005  max mem: 233\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 1.4464 (1.5036)  Acc@1: 58.3333 (61.6870)  Acc@5: 87.5000 (89.5325)  time: 0.0897  data: 0.0004  max mem: 233\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 1.4464 (1.5010)  Acc@1: 58.3333 (61.8000)  Acc@5: 87.5000 (89.6000)  time: 0.0877  data: 0.0004  max mem: 233\n",
            "Test: [Task 8] Total time: 0:00:04 (0.1016 s / it)\n",
            "* Acc@1 61.800 Acc@5 89.600 loss 1.501\n",
            "Test: [Task 9]  [ 0/42]  eta: 0:00:37  Loss: 1.7071 (1.7071)  Acc@1: 33.3333 (33.3333)  Acc@5: 95.8333 (95.8333)  time: 0.8810  data: 0.7778  max mem: 233\n",
            "Test: [Task 9]  [10/42]  eta: 0:00:05  Loss: 1.8666 (1.8569)  Acc@1: 45.8333 (43.1818)  Acc@5: 91.6667 (92.0455)  time: 0.1635  data: 0.0720  max mem: 233\n",
            "Test: [Task 9]  [20/42]  eta: 0:00:02  Loss: 1.8666 (1.8422)  Acc@1: 45.8333 (46.4286)  Acc@5: 91.6667 (91.0714)  time: 0.0903  data: 0.0014  max mem: 233\n",
            "Test: [Task 9]  [30/42]  eta: 0:00:01  Loss: 1.8866 (1.8937)  Acc@1: 45.8333 (44.8925)  Acc@5: 87.5000 (89.2473)  time: 0.0889  data: 0.0009  max mem: 233\n",
            "Test: [Task 9]  [40/42]  eta: 0:00:00  Loss: 1.7662 (1.8492)  Acc@1: 45.8333 (45.9350)  Acc@5: 87.5000 (90.2439)  time: 0.0895  data: 0.0003  max mem: 233\n",
            "Test: [Task 9]  [41/42]  eta: 0:00:00  Loss: 1.7376 (1.8372)  Acc@1: 45.8333 (46.0000)  Acc@5: 91.6667 (90.4000)  time: 0.0880  data: 0.0003  max mem: 233\n",
            "Test: [Task 9] Total time: 0:00:04 (0.1101 s / it)\n",
            "* Acc@1 46.000 Acc@5 90.400 loss 1.837\n",
            "Test: [Task 10]  [ 0/42]  eta: 0:00:14  Loss: 3.6679 (3.6679)  Acc@1: 0.0000 (0.0000)  Acc@5: 20.8333 (20.8333)  time: 0.3541  data: 0.2762  max mem: 233\n",
            "Test: [Task 10]  [10/42]  eta: 0:00:03  Loss: 3.5719 (3.5996)  Acc@1: 0.0000 (1.1364)  Acc@5: 33.3333 (32.1970)  time: 0.1230  data: 0.0371  max mem: 233\n",
            "Test: [Task 10]  [20/42]  eta: 0:00:02  Loss: 3.5719 (3.5784)  Acc@1: 0.0000 (0.9921)  Acc@5: 37.5000 (37.1032)  time: 0.0946  data: 0.0067  max mem: 233\n",
            "Test: [Task 10]  [30/42]  eta: 0:00:01  Loss: 3.6369 (3.5854)  Acc@1: 0.0000 (0.6720)  Acc@5: 37.5000 (37.3656)  time: 0.0897  data: 0.0003  max mem: 233\n",
            "Test: [Task 10]  [40/42]  eta: 0:00:00  Loss: 3.6057 (3.5971)  Acc@1: 0.0000 (0.6098)  Acc@5: 37.5000 (38.1098)  time: 0.0905  data: 0.0003  max mem: 233\n",
            "Test: [Task 10]  [41/42]  eta: 0:00:00  Loss: 3.5937 (3.5911)  Acc@1: 0.0000 (0.6000)  Acc@5: 37.5000 (38.2000)  time: 0.0881  data: 0.0003  max mem: 233\n",
            "Test: [Task 10] Total time: 0:00:04 (0.0999 s / it)\n",
            "* Acc@1 0.600 Acc@5 38.200 loss 3.591\n",
            "[Average accuracy till task10]\tAcc@1: 60.2300\tAcc@5: 86.9200\tLoss: 1.5411\tForgetting: 3.3444\tBackward: 32.3556\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([20832, 384])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 1.0052  Acc@1: 83.3333 (83.2870)  Acc@5: 95.8333 (95.7407)\n",
            "torch.Size([20832, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 0.8424  Acc@1: 87.5000 (85.5556)  Acc@5: 95.8333 (96.9907)\n",
            "torch.Size([20832, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 0.7975  Acc@1: 83.3333 (85.2315)  Acc@5: 100.0000 (98.6574)\n",
            "torch.Size([20832, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 0.8957  Acc@1: 87.5000 (89.3056)  Acc@5: 100.0000 (99.0278)\n",
            "torch.Size([20832, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 0.7830  Acc@1: 87.5000 (88.0093)  Acc@5: 100.0000 (99.2593)\n",
            "torch.Size([20832, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 0.8080  Acc@1: 91.6667 (88.9815)  Acc@5: 100.0000 (99.2130)\n",
            "torch.Size([20832, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.6625  Acc@1: 91.6667 (88.8889)  Acc@5: 100.0000 (99.1204)\n",
            "torch.Size([20832, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 0.5868  Acc@1: 91.6667 (91.1111)  Acc@5: 100.0000 (99.3056)\n",
            "torch.Size([20832, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 0.6781  Acc@1: 91.6667 (90.8333)  Acc@5: 100.0000 (99.7222)\n",
            "torch.Size([20832, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.7337  Acc@1: 87.5000 (90.5556)  Acc@5: 100.0000 (99.5833)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:21  Loss: 1.0724 (1.0724)  Acc@1: 70.8333 (70.8333)  Acc@5: 83.3333 (83.3333)  time: 0.5060  data: 0.4361  max mem: 233\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 1.0724 (1.0988)  Acc@1: 70.8333 (71.5909)  Acc@5: 91.6667 (92.0455)  time: 0.1277  data: 0.0401  max mem: 233\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 1.0672 (1.0601)  Acc@1: 75.0000 (74.2064)  Acc@5: 95.8333 (92.8571)  time: 0.0886  data: 0.0005  max mem: 233\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.9467 (1.0270)  Acc@1: 75.0000 (74.5968)  Acc@5: 95.8333 (93.8172)  time: 0.0877  data: 0.0005  max mem: 233\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.8766 (0.9971)  Acc@1: 79.1667 (75.8130)  Acc@5: 95.8333 (94.6138)  time: 0.0888  data: 0.0002  max mem: 233\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.8745 (0.9886)  Acc@1: 79.1667 (76.1000)  Acc@5: 95.8333 (94.7000)  time: 0.0868  data: 0.0002  max mem: 233\n",
            "Test: [Task 1] Total time: 0:00:04 (0.0998 s / it)\n",
            "* Acc@1 76.100 Acc@5 94.700 loss 0.989\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:28  Loss: 1.2045 (1.2045)  Acc@1: 62.5000 (62.5000)  Acc@5: 91.6667 (91.6667)  time: 0.6867  data: 0.5873  max mem: 233\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 1.2123 (1.2020)  Acc@1: 66.6667 (67.4242)  Acc@5: 91.6667 (92.0455)  time: 0.1457  data: 0.0559  max mem: 233\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 1.2123 (1.2354)  Acc@1: 66.6667 (66.8651)  Acc@5: 91.6667 (91.2698)  time: 0.0901  data: 0.0023  max mem: 233\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 1.1403 (1.2246)  Acc@1: 66.6667 (68.5484)  Acc@5: 91.6667 (91.3979)  time: 0.0892  data: 0.0012  max mem: 233\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 1.0838 (1.2104)  Acc@1: 66.6667 (68.1911)  Acc@5: 95.8333 (91.9715)  time: 0.0905  data: 0.0005  max mem: 233\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 1.0838 (1.2076)  Acc@1: 66.6667 (68.3000)  Acc@5: 95.8333 (92.0000)  time: 0.0890  data: 0.0005  max mem: 233\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1060 s / it)\n",
            "* Acc@1 68.300 Acc@5 92.000 loss 1.208\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:15  Loss: 0.7331 (0.7331)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.3806  data: 0.3028  max mem: 233\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 0.9956 (1.0260)  Acc@1: 79.1667 (76.8939)  Acc@5: 95.8333 (93.1818)  time: 0.1229  data: 0.0360  max mem: 233\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 1.0220 (1.0155)  Acc@1: 79.1667 (76.9841)  Acc@5: 95.8333 (93.4524)  time: 0.0934  data: 0.0048  max mem: 233\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.9600 (0.9943)  Acc@1: 75.0000 (76.3441)  Acc@5: 95.8333 (93.6828)  time: 0.0901  data: 0.0004  max mem: 233\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.9600 (1.0015)  Acc@1: 79.1667 (76.6260)  Acc@5: 91.6667 (93.4959)  time: 0.0910  data: 0.0003  max mem: 233\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.9600 (1.0067)  Acc@1: 79.1667 (76.5000)  Acc@5: 93.7500 (93.5000)  time: 0.0891  data: 0.0002  max mem: 233\n",
            "Test: [Task 3] Total time: 0:00:04 (0.1003 s / it)\n",
            "* Acc@1 76.500 Acc@5 93.500 loss 1.007\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:18  Loss: 1.4891 (1.4891)  Acc@1: 66.6667 (66.6667)  Acc@5: 79.1667 (79.1667)  time: 0.4387  data: 0.3308  max mem: 233\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:03  Loss: 1.1638 (1.1726)  Acc@1: 66.6667 (68.5606)  Acc@5: 91.6667 (91.2879)  time: 0.1224  data: 0.0316  max mem: 233\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 1.1758 (1.1944)  Acc@1: 70.8333 (69.4444)  Acc@5: 91.6667 (91.0714)  time: 0.0914  data: 0.0011  max mem: 233\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 1.1012 (1.1425)  Acc@1: 70.8333 (70.9677)  Acc@5: 91.6667 (91.6667)  time: 0.0919  data: 0.0004  max mem: 233\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 1.0917 (1.1600)  Acc@1: 70.8333 (70.3252)  Acc@5: 91.6667 (91.3618)  time: 0.0923  data: 0.0003  max mem: 233\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 1.0517 (1.1530)  Acc@1: 70.8333 (70.7000)  Acc@5: 91.6667 (91.5000)  time: 0.0909  data: 0.0003  max mem: 233\n",
            "Test: [Task 4] Total time: 0:00:04 (0.1017 s / it)\n",
            "* Acc@1 70.700 Acc@5 91.500 loss 1.153\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:18  Loss: 1.0124 (1.0124)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  time: 0.4471  data: 0.3633  max mem: 233\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:04  Loss: 1.0124 (1.0507)  Acc@1: 70.8333 (72.3485)  Acc@5: 95.8333 (95.4545)  time: 0.1250  data: 0.0337  max mem: 233\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 1.1206 (1.1016)  Acc@1: 70.8333 (72.4206)  Acc@5: 91.6667 (93.8492)  time: 0.0921  data: 0.0010  max mem: 233\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 1.1221 (1.1144)  Acc@1: 70.8333 (71.7742)  Acc@5: 91.6667 (93.8172)  time: 0.0916  data: 0.0016  max mem: 233\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 1.1677 (1.1655)  Acc@1: 66.6667 (70.0203)  Acc@5: 91.6667 (93.0894)  time: 0.0921  data: 0.0015  max mem: 233\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 1.2272 (1.1800)  Acc@1: 66.6667 (69.5000)  Acc@5: 91.6667 (93.0000)  time: 0.0909  data: 0.0014  max mem: 233\n",
            "Test: [Task 5] Total time: 0:00:04 (0.1031 s / it)\n",
            "* Acc@1 69.500 Acc@5 93.000 loss 1.180\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:23  Loss: 0.9812 (0.9812)  Acc@1: 83.3333 (83.3333)  Acc@5: 91.6667 (91.6667)  time: 0.5599  data: 0.4732  max mem: 233\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:04  Loss: 1.1385 (1.1978)  Acc@1: 70.8333 (69.6970)  Acc@5: 95.8333 (95.0758)  time: 0.1361  data: 0.0435  max mem: 233\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 1.1445 (1.2123)  Acc@1: 70.8333 (68.0556)  Acc@5: 95.8333 (94.6429)  time: 0.0931  data: 0.0004  max mem: 233\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 1.2963 (1.2496)  Acc@1: 62.5000 (66.2634)  Acc@5: 91.6667 (93.8172)  time: 0.0929  data: 0.0004  max mem: 233\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.3290 (1.2681)  Acc@1: 62.5000 (65.8537)  Acc@5: 91.6667 (93.5976)  time: 0.0933  data: 0.0003  max mem: 233\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.3430 (1.2712)  Acc@1: 62.5000 (65.9000)  Acc@5: 91.6667 (93.5000)  time: 0.0917  data: 0.0003  max mem: 233\n",
            "Test: [Task 6] Total time: 0:00:04 (0.1057 s / it)\n",
            "* Acc@1 65.900 Acc@5 93.500 loss 1.271\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:19  Loss: 1.4698 (1.4698)  Acc@1: 58.3333 (58.3333)  Acc@5: 87.5000 (87.5000)  time: 0.4630  data: 0.3677  max mem: 233\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:04  Loss: 1.2834 (1.3002)  Acc@1: 62.5000 (64.0152)  Acc@5: 91.6667 (92.4242)  time: 0.1271  data: 0.0343  max mem: 233\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:02  Loss: 1.2531 (1.3021)  Acc@1: 62.5000 (63.8889)  Acc@5: 95.8333 (92.6587)  time: 0.0933  data: 0.0007  max mem: 233\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:01  Loss: 1.2742 (1.3284)  Acc@1: 62.5000 (64.3817)  Acc@5: 91.6667 (91.5323)  time: 0.0935  data: 0.0005  max mem: 233\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 1.2769 (1.3271)  Acc@1: 62.5000 (64.3293)  Acc@5: 91.6667 (91.3618)  time: 0.0934  data: 0.0004  max mem: 233\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 1.2769 (1.3239)  Acc@1: 66.6667 (64.5000)  Acc@5: 91.6667 (91.3000)  time: 0.0919  data: 0.0003  max mem: 233\n",
            "Test: [Task 7] Total time: 0:00:04 (0.1035 s / it)\n",
            "* Acc@1 64.500 Acc@5 91.300 loss 1.324\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:15  Loss: 1.2442 (1.2442)  Acc@1: 62.5000 (62.5000)  Acc@5: 95.8333 (95.8333)  time: 0.3584  data: 0.2790  max mem: 233\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:03  Loss: 1.3959 (1.3698)  Acc@1: 62.5000 (67.0455)  Acc@5: 91.6667 (89.3939)  time: 0.1190  data: 0.0301  max mem: 233\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:02  Loss: 1.2666 (1.3230)  Acc@1: 66.6667 (68.0556)  Acc@5: 87.5000 (89.2857)  time: 0.0936  data: 0.0030  max mem: 233\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:01  Loss: 1.2298 (1.2960)  Acc@1: 66.6667 (68.5484)  Acc@5: 91.6667 (90.7258)  time: 0.0923  data: 0.0015  max mem: 233\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 1.2989 (1.3181)  Acc@1: 62.5000 (67.3781)  Acc@5: 91.6667 (90.4472)  time: 0.0926  data: 0.0012  max mem: 233\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 1.2989 (1.3155)  Acc@1: 62.5000 (67.4000)  Acc@5: 91.6667 (90.5000)  time: 0.0906  data: 0.0011  max mem: 233\n",
            "Test: [Task 8] Total time: 0:00:04 (0.1016 s / it)\n",
            "* Acc@1 67.400 Acc@5 90.500 loss 1.315\n",
            "Test: [Task 9]  [ 0/42]  eta: 0:00:30  Loss: 1.1944 (1.1944)  Acc@1: 66.6667 (66.6667)  Acc@5: 100.0000 (100.0000)  time: 0.7242  data: 0.5770  max mem: 233\n",
            "Test: [Task 9]  [10/42]  eta: 0:00:04  Loss: 1.3323 (1.3202)  Acc@1: 62.5000 (63.2576)  Acc@5: 95.8333 (95.8333)  time: 0.1495  data: 0.0533  max mem: 233\n",
            "Test: [Task 9]  [20/42]  eta: 0:00:02  Loss: 1.3316 (1.2813)  Acc@1: 62.5000 (66.8651)  Acc@5: 95.8333 (95.0397)  time: 0.0919  data: 0.0006  max mem: 233\n",
            "Test: [Task 9]  [30/42]  eta: 0:00:01  Loss: 1.3316 (1.3325)  Acc@1: 62.5000 (65.8602)  Acc@5: 91.6667 (93.8172)  time: 0.0917  data: 0.0004  max mem: 233\n",
            "Test: [Task 9]  [40/42]  eta: 0:00:00  Loss: 1.2103 (1.2841)  Acc@1: 70.8333 (67.6829)  Acc@5: 91.6667 (93.6992)  time: 0.0916  data: 0.0004  max mem: 233\n",
            "Test: [Task 9]  [41/42]  eta: 0:00:00  Loss: 1.1468 (1.2719)  Acc@1: 70.8333 (67.9000)  Acc@5: 91.6667 (93.8000)  time: 0.0899  data: 0.0004  max mem: 233\n",
            "Test: [Task 9] Total time: 0:00:04 (0.1084 s / it)\n",
            "* Acc@1 67.900 Acc@5 93.800 loss 1.272\n",
            "Test: [Task 10]  [ 0/42]  eta: 0:00:17  Loss: 1.8132 (1.8132)  Acc@1: 45.8333 (45.8333)  Acc@5: 91.6667 (91.6667)  time: 0.4114  data: 0.3326  max mem: 233\n",
            "Test: [Task 10]  [10/42]  eta: 0:00:03  Loss: 1.7827 (1.7780)  Acc@1: 45.8333 (45.4545)  Acc@5: 91.6667 (91.2879)  time: 0.1209  data: 0.0310  max mem: 233\n",
            "Test: [Task 10]  [20/42]  eta: 0:00:02  Loss: 1.7827 (1.7534)  Acc@1: 45.8333 (46.2302)  Acc@5: 91.6667 (91.4683)  time: 0.0913  data: 0.0005  max mem: 233\n",
            "Test: [Task 10]  [30/42]  eta: 0:00:01  Loss: 1.7878 (1.7526)  Acc@1: 45.8333 (47.0430)  Acc@5: 87.5000 (90.8602)  time: 0.0909  data: 0.0003  max mem: 233\n",
            "Test: [Task 10]  [40/42]  eta: 0:00:00  Loss: 1.7611 (1.7651)  Acc@1: 45.8333 (45.5285)  Acc@5: 87.5000 (90.4472)  time: 0.0911  data: 0.0002  max mem: 233\n",
            "Test: [Task 10]  [41/42]  eta: 0:00:00  Loss: 1.7396 (1.7585)  Acc@1: 45.8333 (45.7000)  Acc@5: 87.5000 (90.4000)  time: 0.0895  data: 0.0002  max mem: 233\n",
            "Test: [Task 10] Total time: 0:00:04 (0.1001 s / it)\n",
            "* Acc@1 45.700 Acc@5 90.400 loss 1.759\n",
            "[Average accuracy till task10]\tAcc@1: 67.2500\tAcc@5: 92.4200\tLoss: 1.2477\tForgetting: 4.1222\tBackward: 1.2444\n",
            "Total training time: 0:14:52\n",
            "[rank0]:[W1008 14:21:56.711413361 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
          ]
        }
      ],
      "source": [
        "!torchrun --nproc_per_node=1 main.py cifar100_hideprompt_5e --original_model vit_small_patch16_224.dino --model vit_small_patch16_224.dino --batch-size 24 --data-path ./datasets/ --output_dir ./output/cifar100_full_dino_5epoch_10pct --epochs 10 --crct_epochs 10 --sched constant --seed 20 --train_inference_task_only --lr 0.0005 --pct 0.10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "902dd0fc",
      "metadata": {
        "id": "902dd0fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7972b70-a7ad-471c-d9dd-a1e5717a2d10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(subparser_name='cifar100_hideprompt_5e', pct=0.25, batch_size=24, epochs=1, original_model='vit_small_patch16_224.dino', model='vit_small_patch16_224.dino', input_size=224, pretrained=True, drop=0.0, drop_path=0.0, opt='adam', opt_eps=1e-08, opt_betas=(0.9, 0.999), clip_grad=1.0, momentum=0.9, weight_decay=0.0, reinit_optimizer=True, sched='step', lr=0.03, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=0, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, unscale_lr=True, color_jitter=None, aa=None, smoothing=0.1, train_interpolation='bicubic', reprob=0.0, remode='pixel', recount=1, data_path='./datasets/', dataset='Split-CIFAR100', shuffle=False, output_dir='./output/cifar100_full_dino_1epoch_final_25pct', device='cuda', seed=20, eval=False, num_workers=4, pin_mem=True, world_size=1, dist_url='env://', num_tasks=10, train_mask=True, task_inc=False, use_g_prompt=False, g_prompt_length=5, g_prompt_layer_idx=[], use_prefix_tune_for_g_prompt=False, use_e_prompt=True, e_prompt_layer_idx=[0, 1, 2, 3, 4], use_prefix_tune_for_e_prompt=True, larger_prompt_lr=True, prompt_pool=True, size=10, length=5, top_k=1, initializer='uniform', prompt_key=False, prompt_key_init='uniform', use_prompt_mask=True, mask_first_epoch=False, shared_prompt_pool=True, shared_prompt_key=False, batchwise_prompt=False, embedding_key='cls', predefined_key='', pull_constraint=True, pull_constraint_coeff=1.0, same_key_value=False, global_pool='token', head_type='token', freeze=['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed'], crct_epochs=1, train_inference_task_only=False, original_model_mlp_structure=[2], ca_lr=0.005, milestones=[10], trained_original_model='./output/cifar100_full_dino_1epoch_25pct', prompt_momentum=0.1, reg=0.1, not_train_ca=False, ca_epochs=30, ca_storage_efficient_method='multi-centroid', n_centroids=10, print_freq=10, config='cifar100_hideprompt_5e')\n",
            "| distributed init (rank 0): env://\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "[rank0]:[W1006 10:56:51.837160810 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/helpers.py:7: FutureWarning: Importing from timm.models.helpers is deprecated, please import via timm.models\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:886: UserWarning: Overwriting vit_tiny_patch16_224 in registry with vits.hide_prompt_vision_transformer.vit_tiny_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:895: UserWarning: Overwriting vit_tiny_patch16_384 in registry with vits.hide_prompt_vision_transformer.vit_tiny_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:904: UserWarning: Overwriting vit_small_patch32_224 in registry with vits.hide_prompt_vision_transformer.vit_small_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:913: UserWarning: Overwriting vit_small_patch32_384 in registry with vits.hide_prompt_vision_transformer.vit_small_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:922: UserWarning: Overwriting vit_small_patch16_224 in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:932: UserWarning: Overwriting vit_small_patch16_384 in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:942: UserWarning: Overwriting vit_base_patch32_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:952: UserWarning: Overwriting vit_base_patch32_384 in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:962: UserWarning: Overwriting vit_base_patch16_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:972: UserWarning: Overwriting vit_base_patch16_384 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:982: UserWarning: Overwriting vit_base_patch8_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:992: UserWarning: Overwriting vit_large_patch32_224 in registry with vits.hide_prompt_vision_transformer.vit_large_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1001: UserWarning: Overwriting vit_large_patch32_384 in registry with vits.hide_prompt_vision_transformer.vit_large_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1011: UserWarning: Overwriting vit_large_patch16_224 in registry with vits.hide_prompt_vision_transformer.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1021: UserWarning: Overwriting vit_large_patch16_384 in registry with vits.hide_prompt_vision_transformer.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1031: UserWarning: Overwriting vit_large_patch14_224 in registry with vits.hide_prompt_vision_transformer.vit_large_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1040: UserWarning: Overwriting vit_huge_patch14_224 in registry with vits.hide_prompt_vision_transformer.vit_huge_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1049: UserWarning: Overwriting vit_giant_patch14_224 in registry with vits.hide_prompt_vision_transformer.vit_giant_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1058: UserWarning: Overwriting vit_gigantic_patch14_224 in registry with vits.hide_prompt_vision_transformer.vit_gigantic_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1067: UserWarning: Overwriting vit_tiny_patch16_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_tiny_patch16_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1078: UserWarning: Overwriting vit_small_patch32_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_small_patch32_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1089: UserWarning: Overwriting vit_small_patch16_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1100: UserWarning: Overwriting vit_base_patch32_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1111: UserWarning: Overwriting vit_base_patch16_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1122: UserWarning: Overwriting vit_base_patch8_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_base_patch8_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1133: UserWarning: Overwriting vit_large_patch32_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_large_patch32_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1144: UserWarning: Overwriting vit_large_patch16_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_large_patch16_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1155: UserWarning: Overwriting vit_huge_patch14_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_huge_patch14_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1166: UserWarning: Overwriting vit_base_patch16_224_sam in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_sam. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1175: UserWarning: Overwriting vit_base_patch32_224_sam in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_224_sam. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1184: UserWarning: Overwriting vit_small_patch16_224_dino in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_224_dino. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1193: UserWarning: Overwriting vit_small_patch8_224_dino in registry with vits.hide_prompt_vision_transformer.vit_small_patch8_224_dino. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1211: UserWarning: Overwriting vit_base_patch8_224_dino in registry with vits.hide_prompt_vision_transformer.vit_base_patch8_224_dino. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1220: UserWarning: Overwriting vit_base_patch16_224_miil_in21k in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_miil_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1230: UserWarning: Overwriting vit_base_patch16_224_miil in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_miil. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1242: UserWarning: Overwriting vit_base_patch32_plus_256 in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_plus_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1251: UserWarning: Overwriting vit_base_patch16_plus_240 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_plus_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1260: UserWarning: Overwriting vit_base_patch16_rpn_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_rpn_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1271: UserWarning: Overwriting vit_small_patch16_36x1_224 in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_36x1_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1282: UserWarning: Overwriting vit_small_patch16_18x2_224 in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1294: UserWarning: Overwriting vit_base_patch16_18x2_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1331: UserWarning: Overwriting vit_base_patch16_224_dino in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_dino. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "Original train size:  50000\n",
            "Sampled train size:  12500\n",
            "Original train size:  50000\n",
            "Sampled train size:  12500\n",
            "100\n",
            "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, 13, 14, 15, 16, 17, 18, 19], [20, 21, 22, 23, 24, 25, 26, 27, 28, 29], [30, 31, 32, 33, 34, 35, 36, 37, 38, 39], [40, 41, 42, 43, 44, 45, 46, 47, 48, 49], [50, 51, 52, 53, 54, 55, 56, 57, 58, 59], [60, 61, 62, 63, 64, 65, 66, 67, 68, 69], [70, 71, 72, 73, 74, 75, 76, 77, 78, 79], [80, 81, 82, 83, 84, 85, 86, 87, 88, 89], [90, 91, 92, 93, 94, 95, 96, 97, 98, 99]]\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Creating original model: vit_small_patch16_224.dino\n",
            "[Sequential(\n",
            "  (0): Linear(in_features=384, out_features=768, bias=True)\n",
            "  (1): GELU(approximate='none')\n",
            "  (2): Dropout(p=0.0, inplace=False)\n",
            "), Sequential(\n",
            "  (0): Linear(in_features=768, out_features=384, bias=True)\n",
            "  (1): Dropout(p=0.0, inplace=False)\n",
            ")]\n",
            "Creating model: vit_small_patch16_224.dino\n",
            "Namespace(subparser_name='cifar100_hideprompt_5e', pct=0.25, batch_size=24, epochs=1, original_model='vit_small_patch16_224.dino', model='vit_small_patch16_224.dino', input_size=224, pretrained=True, drop=0.0, drop_path=0.0, opt='adam', opt_eps=1e-08, opt_betas=(0.9, 0.999), clip_grad=1.0, momentum=0.9, weight_decay=0.0, reinit_optimizer=True, sched='step', lr=0.03, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=0, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, unscale_lr=True, color_jitter=None, aa=None, smoothing=0.1, train_interpolation='bicubic', reprob=0.0, remode='pixel', recount=1, data_path='./datasets/', dataset='Split-CIFAR100', shuffle=False, output_dir='./output/cifar100_full_dino_1epoch_final_25pct', device='cuda', seed=20, eval=False, num_workers=4, pin_mem=True, world_size=1, dist_url='env://', num_tasks=10, train_mask=True, task_inc=False, use_g_prompt=False, g_prompt_length=5, g_prompt_layer_idx=[], use_prefix_tune_for_g_prompt=False, use_e_prompt=True, e_prompt_layer_idx=[0, 1, 2, 3, 4], use_prefix_tune_for_e_prompt=True, larger_prompt_lr=True, prompt_pool=True, size=10, length=5, top_k=1, initializer='uniform', prompt_key=False, prompt_key_init='uniform', use_prompt_mask=True, mask_first_epoch=False, shared_prompt_pool=True, shared_prompt_key=False, batchwise_prompt=False, embedding_key='cls', predefined_key='', pull_constraint=True, pull_constraint_coeff=1.0, same_key_value=False, global_pool='token', head_type='token', freeze=['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed'], crct_epochs=1, train_inference_task_only=False, original_model_mlp_structure=[2], ca_lr=0.005, milestones=[10], trained_original_model='./output/cifar100_full_dino_1epoch_25pct', prompt_momentum=0.1, reg=0.1, not_train_ca=False, ca_epochs=30, ca_storage_efficient_method='multi-centroid', n_centroids=10, print_freq=10, config='cifar100_hideprompt_5e', rank=0, gpu=0, distributed=True, dist_backend='nccl', nb_classes=100)\n",
            "number of params: 230500\n",
            "Start training for 1 epochs\n",
            "Loading checkpoint from: ./output/cifar100_full_dino_1epoch_25pct/checkpoint/task1_checkpoint.pth\n",
            "[rank0]:[W1006 10:57:02.655868532 reducer.cpp:1457] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/engines/hide_promtp_wtp_and_tap_engine.py:545: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  loss = torch.nn.functional.cross_entropy(sim, torch.range(0, sim.shape[0] - 1).long().to(device))\n",
            "Train: Epoch[1/1]  [ 0/54]  eta: 0:02:14  Lr: 0.002812  Loss: 3.3383  Acc@1: 16.6667 (16.6667)  Acc@5: 58.3333 (58.3333)  time: 2.4911  data: 1.2502  max mem: 1368\n",
            "Train: Epoch[1/1]  [10/54]  eta: 0:00:19  Lr: 0.002812  Loss: 2.2149  Acc@1: 16.6667 (16.2879)  Acc@5: 50.0000 (53.4091)  time: 0.4417  data: 0.1139  max mem: 1370\n",
            "Train: Epoch[1/1]  [20/54]  eta: 0:00:11  Lr: 0.002812  Loss: 2.3892  Acc@1: 16.6667 (18.6508)  Acc@5: 58.3333 (59.7222)  time: 0.2380  data: 0.0004  max mem: 1370\n",
            "Train: Epoch[1/1]  [30/54]  eta: 0:00:07  Lr: 0.002812  Loss: 1.9842  Acc@1: 20.8333 (20.0269)  Acc@5: 75.0000 (64.9194)  time: 0.2412  data: 0.0009  max mem: 1370\n",
            "Train: Epoch[1/1]  [40/54]  eta: 0:00:04  Lr: 0.002812  Loss: 1.6835  Acc@1: 29.1667 (25.7114)  Acc@5: 79.1667 (69.6138)  time: 0.2419  data: 0.0009  max mem: 1370\n",
            "Train: Epoch[1/1]  [50/54]  eta: 0:00:01  Lr: 0.002812  Loss: 1.0219  Acc@1: 45.8333 (30.3922)  Acc@5: 87.5000 (73.5294)  time: 0.2407  data: 0.0003  max mem: 1370\n",
            "Train: Epoch[1/1]  [53/54]  eta: 0:00:00  Lr: 0.002812  Loss: 1.3430  Acc@1: 45.8333 (31.0748)  Acc@5: 87.5000 (74.2991)  time: 0.2397  data: 0.0003  max mem: 1370\n",
            "Train: Epoch[1/1] Total time: 0:00:15 (0.2826 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.3430  Acc@1: 45.8333 (31.0748)  Acc@5: 87.5000 (74.2991)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:18  Loss: 2.9719 (2.9719)  Acc@1: 41.6667 (41.6667)  Acc@5: 54.1667 (54.1667)  Acc@task: 100.0000 (100.0000)  time: 0.4335  data: 0.2702  max mem: 1370\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:05  Loss: 3.2749 (3.2952)  Acc@1: 20.8333 (23.8636)  Acc@5: 50.0000 (48.4848)  Acc@task: 100.0000 (100.0000)  time: 0.1816  data: 0.0251  max mem: 1370\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:03  Loss: 3.2661 (3.2614)  Acc@1: 20.8333 (24.0079)  Acc@5: 45.8333 (48.6111)  Acc@task: 100.0000 (100.0000)  time: 0.1572  data: 0.0004  max mem: 1370\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 3.1822 (3.2148)  Acc@1: 20.8333 (24.1935)  Acc@5: 50.0000 (50.1344)  Acc@task: 100.0000 (100.0000)  time: 0.1578  data: 0.0003  max mem: 1370\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 3.0914 (3.1983)  Acc@1: 25.0000 (24.5935)  Acc@5: 50.0000 (50.3049)  Acc@task: 100.0000 (100.0000)  time: 0.1577  data: 0.0003  max mem: 1370\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 3.0657 (3.1857)  Acc@1: 25.0000 (24.7000)  Acc@5: 50.0000 (50.5000)  Acc@task: 100.0000 (100.0000)  time: 0.1600  data: 0.0002  max mem: 1370\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1670 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@task 100.000 Acc@1 24.700 Acc@5 50.500 loss 3.186\n",
            "[Average accuracy till task1]\tAcc@task: 100.0000\tAcc@1: 24.7000\tAcc@5: 50.5000\tLoss: 3.1857\n",
            "Loading checkpoint from: ./output/cifar100_full_dino_1epoch_25pct/checkpoint/task2_checkpoint.pth\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/engines/hide_promtp_wtp_and_tap_engine.py:540: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  loss = torch.nn.functional.cross_entropy(sim, torch.range(0, sim.shape[0] - 1).long().to(device))\n",
            "Train: Epoch[1/1]  [ 0/52]  eta: 0:00:42  Lr: 0.002812  Loss: 4.3146  Acc@1: 16.6667 (16.6667)  Acc@5: 50.0000 (50.0000)  time: 0.8123  data: 0.3593  max mem: 1371\n",
            "Train: Epoch[1/1]  [10/52]  eta: 0:00:12  Lr: 0.002812  Loss: 3.0070  Acc@1: 12.5000 (11.7424)  Acc@5: 50.0000 (49.6212)  time: 0.3018  data: 0.0350  max mem: 1373\n",
            "Train: Epoch[1/1]  [20/52]  eta: 0:00:08  Lr: 0.002812  Loss: 2.6126  Acc@1: 12.5000 (13.2937)  Acc@5: 54.1667 (53.7698)  time: 0.2520  data: 0.0019  max mem: 1373\n",
            "Train: Epoch[1/1]  [30/52]  eta: 0:00:05  Lr: 0.002812  Loss: 2.6176  Acc@1: 16.6667 (15.4570)  Acc@5: 62.5000 (57.9301)  time: 0.2541  data: 0.0011  max mem: 1373\n",
            "Train: Epoch[1/1]  [40/52]  eta: 0:00:03  Lr: 0.002812  Loss: 2.2188  Acc@1: 20.8333 (20.0203)  Acc@5: 75.0000 (63.9228)  time: 0.2518  data: 0.0007  max mem: 1373\n",
            "Train: Epoch[1/1]  [50/52]  eta: 0:00:00  Lr: 0.002812  Loss: 2.1738  Acc@1: 37.5000 (24.1830)  Acc@5: 83.3333 (67.8105)  time: 0.2497  data: 0.0004  max mem: 1373\n",
            "Train: Epoch[1/1]  [51/52]  eta: 0:00:00  Lr: 0.002812  Loss: 0.5881  Acc@1: 37.5000 (24.2449)  Acc@5: 83.3333 (67.8367)  time: 0.2421  data: 0.0004  max mem: 1373\n",
            "Train: Epoch[1/1] Total time: 0:00:13 (0.2611 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.5881  Acc@1: 37.5000 (24.2449)  Acc@5: 83.3333 (67.8367)\n",
            "torch.Size([5, 2, 5, 6, 64])\n",
            "torch.Size([5, 2, 1, 5, 6, 64])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:23  Loss: 2.9851 (2.9851)  Acc@1: 41.6667 (41.6667)  Acc@5: 54.1667 (54.1667)  Acc@task: 95.8333 (95.8333)  time: 0.5574  data: 0.4006  max mem: 1373\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 3.2782 (3.2882)  Acc@1: 20.8333 (22.3485)  Acc@5: 50.0000 (48.1061)  Acc@task: 91.6667 (90.1515)  time: 0.1988  data: 0.0369  max mem: 1373\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 3.2369 (3.2498)  Acc@1: 20.8333 (22.0238)  Acc@5: 45.8333 (47.2222)  Acc@task: 91.6667 (90.8730)  time: 0.1645  data: 0.0005  max mem: 1373\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 3.1499 (3.2087)  Acc@1: 20.8333 (21.7742)  Acc@5: 50.0000 (49.3280)  Acc@task: 95.8333 (92.2043)  time: 0.1655  data: 0.0004  max mem: 1373\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 3.1012 (3.1911)  Acc@1: 20.8333 (22.7642)  Acc@5: 50.0000 (49.8984)  Acc@task: 95.8333 (92.8862)  time: 0.1653  data: 0.0002  max mem: 1373\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 3.0675 (3.1781)  Acc@1: 20.8333 (23.0000)  Acc@5: 50.0000 (50.1000)  Acc@task: 95.8333 (92.9000)  time: 0.1628  data: 0.0002  max mem: 1373\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1748 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@task 92.900 Acc@1 23.000 Acc@5 50.100 loss 3.178\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:20  Loss: 4.0661 (4.0661)  Acc@1: 4.1667 (4.1667)  Acc@5: 33.3333 (33.3333)  Acc@task: 91.6667 (91.6667)  time: 0.4984  data: 0.3196  max mem: 1373\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 3.9053 (4.0019)  Acc@1: 4.1667 (5.6818)  Acc@5: 29.1667 (31.4394)  Acc@task: 87.5000 (85.2273)  time: 0.1966  data: 0.0306  max mem: 1373\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 3.8179 (3.8824)  Acc@1: 8.3333 (8.1349)  Acc@5: 33.3333 (33.9286)  Acc@task: 87.5000 (85.9127)  time: 0.1673  data: 0.0014  max mem: 1373\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 3.7974 (3.8834)  Acc@1: 8.3333 (8.8710)  Acc@5: 29.1667 (32.2581)  Acc@task: 87.5000 (86.1559)  time: 0.1677  data: 0.0010  max mem: 1373\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 3.8572 (3.8602)  Acc@1: 8.3333 (8.2317)  Acc@5: 29.1667 (33.4350)  Acc@task: 83.3333 (85.6707)  time: 0.1668  data: 0.0006  max mem: 1373\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 3.7602 (3.8452)  Acc@1: 8.3333 (8.4000)  Acc@5: 33.3333 (33.7000)  Acc@task: 83.3333 (85.7000)  time: 0.1642  data: 0.0005  max mem: 1373\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1760 s / it)\n",
            "* Acc@task 85.700 Acc@1 8.400 Acc@5 33.700 loss 3.845\n",
            "[Average accuracy till task2]\tAcc@task: 89.3000\tAcc@1: 15.7000\tAcc@5: 41.9000\tLoss: 3.5116\tForgetting: 0.0000\tBackward: 23.0000\n",
            "torch.Size([23400, 384])\n",
            "Averaged stats: Lr: 0.005000  Loss: 0.4578  Acc@1: 61.6667 (64.7500)  Acc@5: 88.3333 (91.5000)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:17  Loss: 0.8258 (0.8258)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  Acc@task: 95.8333 (95.8333)  time: 0.4097  data: 0.2445  max mem: 1373\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 1.0924 (1.2308)  Acc@1: 66.6667 (66.2879)  Acc@5: 91.6667 (89.3939)  Acc@task: 91.6667 (90.1515)  time: 0.1920  data: 0.0246  max mem: 1373\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:03  Loss: 1.0924 (1.1690)  Acc@1: 66.6667 (68.6508)  Acc@5: 87.5000 (89.6825)  Acc@task: 91.6667 (90.8730)  time: 0.1693  data: 0.0014  max mem: 1373\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.9988 (1.0920)  Acc@1: 75.0000 (70.4301)  Acc@5: 91.6667 (90.9946)  Acc@task: 95.8333 (92.2043)  time: 0.1686  data: 0.0003  max mem: 1373\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.9230 (1.0584)  Acc@1: 75.0000 (71.7480)  Acc@5: 91.6667 (91.2602)  Acc@task: 95.8333 (92.8862)  time: 0.1685  data: 0.0002  max mem: 1373\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.8189 (1.0515)  Acc@1: 75.0000 (72.0000)  Acc@5: 95.8333 (91.4000)  Acc@task: 95.8333 (92.9000)  time: 0.1658  data: 0.0002  max mem: 1373\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1761 s / it)\n",
            "* Acc@task 92.900 Acc@1 72.000 Acc@5 91.400 loss 1.052\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:33  Loss: 0.9910 (0.9910)  Acc@1: 70.8333 (70.8333)  Acc@5: 100.0000 (100.0000)  Acc@task: 91.6667 (91.6667)  time: 0.7950  data: 0.6096  max mem: 1373\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:07  Loss: 0.7770 (0.8610)  Acc@1: 79.1667 (76.1364)  Acc@5: 95.8333 (95.4545)  Acc@task: 87.5000 (85.2273)  time: 0.2262  data: 0.0561  max mem: 1373\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.9005 (0.9159)  Acc@1: 79.1667 (75.9921)  Acc@5: 91.6667 (93.0556)  Acc@task: 87.5000 (85.9127)  time: 0.1687  data: 0.0008  max mem: 1373\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.8855 (0.9031)  Acc@1: 75.0000 (76.8817)  Acc@5: 95.8333 (93.5484)  Acc@task: 87.5000 (86.1559)  time: 0.1679  data: 0.0006  max mem: 1373\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.7929 (0.8734)  Acc@1: 79.1667 (78.1504)  Acc@5: 95.8333 (93.5976)  Acc@task: 83.3333 (85.6707)  time: 0.1670  data: 0.0002  max mem: 1373\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.7785 (0.8688)  Acc@1: 79.1667 (78.3000)  Acc@5: 95.8333 (93.7000)  Acc@task: 83.3333 (85.7000)  time: 0.1643  data: 0.0002  max mem: 1373\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1837 s / it)\n",
            "* Acc@task 85.700 Acc@1 78.300 Acc@5 93.700 loss 0.869\n",
            "[Average accuracy till task2]\tAcc@task: 89.3000\tAcc@1: 75.1500\tAcc@5: 92.5500\tLoss: 0.9602\tForgetting: 0.0000\tBackward: 47.3000\n",
            "Loading checkpoint from: ./output/cifar100_full_dino_1epoch_25pct/checkpoint/task3_checkpoint.pth\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/engines/hide_promtp_wtp_and_tap_engine.py:540: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  loss = torch.nn.functional.cross_entropy(sim, torch.range(0, sim.shape[0] - 1).long().to(device))\n",
            "Train: Epoch[1/1]  [ 0/54]  eta: 0:00:36  Lr: 0.002812  Loss: 3.7012  Acc@1: 20.8333 (20.8333)  Acc@5: 66.6667 (66.6667)  time: 0.6739  data: 0.3532  max mem: 1375\n",
            "Train: Epoch[1/1]  [10/54]  eta: 0:00:12  Lr: 0.002812  Loss: 2.3385  Acc@1: 8.3333 (11.3636)  Acc@5: 58.3333 (59.0909)  time: 0.2948  data: 0.0324  max mem: 1377\n",
            "Train: Epoch[1/1]  [20/54]  eta: 0:00:09  Lr: 0.002812  Loss: 3.3626  Acc@1: 12.5000 (14.4841)  Acc@5: 58.3333 (59.1270)  time: 0.2578  data: 0.0004  max mem: 1377\n",
            "Train: Epoch[1/1]  [30/54]  eta: 0:00:06  Lr: 0.002812  Loss: 2.6021  Acc@1: 20.8333 (18.6828)  Acc@5: 66.6667 (62.9032)  time: 0.2580  data: 0.0007  max mem: 1377\n",
            "Train: Epoch[1/1]  [40/54]  eta: 0:00:03  Lr: 0.002812  Loss: 1.8634  Acc@1: 33.3333 (22.6626)  Acc@5: 75.0000 (66.8699)  time: 0.2563  data: 0.0006  max mem: 1377\n",
            "Train: Epoch[1/1]  [50/54]  eta: 0:00:01  Lr: 0.002812  Loss: 1.6330  Acc@1: 37.5000 (27.4510)  Acc@5: 83.3333 (70.6699)  time: 0.2543  data: 0.0003  max mem: 1377\n",
            "Train: Epoch[1/1]  [53/54]  eta: 0:00:00  Lr: 0.002812  Loss: 0.9978  Acc@1: 45.8333 (29.1051)  Acc@5: 87.5000 (71.5175)  time: 0.2543  data: 0.0003  max mem: 1377\n",
            "Train: Epoch[1/1] Total time: 0:00:14 (0.2655 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.9978  Acc@1: 45.8333 (29.1051)  Acc@5: 87.5000 (71.5175)\n",
            "torch.Size([5, 2, 5, 6, 64])\n",
            "torch.Size([5, 2, 1, 5, 6, 64])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:18  Loss: 0.8272 (0.8272)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  Acc@task: 95.8333 (95.8333)  time: 0.4493  data: 0.2845  max mem: 1377\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:05  Loss: 1.0982 (1.2249)  Acc@1: 66.6667 (67.0455)  Acc@5: 91.6667 (89.0152)  Acc@task: 87.5000 (84.4697)  time: 0.1874  data: 0.0264  max mem: 1377\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:03  Loss: 1.0982 (1.1652)  Acc@1: 66.6667 (69.0476)  Acc@5: 87.5000 (89.4841)  Acc@task: 87.5000 (84.5238)  time: 0.1608  data: 0.0004  max mem: 1377\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.9965 (1.0910)  Acc@1: 75.0000 (70.6989)  Acc@5: 91.6667 (90.9946)  Acc@task: 87.5000 (86.0215)  time: 0.1611  data: 0.0003  max mem: 1377\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.9159 (1.0566)  Acc@1: 75.0000 (71.9512)  Acc@5: 91.6667 (91.2602)  Acc@task: 91.6667 (87.1951)  time: 0.1613  data: 0.0002  max mem: 1377\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.8145 (1.0497)  Acc@1: 75.0000 (72.1000)  Acc@5: 95.8333 (91.4000)  Acc@task: 91.6667 (87.3000)  time: 0.1586  data: 0.0002  max mem: 1377\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1688 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@task 87.300 Acc@1 72.100 Acc@5 91.400 loss 1.050\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:21  Loss: 0.9896 (0.9896)  Acc@1: 70.8333 (70.8333)  Acc@5: 100.0000 (100.0000)  Acc@task: 87.5000 (87.5000)  time: 0.5168  data: 0.3589  max mem: 1377\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.7648 (0.8458)  Acc@1: 79.1667 (76.5152)  Acc@5: 95.8333 (95.4545)  Acc@task: 87.5000 (88.2576)  time: 0.1935  data: 0.0335  max mem: 1377\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:03  Loss: 0.8561 (0.8983)  Acc@1: 79.1667 (76.1905)  Acc@5: 91.6667 (93.0556)  Acc@task: 91.6667 (88.8889)  time: 0.1617  data: 0.0009  max mem: 1377\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.8561 (0.8866)  Acc@1: 75.0000 (77.0161)  Acc@5: 95.8333 (93.5484)  Acc@task: 91.6667 (89.6505)  time: 0.1617  data: 0.0009  max mem: 1377\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.7829 (0.8576)  Acc@1: 79.1667 (78.4553)  Acc@5: 95.8333 (93.8008)  Acc@task: 87.5000 (88.6179)  time: 0.1613  data: 0.0008  max mem: 1377\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.7657 (0.8530)  Acc@1: 79.1667 (78.6000)  Acc@5: 95.8333 (93.9000)  Acc@task: 87.5000 (88.8000)  time: 0.1588  data: 0.0008  max mem: 1377\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1707 s / it)\n",
            "* Acc@task 88.800 Acc@1 78.600 Acc@5 93.900 loss 0.853\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:23  Loss: 4.6593 (4.6593)  Acc@1: 0.0000 (0.0000)  Acc@5: 37.5000 (37.5000)  Acc@task: 79.1667 (79.1667)  time: 0.5558  data: 0.3913  max mem: 1377\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 4.6637 (4.6994)  Acc@1: 4.1667 (3.0303)  Acc@5: 33.3333 (33.3333)  Acc@task: 79.1667 (81.0606)  time: 0.1969  data: 0.0358  max mem: 1377\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:03  Loss: 4.5933 (4.5127)  Acc@1: 4.1667 (3.7698)  Acc@5: 37.5000 (37.8968)  Acc@task: 79.1667 (80.3571)  time: 0.1618  data: 0.0003  max mem: 1377\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 4.4426 (4.5120)  Acc@1: 4.1667 (3.8978)  Acc@5: 41.6667 (39.3817)  Acc@task: 79.1667 (81.5860)  time: 0.1622  data: 0.0003  max mem: 1377\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 4.4426 (4.4885)  Acc@1: 4.1667 (3.9634)  Acc@5: 41.6667 (39.3293)  Acc@task: 79.1667 (81.4024)  time: 0.1615  data: 0.0003  max mem: 1377\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 4.4220 (4.4793)  Acc@1: 4.1667 (4.1000)  Acc@5: 41.6667 (39.5000)  Acc@task: 79.1667 (81.4000)  time: 0.1590  data: 0.0003  max mem: 1377\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1718 s / it)\n",
            "* Acc@task 81.400 Acc@1 4.100 Acc@5 39.500 loss 4.479\n",
            "[Average accuracy till task3]\tAcc@task: 85.8333\tAcc@1: 51.6000\tAcc@5: 74.9333\tLoss: 2.1274\tForgetting: 0.0000\tBackward: 71.1500\n",
            "torch.Size([35160, 384])\n",
            "Averaged stats: Lr: 0.005000  Loss: 0.1613  Acc@1: 84.1667 (80.6667)  Acc@5: 96.6667 (95.7500)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:21  Loss: 0.4780 (0.4780)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 95.8333 (95.8333)  time: 0.5098  data: 0.3343  max mem: 1377\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.7845 (0.7139)  Acc@1: 79.1667 (78.7879)  Acc@5: 100.0000 (96.9697)  Acc@task: 87.5000 (84.4697)  time: 0.1941  data: 0.0314  max mem: 1377\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:03  Loss: 0.6607 (0.6897)  Acc@1: 79.1667 (79.3651)  Acc@5: 95.8333 (96.4286)  Acc@task: 87.5000 (84.5238)  time: 0.1624  data: 0.0014  max mem: 1377\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.6193 (0.6774)  Acc@1: 87.5000 (81.1828)  Acc@5: 95.8333 (96.9086)  Acc@task: 87.5000 (86.0215)  time: 0.1620  data: 0.0012  max mem: 1377\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6193 (0.6734)  Acc@1: 87.5000 (81.6057)  Acc@5: 100.0000 (96.9512)  Acc@task: 91.6667 (87.1951)  time: 0.1616  data: 0.0005  max mem: 1377\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6052 (0.6649)  Acc@1: 87.5000 (81.7000)  Acc@5: 100.0000 (97.0000)  Acc@task: 91.6667 (87.3000)  time: 0.1590  data: 0.0003  max mem: 1377\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1713 s / it)\n",
            "* Acc@task 87.300 Acc@1 81.700 Acc@5 97.000 loss 0.665\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:20  Loss: 0.8926 (0.8926)  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.8333)  Acc@task: 87.5000 (87.5000)  time: 0.4882  data: 0.3204  max mem: 1377\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.9785 (0.9879)  Acc@1: 70.8333 (72.7273)  Acc@5: 95.8333 (94.3182)  Acc@task: 87.5000 (88.2576)  time: 0.1935  data: 0.0297  max mem: 1377\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:03  Loss: 0.9925 (1.0781)  Acc@1: 70.8333 (71.0317)  Acc@5: 91.6667 (92.8571)  Acc@task: 91.6667 (88.8889)  time: 0.1649  data: 0.0005  max mem: 1377\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 1.0078 (1.0627)  Acc@1: 70.8333 (71.5054)  Acc@5: 91.6667 (93.1452)  Acc@task: 91.6667 (89.6505)  time: 0.1650  data: 0.0005  max mem: 1377\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.9633 (1.0423)  Acc@1: 70.8333 (72.0528)  Acc@5: 91.6667 (93.0894)  Acc@task: 87.5000 (88.6179)  time: 0.1637  data: 0.0004  max mem: 1377\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.8647 (1.0292)  Acc@1: 75.0000 (72.3000)  Acc@5: 91.6667 (93.2000)  Acc@task: 87.5000 (88.8000)  time: 0.1611  data: 0.0003  max mem: 1377\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1729 s / it)\n",
            "* Acc@task 88.800 Acc@1 72.300 Acc@5 93.200 loss 1.029\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:35  Loss: 0.8811 (0.8811)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  Acc@task: 79.1667 (79.1667)  time: 0.8470  data: 0.6655  max mem: 1377\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:07  Loss: 0.8811 (0.7848)  Acc@1: 83.3333 (82.1970)  Acc@5: 95.8333 (93.5606)  Acc@task: 79.1667 (81.0606)  time: 0.2273  data: 0.0617  max mem: 1377\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.7571 (0.7519)  Acc@1: 83.3333 (80.9524)  Acc@5: 91.6667 (93.4524)  Acc@task: 79.1667 (80.3571)  time: 0.1650  data: 0.0012  max mem: 1377\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.6777 (0.7217)  Acc@1: 83.3333 (82.7957)  Acc@5: 95.8333 (94.0860)  Acc@task: 79.1667 (81.5860)  time: 0.1649  data: 0.0007  max mem: 1377\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.6707 (0.7303)  Acc@1: 83.3333 (83.1301)  Acc@5: 95.8333 (94.2073)  Acc@task: 79.1667 (81.4024)  time: 0.1651  data: 0.0002  max mem: 1377\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.6434 (0.7210)  Acc@1: 83.3333 (83.1000)  Acc@5: 95.8333 (94.3000)  Acc@task: 79.1667 (81.4000)  time: 0.1622  data: 0.0002  max mem: 1377\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1822 s / it)\n",
            "* Acc@task 81.400 Acc@1 83.100 Acc@5 94.300 loss 0.721\n",
            "[Average accuracy till task3]\tAcc@task: 85.8333\tAcc@1: 79.0333\tAcc@5: 94.8333\tLoss: 0.8051\tForgetting: 3.0000\tBackward: 25.5000\n",
            "Loading checkpoint from: ./output/cifar100_full_dino_1epoch_25pct/checkpoint/task4_checkpoint.pth\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/engines/hide_promtp_wtp_and_tap_engine.py:540: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  loss = torch.nn.functional.cross_entropy(sim, torch.range(0, sim.shape[0] - 1).long().to(device))\n",
            "Train: Epoch[1/1]  [ 0/51]  eta: 0:00:33  Lr: 0.002812  Loss: 3.1986  Acc@1: 20.8333 (20.8333)  Acc@5: 62.5000 (62.5000)  time: 0.6643  data: 0.3610  max mem: 1377\n",
            "Train: Epoch[1/1]  [10/51]  eta: 0:00:12  Lr: 0.002812  Loss: 3.0092  Acc@1: 12.5000 (15.1515)  Acc@5: 66.6667 (64.0152)  time: 0.2935  data: 0.0331  max mem: 1377\n",
            "Train: Epoch[1/1]  [20/51]  eta: 0:00:08  Lr: 0.002812  Loss: 1.9958  Acc@1: 20.8333 (21.0317)  Acc@5: 66.6667 (68.2540)  time: 0.2569  data: 0.0003  max mem: 1377\n",
            "Train: Epoch[1/1]  [30/51]  eta: 0:00:05  Lr: 0.002812  Loss: 1.6392  Acc@1: 29.1667 (25.1344)  Acc@5: 79.1667 (73.1183)  time: 0.2582  data: 0.0006  max mem: 1377\n",
            "Train: Epoch[1/1]  [40/51]  eta: 0:00:02  Lr: 0.002812  Loss: 1.6339  Acc@1: 41.6667 (30.5894)  Acc@5: 83.3333 (76.5244)  time: 0.2590  data: 0.0011  max mem: 1377\n",
            "Train: Epoch[1/1]  [50/51]  eta: 0:00:00  Lr: 0.002812  Loss: 1.3381  Acc@1: 50.0000 (35.6265)  Acc@5: 91.6667 (79.3612)  time: 0.2571  data: 0.0007  max mem: 1377\n",
            "Train: Epoch[1/1] Total time: 0:00:13 (0.2672 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.3381  Acc@1: 50.0000 (35.6265)  Acc@5: 91.6667 (79.3612)\n",
            "torch.Size([5, 2, 5, 6, 64])\n",
            "torch.Size([5, 2, 1, 5, 6, 64])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:18  Loss: 0.4928 (0.4928)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  Acc@task: 95.8333 (95.8333)  time: 0.4353  data: 0.2668  max mem: 1377\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.7840 (0.7214)  Acc@1: 79.1667 (78.4091)  Acc@5: 100.0000 (96.5909)  Acc@task: 87.5000 (85.2273)  time: 0.1879  data: 0.0245  max mem: 1377\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:03  Loss: 0.6623 (0.6930)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (96.4286)  Acc@task: 83.3333 (85.3175)  time: 0.1642  data: 0.0003  max mem: 1377\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.6123 (0.6815)  Acc@1: 83.3333 (80.7796)  Acc@5: 100.0000 (96.9086)  Acc@task: 87.5000 (86.2903)  time: 0.1649  data: 0.0003  max mem: 1377\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6123 (0.6832)  Acc@1: 83.3333 (81.3008)  Acc@5: 100.0000 (96.8496)  Acc@task: 91.6667 (86.8902)  time: 0.1630  data: 0.0002  max mem: 1377\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6121 (0.6745)  Acc@1: 87.5000 (81.4000)  Acc@5: 100.0000 (96.9000)  Acc@task: 91.6667 (87.0000)  time: 0.1603  data: 0.0002  max mem: 1377\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1709 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@task 87.000 Acc@1 81.400 Acc@5 96.900 loss 0.674\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:21  Loss: 0.9358 (0.9358)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  Acc@task: 91.6667 (91.6667)  time: 0.5112  data: 0.3485  max mem: 1377\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.9710 (0.9936)  Acc@1: 70.8333 (72.3485)  Acc@5: 91.6667 (93.5606)  Acc@task: 83.3333 (84.8485)  time: 0.1939  data: 0.0320  max mem: 1377\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:03  Loss: 0.9884 (1.0815)  Acc@1: 75.0000 (71.0317)  Acc@5: 91.6667 (92.0635)  Acc@task: 83.3333 (84.1270)  time: 0.1639  data: 0.0004  max mem: 1377\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.9936 (1.0624)  Acc@1: 75.0000 (71.7742)  Acc@5: 91.6667 (92.7419)  Acc@task: 83.3333 (84.9462)  time: 0.1645  data: 0.0006  max mem: 1377\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.9784 (1.0422)  Acc@1: 75.0000 (72.1545)  Acc@5: 91.6667 (92.7846)  Acc@task: 83.3333 (84.5528)  time: 0.1631  data: 0.0008  max mem: 1377\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.8377 (1.0290)  Acc@1: 75.0000 (72.4000)  Acc@5: 91.6667 (92.9000)  Acc@task: 83.3333 (84.8000)  time: 0.1604  data: 0.0007  max mem: 1377\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1727 s / it)\n",
            "* Acc@task 84.800 Acc@1 72.400 Acc@5 92.900 loss 1.029\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:22  Loss: 0.7918 (0.7918)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  Acc@task: 100.0000 (100.0000)  time: 0.5343  data: 0.3770  max mem: 1377\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 0.7918 (0.7427)  Acc@1: 83.3333 (82.9545)  Acc@5: 95.8333 (94.3182)  Acc@task: 83.3333 (85.6061)  time: 0.1959  data: 0.0345  max mem: 1377\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:03  Loss: 0.6919 (0.7138)  Acc@1: 83.3333 (81.5476)  Acc@5: 95.8333 (93.8492)  Acc@task: 83.3333 (85.3175)  time: 0.1635  data: 0.0003  max mem: 1377\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.6237 (0.6870)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (94.4892)  Acc@task: 87.5000 (86.2903)  time: 0.1643  data: 0.0003  max mem: 1377\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.6237 (0.6922)  Acc@1: 83.3333 (83.6382)  Acc@5: 95.8333 (94.6138)  Acc@task: 87.5000 (86.3821)  time: 0.1633  data: 0.0002  max mem: 1377\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.6131 (0.6838)  Acc@1: 83.3333 (83.6000)  Acc@5: 95.8333 (94.7000)  Acc@task: 87.5000 (86.2000)  time: 0.1607  data: 0.0002  max mem: 1377\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1731 s / it)\n",
            "* Acc@task 86.200 Acc@1 83.600 Acc@5 94.700 loss 0.684\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:21  Loss: 5.6060 (5.6060)  Acc@1: 12.5000 (12.5000)  Acc@5: 33.3333 (33.3333)  Acc@task: 66.6667 (66.6667)  time: 0.5169  data: 0.3547  max mem: 1377\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:06  Loss: 5.5248 (5.5624)  Acc@1: 4.1667 (5.3030)  Acc@5: 25.0000 (26.8939)  Acc@task: 79.1667 (79.1667)  time: 0.1947  data: 0.0325  max mem: 1377\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:03  Loss: 5.4710 (5.5738)  Acc@1: 4.1667 (3.9683)  Acc@5: 25.0000 (26.1905)  Acc@task: 79.1667 (76.7857)  time: 0.1634  data: 0.0004  max mem: 1377\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 5.3377 (5.4845)  Acc@1: 4.1667 (3.8978)  Acc@5: 29.1667 (28.0914)  Acc@task: 75.0000 (76.6129)  time: 0.1638  data: 0.0007  max mem: 1377\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 5.3911 (5.4940)  Acc@1: 4.1667 (3.6585)  Acc@5: 29.1667 (27.2358)  Acc@task: 79.1667 (77.0325)  time: 0.1631  data: 0.0006  max mem: 1377\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 5.4067 (5.4920)  Acc@1: 4.1667 (3.6000)  Acc@5: 25.0000 (27.2000)  Acc@task: 79.1667 (77.2000)  time: 0.1607  data: 0.0006  max mem: 1377\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1725 s / it)\n",
            "* Acc@task 77.200 Acc@1 3.600 Acc@5 27.200 loss 5.492\n",
            "[Average accuracy till task4]\tAcc@task: 83.8000\tAcc@1: 60.2500\tAcc@5: 77.9250\tLoss: 1.9698\tForgetting: 2.0667\tBackward: 74.9667\n",
            "torch.Size([47040, 384])\n",
            "Averaged stats: Lr: 0.005000  Loss: 0.1558  Acc@1: 93.3333 (87.8611)  Acc@5: 100.0000 (97.4722)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:21  Loss: 0.4276 (0.4276)  Acc@1: 91.6667 (91.6667)  Acc@5: 95.8333 (95.8333)  Acc@task: 95.8333 (95.8333)  time: 0.5076  data: 0.3371  max mem: 1377\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.6390 (0.7062)  Acc@1: 83.3333 (79.9242)  Acc@5: 100.0000 (97.3485)  Acc@task: 87.5000 (85.2273)  time: 0.1940  data: 0.0312  max mem: 1377\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:03  Loss: 0.6390 (0.6658)  Acc@1: 79.1667 (81.9444)  Acc@5: 95.8333 (97.0238)  Acc@task: 83.3333 (85.3175)  time: 0.1641  data: 0.0005  max mem: 1377\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.5395 (0.6425)  Acc@1: 87.5000 (82.5269)  Acc@5: 100.0000 (97.4462)  Acc@task: 87.5000 (86.2903)  time: 0.1642  data: 0.0003  max mem: 1377\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.5840 (0.6424)  Acc@1: 83.3333 (83.0285)  Acc@5: 100.0000 (97.5610)  Acc@task: 91.6667 (86.8902)  time: 0.1629  data: 0.0003  max mem: 1377\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.5820 (0.6366)  Acc@1: 87.5000 (83.2000)  Acc@5: 100.0000 (97.6000)  Acc@task: 91.6667 (87.0000)  time: 0.1603  data: 0.0003  max mem: 1377\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1726 s / it)\n",
            "* Acc@task 87.000 Acc@1 83.200 Acc@5 97.600 loss 0.637\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:18  Loss: 1.0521 (1.0521)  Acc@1: 66.6667 (66.6667)  Acc@5: 95.8333 (95.8333)  Acc@task: 91.6667 (91.6667)  time: 0.4394  data: 0.2627  max mem: 1377\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.8915 (0.9303)  Acc@1: 79.1667 (75.7576)  Acc@5: 95.8333 (96.2121)  Acc@task: 83.3333 (84.8485)  time: 0.1922  data: 0.0284  max mem: 1377\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:03  Loss: 0.8408 (0.9648)  Acc@1: 75.0000 (73.6111)  Acc@5: 95.8333 (95.4365)  Acc@task: 83.3333 (84.1270)  time: 0.1658  data: 0.0034  max mem: 1377\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.7992 (0.9575)  Acc@1: 75.0000 (73.5215)  Acc@5: 95.8333 (95.1613)  Acc@task: 83.3333 (84.9462)  time: 0.1629  data: 0.0011  max mem: 1377\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.8351 (0.9318)  Acc@1: 75.0000 (73.6789)  Acc@5: 95.8333 (95.3252)  Acc@task: 83.3333 (84.5528)  time: 0.1617  data: 0.0002  max mem: 1377\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.8199 (0.9238)  Acc@1: 75.0000 (73.9000)  Acc@5: 95.8333 (95.4000)  Acc@task: 83.3333 (84.8000)  time: 0.1590  data: 0.0002  max mem: 1377\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1712 s / it)\n",
            "* Acc@task 84.800 Acc@1 73.900 Acc@5 95.400 loss 0.924\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:17  Loss: 0.5273 (0.5273)  Acc@1: 87.5000 (87.5000)  Acc@5: 95.8333 (95.8333)  Acc@task: 100.0000 (100.0000)  time: 0.4165  data: 0.2588  max mem: 1377\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 0.7903 (0.7947)  Acc@1: 79.1667 (80.6818)  Acc@5: 95.8333 (94.6970)  Acc@task: 83.3333 (85.6061)  time: 0.1891  data: 0.0243  max mem: 1377\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:03  Loss: 0.7621 (0.7461)  Acc@1: 79.1667 (80.5556)  Acc@5: 95.8333 (95.4365)  Acc@task: 83.3333 (85.3175)  time: 0.1655  data: 0.0006  max mem: 1377\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.5778 (0.6886)  Acc@1: 83.3333 (81.3172)  Acc@5: 95.8333 (95.8333)  Acc@task: 87.5000 (86.2903)  time: 0.1638  data: 0.0003  max mem: 1377\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.5833 (0.6664)  Acc@1: 83.3333 (82.4187)  Acc@5: 95.8333 (95.9350)  Acc@task: 87.5000 (86.3821)  time: 0.1630  data: 0.0002  max mem: 1377\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.5833 (0.6778)  Acc@1: 83.3333 (82.1000)  Acc@5: 95.8333 (96.0000)  Acc@task: 87.5000 (86.2000)  time: 0.1604  data: 0.0002  max mem: 1377\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1720 s / it)\n",
            "* Acc@task 86.200 Acc@1 82.100 Acc@5 96.000 loss 0.678\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:34  Loss: 1.1782 (1.1782)  Acc@1: 66.6667 (66.6667)  Acc@5: 87.5000 (87.5000)  Acc@task: 66.6667 (66.6667)  time: 0.8296  data: 0.6180  max mem: 1377\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:07  Loss: 0.4967 (0.6335)  Acc@1: 83.3333 (80.3030)  Acc@5: 95.8333 (95.4545)  Acc@task: 79.1667 (79.1667)  time: 0.2240  data: 0.0573  max mem: 1377\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.6568 (0.7155)  Acc@1: 79.1667 (79.3651)  Acc@5: 95.8333 (95.4365)  Acc@task: 79.1667 (76.7857)  time: 0.1637  data: 0.0010  max mem: 1377\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.6828 (0.7072)  Acc@1: 79.1667 (79.8387)  Acc@5: 95.8333 (95.4301)  Acc@task: 75.0000 (76.6129)  time: 0.1637  data: 0.0008  max mem: 1377\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7137 (0.7263)  Acc@1: 79.1667 (79.6748)  Acc@5: 95.8333 (94.8171)  Acc@task: 79.1667 (77.0325)  time: 0.1632  data: 0.0005  max mem: 1377\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.7104 (0.7171)  Acc@1: 79.1667 (79.9000)  Acc@5: 95.8333 (94.8000)  Acc@task: 79.1667 (77.2000)  time: 0.1607  data: 0.0004  max mem: 1377\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1802 s / it)\n",
            "* Acc@task 77.200 Acc@1 79.900 Acc@5 94.800 loss 0.717\n",
            "[Average accuracy till task4]\tAcc@task: 83.8000\tAcc@1: 79.7750\tAcc@5: 95.9500\tLoss: 0.7388\tForgetting: 1.8000\tBackward: 17.7000\n",
            "Loading checkpoint from: ./output/cifar100_full_dino_1epoch_25pct/checkpoint/task5_checkpoint.pth\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/engines/hide_promtp_wtp_and_tap_engine.py:540: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  loss = torch.nn.functional.cross_entropy(sim, torch.range(0, sim.shape[0] - 1).long().to(device))\n",
            "Train: Epoch[1/1]  [ 0/53]  eta: 0:00:43  Lr: 0.002812  Loss: 4.2997  Acc@1: 4.1667 (4.1667)  Acc@5: 25.0000 (25.0000)  time: 0.8241  data: 0.5660  max mem: 1377\n",
            "Train: Epoch[1/1]  [10/53]  eta: 0:00:13  Lr: 0.002812  Loss: 3.0137  Acc@1: 16.6667 (17.8030)  Acc@5: 50.0000 (49.6212)  time: 0.3057  data: 0.0517  max mem: 1378\n",
            "Train: Epoch[1/1]  [20/53]  eta: 0:00:09  Lr: 0.002812  Loss: 2.2980  Acc@1: 20.8333 (22.2222)  Acc@5: 62.5000 (58.7302)  time: 0.2548  data: 0.0002  max mem: 1378\n",
            "Train: Epoch[1/1]  [30/53]  eta: 0:00:06  Lr: 0.002812  Loss: 1.7888  Acc@1: 33.3333 (26.7473)  Acc@5: 70.8333 (64.5161)  time: 0.2558  data: 0.0007  max mem: 1378\n",
            "Train: Epoch[1/1]  [40/53]  eta: 0:00:03  Lr: 0.002812  Loss: 1.3446  Acc@1: 41.6667 (31.0976)  Acc@5: 83.3333 (70.3252)  time: 0.2557  data: 0.0010  max mem: 1378\n",
            "Train: Epoch[1/1]  [50/53]  eta: 0:00:00  Lr: 0.002812  Loss: 1.2802  Acc@1: 45.8333 (34.7222)  Acc@5: 87.5000 (73.6111)  time: 0.2552  data: 0.0005  max mem: 1378\n",
            "Train: Epoch[1/1]  [52/53]  eta: 0:00:00  Lr: 0.002812  Loss: 1.2615  Acc@1: 50.0000 (35.4200)  Acc@5: 87.5000 (74.0888)  time: 0.2499  data: 0.0003  max mem: 1378\n",
            "Train: Epoch[1/1] Total time: 0:00:14 (0.2657 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.2615  Acc@1: 50.0000 (35.4200)  Acc@5: 87.5000 (74.0888)\n",
            "torch.Size([5, 2, 5, 6, 64])\n",
            "torch.Size([5, 2, 1, 5, 6, 64])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:21  Loss: 0.4294 (0.4294)  Acc@1: 91.6667 (91.6667)  Acc@5: 95.8333 (95.8333)  Acc@task: 87.5000 (87.5000)  time: 0.5187  data: 0.3513  max mem: 1378\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.7074 (0.7161)  Acc@1: 79.1667 (79.5455)  Acc@5: 100.0000 (97.3485)  Acc@task: 87.5000 (84.0909)  time: 0.1950  data: 0.0322  max mem: 1378\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:03  Loss: 0.6902 (0.6785)  Acc@1: 79.1667 (81.5476)  Acc@5: 95.8333 (97.2222)  Acc@task: 83.3333 (84.3254)  time: 0.1627  data: 0.0004  max mem: 1378\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.5396 (0.6637)  Acc@1: 87.5000 (82.2581)  Acc@5: 100.0000 (97.4462)  Acc@task: 83.3333 (84.9462)  time: 0.1622  data: 0.0004  max mem: 1378\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.5830 (0.6589)  Acc@1: 83.3333 (82.7236)  Acc@5: 100.0000 (97.5610)  Acc@task: 87.5000 (85.6707)  time: 0.1616  data: 0.0002  max mem: 1378\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.5821 (0.6527)  Acc@1: 87.5000 (82.9000)  Acc@5: 100.0000 (97.6000)  Acc@task: 87.5000 (85.8000)  time: 0.1592  data: 0.0002  max mem: 1378\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1716 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@task 85.800 Acc@1 82.900 Acc@5 97.600 loss 0.653\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:22  Loss: 1.1382 (1.1382)  Acc@1: 66.6667 (66.6667)  Acc@5: 95.8333 (95.8333)  Acc@task: 87.5000 (87.5000)  time: 0.5264  data: 0.3566  max mem: 1378\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.8878 (0.9484)  Acc@1: 79.1667 (75.3788)  Acc@5: 95.8333 (96.2121)  Acc@task: 83.3333 (84.8485)  time: 0.1958  data: 0.0328  max mem: 1378\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:03  Loss: 0.8396 (0.9833)  Acc@1: 75.0000 (73.2143)  Acc@5: 95.8333 (95.2381)  Acc@task: 83.3333 (82.3413)  time: 0.1635  data: 0.0007  max mem: 1378\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.7992 (0.9705)  Acc@1: 75.0000 (73.3871)  Acc@5: 95.8333 (95.0269)  Acc@task: 83.3333 (83.4677)  time: 0.1630  data: 0.0010  max mem: 1378\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.8354 (0.9413)  Acc@1: 75.0000 (73.4756)  Acc@5: 95.8333 (95.2236)  Acc@task: 83.3333 (83.4350)  time: 0.1623  data: 0.0006  max mem: 1378\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.8190 (0.9359)  Acc@1: 75.0000 (73.6000)  Acc@5: 95.8333 (95.2000)  Acc@task: 87.5000 (83.5000)  time: 0.1598  data: 0.0005  max mem: 1378\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1724 s / it)\n",
            "* Acc@task 83.500 Acc@1 73.600 Acc@5 95.200 loss 0.936\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:17  Loss: 0.5258 (0.5258)  Acc@1: 87.5000 (87.5000)  Acc@5: 95.8333 (95.8333)  Acc@task: 95.8333 (95.8333)  time: 0.4123  data: 0.2565  max mem: 1378\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 0.7647 (0.7836)  Acc@1: 79.1667 (80.3030)  Acc@5: 95.8333 (95.0758)  Acc@task: 83.3333 (83.7121)  time: 0.1924  data: 0.0310  max mem: 1378\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:03  Loss: 0.7287 (0.7361)  Acc@1: 79.1667 (80.3571)  Acc@5: 95.8333 (95.6349)  Acc@task: 83.3333 (83.9286)  time: 0.1681  data: 0.0044  max mem: 1378\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.5657 (0.6782)  Acc@1: 83.3333 (81.5860)  Acc@5: 95.8333 (95.9677)  Acc@task: 83.3333 (84.0054)  time: 0.1650  data: 0.0003  max mem: 1378\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.5679 (0.6579)  Acc@1: 83.3333 (82.8252)  Acc@5: 95.8333 (96.0366)  Acc@task: 83.3333 (84.2480)  time: 0.1643  data: 0.0002  max mem: 1378\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.5679 (0.6691)  Acc@1: 83.3333 (82.5000)  Acc@5: 95.8333 (96.1000)  Acc@task: 83.3333 (84.2000)  time: 0.1618  data: 0.0002  max mem: 1378\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1728 s / it)\n",
            "* Acc@task 84.200 Acc@1 82.500 Acc@5 96.100 loss 0.669\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:22  Loss: 1.1874 (1.1874)  Acc@1: 62.5000 (62.5000)  Acc@5: 87.5000 (87.5000)  Acc@task: 75.0000 (75.0000)  time: 0.5305  data: 0.3498  max mem: 1378\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:06  Loss: 0.4816 (0.6248)  Acc@1: 83.3333 (79.5455)  Acc@5: 95.8333 (96.2121)  Acc@task: 83.3333 (81.0606)  time: 0.1973  data: 0.0330  max mem: 1378\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:03  Loss: 0.6167 (0.6457)  Acc@1: 79.1667 (80.1587)  Acc@5: 95.8333 (96.4286)  Acc@task: 83.3333 (81.1508)  time: 0.1641  data: 0.0016  max mem: 1378\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.6512 (0.6388)  Acc@1: 79.1667 (81.0484)  Acc@5: 95.8333 (96.3710)  Acc@task: 79.1667 (80.6452)  time: 0.1633  data: 0.0014  max mem: 1378\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.6499 (0.6624)  Acc@1: 79.1667 (80.9959)  Acc@5: 95.8333 (95.7317)  Acc@task: 79.1667 (81.0976)  time: 0.1628  data: 0.0006  max mem: 1378\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.5874 (0.6548)  Acc@1: 79.1667 (81.2000)  Acc@5: 95.8333 (95.7000)  Acc@task: 79.1667 (81.2000)  time: 0.1601  data: 0.0006  max mem: 1378\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1729 s / it)\n",
            "* Acc@task 81.200 Acc@1 81.200 Acc@5 95.700 loss 0.655\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:25  Loss: 6.0782 (6.0782)  Acc@1: 0.0000 (0.0000)  Acc@5: 29.1667 (29.1667)  Acc@task: 83.3333 (83.3333)  time: 0.6179  data: 0.4658  max mem: 1378\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 7.0692 (6.8695)  Acc@1: 0.0000 (0.3788)  Acc@5: 20.8333 (20.0758)  Acc@task: 75.0000 (77.2727)  time: 0.2051  data: 0.0428  max mem: 1378\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 7.1110 (6.9929)  Acc@1: 0.0000 (0.1984)  Acc@5: 20.8333 (20.0397)  Acc@task: 75.0000 (75.1984)  time: 0.1645  data: 0.0004  max mem: 1378\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 6.9756 (6.9732)  Acc@1: 0.0000 (0.2688)  Acc@5: 20.8333 (20.0269)  Acc@task: 75.0000 (75.6720)  time: 0.1649  data: 0.0004  max mem: 1378\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 6.9707 (6.9989)  Acc@1: 0.0000 (0.3049)  Acc@5: 16.6667 (18.6992)  Acc@task: 75.0000 (74.1870)  time: 0.1644  data: 0.0003  max mem: 1378\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 6.9707 (7.0042)  Acc@1: 0.0000 (0.3000)  Acc@5: 16.6667 (18.7000)  Acc@task: 75.0000 (74.0000)  time: 0.1620  data: 0.0003  max mem: 1378\n",
            "Test: [Task 5] Total time: 0:00:07 (0.1760 s / it)\n",
            "* Acc@task 74.000 Acc@1 0.300 Acc@5 18.700 loss 7.004\n",
            "[Average accuracy till task5]\tAcc@task: 81.7400\tAcc@1: 64.1000\tAcc@5: 80.6600\tLoss: 1.9833\tForgetting: 1.5250\tBackward: 76.0250\n",
            "torch.Size([58680, 384])\n",
            "Averaged stats: Lr: 0.005000  Loss: 0.0693  Acc@1: 95.8333 (90.6250)  Acc@5: 100.0000 (97.8750)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:35  Loss: 0.7462 (0.7462)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 87.5000 (87.5000)  time: 0.8404  data: 0.6463  max mem: 1378\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:07  Loss: 0.8767 (0.8819)  Acc@1: 79.1667 (77.6515)  Acc@5: 95.8333 (95.4545)  Acc@task: 87.5000 (84.0909)  time: 0.2259  data: 0.0604  max mem: 1378\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.8767 (0.9045)  Acc@1: 79.1667 (78.1746)  Acc@5: 95.8333 (95.6349)  Acc@task: 83.3333 (84.3254)  time: 0.1631  data: 0.0011  max mem: 1378\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.7761 (0.8715)  Acc@1: 79.1667 (77.8226)  Acc@5: 95.8333 (96.1022)  Acc@task: 83.3333 (84.9462)  time: 0.1619  data: 0.0004  max mem: 1378\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.7106 (0.8464)  Acc@1: 79.1667 (78.6585)  Acc@5: 100.0000 (96.4431)  Acc@task: 87.5000 (85.6707)  time: 0.1618  data: 0.0003  max mem: 1378\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6337 (0.8370)  Acc@1: 79.1667 (78.9000)  Acc@5: 100.0000 (96.5000)  Acc@task: 87.5000 (85.8000)  time: 0.1592  data: 0.0003  max mem: 1378\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1794 s / it)\n",
            "* Acc@task 85.800 Acc@1 78.900 Acc@5 96.500 loss 0.837\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:22  Loss: 0.8172 (0.8172)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 87.5000 (87.5000)  time: 0.5407  data: 0.3841  max mem: 1378\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.8172 (0.8356)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (97.7273)  Acc@task: 83.3333 (84.8485)  time: 0.1977  data: 0.0354  max mem: 1378\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.9219 (0.9167)  Acc@1: 75.0000 (76.9841)  Acc@5: 95.8333 (96.4286)  Acc@task: 83.3333 (82.3413)  time: 0.1644  data: 0.0004  max mem: 1378\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.9219 (0.8960)  Acc@1: 75.0000 (77.5538)  Acc@5: 95.8333 (96.1022)  Acc@task: 83.3333 (83.4677)  time: 0.1642  data: 0.0004  max mem: 1378\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.7344 (0.8580)  Acc@1: 79.1667 (78.0488)  Acc@5: 95.8333 (96.2398)  Acc@task: 83.3333 (83.4350)  time: 0.1627  data: 0.0003  max mem: 1378\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.7344 (0.8585)  Acc@1: 79.1667 (78.0000)  Acc@5: 95.8333 (96.1000)  Acc@task: 87.5000 (83.5000)  time: 0.1602  data: 0.0003  max mem: 1378\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1742 s / it)\n",
            "* Acc@task 83.500 Acc@1 78.000 Acc@5 96.100 loss 0.858\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:35  Loss: 0.6091 (0.6091)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  Acc@task: 95.8333 (95.8333)  time: 0.8548  data: 0.6400  max mem: 1378\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:07  Loss: 0.8802 (0.9078)  Acc@1: 79.1667 (79.9242)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (83.7121)  time: 0.2250  data: 0.0585  max mem: 1378\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.8175 (0.8235)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (83.9286)  time: 0.1631  data: 0.0003  max mem: 1378\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.6411 (0.7493)  Acc@1: 79.1667 (79.9731)  Acc@5: 95.8333 (95.9677)  Acc@task: 83.3333 (84.0054)  time: 0.1640  data: 0.0003  max mem: 1378\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.6411 (0.7489)  Acc@1: 79.1667 (80.4878)  Acc@5: 95.8333 (95.9350)  Acc@task: 83.3333 (84.2480)  time: 0.1631  data: 0.0002  max mem: 1378\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.6411 (0.7550)  Acc@1: 79.1667 (80.2000)  Acc@5: 95.8333 (95.9000)  Acc@task: 83.3333 (84.2000)  time: 0.1606  data: 0.0002  max mem: 1378\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1803 s / it)\n",
            "* Acc@task 84.200 Acc@1 80.200 Acc@5 95.900 loss 0.755\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:22  Loss: 0.8509 (0.8509)  Acc@1: 66.6667 (66.6667)  Acc@5: 91.6667 (91.6667)  Acc@task: 75.0000 (75.0000)  time: 0.5380  data: 0.3815  max mem: 1378\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:06  Loss: 0.6600 (0.6138)  Acc@1: 83.3333 (80.6818)  Acc@5: 95.8333 (96.9697)  Acc@task: 83.3333 (81.0606)  time: 0.1971  data: 0.0351  max mem: 1378\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.6600 (0.6792)  Acc@1: 83.3333 (80.5556)  Acc@5: 95.8333 (97.2222)  Acc@task: 83.3333 (81.1508)  time: 0.1642  data: 0.0004  max mem: 1378\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.7238 (0.6903)  Acc@1: 79.1667 (80.6452)  Acc@5: 100.0000 (97.0430)  Acc@task: 79.1667 (80.6452)  time: 0.1642  data: 0.0008  max mem: 1378\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7954 (0.7460)  Acc@1: 79.1667 (79.8781)  Acc@5: 95.8333 (96.1382)  Acc@task: 79.1667 (81.0976)  time: 0.1622  data: 0.0010  max mem: 1378\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.7669 (0.7334)  Acc@1: 79.1667 (80.1000)  Acc@5: 95.8333 (96.2000)  Acc@task: 79.1667 (81.2000)  time: 0.1596  data: 0.0010  max mem: 1378\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1731 s / it)\n",
            "* Acc@task 81.200 Acc@1 80.100 Acc@5 96.200 loss 0.733\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:33  Loss: 0.1280 (0.1280)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.8046  data: 0.6186  max mem: 1378\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:07  Loss: 0.7640 (0.6590)  Acc@1: 79.1667 (80.3030)  Acc@5: 100.0000 (98.1061)  Acc@task: 75.0000 (77.2727)  time: 0.2215  data: 0.0577  max mem: 1378\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.7795 (0.6918)  Acc@1: 79.1667 (79.7619)  Acc@5: 95.8333 (96.8254)  Acc@task: 75.0000 (75.1984)  time: 0.1627  data: 0.0017  max mem: 1378\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.7479 (0.6939)  Acc@1: 79.1667 (80.7796)  Acc@5: 95.8333 (96.5054)  Acc@task: 75.0000 (75.6720)  time: 0.1619  data: 0.0011  max mem: 1378\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.6959 (0.7313)  Acc@1: 79.1667 (80.2846)  Acc@5: 95.8333 (96.3415)  Acc@task: 75.0000 (74.1870)  time: 0.1618  data: 0.0002  max mem: 1378\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.7033 (0.7637)  Acc@1: 79.1667 (80.0000)  Acc@5: 95.8333 (96.2000)  Acc@task: 75.0000 (74.0000)  time: 0.1592  data: 0.0002  max mem: 1378\n",
            "Test: [Task 5] Total time: 0:00:07 (0.1785 s / it)\n",
            "* Acc@task 74.000 Acc@1 80.000 Acc@5 96.200 loss 0.764\n",
            "[Average accuracy till task5]\tAcc@task: 81.7400\tAcc@1: 79.4400\tAcc@5: 96.1800\tLoss: 0.7895\tForgetting: 1.8750\tBackward: 12.8000\n",
            "Loading checkpoint from: ./output/cifar100_full_dino_1epoch_25pct/checkpoint/task6_checkpoint.pth\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/engines/hide_promtp_wtp_and_tap_engine.py:540: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  loss = torch.nn.functional.cross_entropy(sim, torch.range(0, sim.shape[0] - 1).long().to(device))\n",
            "Train: Epoch[1/1]  [ 0/53]  eta: 0:00:37  Lr: 0.002812  Loss: 3.7415  Acc@1: 0.0000 (0.0000)  Acc@5: 66.6667 (66.6667)  time: 0.7059  data: 0.4317  max mem: 1378\n",
            "Train: Epoch[1/1]  [10/53]  eta: 0:00:12  Lr: 0.002812  Loss: 3.3423  Acc@1: 8.3333 (9.0909)  Acc@5: 54.1667 (56.4394)  time: 0.2978  data: 0.0397  max mem: 1378\n",
            "Train: Epoch[1/1]  [20/53]  eta: 0:00:09  Lr: 0.002812  Loss: 2.2053  Acc@1: 12.5000 (13.6905)  Acc@5: 54.1667 (61.7064)  time: 0.2574  data: 0.0009  max mem: 1378\n",
            "Train: Epoch[1/1]  [30/53]  eta: 0:00:06  Lr: 0.002812  Loss: 1.8214  Acc@1: 25.0000 (19.4892)  Acc@5: 70.8333 (67.2043)  time: 0.2572  data: 0.0009  max mem: 1378\n",
            "Train: Epoch[1/1]  [40/53]  eta: 0:00:03  Lr: 0.002812  Loss: 1.6542  Acc@1: 33.3333 (24.6951)  Acc@5: 83.3333 (72.0528)  time: 0.2563  data: 0.0004  max mem: 1378\n",
            "Train: Epoch[1/1]  [50/53]  eta: 0:00:00  Lr: 0.002812  Loss: 1.8920  Acc@1: 45.8333 (29.3301)  Acc@5: 87.5000 (75.2451)  time: 0.2562  data: 0.0003  max mem: 1378\n",
            "Train: Epoch[1/1]  [52/53]  eta: 0:00:00  Lr: 0.002812  Loss: 1.2782  Acc@1: 45.8333 (29.9920)  Acc@5: 91.6667 (75.7359)  time: 0.2490  data: 0.0002  max mem: 1378\n",
            "Train: Epoch[1/1] Total time: 0:00:13 (0.2641 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.2782  Acc@1: 45.8333 (29.9920)  Acc@5: 91.6667 (75.7359)\n",
            "torch.Size([5, 2, 5, 6, 64])\n",
            "torch.Size([5, 2, 1, 5, 6, 64])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:19  Loss: 0.7443 (0.7443)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.4700  data: 0.3086  max mem: 1378\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.8754 (0.8848)  Acc@1: 79.1667 (77.6515)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (83.3333)  time: 0.1901  data: 0.0289  max mem: 1378\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:03  Loss: 0.8794 (0.9481)  Acc@1: 79.1667 (77.7778)  Acc@5: 95.8333 (95.0397)  Acc@task: 83.3333 (81.9444)  time: 0.1631  data: 0.0007  max mem: 1378\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.8092 (0.9020)  Acc@1: 75.0000 (77.4194)  Acc@5: 95.8333 (95.6989)  Acc@task: 83.3333 (82.6613)  time: 0.1630  data: 0.0004  max mem: 1378\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.7718 (0.8769)  Acc@1: 75.0000 (78.1504)  Acc@5: 100.0000 (96.0366)  Acc@task: 87.5000 (83.2317)  time: 0.1616  data: 0.0003  max mem: 1378\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6320 (0.8668)  Acc@1: 79.1667 (78.4000)  Acc@5: 100.0000 (96.1000)  Acc@task: 87.5000 (83.4000)  time: 0.1591  data: 0.0003  max mem: 1378\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1714 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@task 83.400 Acc@1 78.400 Acc@5 96.100 loss 0.867\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:34  Loss: 0.7939 (0.7939)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (83.3333)  time: 0.8262  data: 0.6261  max mem: 1378\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:07  Loss: 0.7939 (0.8331)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (98.1061)  Acc@task: 83.3333 (83.3333)  time: 0.2243  data: 0.0581  max mem: 1378\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.9262 (0.9283)  Acc@1: 75.0000 (76.7857)  Acc@5: 95.8333 (97.0238)  Acc@task: 83.3333 (80.7540)  time: 0.1641  data: 0.0009  max mem: 1378\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.9262 (0.9052)  Acc@1: 75.0000 (77.4194)  Acc@5: 95.8333 (96.5054)  Acc@task: 79.1667 (81.1828)  time: 0.1639  data: 0.0005  max mem: 1378\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.7376 (0.8675)  Acc@1: 79.1667 (77.9472)  Acc@5: 95.8333 (96.5447)  Acc@task: 79.1667 (81.5041)  time: 0.1632  data: 0.0003  max mem: 1378\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.7376 (0.8677)  Acc@1: 79.1667 (77.9000)  Acc@5: 95.8333 (96.4000)  Acc@task: 83.3333 (81.6000)  time: 0.1608  data: 0.0003  max mem: 1378\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1803 s / it)\n",
            "* Acc@task 81.600 Acc@1 77.900 Acc@5 96.400 loss 0.868\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:26  Loss: 0.6123 (0.6123)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  Acc@task: 91.6667 (91.6667)  time: 0.6196  data: 0.4693  max mem: 1378\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 0.8777 (0.9021)  Acc@1: 79.1667 (79.9242)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (83.3333)  time: 0.2046  data: 0.0429  max mem: 1378\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.8338 (0.8253)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.6349)  Acc@task: 83.3333 (83.9286)  time: 0.1639  data: 0.0004  max mem: 1378\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.6373 (0.7503)  Acc@1: 79.1667 (80.1075)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (83.6022)  time: 0.1649  data: 0.0004  max mem: 1378\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.6373 (0.7486)  Acc@1: 79.1667 (80.4878)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (84.0447)  time: 0.1638  data: 0.0003  max mem: 1378\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.6373 (0.7547)  Acc@1: 79.1667 (80.2000)  Acc@5: 95.8333 (95.8000)  Acc@task: 83.3333 (84.0000)  time: 0.1613  data: 0.0003  max mem: 1378\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1768 s / it)\n",
            "* Acc@task 84.000 Acc@1 80.200 Acc@5 95.800 loss 0.755\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:33  Loss: 0.7510 (0.7510)  Acc@1: 66.6667 (66.6667)  Acc@5: 95.8333 (95.8333)  Acc@task: 75.0000 (75.0000)  time: 0.7962  data: 0.6281  max mem: 1378\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:07  Loss: 0.6529 (0.5842)  Acc@1: 83.3333 (81.0606)  Acc@5: 95.8333 (97.3485)  Acc@task: 79.1667 (81.0606)  time: 0.2215  data: 0.0581  max mem: 1378\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.6529 (0.6543)  Acc@1: 83.3333 (80.7540)  Acc@5: 95.8333 (97.4206)  Acc@task: 79.1667 (81.3492)  time: 0.1647  data: 0.0008  max mem: 1378\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.6942 (0.6533)  Acc@1: 83.3333 (81.3172)  Acc@5: 100.0000 (97.3118)  Acc@task: 83.3333 (81.7204)  time: 0.1651  data: 0.0006  max mem: 1378\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7225 (0.7109)  Acc@1: 79.1667 (80.7927)  Acc@5: 95.8333 (96.5447)  Acc@task: 83.3333 (81.6057)  time: 0.1641  data: 0.0004  max mem: 1378\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.7025 (0.6991)  Acc@1: 79.1667 (81.0000)  Acc@5: 95.8333 (96.6000)  Acc@task: 83.3333 (81.8000)  time: 0.1616  data: 0.0004  max mem: 1378\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1806 s / it)\n",
            "* Acc@task 81.800 Acc@1 81.000 Acc@5 96.600 loss 0.699\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:23  Loss: 0.1247 (0.1247)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.5562  data: 0.3898  max mem: 1378\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 0.5846 (0.5959)  Acc@1: 83.3333 (81.4394)  Acc@5: 100.0000 (98.4848)  Acc@task: 83.3333 (83.3333)  time: 0.1991  data: 0.0361  max mem: 1378\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.7194 (0.6454)  Acc@1: 79.1667 (80.7540)  Acc@5: 95.8333 (97.6190)  Acc@task: 79.1667 (79.7619)  time: 0.1645  data: 0.0005  max mem: 1378\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.7058 (0.6507)  Acc@1: 79.1667 (81.4516)  Acc@5: 95.8333 (97.0430)  Acc@task: 75.0000 (79.4355)  time: 0.1653  data: 0.0004  max mem: 1378\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.5990 (0.6802)  Acc@1: 83.3333 (81.3008)  Acc@5: 95.8333 (96.7480)  Acc@task: 75.0000 (78.6585)  time: 0.1639  data: 0.0004  max mem: 1378\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.6413 (0.7134)  Acc@1: 83.3333 (81.0000)  Acc@5: 95.8333 (96.6000)  Acc@task: 75.0000 (78.5000)  time: 0.1613  data: 0.0004  max mem: 1378\n",
            "Test: [Task 5] Total time: 0:00:07 (0.1753 s / it)\n",
            "* Acc@task 78.500 Acc@1 81.000 Acc@5 96.600 loss 0.713\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:24  Loss: 8.5618 (8.5618)  Acc@1: 0.0000 (0.0000)  Acc@5: 8.3333 (8.3333)  Acc@task: 83.3333 (83.3333)  time: 0.5844  data: 0.4197  max mem: 1378\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:06  Loss: 8.3658 (8.4941)  Acc@1: 0.0000 (0.0000)  Acc@5: 8.3333 (9.0909)  Acc@task: 75.0000 (73.8636)  time: 0.2018  data: 0.0385  max mem: 1378\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:04  Loss: 8.2608 (8.3064)  Acc@1: 0.0000 (0.0000)  Acc@5: 8.3333 (9.9206)  Acc@task: 70.8333 (73.0159)  time: 0.1647  data: 0.0003  max mem: 1378\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 8.1219 (8.3489)  Acc@1: 0.0000 (0.0000)  Acc@5: 8.3333 (9.1398)  Acc@task: 70.8333 (70.9677)  time: 0.1654  data: 0.0003  max mem: 1378\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 8.1138 (8.2743)  Acc@1: 0.0000 (0.0000)  Acc@5: 8.3333 (10.0610)  Acc@task: 70.8333 (70.8333)  time: 0.1640  data: 0.0003  max mem: 1378\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 8.1138 (8.2928)  Acc@1: 0.0000 (0.0000)  Acc@5: 8.3333 (10.1000)  Acc@task: 70.8333 (71.0000)  time: 0.1615  data: 0.0002  max mem: 1378\n",
            "Test: [Task 6] Total time: 0:00:07 (0.1757 s / it)\n",
            "* Acc@task 71.000 Acc@1 0.000 Acc@5 10.100 loss 8.293\n",
            "[Average accuracy till task6]\tAcc@task: 80.0500\tAcc@1: 66.4167\tAcc@5: 81.9333\tLoss: 2.0324\tForgetting: 1.7600\tBackward: 76.4200\n",
            "torch.Size([70560, 384])\n",
            "Averaged stats: Lr: 0.005000  Loss: 0.1474  Acc@1: 96.6667 (91.8000)  Acc@5: 100.0000 (98.4333)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:23  Loss: 0.8307 (0.8307)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (83.3333)  time: 0.5583  data: 0.4065  max mem: 1378\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.8209 (0.8379)  Acc@1: 79.1667 (78.0303)  Acc@5: 95.8333 (96.5909)  Acc@task: 83.3333 (83.3333)  time: 0.1989  data: 0.0373  max mem: 1378\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.8209 (0.8920)  Acc@1: 79.1667 (77.5794)  Acc@5: 95.8333 (95.2381)  Acc@task: 83.3333 (81.9444)  time: 0.1638  data: 0.0011  max mem: 1378\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.8045 (0.8418)  Acc@1: 79.1667 (78.7634)  Acc@5: 95.8333 (95.5645)  Acc@task: 83.3333 (82.6613)  time: 0.1629  data: 0.0015  max mem: 1378\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6455 (0.8096)  Acc@1: 79.1667 (79.0650)  Acc@5: 100.0000 (95.9350)  Acc@task: 87.5000 (83.2317)  time: 0.1614  data: 0.0007  max mem: 1378\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6427 (0.8039)  Acc@1: 79.1667 (79.2000)  Acc@5: 100.0000 (96.0000)  Acc@task: 87.5000 (83.4000)  time: 0.1588  data: 0.0007  max mem: 1378\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1730 s / it)\n",
            "* Acc@task 83.400 Acc@1 79.200 Acc@5 96.000 loss 0.804\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:25  Loss: 0.9258 (0.9258)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (83.3333)  time: 0.6075  data: 0.4516  max mem: 1378\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.9199 (0.9351)  Acc@1: 79.1667 (77.2727)  Acc@5: 95.8333 (96.2121)  Acc@task: 83.3333 (83.3333)  time: 0.2026  data: 0.0413  max mem: 1378\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.9199 (1.0060)  Acc@1: 75.0000 (75.5952)  Acc@5: 95.8333 (95.0397)  Acc@task: 83.3333 (80.7540)  time: 0.1633  data: 0.0003  max mem: 1378\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.9492 (0.9847)  Acc@1: 70.8333 (75.8065)  Acc@5: 95.8333 (95.2957)  Acc@task: 79.1667 (81.1828)  time: 0.1639  data: 0.0004  max mem: 1378\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.8667 (0.9476)  Acc@1: 79.1667 (76.4228)  Acc@5: 95.8333 (95.1220)  Acc@task: 79.1667 (81.5041)  time: 0.1624  data: 0.0003  max mem: 1378\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.8667 (0.9475)  Acc@1: 79.1667 (76.5000)  Acc@5: 95.8333 (95.0000)  Acc@task: 83.3333 (81.6000)  time: 0.1599  data: 0.0003  max mem: 1378\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1742 s / it)\n",
            "* Acc@task 81.600 Acc@1 76.500 Acc@5 95.000 loss 0.948\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:19  Loss: 0.5822 (0.5822)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  Acc@task: 91.6667 (91.6667)  time: 0.4651  data: 0.3020  max mem: 1378\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 0.7644 (0.8245)  Acc@1: 79.1667 (80.6818)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (83.3333)  time: 0.1908  data: 0.0290  max mem: 1378\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:03  Loss: 0.7644 (0.7796)  Acc@1: 79.1667 (80.3571)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (83.9286)  time: 0.1627  data: 0.0017  max mem: 1378\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.6090 (0.7172)  Acc@1: 83.3333 (81.3172)  Acc@5: 95.8333 (96.3710)  Acc@task: 83.3333 (83.6022)  time: 0.1619  data: 0.0011  max mem: 1378\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.5917 (0.7113)  Acc@1: 83.3333 (81.4024)  Acc@5: 95.8333 (96.4431)  Acc@task: 83.3333 (84.0447)  time: 0.1615  data: 0.0004  max mem: 1378\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.5917 (0.7115)  Acc@1: 83.3333 (81.3000)  Acc@5: 95.8333 (96.5000)  Acc@task: 83.3333 (84.0000)  time: 0.1590  data: 0.0004  max mem: 1378\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1702 s / it)\n",
            "* Acc@task 84.000 Acc@1 81.300 Acc@5 96.500 loss 0.711\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:26  Loss: 0.5869 (0.5869)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 75.0000 (75.0000)  time: 0.6423  data: 0.4845  max mem: 1378\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:06  Loss: 0.6229 (0.6226)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (97.7273)  Acc@task: 79.1667 (81.0606)  time: 0.2057  data: 0.0443  max mem: 1378\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.7004 (0.6740)  Acc@1: 83.3333 (81.3492)  Acc@5: 95.8333 (97.0238)  Acc@task: 79.1667 (81.3492)  time: 0.1635  data: 0.0003  max mem: 1378\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.7458 (0.6593)  Acc@1: 79.1667 (81.8548)  Acc@5: 95.8333 (96.9086)  Acc@task: 83.3333 (81.7204)  time: 0.1641  data: 0.0003  max mem: 1378\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7554 (0.7303)  Acc@1: 83.3333 (81.1992)  Acc@5: 95.8333 (96.1382)  Acc@task: 83.3333 (81.6057)  time: 0.1627  data: 0.0002  max mem: 1378\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.7554 (0.7187)  Acc@1: 83.3333 (81.4000)  Acc@5: 95.8333 (96.2000)  Acc@task: 83.3333 (81.8000)  time: 0.1601  data: 0.0002  max mem: 1378\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1752 s / it)\n",
            "* Acc@task 81.800 Acc@1 81.400 Acc@5 96.200 loss 0.719\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:19  Loss: 0.2656 (0.2656)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.4651  data: 0.2912  max mem: 1378\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 0.7127 (0.7099)  Acc@1: 79.1667 (78.7879)  Acc@5: 95.8333 (96.9697)  Acc@task: 83.3333 (83.3333)  time: 0.2027  data: 0.0376  max mem: 1378\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.7139 (0.7233)  Acc@1: 75.0000 (78.9683)  Acc@5: 95.8333 (96.2302)  Acc@task: 79.1667 (79.7619)  time: 0.1693  data: 0.0068  max mem: 1378\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.7040 (0.7213)  Acc@1: 79.1667 (79.3011)  Acc@5: 95.8333 (96.1022)  Acc@task: 75.0000 (79.4355)  time: 0.1617  data: 0.0008  max mem: 1378\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.7947 (0.7834)  Acc@1: 75.0000 (78.2520)  Acc@5: 95.8333 (95.9350)  Acc@task: 75.0000 (78.6585)  time: 0.1615  data: 0.0002  max mem: 1378\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.7955 (0.8217)  Acc@1: 75.0000 (77.8000)  Acc@5: 95.8333 (95.7000)  Acc@task: 75.0000 (78.5000)  time: 0.1590  data: 0.0002  max mem: 1378\n",
            "Test: [Task 5] Total time: 0:00:07 (0.1735 s / it)\n",
            "* Acc@task 78.500 Acc@1 77.800 Acc@5 95.700 loss 0.822\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:27  Loss: 0.4657 (0.4657)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (83.3333)  time: 0.6470  data: 0.4810  max mem: 1378\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:06  Loss: 0.9612 (0.9429)  Acc@1: 79.1667 (75.7576)  Acc@5: 95.8333 (95.0758)  Acc@task: 75.0000 (73.8636)  time: 0.2065  data: 0.0440  max mem: 1378\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:04  Loss: 0.9556 (0.9152)  Acc@1: 75.0000 (74.4048)  Acc@5: 95.8333 (96.2302)  Acc@task: 70.8333 (73.0159)  time: 0.1635  data: 0.0003  max mem: 1378\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 0.8430 (0.9244)  Acc@1: 70.8333 (74.1936)  Acc@5: 95.8333 (96.2366)  Acc@task: 70.8333 (70.9677)  time: 0.1633  data: 0.0004  max mem: 1378\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 0.8214 (0.9343)  Acc@1: 75.0000 (74.1870)  Acc@5: 95.8333 (95.8333)  Acc@task: 70.8333 (70.8333)  time: 0.1616  data: 0.0003  max mem: 1378\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 0.8214 (0.9352)  Acc@1: 75.0000 (74.0000)  Acc@5: 95.8333 (95.9000)  Acc@task: 70.8333 (71.0000)  time: 0.1590  data: 0.0003  max mem: 1378\n",
            "Test: [Task 6] Total time: 0:00:07 (0.1757 s / it)\n",
            "* Acc@task 71.000 Acc@1 74.000 Acc@5 95.900 loss 0.935\n",
            "[Average accuracy till task6]\tAcc@task: 80.0500\tAcc@1: 78.3667\tAcc@5: 95.8833\tLoss: 0.8231\tForgetting: 1.9600\tBackward: 10.0400\n",
            "Loading checkpoint from: ./output/cifar100_full_dino_1epoch_25pct/checkpoint/task7_checkpoint.pth\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/engines/hide_promtp_wtp_and_tap_engine.py:540: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  loss = torch.nn.functional.cross_entropy(sim, torch.range(0, sim.shape[0] - 1).long().to(device))\n",
            "Train: Epoch[1/1]  [ 0/53]  eta: 0:00:43  Lr: 0.002812  Loss: 3.6289  Acc@1: 8.3333 (8.3333)  Acc@5: 54.1667 (54.1667)  time: 0.8136  data: 0.4588  max mem: 1378\n",
            "Train: Epoch[1/1]  [10/53]  eta: 0:00:13  Lr: 0.002812  Loss: 2.8104  Acc@1: 12.5000 (13.6364)  Acc@5: 54.1667 (53.4091)  time: 0.3122  data: 0.0428  max mem: 1380\n",
            "Train: Epoch[1/1]  [20/53]  eta: 0:00:09  Lr: 0.002812  Loss: 2.1245  Acc@1: 20.8333 (19.2460)  Acc@5: 58.3333 (61.9048)  time: 0.2586  data: 0.0007  max mem: 1380\n",
            "Train: Epoch[1/1]  [30/53]  eta: 0:00:06  Lr: 0.002812  Loss: 1.9976  Acc@1: 29.1667 (23.5215)  Acc@5: 75.0000 (67.6075)  time: 0.2559  data: 0.0003  max mem: 1380\n",
            "Train: Epoch[1/1]  [40/53]  eta: 0:00:03  Lr: 0.002812  Loss: 1.6522  Acc@1: 37.5000 (28.7602)  Acc@5: 83.3333 (71.5447)  time: 0.2568  data: 0.0003  max mem: 1380\n",
            "Train: Epoch[1/1]  [50/53]  eta: 0:00:00  Lr: 0.002812  Loss: 1.2556  Acc@1: 50.0000 (33.9052)  Acc@5: 87.5000 (75.0817)  time: 0.2568  data: 0.0003  max mem: 1380\n",
            "Train: Epoch[1/1]  [52/53]  eta: 0:00:00  Lr: 0.002812  Loss: 1.7579  Acc@1: 50.0000 (34.5942)  Acc@5: 87.5000 (75.7289)  time: 0.2553  data: 0.0003  max mem: 1380\n",
            "Train: Epoch[1/1] Total time: 0:00:14 (0.2703 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.7579  Acc@1: 50.0000 (34.5942)  Acc@5: 87.5000 (75.7289)\n",
            "torch.Size([5, 2, 5, 6, 64])\n",
            "torch.Size([5, 2, 1, 5, 6, 64])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:22  Loss: 0.8196 (0.8196)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  Acc@task: 79.1667 (79.1667)  time: 0.5425  data: 0.3817  max mem: 1380\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.8196 (0.8590)  Acc@1: 79.1667 (78.7879)  Acc@5: 95.8333 (95.4545)  Acc@task: 79.1667 (81.8182)  time: 0.1971  data: 0.0353  max mem: 1380\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:03  Loss: 0.7481 (0.8993)  Acc@1: 79.1667 (77.7778)  Acc@5: 95.8333 (95.0397)  Acc@task: 79.1667 (80.5556)  time: 0.1635  data: 0.0005  max mem: 1380\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.7481 (0.8505)  Acc@1: 79.1667 (78.6290)  Acc@5: 95.8333 (95.4301)  Acc@task: 79.1667 (81.0484)  time: 0.1642  data: 0.0011  max mem: 1380\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6981 (0.8172)  Acc@1: 79.1667 (78.8618)  Acc@5: 100.0000 (95.8333)  Acc@task: 83.3333 (81.9106)  time: 0.1632  data: 0.0011  max mem: 1380\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6469 (0.8114)  Acc@1: 79.1667 (79.0000)  Acc@5: 100.0000 (95.9000)  Acc@task: 83.3333 (82.1000)  time: 0.1608  data: 0.0011  max mem: 1380\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1737 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@task 82.100 Acc@1 79.000 Acc@5 95.900 loss 0.811\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:24  Loss: 1.0718 (1.0718)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 79.1667 (79.1667)  time: 0.5940  data: 0.4370  max mem: 1380\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.9180 (0.9595)  Acc@1: 79.1667 (77.2727)  Acc@5: 95.8333 (95.8333)  Acc@task: 79.1667 (79.9242)  time: 0.2017  data: 0.0400  max mem: 1380\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.9180 (1.0234)  Acc@1: 75.0000 (75.7937)  Acc@5: 95.8333 (95.0397)  Acc@task: 79.1667 (79.1667)  time: 0.1634  data: 0.0003  max mem: 1380\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 1.0037 (0.9992)  Acc@1: 70.8333 (75.6720)  Acc@5: 95.8333 (95.2957)  Acc@task: 79.1667 (80.1075)  time: 0.1635  data: 0.0003  max mem: 1380\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.8622 (0.9610)  Acc@1: 79.1667 (76.2195)  Acc@5: 95.8333 (95.1220)  Acc@task: 79.1667 (80.3862)  time: 0.1631  data: 0.0002  max mem: 1380\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.8622 (0.9607)  Acc@1: 79.1667 (76.3000)  Acc@5: 95.8333 (95.0000)  Acc@task: 79.1667 (80.4000)  time: 0.1604  data: 0.0002  max mem: 1380\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1744 s / it)\n",
            "* Acc@task 80.400 Acc@1 76.300 Acc@5 95.000 loss 0.961\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:17  Loss: 0.5926 (0.5926)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 87.5000 (87.5000)  time: 0.4188  data: 0.2635  max mem: 1380\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 0.7687 (0.8311)  Acc@1: 79.1667 (80.3030)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (81.8182)  time: 0.1883  data: 0.0244  max mem: 1380\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:03  Loss: 0.7764 (0.7875)  Acc@1: 79.1667 (79.9603)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (82.7381)  time: 0.1653  data: 0.0008  max mem: 1380\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.6080 (0.7238)  Acc@1: 83.3333 (81.0484)  Acc@5: 95.8333 (96.3710)  Acc@task: 83.3333 (82.1237)  time: 0.1639  data: 0.0011  max mem: 1380\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.5922 (0.7179)  Acc@1: 83.3333 (81.3008)  Acc@5: 95.8333 (96.4431)  Acc@task: 83.3333 (82.9268)  time: 0.1636  data: 0.0008  max mem: 1380\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.5922 (0.7178)  Acc@1: 83.3333 (81.2000)  Acc@5: 95.8333 (96.5000)  Acc@task: 83.3333 (82.9000)  time: 0.1611  data: 0.0007  max mem: 1380\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1714 s / it)\n",
            "* Acc@task 82.900 Acc@1 81.200 Acc@5 96.500 loss 0.718\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:22  Loss: 0.5915 (0.5915)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 66.6667 (66.6667)  time: 0.5386  data: 0.3679  max mem: 1380\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:06  Loss: 0.5915 (0.6040)  Acc@1: 83.3333 (83.7121)  Acc@5: 100.0000 (97.7273)  Acc@task: 79.1667 (79.1667)  time: 0.1976  data: 0.0337  max mem: 1380\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.6898 (0.6727)  Acc@1: 83.3333 (81.5476)  Acc@5: 95.8333 (97.0238)  Acc@task: 79.1667 (80.5556)  time: 0.1646  data: 0.0003  max mem: 1380\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.7048 (0.6578)  Acc@1: 79.1667 (82.1237)  Acc@5: 95.8333 (96.9086)  Acc@task: 83.3333 (80.7796)  time: 0.1654  data: 0.0003  max mem: 1380\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7048 (0.7285)  Acc@1: 83.3333 (81.4024)  Acc@5: 95.8333 (96.2398)  Acc@task: 79.1667 (80.3862)  time: 0.1646  data: 0.0002  max mem: 1380\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.6936 (0.7169)  Acc@1: 83.3333 (81.6000)  Acc@5: 95.8333 (96.3000)  Acc@task: 79.1667 (80.5000)  time: 0.1621  data: 0.0002  max mem: 1380\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1743 s / it)\n",
            "* Acc@task 80.500 Acc@1 81.600 Acc@5 96.300 loss 0.717\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:25  Loss: 0.2620 (0.2620)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.6188  data: 0.4549  max mem: 1380\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 0.7012 (0.7088)  Acc@1: 79.1667 (78.7879)  Acc@5: 95.8333 (96.9697)  Acc@task: 83.3333 (84.0909)  time: 0.2058  data: 0.0418  max mem: 1380\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.7136 (0.7184)  Acc@1: 75.0000 (79.1667)  Acc@5: 95.8333 (96.4286)  Acc@task: 83.3333 (82.1429)  time: 0.1650  data: 0.0013  max mem: 1380\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.7032 (0.7107)  Acc@1: 79.1667 (79.4355)  Acc@5: 95.8333 (96.2366)  Acc@task: 79.1667 (81.8548)  time: 0.1653  data: 0.0015  max mem: 1380\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.7572 (0.7719)  Acc@1: 75.0000 (78.5569)  Acc@5: 95.8333 (96.1382)  Acc@task: 79.1667 (80.6911)  time: 0.1650  data: 0.0006  max mem: 1380\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.7930 (0.8104)  Acc@1: 75.0000 (78.1000)  Acc@5: 95.8333 (95.9000)  Acc@task: 79.1667 (80.5000)  time: 0.1623  data: 0.0005  max mem: 1380\n",
            "Test: [Task 5] Total time: 0:00:07 (0.1765 s / it)\n",
            "* Acc@task 80.500 Acc@1 78.100 Acc@5 95.900 loss 0.810\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:19  Loss: 0.4654 (0.4654)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (83.3333)  time: 0.4682  data: 0.3035  max mem: 1380\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:06  Loss: 0.8701 (0.8771)  Acc@1: 79.1667 (77.2727)  Acc@5: 95.8333 (95.4545)  Acc@task: 79.1667 (79.5455)  time: 0.1925  data: 0.0300  max mem: 1380\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:03  Loss: 0.9120 (0.8497)  Acc@1: 75.0000 (76.3889)  Acc@5: 95.8333 (96.4286)  Acc@task: 79.1667 (78.9683)  time: 0.1649  data: 0.0015  max mem: 1380\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 0.8036 (0.8601)  Acc@1: 75.0000 (75.9409)  Acc@5: 95.8333 (96.5054)  Acc@task: 75.0000 (77.5538)  time: 0.1641  data: 0.0003  max mem: 1380\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 0.7836 (0.8695)  Acc@1: 79.1667 (76.1179)  Acc@5: 95.8333 (96.2398)  Acc@task: 75.0000 (77.1341)  time: 0.1638  data: 0.0002  max mem: 1380\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 0.7836 (0.8712)  Acc@1: 79.1667 (76.0000)  Acc@5: 95.8333 (96.3000)  Acc@task: 75.0000 (77.3000)  time: 0.1611  data: 0.0002  max mem: 1380\n",
            "Test: [Task 6] Total time: 0:00:07 (0.1724 s / it)\n",
            "* Acc@task 77.300 Acc@1 76.000 Acc@5 96.300 loss 0.871\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:30  Loss: 8.0949 (8.0949)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  Acc@task: 75.0000 (75.0000)  time: 0.7162  data: 0.4928  max mem: 1380\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:06  Loss: 8.0415 (7.8061)  Acc@1: 0.0000 (0.3788)  Acc@5: 8.3333 (8.7121)  Acc@task: 62.5000 (65.1515)  time: 0.2137  data: 0.0470  max mem: 1380\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:04  Loss: 7.7912 (7.7260)  Acc@1: 0.0000 (0.1984)  Acc@5: 8.3333 (9.1270)  Acc@task: 62.5000 (65.6746)  time: 0.1628  data: 0.0019  max mem: 1380\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:02  Loss: 7.9249 (7.8119)  Acc@1: 0.0000 (0.1344)  Acc@5: 8.3333 (9.2742)  Acc@task: 66.6667 (66.3979)  time: 0.1620  data: 0.0009  max mem: 1380\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 7.9268 (7.8213)  Acc@1: 0.0000 (0.1016)  Acc@5: 8.3333 (9.2480)  Acc@task: 66.6667 (67.3781)  time: 0.1617  data: 0.0002  max mem: 1380\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 7.9249 (7.7992)  Acc@1: 0.0000 (0.2000)  Acc@5: 8.3333 (9.4000)  Acc@task: 68.7500 (67.4000)  time: 0.1591  data: 0.0002  max mem: 1380\n",
            "Test: [Task 7] Total time: 0:00:07 (0.1763 s / it)\n",
            "* Acc@task 67.400 Acc@1 0.200 Acc@5 9.400 loss 7.799\n",
            "[Average accuracy till task7]\tAcc@task: 78.7286\tAcc@1: 67.4857\tAcc@5: 83.6143\tLoss: 1.8125\tForgetting: 1.9167\tBackward: 75.9667\n",
            "torch.Size([82440, 384])\n",
            "Averaged stats: Lr: 0.005000  Loss: 0.0510  Acc@1: 97.5000 (93.3750)  Acc@5: 100.0000 (98.6250)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:24  Loss: 0.7024 (0.7024)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 79.1667 (79.1667)  time: 0.5808  data: 0.4212  max mem: 1380\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.7350 (0.8023)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (98.1061)  Acc@task: 79.1667 (81.8182)  time: 0.2005  data: 0.0388  max mem: 1380\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.7714 (0.8432)  Acc@1: 79.1667 (78.5714)  Acc@5: 95.8333 (96.6270)  Acc@task: 79.1667 (80.5556)  time: 0.1632  data: 0.0004  max mem: 1380\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.6083 (0.8025)  Acc@1: 75.0000 (78.7634)  Acc@5: 95.8333 (96.9086)  Acc@task: 79.1667 (81.0484)  time: 0.1632  data: 0.0003  max mem: 1380\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6083 (0.7664)  Acc@1: 83.3333 (79.8781)  Acc@5: 100.0000 (96.9512)  Acc@task: 83.3333 (81.9106)  time: 0.1619  data: 0.0005  max mem: 1380\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.5695 (0.7593)  Acc@1: 83.3333 (80.1000)  Acc@5: 100.0000 (97.0000)  Acc@task: 83.3333 (82.1000)  time: 0.1593  data: 0.0005  max mem: 1380\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1751 s / it)\n",
            "* Acc@task 82.100 Acc@1 80.100 Acc@5 97.000 loss 0.759\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:34  Loss: 1.0864 (1.0864)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  Acc@task: 79.1667 (79.1667)  time: 0.8138  data: 0.6107  max mem: 1380\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:07  Loss: 0.8889 (0.9136)  Acc@1: 79.1667 (78.0303)  Acc@5: 95.8333 (96.5909)  Acc@task: 79.1667 (79.9242)  time: 0.2222  data: 0.0563  max mem: 1380\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.8889 (0.9896)  Acc@1: 75.0000 (75.7937)  Acc@5: 95.8333 (95.4365)  Acc@task: 79.1667 (79.1667)  time: 0.1627  data: 0.0006  max mem: 1380\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.9389 (0.9682)  Acc@1: 75.0000 (76.2097)  Acc@5: 95.8333 (95.5645)  Acc@task: 79.1667 (80.1075)  time: 0.1622  data: 0.0003  max mem: 1380\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.8091 (0.9444)  Acc@1: 79.1667 (76.4228)  Acc@5: 95.8333 (95.4268)  Acc@task: 79.1667 (80.3862)  time: 0.1616  data: 0.0002  max mem: 1380\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.8091 (0.9415)  Acc@1: 79.1667 (76.4000)  Acc@5: 95.8333 (95.3000)  Acc@task: 79.1667 (80.4000)  time: 0.1591  data: 0.0002  max mem: 1380\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1788 s / it)\n",
            "* Acc@task 80.400 Acc@1 76.400 Acc@5 95.300 loss 0.942\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:21  Loss: 0.6327 (0.6327)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  Acc@task: 87.5000 (87.5000)  time: 0.5221  data: 0.3589  max mem: 1380\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 0.8440 (0.9068)  Acc@1: 79.1667 (77.2727)  Acc@5: 95.8333 (96.2121)  Acc@task: 83.3333 (81.8182)  time: 0.1949  data: 0.0329  max mem: 1380\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:03  Loss: 0.8440 (0.8542)  Acc@1: 79.1667 (77.7778)  Acc@5: 95.8333 (95.4365)  Acc@task: 83.3333 (82.7381)  time: 0.1632  data: 0.0003  max mem: 1380\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.6675 (0.7830)  Acc@1: 79.1667 (78.8979)  Acc@5: 95.8333 (95.6989)  Acc@task: 83.3333 (82.1237)  time: 0.1632  data: 0.0004  max mem: 1380\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.7077 (0.7724)  Acc@1: 79.1667 (79.6748)  Acc@5: 95.8333 (95.6301)  Acc@task: 83.3333 (82.9268)  time: 0.1615  data: 0.0005  max mem: 1380\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.6840 (0.7703)  Acc@1: 79.1667 (79.6000)  Acc@5: 95.8333 (95.7000)  Acc@task: 83.3333 (82.9000)  time: 0.1589  data: 0.0005  max mem: 1380\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1730 s / it)\n",
            "* Acc@task 82.900 Acc@1 79.600 Acc@5 95.700 loss 0.770\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:25  Loss: 1.0062 (1.0062)  Acc@1: 62.5000 (62.5000)  Acc@5: 87.5000 (87.5000)  Acc@task: 66.6667 (66.6667)  time: 0.6133  data: 0.4360  max mem: 1380\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:06  Loss: 0.8558 (0.7580)  Acc@1: 79.1667 (80.3030)  Acc@5: 95.8333 (94.6970)  Acc@task: 79.1667 (79.1667)  time: 0.2041  data: 0.0399  max mem: 1380\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.8997 (0.8338)  Acc@1: 79.1667 (78.3730)  Acc@5: 95.8333 (94.8413)  Acc@task: 79.1667 (80.5556)  time: 0.1637  data: 0.0004  max mem: 1380\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.8997 (0.8221)  Acc@1: 75.0000 (78.4946)  Acc@5: 95.8333 (94.8925)  Acc@task: 83.3333 (80.7796)  time: 0.1641  data: 0.0004  max mem: 1380\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.8968 (0.8774)  Acc@1: 79.1667 (78.5569)  Acc@5: 95.8333 (94.4106)  Acc@task: 79.1667 (80.3862)  time: 0.1633  data: 0.0002  max mem: 1380\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.8968 (0.8645)  Acc@1: 79.1667 (78.7000)  Acc@5: 95.8333 (94.5000)  Acc@task: 79.1667 (80.5000)  time: 0.1609  data: 0.0002  max mem: 1380\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1750 s / it)\n",
            "* Acc@task 80.500 Acc@1 78.700 Acc@5 94.500 loss 0.864\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:24  Loss: 0.2633 (0.2633)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.5830  data: 0.4172  max mem: 1380\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 0.6793 (0.6729)  Acc@1: 83.3333 (81.4394)  Acc@5: 95.8333 (96.9697)  Acc@task: 83.3333 (84.0909)  time: 0.2002  data: 0.0382  max mem: 1380\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.7476 (0.7082)  Acc@1: 79.1667 (80.3571)  Acc@5: 95.8333 (96.6270)  Acc@task: 83.3333 (82.1429)  time: 0.1637  data: 0.0004  max mem: 1380\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.6723 (0.7002)  Acc@1: 79.1667 (81.0484)  Acc@5: 95.8333 (95.9677)  Acc@task: 79.1667 (81.8548)  time: 0.1642  data: 0.0010  max mem: 1380\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.6752 (0.7531)  Acc@1: 79.1667 (80.3862)  Acc@5: 95.8333 (95.4268)  Acc@task: 79.1667 (80.6911)  time: 0.1622  data: 0.0009  max mem: 1380\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.6980 (0.7919)  Acc@1: 79.1667 (79.8000)  Acc@5: 95.8333 (95.2000)  Acc@task: 79.1667 (80.5000)  time: 0.1596  data: 0.0009  max mem: 1380\n",
            "Test: [Task 5] Total time: 0:00:07 (0.1737 s / it)\n",
            "* Acc@task 80.500 Acc@1 79.800 Acc@5 95.200 loss 0.792\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:22  Loss: 0.6149 (0.6149)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (83.3333)  time: 0.5433  data: 0.3728  max mem: 1380\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:06  Loss: 1.0164 (1.0716)  Acc@1: 70.8333 (71.9697)  Acc@5: 95.8333 (94.3182)  Acc@task: 79.1667 (79.5455)  time: 0.1974  data: 0.0343  max mem: 1380\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:04  Loss: 1.0164 (1.0751)  Acc@1: 70.8333 (70.4365)  Acc@5: 95.8333 (94.6429)  Acc@task: 79.1667 (78.9683)  time: 0.1640  data: 0.0004  max mem: 1380\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 1.1588 (1.1099)  Acc@1: 70.8333 (70.4301)  Acc@5: 95.8333 (94.8925)  Acc@task: 75.0000 (77.5538)  time: 0.1645  data: 0.0004  max mem: 1380\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.1616 (1.1086)  Acc@1: 70.8333 (70.5285)  Acc@5: 95.8333 (94.5122)  Acc@task: 75.0000 (77.1341)  time: 0.1632  data: 0.0004  max mem: 1380\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.1616 (1.1073)  Acc@1: 70.8333 (70.5000)  Acc@5: 95.8333 (94.5000)  Acc@task: 75.0000 (77.3000)  time: 0.1606  data: 0.0004  max mem: 1380\n",
            "Test: [Task 6] Total time: 0:00:07 (0.1735 s / it)\n",
            "* Acc@task 77.300 Acc@1 70.500 Acc@5 94.500 loss 1.107\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:21  Loss: 1.0503 (1.0503)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  Acc@task: 75.0000 (75.0000)  time: 0.5164  data: 0.3516  max mem: 1380\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:06  Loss: 0.7744 (0.7655)  Acc@1: 75.0000 (76.5152)  Acc@5: 95.8333 (94.6970)  Acc@task: 62.5000 (65.1515)  time: 0.1945  data: 0.0330  max mem: 1380\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:03  Loss: 0.8505 (0.9020)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (94.4444)  Acc@task: 62.5000 (65.6746)  time: 0.1640  data: 0.0011  max mem: 1380\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:02  Loss: 0.9312 (0.9609)  Acc@1: 70.8333 (73.9247)  Acc@5: 95.8333 (93.0108)  Acc@task: 66.6667 (66.3979)  time: 0.1652  data: 0.0012  max mem: 1380\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 0.9270 (0.9833)  Acc@1: 70.8333 (73.6789)  Acc@5: 91.6667 (92.7846)  Acc@task: 66.6667 (67.3781)  time: 0.1639  data: 0.0009  max mem: 1380\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 0.9235 (0.9706)  Acc@1: 70.8333 (74.0000)  Acc@5: 91.6667 (92.9000)  Acc@task: 68.7500 (67.4000)  time: 0.1613  data: 0.0008  max mem: 1380\n",
            "Test: [Task 7] Total time: 0:00:07 (0.1736 s / it)\n",
            "* Acc@task 67.400 Acc@1 74.000 Acc@5 92.900 loss 0.971\n",
            "[Average accuracy till task7]\tAcc@task: 78.7286\tAcc@1: 77.0143\tAcc@5: 95.0143\tLoss: 0.8865\tForgetting: 2.4833\tBackward: 7.5167\n",
            "Loading checkpoint from: ./output/cifar100_full_dino_1epoch_25pct/checkpoint/task8_checkpoint.pth\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/engines/hide_promtp_wtp_and_tap_engine.py:540: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  loss = torch.nn.functional.cross_entropy(sim, torch.range(0, sim.shape[0] - 1).long().to(device))\n",
            "Train: Epoch[1/1]  [ 0/52]  eta: 0:00:39  Lr: 0.002812  Loss: 4.3561  Acc@1: 12.5000 (12.5000)  Acc@5: 41.6667 (41.6667)  time: 0.7523  data: 0.4804  max mem: 1380\n",
            "Train: Epoch[1/1]  [10/52]  eta: 0:00:12  Lr: 0.002812  Loss: 3.1365  Acc@1: 12.5000 (12.5000)  Acc@5: 50.0000 (48.1061)  time: 0.3056  data: 0.0442  max mem: 1380\n",
            "Train: Epoch[1/1]  [20/52]  eta: 0:00:09  Lr: 0.002812  Loss: 2.7898  Acc@1: 12.5000 (16.4683)  Acc@5: 54.1667 (55.5556)  time: 0.2609  data: 0.0005  max mem: 1380\n",
            "Train: Epoch[1/1]  [30/52]  eta: 0:00:06  Lr: 0.002812  Loss: 1.9587  Acc@1: 25.0000 (20.2957)  Acc@5: 66.6667 (60.2151)  time: 0.2611  data: 0.0003  max mem: 1380\n",
            "Train: Epoch[1/1]  [40/52]  eta: 0:00:03  Lr: 0.002812  Loss: 2.2126  Acc@1: 33.3333 (26.2195)  Acc@5: 79.1667 (66.4634)  time: 0.2635  data: 0.0006  max mem: 1380\n",
            "Train: Epoch[1/1]  [50/52]  eta: 0:00:00  Lr: 0.002812  Loss: 1.5635  Acc@1: 45.8333 (31.4542)  Acc@5: 83.3333 (70.7516)  time: 0.2628  data: 0.0007  max mem: 1380\n",
            "Train: Epoch[1/1]  [51/52]  eta: 0:00:00  Lr: 0.002812  Loss: 1.8123  Acc@1: 45.8333 (31.5534)  Acc@5: 83.3333 (70.8738)  time: 0.2566  data: 0.0007  max mem: 1380\n",
            "Train: Epoch[1/1] Total time: 0:00:14 (0.2707 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.8123  Acc@1: 45.8333 (31.5534)  Acc@5: 83.3333 (70.8738)\n",
            "torch.Size([5, 2, 5, 6, 64])\n",
            "torch.Size([5, 2, 1, 5, 6, 64])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:35  Loss: 0.7023 (0.7023)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 79.1667 (79.1667)  time: 0.8508  data: 0.6180  max mem: 1380\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:07  Loss: 0.7348 (0.8166)  Acc@1: 79.1667 (78.7879)  Acc@5: 100.0000 (97.7273)  Acc@task: 79.1667 (80.3030)  time: 0.2265  data: 0.0568  max mem: 1380\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.7920 (0.8641)  Acc@1: 75.0000 (78.3730)  Acc@5: 95.8333 (96.2302)  Acc@task: 79.1667 (79.5635)  time: 0.1630  data: 0.0012  max mem: 1380\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.6283 (0.8182)  Acc@1: 75.0000 (78.7634)  Acc@5: 95.8333 (96.5054)  Acc@task: 83.3333 (80.3763)  time: 0.1618  data: 0.0011  max mem: 1380\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6079 (0.7794)  Acc@1: 83.3333 (79.8781)  Acc@5: 95.8333 (96.6463)  Acc@task: 83.3333 (81.4024)  time: 0.1617  data: 0.0003  max mem: 1380\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.5695 (0.7720)  Acc@1: 83.3333 (80.1000)  Acc@5: 100.0000 (96.7000)  Acc@task: 87.5000 (81.6000)  time: 0.1590  data: 0.0003  max mem: 1380\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1800 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@task 81.600 Acc@1 80.100 Acc@5 96.700 loss 0.772\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:23  Loss: 1.3161 (1.3161)  Acc@1: 83.3333 (83.3333)  Acc@5: 91.6667 (91.6667)  Acc@task: 75.0000 (75.0000)  time: 0.5634  data: 0.4102  max mem: 1380\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.8943 (0.9484)  Acc@1: 79.1667 (78.0303)  Acc@5: 95.8333 (95.8333)  Acc@task: 75.0000 (76.5152)  time: 0.1986  data: 0.0377  max mem: 1380\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.8943 (1.0178)  Acc@1: 75.0000 (75.5952)  Acc@5: 95.8333 (95.0397)  Acc@task: 75.0000 (76.9841)  time: 0.1634  data: 0.0004  max mem: 1380\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.9387 (0.9858)  Acc@1: 75.0000 (75.9409)  Acc@5: 95.8333 (95.2957)  Acc@task: 79.1667 (78.0914)  time: 0.1638  data: 0.0003  max mem: 1380\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.8062 (0.9577)  Acc@1: 75.0000 (76.0163)  Acc@5: 95.8333 (95.3252)  Acc@task: 79.1667 (78.8618)  time: 0.1620  data: 0.0003  max mem: 1380\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.8062 (0.9545)  Acc@1: 75.0000 (76.0000)  Acc@5: 95.8333 (95.2000)  Acc@task: 79.1667 (78.8000)  time: 0.1593  data: 0.0002  max mem: 1380\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1741 s / it)\n",
            "* Acc@task 78.800 Acc@1 76.000 Acc@5 95.200 loss 0.954\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:36  Loss: 0.6328 (0.6328)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  Acc@task: 87.5000 (87.5000)  time: 0.8638  data: 0.6610  max mem: 1380\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:07  Loss: 0.8435 (0.9082)  Acc@1: 79.1667 (76.8939)  Acc@5: 95.8333 (96.2121)  Acc@task: 83.3333 (80.6818)  time: 0.2271  data: 0.0608  max mem: 1380\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.8435 (0.8559)  Acc@1: 79.1667 (77.5794)  Acc@5: 95.8333 (95.6349)  Acc@task: 79.1667 (81.1508)  time: 0.1628  data: 0.0006  max mem: 1380\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.6812 (0.7835)  Acc@1: 79.1667 (78.8979)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (81.0484)  time: 0.1620  data: 0.0009  max mem: 1380\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.7075 (0.7717)  Acc@1: 79.1667 (79.6748)  Acc@5: 95.8333 (95.7317)  Acc@task: 83.3333 (82.1138)  time: 0.1616  data: 0.0009  max mem: 1380\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.6837 (0.7696)  Acc@1: 79.1667 (79.6000)  Acc@5: 95.8333 (95.8000)  Acc@task: 83.3333 (82.0000)  time: 0.1590  data: 0.0009  max mem: 1380\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1798 s / it)\n",
            "* Acc@task 82.000 Acc@1 79.600 Acc@5 95.800 loss 0.770\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:27  Loss: 1.0312 (1.0312)  Acc@1: 62.5000 (62.5000)  Acc@5: 87.5000 (87.5000)  Acc@task: 66.6667 (66.6667)  time: 0.6484  data: 0.4934  max mem: 1380\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:06  Loss: 0.8002 (0.7555)  Acc@1: 79.1667 (80.3030)  Acc@5: 95.8333 (94.6970)  Acc@task: 75.0000 (76.1364)  time: 0.2069  data: 0.0451  max mem: 1380\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.8927 (0.8328)  Acc@1: 79.1667 (78.3730)  Acc@5: 95.8333 (94.8413)  Acc@task: 79.1667 (77.1825)  time: 0.1637  data: 0.0003  max mem: 1380\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.8992 (0.8212)  Acc@1: 75.0000 (78.4946)  Acc@5: 95.8333 (94.8925)  Acc@task: 79.1667 (77.6882)  time: 0.1642  data: 0.0004  max mem: 1380\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.9001 (0.8761)  Acc@1: 79.1667 (78.5569)  Acc@5: 95.8333 (94.4106)  Acc@task: 79.1667 (77.8455)  time: 0.1631  data: 0.0004  max mem: 1380\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.9001 (0.8631)  Acc@1: 79.1667 (78.7000)  Acc@5: 95.8333 (94.5000)  Acc@task: 79.1667 (78.0000)  time: 0.1606  data: 0.0004  max mem: 1380\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1771 s / it)\n",
            "* Acc@task 78.000 Acc@1 78.700 Acc@5 94.500 loss 0.863\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:31  Loss: 0.2631 (0.2631)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.7492  data: 0.5605  max mem: 1380\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 0.6775 (0.6675)  Acc@1: 83.3333 (81.4394)  Acc@5: 95.8333 (96.9697)  Acc@task: 79.1667 (82.1970)  time: 0.2165  data: 0.0513  max mem: 1380\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.7471 (0.6999)  Acc@1: 79.1667 (80.5556)  Acc@5: 95.8333 (96.6270)  Acc@task: 79.1667 (81.3492)  time: 0.1644  data: 0.0004  max mem: 1380\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.7096 (0.6986)  Acc@1: 79.1667 (81.1828)  Acc@5: 95.8333 (95.9677)  Acc@task: 79.1667 (80.7796)  time: 0.1641  data: 0.0003  max mem: 1380\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.7096 (0.7530)  Acc@1: 79.1667 (80.4878)  Acc@5: 95.8333 (95.4268)  Acc@task: 75.0000 (79.7764)  time: 0.1636  data: 0.0002  max mem: 1380\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.7120 (0.7910)  Acc@1: 79.1667 (80.0000)  Acc@5: 95.8333 (95.2000)  Acc@task: 75.0000 (79.6000)  time: 0.1611  data: 0.0002  max mem: 1380\n",
            "Test: [Task 5] Total time: 0:00:07 (0.1792 s / it)\n",
            "* Acc@task 79.600 Acc@1 80.000 Acc@5 95.200 loss 0.791\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:24  Loss: 0.6129 (0.6129)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 87.5000 (87.5000)  time: 0.5872  data: 0.4280  max mem: 1380\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:06  Loss: 1.0163 (1.0076)  Acc@1: 75.0000 (73.4849)  Acc@5: 95.8333 (95.4545)  Acc@task: 83.3333 (81.0606)  time: 0.2018  data: 0.0393  max mem: 1380\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:04  Loss: 1.0163 (1.0364)  Acc@1: 70.8333 (71.4286)  Acc@5: 95.8333 (95.2381)  Acc@task: 79.1667 (80.3571)  time: 0.1646  data: 0.0004  max mem: 1380\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 1.1319 (1.0895)  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.1613)  Acc@task: 75.0000 (77.6882)  time: 0.1651  data: 0.0008  max mem: 1380\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.1619 (1.0902)  Acc@1: 70.8333 (70.7317)  Acc@5: 95.8333 (94.7154)  Acc@task: 70.8333 (76.5244)  time: 0.1635  data: 0.0010  max mem: 1380\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.1619 (1.0891)  Acc@1: 70.8333 (70.7000)  Acc@5: 95.8333 (94.7000)  Acc@task: 70.8333 (76.6000)  time: 0.1610  data: 0.0010  max mem: 1380\n",
            "Test: [Task 6] Total time: 0:00:07 (0.1750 s / it)\n",
            "* Acc@task 76.600 Acc@1 70.700 Acc@5 94.700 loss 1.089\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:23  Loss: 1.0219 (1.0219)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  Acc@task: 79.1667 (79.1667)  time: 0.5614  data: 0.4047  max mem: 1380\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:06  Loss: 0.6579 (0.7360)  Acc@1: 79.1667 (77.6515)  Acc@5: 95.8333 (95.0758)  Acc@task: 75.0000 (71.2121)  time: 0.1998  data: 0.0374  max mem: 1380\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:04  Loss: 0.8023 (0.8675)  Acc@1: 75.0000 (75.9921)  Acc@5: 95.8333 (95.0397)  Acc@task: 70.8333 (71.6270)  time: 0.1647  data: 0.0006  max mem: 1380\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:02  Loss: 0.9305 (0.9270)  Acc@1: 70.8333 (74.5968)  Acc@5: 95.8333 (93.6828)  Acc@task: 70.8333 (70.9677)  time: 0.1652  data: 0.0005  max mem: 1380\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 0.9252 (0.9538)  Acc@1: 70.8333 (74.1870)  Acc@5: 91.6667 (93.2927)  Acc@task: 70.8333 (71.2398)  time: 0.1638  data: 0.0003  max mem: 1380\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 0.8888 (0.9418)  Acc@1: 70.8333 (74.5000)  Acc@5: 91.6667 (93.4000)  Acc@task: 70.8333 (71.2000)  time: 0.1613  data: 0.0003  max mem: 1380\n",
            "Test: [Task 7] Total time: 0:00:07 (0.1746 s / it)\n",
            "* Acc@task 71.200 Acc@1 74.500 Acc@5 93.400 loss 0.942\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:22  Loss: 9.0950 (9.0950)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  Acc@task: 70.8333 (70.8333)  time: 0.5272  data: 0.3596  max mem: 1380\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:06  Loss: 9.0458 (8.9358)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.3788)  Acc@task: 58.3333 (59.8485)  time: 0.1951  data: 0.0330  max mem: 1380\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:03  Loss: 8.6134 (8.8611)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (1.5873)  Acc@task: 58.3333 (59.3254)  time: 0.1637  data: 0.0004  max mem: 1380\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:02  Loss: 8.7613 (8.9338)  Acc@1: 0.0000 (0.0000)  Acc@5: 4.1667 (2.0161)  Acc@task: 58.3333 (59.6774)  time: 0.1642  data: 0.0008  max mem: 1380\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 8.9225 (8.9140)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (1.9309)  Acc@task: 58.3333 (59.1463)  time: 0.1630  data: 0.0008  max mem: 1380\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 8.9472 (8.9177)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (1.9000)  Acc@task: 58.3333 (59.5000)  time: 0.1604  data: 0.0007  max mem: 1380\n",
            "Test: [Task 8] Total time: 0:00:07 (0.1729 s / it)\n",
            "* Acc@task 59.500 Acc@1 0.000 Acc@5 1.900 loss 8.918\n",
            "[Average accuracy till task8]\tAcc@task: 75.9125\tAcc@1: 67.4500\tAcc@5: 83.4250\tLoss: 1.8874\tForgetting: 2.6571\tBackward: 74.7143\n",
            "torch.Size([94320, 384])\n",
            "Averaged stats: Lr: 0.005000  Loss: 0.0732  Acc@1: 96.6667 (92.9881)  Acc@5: 100.0000 (98.6310)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:18  Loss: 0.6061 (0.6061)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  Acc@task: 79.1667 (79.1667)  time: 0.4312  data: 0.2653  max mem: 1380\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.6498 (0.8067)  Acc@1: 79.1667 (78.0303)  Acc@5: 100.0000 (97.7273)  Acc@task: 79.1667 (80.3030)  time: 0.1926  data: 0.0288  max mem: 1380\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:03  Loss: 0.7083 (0.8696)  Acc@1: 79.1667 (78.5714)  Acc@5: 95.8333 (96.0317)  Acc@task: 79.1667 (79.5635)  time: 0.1665  data: 0.0028  max mem: 1380\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.7144 (0.8260)  Acc@1: 75.0000 (78.8979)  Acc@5: 95.8333 (96.2366)  Acc@task: 83.3333 (80.3763)  time: 0.1629  data: 0.0004  max mem: 1380\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.7144 (0.7907)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (96.4431)  Acc@task: 83.3333 (81.4024)  time: 0.1617  data: 0.0003  max mem: 1380\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6345 (0.7821)  Acc@1: 79.1667 (79.4000)  Acc@5: 95.8333 (96.5000)  Acc@task: 87.5000 (81.6000)  time: 0.1592  data: 0.0003  max mem: 1380\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1727 s / it)\n",
            "* Acc@task 81.600 Acc@1 79.400 Acc@5 96.500 loss 0.782\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:34  Loss: 1.4198 (1.4198)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  Acc@task: 75.0000 (75.0000)  time: 0.8251  data: 0.6426  max mem: 1380\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:07  Loss: 1.1234 (1.0532)  Acc@1: 79.1667 (76.5152)  Acc@5: 95.8333 (95.0758)  Acc@task: 75.0000 (76.5152)  time: 0.2240  data: 0.0602  max mem: 1380\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 1.0296 (1.1209)  Acc@1: 79.1667 (74.8016)  Acc@5: 95.8333 (94.4444)  Acc@task: 75.0000 (76.9841)  time: 0.1632  data: 0.0014  max mem: 1380\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 1.0176 (1.0631)  Acc@1: 75.0000 (75.4032)  Acc@5: 95.8333 (95.0269)  Acc@task: 79.1667 (78.0914)  time: 0.1621  data: 0.0006  max mem: 1380\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.8919 (1.0189)  Acc@1: 75.0000 (75.3049)  Acc@5: 95.8333 (95.1220)  Acc@task: 79.1667 (78.8618)  time: 0.1623  data: 0.0002  max mem: 1380\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.8919 (1.0166)  Acc@1: 75.0000 (75.3000)  Acc@5: 95.8333 (95.0000)  Acc@task: 79.1667 (78.8000)  time: 0.1598  data: 0.0002  max mem: 1380\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1793 s / it)\n",
            "* Acc@task 78.800 Acc@1 75.300 Acc@5 95.000 loss 1.017\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:22  Loss: 0.6768 (0.6768)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  Acc@task: 87.5000 (87.5000)  time: 0.5387  data: 0.3763  max mem: 1380\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 0.7375 (0.8223)  Acc@1: 79.1667 (80.6818)  Acc@5: 95.8333 (96.2121)  Acc@task: 83.3333 (80.6818)  time: 0.1968  data: 0.0349  max mem: 1380\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:03  Loss: 0.7206 (0.7632)  Acc@1: 79.1667 (80.7540)  Acc@5: 95.8333 (95.6349)  Acc@task: 79.1667 (81.1508)  time: 0.1639  data: 0.0006  max mem: 1380\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.5715 (0.7068)  Acc@1: 79.1667 (81.7204)  Acc@5: 95.8333 (96.3710)  Acc@task: 83.3333 (81.0484)  time: 0.1643  data: 0.0005  max mem: 1380\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.5715 (0.7158)  Acc@1: 83.3333 (81.8089)  Acc@5: 95.8333 (96.3415)  Acc@task: 83.3333 (82.1138)  time: 0.1630  data: 0.0004  max mem: 1380\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.5509 (0.7075)  Acc@1: 83.3333 (81.9000)  Acc@5: 95.8333 (96.4000)  Acc@task: 83.3333 (82.0000)  time: 0.1603  data: 0.0003  max mem: 1380\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1740 s / it)\n",
            "* Acc@task 82.000 Acc@1 81.900 Acc@5 96.400 loss 0.707\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:33  Loss: 1.1044 (1.1044)  Acc@1: 58.3333 (58.3333)  Acc@5: 87.5000 (87.5000)  Acc@task: 66.6667 (66.6667)  time: 0.7911  data: 0.6367  max mem: 1380\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:07  Loss: 0.8674 (0.8554)  Acc@1: 83.3333 (78.4091)  Acc@5: 95.8333 (94.3182)  Acc@task: 75.0000 (76.1364)  time: 0.2206  data: 0.0589  max mem: 1380\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.9441 (0.9699)  Acc@1: 75.0000 (75.7937)  Acc@5: 95.8333 (95.2381)  Acc@task: 79.1667 (77.1825)  time: 0.1636  data: 0.0008  max mem: 1380\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.9486 (0.9416)  Acc@1: 75.0000 (76.2097)  Acc@5: 95.8333 (95.4301)  Acc@task: 79.1667 (77.6882)  time: 0.1628  data: 0.0004  max mem: 1380\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.9604 (1.0072)  Acc@1: 75.0000 (76.4228)  Acc@5: 91.6667 (94.1057)  Acc@task: 79.1667 (77.8455)  time: 0.1616  data: 0.0003  max mem: 1380\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.9604 (0.9916)  Acc@1: 75.0000 (76.7000)  Acc@5: 91.6667 (94.1000)  Acc@task: 79.1667 (78.0000)  time: 0.1591  data: 0.0003  max mem: 1380\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1784 s / it)\n",
            "* Acc@task 78.000 Acc@1 76.700 Acc@5 94.100 loss 0.992\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:22  Loss: 0.4869 (0.4869)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.5249  data: 0.3458  max mem: 1380\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 0.7821 (0.7420)  Acc@1: 75.0000 (77.2727)  Acc@5: 95.8333 (96.9697)  Acc@task: 79.1667 (82.1970)  time: 0.1960  data: 0.0317  max mem: 1380\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:03  Loss: 0.8075 (0.8096)  Acc@1: 75.0000 (77.5794)  Acc@5: 95.8333 (96.2302)  Acc@task: 79.1667 (81.3492)  time: 0.1637  data: 0.0004  max mem: 1380\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.8722 (0.8270)  Acc@1: 79.1667 (77.5538)  Acc@5: 95.8333 (95.4301)  Acc@task: 79.1667 (80.7796)  time: 0.1632  data: 0.0004  max mem: 1380\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.8922 (0.8962)  Acc@1: 75.0000 (76.5244)  Acc@5: 95.8333 (95.6301)  Acc@task: 75.0000 (79.7764)  time: 0.1625  data: 0.0002  max mem: 1380\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.9297 (0.9345)  Acc@1: 70.8333 (76.1000)  Acc@5: 95.8333 (95.4000)  Acc@task: 75.0000 (79.6000)  time: 0.1600  data: 0.0002  max mem: 1380\n",
            "Test: [Task 5] Total time: 0:00:07 (0.1735 s / it)\n",
            "* Acc@task 79.600 Acc@1 76.100 Acc@5 95.400 loss 0.934\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:32  Loss: 0.7327 (0.7327)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  Acc@task: 87.5000 (87.5000)  time: 0.7853  data: 0.5996  max mem: 1380\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:07  Loss: 1.2257 (1.0952)  Acc@1: 75.0000 (74.2424)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (81.0606)  time: 0.2189  data: 0.0550  max mem: 1380\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:04  Loss: 1.0989 (1.0848)  Acc@1: 70.8333 (72.8175)  Acc@5: 95.8333 (95.4365)  Acc@task: 79.1667 (80.3571)  time: 0.1632  data: 0.0004  max mem: 1380\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 1.1235 (1.1302)  Acc@1: 70.8333 (71.6398)  Acc@5: 95.8333 (95.0269)  Acc@task: 75.0000 (77.6882)  time: 0.1639  data: 0.0003  max mem: 1380\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.1550 (1.1458)  Acc@1: 70.8333 (72.1545)  Acc@5: 95.8333 (94.5122)  Acc@task: 70.8333 (76.5244)  time: 0.1624  data: 0.0002  max mem: 1380\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.1550 (1.1461)  Acc@1: 70.8333 (72.1000)  Acc@5: 95.8333 (94.4000)  Acc@task: 70.8333 (76.6000)  time: 0.1598  data: 0.0002  max mem: 1380\n",
            "Test: [Task 6] Total time: 0:00:07 (0.1786 s / it)\n",
            "* Acc@task 76.600 Acc@1 72.100 Acc@5 94.400 loss 1.146\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:22  Loss: 0.9649 (0.9649)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  Acc@task: 79.1667 (79.1667)  time: 0.5449  data: 0.3575  max mem: 1380\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:06  Loss: 0.7828 (0.7591)  Acc@1: 79.1667 (78.4091)  Acc@5: 95.8333 (93.9394)  Acc@task: 75.0000 (71.2121)  time: 0.1979  data: 0.0329  max mem: 1380\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:04  Loss: 0.8363 (0.9080)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (93.6508)  Acc@task: 70.8333 (71.6270)  time: 0.1637  data: 0.0004  max mem: 1380\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:02  Loss: 1.1583 (0.9900)  Acc@1: 70.8333 (73.6559)  Acc@5: 91.6667 (93.1452)  Acc@task: 70.8333 (70.9677)  time: 0.1631  data: 0.0009  max mem: 1380\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 1.1364 (0.9985)  Acc@1: 70.8333 (73.4756)  Acc@5: 95.8333 (93.2927)  Acc@task: 70.8333 (71.2398)  time: 0.1616  data: 0.0010  max mem: 1380\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 1.1299 (0.9881)  Acc@1: 70.8333 (73.7000)  Acc@5: 95.8333 (93.3000)  Acc@task: 70.8333 (71.2000)  time: 0.1591  data: 0.0010  max mem: 1380\n",
            "Test: [Task 7] Total time: 0:00:07 (0.1727 s / it)\n",
            "* Acc@task 71.200 Acc@1 73.700 Acc@5 93.300 loss 0.988\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:25  Loss: 0.9355 (0.9355)  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  Acc@task: 70.8333 (70.8333)  time: 0.6123  data: 0.4563  max mem: 1380\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:06  Loss: 0.9866 (1.1660)  Acc@1: 75.0000 (70.8333)  Acc@5: 91.6667 (91.2879)  Acc@task: 58.3333 (59.8485)  time: 0.2030  data: 0.0421  max mem: 1380\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:04  Loss: 0.9273 (1.1091)  Acc@1: 75.0000 (71.4286)  Acc@5: 91.6667 (93.2540)  Acc@task: 58.3333 (59.3254)  time: 0.1634  data: 0.0005  max mem: 1380\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:02  Loss: 0.9500 (1.0910)  Acc@1: 70.8333 (71.2366)  Acc@5: 95.8333 (93.8172)  Acc@task: 58.3333 (59.6774)  time: 0.1634  data: 0.0003  max mem: 1380\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 1.1581 (1.1347)  Acc@1: 66.6667 (70.2236)  Acc@5: 95.8333 (93.1911)  Acc@task: 58.3333 (59.1463)  time: 0.1616  data: 0.0003  max mem: 1380\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 1.2357 (1.1401)  Acc@1: 66.6667 (70.1000)  Acc@5: 95.8333 (93.3000)  Acc@task: 58.3333 (59.5000)  time: 0.1591  data: 0.0002  max mem: 1380\n",
            "Test: [Task 8] Total time: 0:00:07 (0.1740 s / it)\n",
            "* Acc@task 59.500 Acc@1 70.100 Acc@5 93.300 loss 1.140\n",
            "[Average accuracy till task8]\tAcc@task: 75.9125\tAcc@1: 75.6625\tAcc@5: 94.8000\tLoss: 0.9633\tForgetting: 2.6857\tBackward: 5.8857\n",
            "Loading checkpoint from: ./output/cifar100_full_dino_1epoch_25pct/checkpoint/task9_checkpoint.pth\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/engines/hide_promtp_wtp_and_tap_engine.py:540: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  loss = torch.nn.functional.cross_entropy(sim, torch.range(0, sim.shape[0] - 1).long().to(device))\n",
            "Train: Epoch[1/1]  [ 0/52]  eta: 0:00:33  Lr: 0.002812  Loss: 3.6073  Acc@1: 12.5000 (12.5000)  Acc@5: 41.6667 (41.6667)  time: 0.6412  data: 0.3528  max mem: 1380\n",
            "Train: Epoch[1/1]  [10/52]  eta: 0:00:12  Lr: 0.002812  Loss: 2.5306  Acc@1: 12.5000 (14.3939)  Acc@5: 58.3333 (59.8485)  time: 0.2976  data: 0.0325  max mem: 1380\n",
            "Train: Epoch[1/1]  [20/52]  eta: 0:00:09  Lr: 0.002812  Loss: 2.3649  Acc@1: 20.8333 (21.4286)  Acc@5: 70.8333 (67.8571)  time: 0.2633  data: 0.0010  max mem: 1380\n",
            "Train: Epoch[1/1]  [30/52]  eta: 0:00:06  Lr: 0.002812  Loss: 1.8143  Acc@1: 37.5000 (28.4946)  Acc@5: 79.1667 (73.9247)  time: 0.2629  data: 0.0009  max mem: 1380\n",
            "Train: Epoch[1/1]  [40/52]  eta: 0:00:03  Lr: 0.002812  Loss: 1.6581  Acc@1: 45.8333 (34.4512)  Acc@5: 87.5000 (77.3374)  time: 0.2630  data: 0.0003  max mem: 1380\n",
            "Train: Epoch[1/1]  [50/52]  eta: 0:00:00  Lr: 0.002812  Loss: 1.3256  Acc@1: 54.1667 (38.1536)  Acc@5: 91.6667 (80.2288)  time: 0.2634  data: 0.0003  max mem: 1380\n",
            "Train: Epoch[1/1]  [51/52]  eta: 0:00:00  Lr: 0.002812  Loss: 1.3597  Acc@1: 54.1667 (38.2114)  Acc@5: 91.6667 (80.3252)  time: 0.2542  data: 0.0003  max mem: 1380\n",
            "Train: Epoch[1/1] Total time: 0:00:13 (0.2687 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.3597  Acc@1: 54.1667 (38.2114)  Acc@5: 91.6667 (80.3252)\n",
            "torch.Size([5, 2, 5, 6, 64])\n",
            "torch.Size([5, 2, 1, 5, 6, 64])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:22  Loss: 0.6638 (0.6638)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 79.1667 (79.1667)  time: 0.5357  data: 0.3721  max mem: 1380\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.7107 (0.8186)  Acc@1: 79.1667 (77.6515)  Acc@5: 100.0000 (97.7273)  Acc@task: 79.1667 (78.0303)  time: 0.1954  data: 0.0341  max mem: 1380\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:03  Loss: 0.7474 (0.8813)  Acc@1: 79.1667 (77.9762)  Acc@5: 95.8333 (96.2302)  Acc@task: 79.1667 (78.5714)  time: 0.1625  data: 0.0003  max mem: 1380\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.7003 (0.8353)  Acc@1: 75.0000 (78.4946)  Acc@5: 95.8333 (96.3710)  Acc@task: 79.1667 (79.5699)  time: 0.1628  data: 0.0003  max mem: 1380\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.7003 (0.8012)  Acc@1: 79.1667 (78.7602)  Acc@5: 95.8333 (96.5447)  Acc@task: 83.3333 (80.6911)  time: 0.1619  data: 0.0003  max mem: 1380\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6345 (0.7923)  Acc@1: 79.1667 (79.0000)  Acc@5: 95.8333 (96.6000)  Acc@task: 83.3333 (80.9000)  time: 0.1593  data: 0.0003  max mem: 1380\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1740 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@task 80.900 Acc@1 79.000 Acc@5 96.600 loss 0.792\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:35  Loss: 1.4215 (1.4215)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  Acc@task: 66.6667 (66.6667)  time: 0.8384  data: 0.6631  max mem: 1380\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:07  Loss: 1.0398 (1.0561)  Acc@1: 79.1667 (76.5152)  Acc@5: 95.8333 (95.0758)  Acc@task: 79.1667 (76.8939)  time: 0.2254  data: 0.0626  max mem: 1380\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 1.0296 (1.1359)  Acc@1: 75.0000 (74.6032)  Acc@5: 95.8333 (94.4444)  Acc@task: 75.0000 (75.1984)  time: 0.1635  data: 0.0015  max mem: 1380\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 1.0183 (1.0819)  Acc@1: 75.0000 (75.2688)  Acc@5: 95.8333 (95.0269)  Acc@task: 75.0000 (75.8065)  time: 0.1630  data: 0.0004  max mem: 1380\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.9101 (1.0386)  Acc@1: 75.0000 (75.3049)  Acc@5: 95.8333 (95.1220)  Acc@task: 75.0000 (76.8293)  time: 0.1622  data: 0.0003  max mem: 1380\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.9101 (1.0359)  Acc@1: 75.0000 (75.3000)  Acc@5: 95.8333 (95.0000)  Acc@task: 75.0000 (76.8000)  time: 0.1596  data: 0.0002  max mem: 1380\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1797 s / it)\n",
            "* Acc@task 76.800 Acc@1 75.300 Acc@5 95.000 loss 1.036\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:18  Loss: 0.6770 (0.6770)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  Acc@task: 87.5000 (87.5000)  time: 0.4325  data: 0.2721  max mem: 1380\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:05  Loss: 0.7397 (0.8247)  Acc@1: 79.1667 (80.3030)  Acc@5: 95.8333 (96.2121)  Acc@task: 83.3333 (80.6818)  time: 0.1870  data: 0.0250  max mem: 1380\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:03  Loss: 0.7180 (0.7722)  Acc@1: 79.1667 (80.1587)  Acc@5: 95.8333 (95.6349)  Acc@task: 79.1667 (80.9524)  time: 0.1637  data: 0.0004  max mem: 1380\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.5808 (0.7123)  Acc@1: 79.1667 (81.3172)  Acc@5: 95.8333 (96.2366)  Acc@task: 79.1667 (80.5108)  time: 0.1641  data: 0.0003  max mem: 1380\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.5528 (0.7205)  Acc@1: 83.3333 (81.5041)  Acc@5: 95.8333 (96.2398)  Acc@task: 83.3333 (81.5041)  time: 0.1631  data: 0.0003  max mem: 1380\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.5259 (0.7121)  Acc@1: 83.3333 (81.6000)  Acc@5: 95.8333 (96.3000)  Acc@task: 83.3333 (81.4000)  time: 0.1605  data: 0.0003  max mem: 1380\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1718 s / it)\n",
            "* Acc@task 81.400 Acc@1 81.600 Acc@5 96.300 loss 0.712\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:29  Loss: 1.0583 (1.0583)  Acc@1: 62.5000 (62.5000)  Acc@5: 91.6667 (91.6667)  Acc@task: 62.5000 (62.5000)  time: 0.6949  data: 0.4926  max mem: 1380\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:06  Loss: 0.7852 (0.8462)  Acc@1: 83.3333 (78.7879)  Acc@5: 95.8333 (94.6970)  Acc@task: 75.0000 (74.2424)  time: 0.2132  data: 0.0454  max mem: 1380\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.9448 (0.9695)  Acc@1: 75.0000 (75.7937)  Acc@5: 95.8333 (95.4365)  Acc@task: 75.0000 (75.5952)  time: 0.1650  data: 0.0005  max mem: 1380\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.9706 (0.9396)  Acc@1: 75.0000 (76.4785)  Acc@5: 95.8333 (95.4301)  Acc@task: 79.1667 (76.6129)  time: 0.1650  data: 0.0004  max mem: 1380\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.9706 (1.0015)  Acc@1: 75.0000 (76.6260)  Acc@5: 91.6667 (94.1057)  Acc@task: 79.1667 (77.2358)  time: 0.1648  data: 0.0003  max mem: 1380\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.9706 (0.9861)  Acc@1: 75.0000 (76.9000)  Acc@5: 91.6667 (94.1000)  Acc@task: 79.1667 (77.4000)  time: 0.1622  data: 0.0003  max mem: 1380\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1786 s / it)\n",
            "* Acc@task 77.400 Acc@1 76.900 Acc@5 94.100 loss 0.986\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:23  Loss: 0.5200 (0.5200)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  Acc@task: 79.1667 (79.1667)  time: 0.5558  data: 0.3937  max mem: 1380\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 0.7822 (0.7351)  Acc@1: 79.1667 (77.6515)  Acc@5: 95.8333 (96.9697)  Acc@task: 83.3333 (81.4394)  time: 0.1998  data: 0.0368  max mem: 1380\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.8112 (0.8062)  Acc@1: 75.0000 (77.7778)  Acc@5: 95.8333 (96.2302)  Acc@task: 83.3333 (80.1587)  time: 0.1650  data: 0.0008  max mem: 1380\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.8684 (0.8194)  Acc@1: 79.1667 (77.6882)  Acc@5: 95.8333 (95.4301)  Acc@task: 79.1667 (79.5699)  time: 0.1652  data: 0.0007  max mem: 1380\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.8684 (0.8870)  Acc@1: 75.0000 (76.5244)  Acc@5: 95.8333 (95.6301)  Acc@task: 79.1667 (78.7602)  time: 0.1641  data: 0.0007  max mem: 1380\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.8722 (0.9255)  Acc@1: 70.8333 (76.1000)  Acc@5: 95.8333 (95.4000)  Acc@task: 75.0000 (78.6000)  time: 0.1616  data: 0.0007  max mem: 1380\n",
            "Test: [Task 5] Total time: 0:00:07 (0.1753 s / it)\n",
            "* Acc@task 78.600 Acc@1 76.100 Acc@5 95.400 loss 0.925\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:38  Loss: 0.7324 (0.7324)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  Acc@task: 91.6667 (91.6667)  time: 0.9101  data: 0.7359  max mem: 1380\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:07  Loss: 1.1242 (1.0761)  Acc@1: 75.0000 (74.6212)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (82.5758)  time: 0.2317  data: 0.0672  max mem: 1380\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:04  Loss: 1.0635 (1.0727)  Acc@1: 75.0000 (73.2143)  Acc@5: 95.8333 (95.4365)  Acc@task: 79.1667 (80.5556)  time: 0.1644  data: 0.0003  max mem: 1380\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 1.1232 (1.1188)  Acc@1: 70.8333 (71.7742)  Acc@5: 95.8333 (95.0269)  Acc@task: 79.1667 (78.0914)  time: 0.1640  data: 0.0003  max mem: 1380\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.1554 (1.1311)  Acc@1: 70.8333 (72.3577)  Acc@5: 95.8333 (94.7154)  Acc@task: 70.8333 (76.6260)  time: 0.1630  data: 0.0002  max mem: 1380\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.1554 (1.1318)  Acc@1: 70.8333 (72.3000)  Acc@5: 95.8333 (94.6000)  Acc@task: 70.8333 (76.7000)  time: 0.1604  data: 0.0002  max mem: 1380\n",
            "Test: [Task 6] Total time: 0:00:07 (0.1823 s / it)\n",
            "* Acc@task 76.700 Acc@1 72.300 Acc@5 94.600 loss 1.132\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:26  Loss: 0.9648 (0.9648)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  Acc@task: 79.1667 (79.1667)  time: 0.6197  data: 0.4702  max mem: 1380\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:06  Loss: 0.7842 (0.7332)  Acc@1: 79.1667 (79.9242)  Acc@5: 95.8333 (93.9394)  Acc@task: 70.8333 (72.3485)  time: 0.2045  data: 0.0431  max mem: 1380\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:04  Loss: 0.8046 (0.8868)  Acc@1: 79.1667 (75.9921)  Acc@5: 91.6667 (93.6508)  Acc@task: 70.8333 (72.8175)  time: 0.1638  data: 0.0004  max mem: 1380\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:02  Loss: 1.1579 (0.9727)  Acc@1: 70.8333 (74.7312)  Acc@5: 91.6667 (93.1452)  Acc@task: 70.8333 (72.4462)  time: 0.1638  data: 0.0005  max mem: 1380\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 1.1366 (0.9800)  Acc@1: 70.8333 (74.4919)  Acc@5: 95.8333 (93.3943)  Acc@task: 70.8333 (72.2561)  time: 0.1622  data: 0.0005  max mem: 1380\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 1.1080 (0.9706)  Acc@1: 70.8333 (74.7000)  Acc@5: 95.8333 (93.4000)  Acc@task: 70.8333 (72.3000)  time: 0.1596  data: 0.0005  max mem: 1380\n",
            "Test: [Task 7] Total time: 0:00:07 (0.1746 s / it)\n",
            "* Acc@task 72.300 Acc@1 74.700 Acc@5 93.400 loss 0.971\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:20  Loss: 0.9308 (0.9308)  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  Acc@task: 79.1667 (79.1667)  time: 0.4874  data: 0.3148  max mem: 1380\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:06  Loss: 0.9838 (1.1496)  Acc@1: 75.0000 (71.2121)  Acc@5: 91.6667 (91.2879)  Acc@task: 70.8333 (68.1818)  time: 0.1927  data: 0.0290  max mem: 1380\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:03  Loss: 0.9371 (1.0892)  Acc@1: 75.0000 (71.6270)  Acc@5: 95.8333 (93.2540)  Acc@task: 70.8333 (68.2540)  time: 0.1638  data: 0.0004  max mem: 1380\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:02  Loss: 0.8494 (1.0440)  Acc@1: 75.0000 (71.9086)  Acc@5: 95.8333 (93.8172)  Acc@task: 70.8333 (69.3548)  time: 0.1633  data: 0.0003  max mem: 1380\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 1.0641 (1.0732)  Acc@1: 70.8333 (70.9350)  Acc@5: 95.8333 (93.4959)  Acc@task: 66.6667 (68.3943)  time: 0.1622  data: 0.0003  max mem: 1380\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 1.0641 (1.0699)  Acc@1: 68.7500 (70.9000)  Acc@5: 95.8333 (93.6000)  Acc@task: 66.6667 (68.7000)  time: 0.1597  data: 0.0003  max mem: 1380\n",
            "Test: [Task 8] Total time: 0:00:07 (0.1717 s / it)\n",
            "* Acc@task 68.700 Acc@1 70.900 Acc@5 93.600 loss 1.070\n",
            "Test: [Task 9]  [ 0/42]  eta: 0:00:24  Loss: 8.6468 (8.6468)  Acc@1: 0.0000 (0.0000)  Acc@5: 12.5000 (12.5000)  Acc@task: 70.8333 (70.8333)  time: 0.5936  data: 0.4360  max mem: 1380\n",
            "Test: [Task 9]  [10/42]  eta: 0:00:06  Loss: 8.5091 (8.3476)  Acc@1: 0.0000 (0.0000)  Acc@5: 8.3333 (9.0909)  Acc@task: 70.8333 (71.2121)  time: 0.2022  data: 0.0401  max mem: 1380\n",
            "Test: [Task 9]  [20/42]  eta: 0:00:04  Loss: 8.0636 (8.2155)  Acc@1: 0.0000 (0.0000)  Acc@5: 8.3333 (8.5317)  Acc@task: 66.6667 (68.2540)  time: 0.1642  data: 0.0007  max mem: 1380\n",
            "Test: [Task 9]  [30/42]  eta: 0:00:02  Loss: 8.0636 (8.2811)  Acc@1: 0.0000 (0.0000)  Acc@5: 8.3333 (8.7366)  Acc@task: 66.6667 (67.4731)  time: 0.1635  data: 0.0012  max mem: 1380\n",
            "Test: [Task 9]  [40/42]  eta: 0:00:00  Loss: 8.2641 (8.2925)  Acc@1: 0.0000 (0.0000)  Acc@5: 8.3333 (9.0447)  Acc@task: 66.6667 (67.8862)  time: 0.1620  data: 0.0009  max mem: 1380\n",
            "Test: [Task 9]  [41/42]  eta: 0:00:00  Loss: 8.2641 (8.2908)  Acc@1: 0.0000 (0.0000)  Acc@5: 8.3333 (9.1000)  Acc@task: 66.6667 (68.3000)  time: 0.1595  data: 0.0008  max mem: 1380\n",
            "Test: [Task 9] Total time: 0:00:07 (0.1742 s / it)\n",
            "* Acc@task 68.300 Acc@1 0.000 Acc@5 9.100 loss 8.291\n",
            "[Average accuracy till task9]\tAcc@task: 75.6778\tAcc@1: 67.4222\tAcc@5: 85.3444\tLoss: 1.7683\tForgetting: 2.8125\tBackward: 73.7750\n",
            "torch.Size([106320, 384])\n",
            "Averaged stats: Lr: 0.005000  Loss: 0.1640  Acc@1: 97.5000 (94.2813)  Acc@5: 100.0000 (99.1250)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:23  Loss: 0.9150 (0.9150)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 79.1667 (79.1667)  time: 0.5688  data: 0.4018  max mem: 1380\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.7887 (0.7867)  Acc@1: 79.1667 (79.9242)  Acc@5: 100.0000 (97.7273)  Acc@task: 79.1667 (78.0303)  time: 0.1988  data: 0.0368  max mem: 1380\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.7524 (0.8432)  Acc@1: 79.1667 (79.7619)  Acc@5: 95.8333 (95.8333)  Acc@task: 79.1667 (78.5714)  time: 0.1631  data: 0.0003  max mem: 1380\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.6732 (0.7791)  Acc@1: 83.3333 (80.6452)  Acc@5: 95.8333 (96.1022)  Acc@task: 79.1667 (79.5699)  time: 0.1630  data: 0.0003  max mem: 1380\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.5894 (0.7433)  Acc@1: 83.3333 (81.0976)  Acc@5: 100.0000 (96.4431)  Acc@task: 83.3333 (80.6911)  time: 0.1616  data: 0.0003  max mem: 1380\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.5412 (0.7356)  Acc@1: 83.3333 (81.3000)  Acc@5: 100.0000 (96.5000)  Acc@task: 83.3333 (80.9000)  time: 0.1590  data: 0.0003  max mem: 1380\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1740 s / it)\n",
            "* Acc@task 80.900 Acc@1 81.300 Acc@5 96.500 loss 0.736\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:38  Loss: 1.5279 (1.5279)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  Acc@task: 66.6667 (66.6667)  time: 0.9262  data: 0.7242  max mem: 1380\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:07  Loss: 1.1695 (1.1358)  Acc@1: 75.0000 (72.3485)  Acc@5: 95.8333 (95.4545)  Acc@task: 79.1667 (76.8939)  time: 0.2319  data: 0.0671  max mem: 1380\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 1.1459 (1.2949)  Acc@1: 70.8333 (69.4444)  Acc@5: 95.8333 (94.2460)  Acc@task: 75.0000 (75.1984)  time: 0.1631  data: 0.0009  max mem: 1380\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 1.1026 (1.2379)  Acc@1: 70.8333 (70.4301)  Acc@5: 95.8333 (94.7581)  Acc@task: 75.0000 (75.8065)  time: 0.1637  data: 0.0004  max mem: 1380\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.9516 (1.1682)  Acc@1: 70.8333 (71.3415)  Acc@5: 95.8333 (94.7154)  Acc@task: 75.0000 (76.8293)  time: 0.1623  data: 0.0002  max mem: 1380\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.9516 (1.1711)  Acc@1: 70.8333 (71.3000)  Acc@5: 95.8333 (94.6000)  Acc@task: 75.0000 (76.8000)  time: 0.1598  data: 0.0002  max mem: 1380\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1817 s / it)\n",
            "* Acc@task 76.800 Acc@1 71.300 Acc@5 94.600 loss 1.171\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:19  Loss: 0.8102 (0.8102)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 87.5000 (87.5000)  time: 0.4596  data: 0.3039  max mem: 1380\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 0.9282 (0.9759)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (93.9394)  Acc@task: 83.3333 (80.6818)  time: 0.1909  data: 0.0281  max mem: 1380\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:03  Loss: 0.9282 (0.9607)  Acc@1: 75.0000 (76.3889)  Acc@5: 95.8333 (94.4444)  Acc@task: 79.1667 (80.9524)  time: 0.1639  data: 0.0004  max mem: 1380\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.8223 (0.9024)  Acc@1: 79.1667 (77.0161)  Acc@5: 95.8333 (95.1613)  Acc@task: 79.1667 (80.5108)  time: 0.1630  data: 0.0003  max mem: 1380\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.7414 (0.8919)  Acc@1: 79.1667 (77.7439)  Acc@5: 95.8333 (95.2236)  Acc@task: 83.3333 (81.5041)  time: 0.1620  data: 0.0004  max mem: 1380\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.7187 (0.8817)  Acc@1: 79.1667 (77.8000)  Acc@5: 95.8333 (95.3000)  Acc@task: 83.3333 (81.4000)  time: 0.1594  data: 0.0004  max mem: 1380\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1722 s / it)\n",
            "* Acc@task 81.400 Acc@1 77.800 Acc@5 95.300 loss 0.882\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:33  Loss: 1.0291 (1.0291)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  Acc@task: 62.5000 (62.5000)  time: 0.8003  data: 0.6040  max mem: 1380\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:07  Loss: 0.7143 (0.8299)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.0758)  Acc@task: 75.0000 (74.2424)  time: 0.2221  data: 0.0555  max mem: 1380\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.8650 (0.8958)  Acc@1: 79.1667 (78.3730)  Acc@5: 91.6667 (94.2460)  Acc@task: 75.0000 (75.5952)  time: 0.1634  data: 0.0005  max mem: 1380\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.9047 (0.8613)  Acc@1: 75.0000 (78.7634)  Acc@5: 91.6667 (94.2204)  Acc@task: 79.1667 (76.6129)  time: 0.1629  data: 0.0003  max mem: 1380\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.9137 (0.9391)  Acc@1: 79.1667 (77.9472)  Acc@5: 91.6667 (93.4959)  Acc@task: 79.1667 (77.2358)  time: 0.1623  data: 0.0003  max mem: 1380\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.8822 (0.9218)  Acc@1: 79.1667 (78.2000)  Acc@5: 95.8333 (93.6000)  Acc@task: 79.1667 (77.4000)  time: 0.1597  data: 0.0003  max mem: 1380\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1794 s / it)\n",
            "* Acc@task 77.400 Acc@1 78.200 Acc@5 93.600 loss 0.922\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:24  Loss: 0.4071 (0.4071)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  Acc@task: 79.1667 (79.1667)  time: 0.5902  data: 0.4247  max mem: 1380\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 0.7083 (0.7132)  Acc@1: 79.1667 (78.7879)  Acc@5: 95.8333 (97.3485)  Acc@task: 83.3333 (81.4394)  time: 0.2016  data: 0.0389  max mem: 1380\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.8358 (0.7945)  Acc@1: 79.1667 (78.3730)  Acc@5: 95.8333 (96.2302)  Acc@task: 83.3333 (80.1587)  time: 0.1635  data: 0.0003  max mem: 1380\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.8792 (0.8083)  Acc@1: 75.0000 (78.2258)  Acc@5: 95.8333 (95.5645)  Acc@task: 79.1667 (79.5699)  time: 0.1641  data: 0.0005  max mem: 1380\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.8485 (0.8831)  Acc@1: 75.0000 (76.9309)  Acc@5: 95.8333 (95.4268)  Acc@task: 79.1667 (78.7602)  time: 0.1634  data: 0.0006  max mem: 1380\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.8829 (0.9254)  Acc@1: 75.0000 (76.5000)  Acc@5: 91.6667 (95.1000)  Acc@task: 75.0000 (78.6000)  time: 0.1610  data: 0.0006  max mem: 1380\n",
            "Test: [Task 5] Total time: 0:00:07 (0.1755 s / it)\n",
            "* Acc@task 78.600 Acc@1 76.500 Acc@5 95.100 loss 0.925\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:24  Loss: 0.7210 (0.7210)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 91.6667 (91.6667)  time: 0.5755  data: 0.3980  max mem: 1380\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:06  Loss: 1.2107 (1.1491)  Acc@1: 66.6667 (70.4545)  Acc@5: 95.8333 (93.9394)  Acc@task: 83.3333 (82.5758)  time: 0.2046  data: 0.0402  max mem: 1380\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:04  Loss: 1.1715 (1.1634)  Acc@1: 66.6667 (69.6429)  Acc@5: 95.8333 (94.0476)  Acc@task: 79.1667 (80.5556)  time: 0.1660  data: 0.0024  max mem: 1380\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 1.1715 (1.2009)  Acc@1: 66.6667 (69.3548)  Acc@5: 95.8333 (93.8172)  Acc@task: 79.1667 (78.0914)  time: 0.1641  data: 0.0003  max mem: 1380\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.2482 (1.2218)  Acc@1: 70.8333 (70.1220)  Acc@5: 91.6667 (93.4959)  Acc@task: 70.8333 (76.6260)  time: 0.1630  data: 0.0002  max mem: 1380\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.1462 (1.2191)  Acc@1: 70.8333 (70.1000)  Acc@5: 91.6667 (93.5000)  Acc@task: 70.8333 (76.7000)  time: 0.1606  data: 0.0002  max mem: 1380\n",
            "Test: [Task 6] Total time: 0:00:07 (0.1752 s / it)\n",
            "* Acc@task 76.700 Acc@1 70.100 Acc@5 93.500 loss 1.219\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:23  Loss: 1.1062 (1.1062)  Acc@1: 66.6667 (66.6667)  Acc@5: 95.8333 (95.8333)  Acc@task: 79.1667 (79.1667)  time: 0.5576  data: 0.3990  max mem: 1380\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:06  Loss: 1.1062 (0.9602)  Acc@1: 75.0000 (71.5909)  Acc@5: 95.8333 (94.3182)  Acc@task: 70.8333 (72.3485)  time: 0.1984  data: 0.0369  max mem: 1380\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:04  Loss: 1.1049 (1.0867)  Acc@1: 75.0000 (70.4365)  Acc@5: 95.8333 (93.8492)  Acc@task: 70.8333 (72.8175)  time: 0.1636  data: 0.0006  max mem: 1380\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:02  Loss: 1.3375 (1.1571)  Acc@1: 70.8333 (70.1613)  Acc@5: 91.6667 (92.4731)  Acc@task: 70.8333 (72.4462)  time: 0.1643  data: 0.0006  max mem: 1380\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 1.2061 (1.1464)  Acc@1: 70.8333 (70.1220)  Acc@5: 91.6667 (92.5813)  Acc@task: 70.8333 (72.2561)  time: 0.1627  data: 0.0006  max mem: 1380\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 1.0185 (1.1314)  Acc@1: 70.8333 (70.4000)  Acc@5: 91.6667 (92.6000)  Acc@task: 70.8333 (72.3000)  time: 0.1601  data: 0.0006  max mem: 1380\n",
            "Test: [Task 7] Total time: 0:00:07 (0.1734 s / it)\n",
            "* Acc@task 72.300 Acc@1 70.400 Acc@5 92.600 loss 1.131\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:18  Loss: 1.0259 (1.0259)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  Acc@task: 79.1667 (79.1667)  time: 0.4336  data: 0.2712  max mem: 1380\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:06  Loss: 1.0485 (1.1905)  Acc@1: 75.0000 (70.8333)  Acc@5: 91.6667 (91.6667)  Acc@task: 70.8333 (68.1818)  time: 0.1883  data: 0.0262  max mem: 1380\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:03  Loss: 0.9462 (1.1551)  Acc@1: 70.8333 (70.0397)  Acc@5: 95.8333 (93.0556)  Acc@task: 70.8333 (68.2540)  time: 0.1638  data: 0.0010  max mem: 1380\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:02  Loss: 0.8907 (1.0915)  Acc@1: 70.8333 (71.3710)  Acc@5: 95.8333 (93.6828)  Acc@task: 70.8333 (69.3548)  time: 0.1634  data: 0.0003  max mem: 1380\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 0.9989 (1.1221)  Acc@1: 70.8333 (71.2398)  Acc@5: 91.6667 (93.2927)  Acc@task: 66.6667 (68.3943)  time: 0.1623  data: 0.0004  max mem: 1380\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 1.0740 (1.1210)  Acc@1: 70.8333 (71.3000)  Acc@5: 91.6667 (93.4000)  Acc@task: 66.6667 (68.7000)  time: 0.1596  data: 0.0004  max mem: 1380\n",
            "Test: [Task 8] Total time: 0:00:07 (0.1703 s / it)\n",
            "* Acc@task 68.700 Acc@1 71.300 Acc@5 93.400 loss 1.121\n",
            "Test: [Task 9]  [ 0/42]  eta: 0:00:17  Loss: 0.8470 (0.8470)  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  Acc@task: 70.8333 (70.8333)  time: 0.4285  data: 0.2673  max mem: 1380\n",
            "Test: [Task 9]  [10/42]  eta: 0:00:05  Loss: 0.9455 (0.9547)  Acc@1: 75.0000 (73.4849)  Acc@5: 95.8333 (96.2121)  Acc@task: 70.8333 (71.2121)  time: 0.1871  data: 0.0250  max mem: 1380\n",
            "Test: [Task 9]  [20/42]  eta: 0:00:03  Loss: 0.8987 (0.8832)  Acc@1: 75.0000 (75.5952)  Acc@5: 95.8333 (95.8333)  Acc@task: 66.6667 (68.2540)  time: 0.1646  data: 0.0010  max mem: 1380\n",
            "Test: [Task 9]  [30/42]  eta: 0:00:02  Loss: 1.0348 (1.0160)  Acc@1: 70.8333 (72.7151)  Acc@5: 91.6667 (94.0860)  Acc@task: 66.6667 (67.4731)  time: 0.1641  data: 0.0015  max mem: 1380\n",
            "Test: [Task 9]  [40/42]  eta: 0:00:00  Loss: 0.8694 (0.9614)  Acc@1: 70.8333 (73.7805)  Acc@5: 91.6667 (94.7154)  Acc@task: 66.6667 (67.8862)  time: 0.1627  data: 0.0012  max mem: 1380\n",
            "Test: [Task 9]  [41/42]  eta: 0:00:00  Loss: 0.8694 (0.9468)  Acc@1: 70.8333 (74.1000)  Acc@5: 91.6667 (94.8000)  Acc@task: 66.6667 (68.3000)  time: 0.1602  data: 0.0011  max mem: 1380\n",
            "Test: [Task 9] Total time: 0:00:07 (0.1708 s / it)\n",
            "* Acc@task 68.300 Acc@1 74.100 Acc@5 94.800 loss 0.947\n",
            "[Average accuracy till task9]\tAcc@task: 75.6778\tAcc@1: 74.5556\tAcc@5: 94.3778\tLoss: 1.0060\tForgetting: 3.5500\tBackward: 4.1000\n",
            "Loading checkpoint from: ./output/cifar100_full_dino_1epoch_25pct/checkpoint/task10_checkpoint.pth\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/engines/hide_promtp_wtp_and_tap_engine.py:540: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  loss = torch.nn.functional.cross_entropy(sim, torch.range(0, sim.shape[0] - 1).long().to(device))\n",
            "Train: Epoch[1/1]  [ 0/52]  eta: 0:00:37  Lr: 0.002812  Loss: 3.9067  Acc@1: 8.3333 (8.3333)  Acc@5: 62.5000 (62.5000)  time: 0.7116  data: 0.4408  max mem: 1380\n",
            "Train: Epoch[1/1]  [10/52]  eta: 0:00:12  Lr: 0.002812  Loss: 3.0905  Acc@1: 12.5000 (13.2576)  Acc@5: 62.5000 (60.6061)  time: 0.3047  data: 0.0404  max mem: 1380\n",
            "Train: Epoch[1/1]  [20/52]  eta: 0:00:09  Lr: 0.002812  Loss: 2.4346  Acc@1: 12.5000 (16.6667)  Acc@5: 66.6667 (63.2937)  time: 0.2638  data: 0.0003  max mem: 1380\n",
            "Train: Epoch[1/1]  [30/52]  eta: 0:00:06  Lr: 0.002812  Loss: 1.8895  Acc@1: 37.5000 (24.8656)  Acc@5: 75.0000 (69.8925)  time: 0.2632  data: 0.0004  max mem: 1380\n",
            "Train: Epoch[1/1]  [40/52]  eta: 0:00:03  Lr: 0.002812  Loss: 1.5842  Acc@1: 41.6667 (30.7927)  Acc@5: 83.3333 (74.4919)  time: 0.2655  data: 0.0005  max mem: 1380\n",
            "Train: Epoch[1/1]  [50/52]  eta: 0:00:00  Lr: 0.002812  Loss: 1.1789  Acc@1: 54.1667 (36.4379)  Acc@5: 91.6667 (77.7778)  time: 0.2660  data: 0.0006  max mem: 1380\n",
            "Train: Epoch[1/1]  [51/52]  eta: 0:00:00  Lr: 0.002812  Loss: 1.7817  Acc@1: 54.1667 (36.4744)  Acc@5: 87.5000 (77.8229)  time: 0.2577  data: 0.0006  max mem: 1380\n",
            "Train: Epoch[1/1] Total time: 0:00:14 (0.2716 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.7817  Acc@1: 54.1667 (36.4744)  Acc@5: 87.5000 (77.8229)\n",
            "torch.Size([5, 2, 5, 6, 64])\n",
            "torch.Size([5, 2, 1, 5, 6, 64])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:32  Loss: 0.9249 (0.9249)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.7692  data: 0.5869  max mem: 1380\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:07  Loss: 0.7884 (0.7884)  Acc@1: 79.1667 (79.9242)  Acc@5: 100.0000 (97.7273)  Acc@task: 79.1667 (78.4091)  time: 0.2195  data: 0.0543  max mem: 1380\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.6936 (0.8381)  Acc@1: 79.1667 (79.9603)  Acc@5: 95.8333 (95.8333)  Acc@task: 79.1667 (78.9683)  time: 0.1638  data: 0.0011  max mem: 1380\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.6146 (0.7778)  Acc@1: 83.3333 (80.7796)  Acc@5: 95.8333 (96.1022)  Acc@task: 83.3333 (79.7043)  time: 0.1633  data: 0.0015  max mem: 1380\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6018 (0.7444)  Acc@1: 83.3333 (81.0976)  Acc@5: 100.0000 (96.3415)  Acc@task: 83.3333 (80.4878)  time: 0.1630  data: 0.0011  max mem: 1380\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.5708 (0.7367)  Acc@1: 83.3333 (81.3000)  Acc@5: 100.0000 (96.4000)  Acc@task: 87.5000 (80.7000)  time: 0.1602  data: 0.0010  max mem: 1380\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1788 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@task 80.700 Acc@1 81.300 Acc@5 96.400 loss 0.737\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:17  Loss: 1.5276 (1.5276)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  Acc@task: 66.6667 (66.6667)  time: 0.4125  data: 0.2548  max mem: 1380\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 1.1497 (1.1670)  Acc@1: 70.8333 (72.3485)  Acc@5: 95.8333 (95.0758)  Acc@task: 75.0000 (75.3788)  time: 0.1876  data: 0.0240  max mem: 1380\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:03  Loss: 1.1456 (1.3368)  Acc@1: 70.8333 (68.8492)  Acc@5: 95.8333 (94.0476)  Acc@task: 75.0000 (73.0159)  time: 0.1642  data: 0.0006  max mem: 1380\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 1.1024 (1.2634)  Acc@1: 70.8333 (69.8925)  Acc@5: 95.8333 (94.6237)  Acc@task: 75.0000 (74.5968)  time: 0.1633  data: 0.0003  max mem: 1380\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.9546 (1.1924)  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (94.7154)  Acc@task: 75.0000 (75.5081)  time: 0.1630  data: 0.0002  max mem: 1380\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.9546 (1.1948)  Acc@1: 70.8333 (70.8000)  Acc@5: 95.8333 (94.6000)  Acc@task: 75.0000 (75.5000)  time: 0.1604  data: 0.0002  max mem: 1380\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1705 s / it)\n",
            "* Acc@task 75.500 Acc@1 70.800 Acc@5 94.600 loss 1.195\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:33  Loss: 0.8101 (0.8101)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 87.5000 (87.5000)  time: 0.7867  data: 0.5757  max mem: 1380\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:07  Loss: 0.9209 (0.9738)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (93.9394)  Acc@task: 83.3333 (81.8182)  time: 0.2206  data: 0.0539  max mem: 1380\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.9209 (0.9624)  Acc@1: 75.0000 (76.3889)  Acc@5: 95.8333 (94.4444)  Acc@task: 83.3333 (81.7460)  time: 0.1642  data: 0.0014  max mem: 1380\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.8225 (0.9047)  Acc@1: 79.1667 (77.0161)  Acc@5: 95.8333 (95.1613)  Acc@task: 79.1667 (80.7796)  time: 0.1640  data: 0.0006  max mem: 1380\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.7409 (0.8944)  Acc@1: 79.1667 (77.6423)  Acc@5: 95.8333 (95.2236)  Acc@task: 83.3333 (81.4024)  time: 0.1631  data: 0.0002  max mem: 1380\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.7209 (0.8840)  Acc@1: 79.1667 (77.7000)  Acc@5: 95.8333 (95.3000)  Acc@task: 83.3333 (81.3000)  time: 0.1604  data: 0.0002  max mem: 1380\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1794 s / it)\n",
            "* Acc@task 81.300 Acc@1 77.700 Acc@5 95.300 loss 0.884\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:23  Loss: 1.0315 (1.0315)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  Acc@task: 62.5000 (62.5000)  time: 0.5574  data: 0.3879  max mem: 1380\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:06  Loss: 0.8066 (0.8426)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.0758)  Acc@task: 70.8333 (70.8333)  time: 0.1989  data: 0.0355  max mem: 1380\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.8784 (0.9025)  Acc@1: 79.1667 (78.3730)  Acc@5: 91.6667 (94.2460)  Acc@task: 75.0000 (73.4127)  time: 0.1644  data: 0.0003  max mem: 1380\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.8958 (0.8629)  Acc@1: 75.0000 (78.8979)  Acc@5: 91.6667 (94.2204)  Acc@task: 75.0000 (74.1936)  time: 0.1653  data: 0.0003  max mem: 1380\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.8958 (0.9396)  Acc@1: 79.1667 (78.0488)  Acc@5: 91.6667 (93.4959)  Acc@task: 75.0000 (74.5935)  time: 0.1640  data: 0.0003  max mem: 1380\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.8803 (0.9223)  Acc@1: 79.1667 (78.3000)  Acc@5: 95.8333 (93.6000)  Acc@task: 75.0000 (74.8000)  time: 0.1615  data: 0.0003  max mem: 1380\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1761 s / it)\n",
            "* Acc@task 74.800 Acc@1 78.300 Acc@5 93.600 loss 0.922\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:31  Loss: 0.4070 (0.4070)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  Acc@task: 79.1667 (79.1667)  time: 0.7496  data: 0.5552  max mem: 1380\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 0.7067 (0.7162)  Acc@1: 79.1667 (78.4091)  Acc@5: 95.8333 (97.3485)  Acc@task: 83.3333 (80.3030)  time: 0.2172  data: 0.0516  max mem: 1380\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.8345 (0.7879)  Acc@1: 79.1667 (78.3730)  Acc@5: 95.8333 (96.2302)  Acc@task: 79.1667 (78.5714)  time: 0.1645  data: 0.0009  max mem: 1380\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.8792 (0.8060)  Acc@1: 79.1667 (78.2258)  Acc@5: 95.8333 (95.5645)  Acc@task: 79.1667 (78.6290)  time: 0.1642  data: 0.0005  max mem: 1380\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.8752 (0.8812)  Acc@1: 75.0000 (77.0325)  Acc@5: 95.8333 (95.4268)  Acc@task: 75.0000 (78.0488)  time: 0.1632  data: 0.0003  max mem: 1380\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.8780 (0.9235)  Acc@1: 75.0000 (76.6000)  Acc@5: 91.6667 (95.1000)  Acc@task: 75.0000 (77.7000)  time: 0.1605  data: 0.0002  max mem: 1380\n",
            "Test: [Task 5] Total time: 0:00:07 (0.1788 s / it)\n",
            "* Acc@task 77.700 Acc@1 76.600 Acc@5 95.100 loss 0.923\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:18  Loss: 0.7209 (0.7209)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 91.6667 (91.6667)  time: 0.4294  data: 0.2746  max mem: 1380\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:05  Loss: 1.2078 (1.1591)  Acc@1: 66.6667 (70.0758)  Acc@5: 95.8333 (93.9394)  Acc@task: 79.1667 (79.1667)  time: 0.1866  data: 0.0257  max mem: 1380\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:03  Loss: 1.1714 (1.1678)  Acc@1: 66.6667 (69.4444)  Acc@5: 95.8333 (94.0476)  Acc@task: 79.1667 (78.7698)  time: 0.1636  data: 0.0006  max mem: 1380\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 1.1714 (1.2069)  Acc@1: 66.6667 (69.2204)  Acc@5: 95.8333 (93.5484)  Acc@task: 70.8333 (75.6720)  time: 0.1641  data: 0.0004  max mem: 1380\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.2476 (1.2259)  Acc@1: 70.8333 (70.0203)  Acc@5: 91.6667 (93.2927)  Acc@task: 70.8333 (74.7968)  time: 0.1628  data: 0.0004  max mem: 1380\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.1461 (1.2231)  Acc@1: 70.8333 (70.0000)  Acc@5: 91.6667 (93.2000)  Acc@task: 70.8333 (74.9000)  time: 0.1604  data: 0.0004  max mem: 1380\n",
            "Test: [Task 6] Total time: 0:00:07 (0.1712 s / it)\n",
            "* Acc@task 74.900 Acc@1 70.000 Acc@5 93.200 loss 1.223\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:33  Loss: 1.1062 (1.1062)  Acc@1: 66.6667 (66.6667)  Acc@5: 95.8333 (95.8333)  Acc@task: 79.1667 (79.1667)  time: 0.7900  data: 0.5922  max mem: 1380\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:07  Loss: 1.1062 (0.9705)  Acc@1: 75.0000 (71.2121)  Acc@5: 95.8333 (93.9394)  Acc@task: 75.0000 (71.5909)  time: 0.2210  data: 0.0548  max mem: 1380\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:04  Loss: 1.1348 (1.0868)  Acc@1: 75.0000 (70.4365)  Acc@5: 91.6667 (93.6508)  Acc@task: 75.0000 (72.4206)  time: 0.1641  data: 0.0007  max mem: 1380\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:02  Loss: 1.3059 (1.1569)  Acc@1: 70.8333 (70.1613)  Acc@5: 91.6667 (92.3387)  Acc@task: 75.0000 (72.3118)  time: 0.1640  data: 0.0003  max mem: 1380\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 1.2060 (1.1458)  Acc@1: 70.8333 (70.1220)  Acc@5: 91.6667 (92.3781)  Acc@task: 75.0000 (72.4594)  time: 0.1635  data: 0.0003  max mem: 1380\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 1.0228 (1.1306)  Acc@1: 70.8333 (70.4000)  Acc@5: 91.6667 (92.4000)  Acc@task: 75.0000 (72.6000)  time: 0.1609  data: 0.0002  max mem: 1380\n",
            "Test: [Task 7] Total time: 0:00:07 (0.1796 s / it)\n",
            "* Acc@task 72.600 Acc@1 70.400 Acc@5 92.400 loss 1.131\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:17  Loss: 1.0252 (1.0252)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  Acc@task: 79.1667 (79.1667)  time: 0.4243  data: 0.2607  max mem: 1380\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:06  Loss: 1.0328 (1.1581)  Acc@1: 75.0000 (70.8333)  Acc@5: 91.6667 (91.6667)  Acc@task: 70.8333 (71.2121)  time: 0.1894  data: 0.0271  max mem: 1380\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:03  Loss: 0.9876 (1.1109)  Acc@1: 75.0000 (71.0317)  Acc@5: 95.8333 (93.2540)  Acc@task: 70.8333 (71.4286)  time: 0.1652  data: 0.0020  max mem: 1380\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:02  Loss: 0.8811 (1.0582)  Acc@1: 75.0000 (72.0430)  Acc@5: 95.8333 (93.9516)  Acc@task: 70.8333 (71.5054)  time: 0.1636  data: 0.0004  max mem: 1380\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 0.9956 (1.0975)  Acc@1: 70.8333 (71.6463)  Acc@5: 95.8333 (93.4959)  Acc@task: 66.6667 (70.2236)  time: 0.1628  data: 0.0007  max mem: 1380\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 1.0565 (1.0970)  Acc@1: 70.8333 (71.7000)  Acc@5: 95.8333 (93.6000)  Acc@task: 66.6667 (70.3000)  time: 0.1602  data: 0.0006  max mem: 1380\n",
            "Test: [Task 8] Total time: 0:00:07 (0.1718 s / it)\n",
            "* Acc@task 70.300 Acc@1 71.700 Acc@5 93.600 loss 1.097\n",
            "Test: [Task 9]  [ 0/42]  eta: 0:00:31  Loss: 0.8443 (0.8443)  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.7511  data: 0.5779  max mem: 1380\n",
            "Test: [Task 9]  [10/42]  eta: 0:00:06  Loss: 0.9256 (0.9424)  Acc@1: 75.0000 (73.4849)  Acc@5: 95.8333 (96.2121)  Acc@task: 70.8333 (76.1364)  time: 0.2165  data: 0.0530  max mem: 1380\n",
            "Test: [Task 9]  [20/42]  eta: 0:00:04  Loss: 0.8135 (0.8482)  Acc@1: 75.0000 (76.3889)  Acc@5: 95.8333 (96.0317)  Acc@task: 75.0000 (76.3889)  time: 0.1643  data: 0.0004  max mem: 1380\n",
            "Test: [Task 9]  [30/42]  eta: 0:00:02  Loss: 0.9058 (0.9582)  Acc@1: 75.0000 (74.1936)  Acc@5: 95.8333 (94.4892)  Acc@task: 75.0000 (75.9409)  time: 0.1646  data: 0.0003  max mem: 1380\n",
            "Test: [Task 9]  [40/42]  eta: 0:00:00  Loss: 0.8749 (0.9017)  Acc@1: 75.0000 (75.4065)  Acc@5: 95.8333 (95.2236)  Acc@task: 79.1667 (76.7276)  time: 0.1631  data: 0.0002  max mem: 1380\n",
            "Test: [Task 9]  [41/42]  eta: 0:00:00  Loss: 0.8749 (0.8885)  Acc@1: 75.0000 (75.7000)  Acc@5: 95.8333 (95.3000)  Acc@task: 79.1667 (77.0000)  time: 0.1605  data: 0.0002  max mem: 1380\n",
            "Test: [Task 9] Total time: 0:00:07 (0.1784 s / it)\n",
            "* Acc@task 77.000 Acc@1 75.700 Acc@5 95.300 loss 0.888\n",
            "Test: [Task 10]  [ 0/42]  eta: 0:00:18  Loss: 9.5899 (9.5899)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  Acc@task: 62.5000 (62.5000)  time: 0.4516  data: 0.2900  max mem: 1380\n",
            "Test: [Task 10]  [10/42]  eta: 0:00:06  Loss: 9.4368 (9.3432)  Acc@1: 0.0000 (0.0000)  Acc@5: 4.1667 (3.7879)  Acc@task: 58.3333 (56.8182)  time: 0.1886  data: 0.0268  max mem: 1380\n",
            "Test: [Task 10]  [20/42]  eta: 0:00:03  Loss: 9.4245 (9.3443)  Acc@1: 0.0000 (0.0000)  Acc@5: 4.1667 (3.7698)  Acc@task: 58.3333 (58.7302)  time: 0.1631  data: 0.0007  max mem: 1380\n",
            "Test: [Task 10]  [30/42]  eta: 0:00:02  Loss: 9.4845 (9.4495)  Acc@1: 0.0000 (0.0000)  Acc@5: 4.1667 (3.6290)  Acc@task: 58.3333 (58.6022)  time: 0.1632  data: 0.0009  max mem: 1380\n",
            "Test: [Task 10]  [40/42]  eta: 0:00:00  Loss: 9.6460 (9.4523)  Acc@1: 0.0000 (0.0000)  Acc@5: 4.1667 (3.8618)  Acc@task: 54.1667 (57.4187)  time: 0.1619  data: 0.0008  max mem: 1380\n",
            "Test: [Task 10]  [41/42]  eta: 0:00:00  Loss: 9.6460 (9.4641)  Acc@1: 0.0000 (0.0000)  Acc@5: 4.1667 (3.8000)  Acc@task: 54.1667 (57.7000)  time: 0.1592  data: 0.0008  max mem: 1380\n",
            "Test: [Task 10] Total time: 0:00:07 (0.1703 s / it)\n",
            "* Acc@task 57.700 Acc@1 0.000 Acc@5 3.800 loss 9.464\n",
            "[Average accuracy till task10]\tAcc@task: 74.2500\tAcc@1: 67.2500\tAcc@5: 85.3300\tLoss: 1.8465\tForgetting: 3.7000\tBackward: 72.8778\n",
            "torch.Size([118320, 384])\n",
            "Averaged stats: Lr: 0.005000  Loss: 0.1195  Acc@1: 96.6667 (93.4259)  Acc@5: 100.0000 (99.0556)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:20  Loss: 0.9534 (0.9534)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (83.3333)  time: 0.4982  data: 0.3436  max mem: 1380\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.9401 (0.9642)  Acc@1: 75.0000 (75.7576)  Acc@5: 95.8333 (96.9697)  Acc@task: 79.1667 (78.4091)  time: 0.1928  data: 0.0319  max mem: 1380\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:03  Loss: 0.8749 (1.0307)  Acc@1: 75.0000 (76.7857)  Acc@5: 95.8333 (95.4365)  Acc@task: 79.1667 (78.9683)  time: 0.1634  data: 0.0005  max mem: 1380\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.7324 (0.9547)  Acc@1: 75.0000 (77.4194)  Acc@5: 95.8333 (95.6989)  Acc@task: 83.3333 (79.7043)  time: 0.1632  data: 0.0003  max mem: 1380\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6772 (0.9076)  Acc@1: 79.1667 (78.0488)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (80.4878)  time: 0.1621  data: 0.0003  max mem: 1380\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6208 (0.8968)  Acc@1: 83.3333 (78.3000)  Acc@5: 95.8333 (95.9000)  Acc@task: 87.5000 (80.7000)  time: 0.1597  data: 0.0003  max mem: 1380\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1726 s / it)\n",
            "* Acc@task 80.700 Acc@1 78.300 Acc@5 95.900 loss 0.897\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:28  Loss: 1.5278 (1.5278)  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  Acc@task: 66.6667 (66.6667)  time: 0.6846  data: 0.4897  max mem: 1380\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 1.1471 (1.1245)  Acc@1: 79.1667 (75.7576)  Acc@5: 95.8333 (95.8333)  Acc@task: 75.0000 (75.3788)  time: 0.2112  data: 0.0466  max mem: 1380\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 1.1462 (1.2940)  Acc@1: 75.0000 (71.6270)  Acc@5: 95.8333 (94.0476)  Acc@task: 75.0000 (73.0159)  time: 0.1635  data: 0.0018  max mem: 1380\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 1.0714 (1.1983)  Acc@1: 70.8333 (72.8495)  Acc@5: 91.6667 (94.0860)  Acc@task: 75.0000 (74.5968)  time: 0.1625  data: 0.0008  max mem: 1380\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.8837 (1.1303)  Acc@1: 75.0000 (73.1707)  Acc@5: 95.8333 (94.4106)  Acc@task: 75.0000 (75.5081)  time: 0.1618  data: 0.0003  max mem: 1380\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.8837 (1.1266)  Acc@1: 75.0000 (73.2000)  Acc@5: 95.8333 (94.3000)  Acc@task: 75.0000 (75.5000)  time: 0.1591  data: 0.0003  max mem: 1380\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1763 s / it)\n",
            "* Acc@task 75.500 Acc@1 73.200 Acc@5 94.300 loss 1.127\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:26  Loss: 0.7129 (0.7129)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 87.5000 (87.5000)  time: 0.6230  data: 0.4714  max mem: 1380\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 0.7129 (0.8328)  Acc@1: 79.1667 (81.0606)  Acc@5: 95.8333 (96.2121)  Acc@task: 83.3333 (81.8182)  time: 0.2047  data: 0.0434  max mem: 1380\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.7464 (0.8191)  Acc@1: 79.1667 (80.9524)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (81.7460)  time: 0.1638  data: 0.0005  max mem: 1380\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.7194 (0.7492)  Acc@1: 83.3333 (81.8548)  Acc@5: 95.8333 (96.3710)  Acc@task: 79.1667 (80.7796)  time: 0.1641  data: 0.0003  max mem: 1380\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.5097 (0.7562)  Acc@1: 83.3333 (81.9106)  Acc@5: 95.8333 (96.2398)  Acc@task: 83.3333 (81.4024)  time: 0.1626  data: 0.0003  max mem: 1380\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.4939 (0.7479)  Acc@1: 87.5000 (82.0000)  Acc@5: 95.8333 (96.3000)  Acc@task: 83.3333 (81.3000)  time: 0.1601  data: 0.0003  max mem: 1380\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1765 s / it)\n",
            "* Acc@task 81.300 Acc@1 82.000 Acc@5 96.300 loss 0.748\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:32  Loss: 1.3381 (1.3381)  Acc@1: 58.3333 (58.3333)  Acc@5: 83.3333 (83.3333)  Acc@task: 62.5000 (62.5000)  time: 0.7786  data: 0.5595  max mem: 1380\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:07  Loss: 1.1148 (1.1519)  Acc@1: 75.0000 (72.7273)  Acc@5: 91.6667 (90.5303)  Acc@task: 70.8333 (70.8333)  time: 0.2214  data: 0.0525  max mem: 1380\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 1.1547 (1.1907)  Acc@1: 75.0000 (71.8254)  Acc@5: 91.6667 (91.0714)  Acc@task: 75.0000 (73.4127)  time: 0.1644  data: 0.0011  max mem: 1380\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 1.1904 (1.1437)  Acc@1: 70.8333 (72.8495)  Acc@5: 91.6667 (91.8011)  Acc@task: 75.0000 (74.1936)  time: 0.1632  data: 0.0004  max mem: 1380\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 1.0264 (1.2059)  Acc@1: 75.0000 (72.5610)  Acc@5: 91.6667 (91.2602)  Acc@task: 75.0000 (74.5935)  time: 0.1632  data: 0.0003  max mem: 1380\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 1.0264 (1.1887)  Acc@1: 75.0000 (72.9000)  Acc@5: 91.6667 (91.3000)  Acc@task: 75.0000 (74.8000)  time: 0.1604  data: 0.0003  max mem: 1380\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1794 s / it)\n",
            "* Acc@task 74.800 Acc@1 72.900 Acc@5 91.300 loss 1.189\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:17  Loss: 0.5046 (0.5046)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 79.1667 (79.1667)  time: 0.4102  data: 0.2538  max mem: 1380\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 0.7489 (0.7875)  Acc@1: 79.1667 (76.8939)  Acc@5: 95.8333 (96.2121)  Acc@task: 83.3333 (80.3030)  time: 0.1884  data: 0.0271  max mem: 1380\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:03  Loss: 0.9557 (0.9055)  Acc@1: 75.0000 (75.3968)  Acc@5: 95.8333 (95.0397)  Acc@task: 79.1667 (78.5714)  time: 0.1653  data: 0.0024  max mem: 1380\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 1.0418 (0.9300)  Acc@1: 75.0000 (75.8065)  Acc@5: 91.6667 (94.2204)  Acc@task: 79.1667 (78.6290)  time: 0.1639  data: 0.0003  max mem: 1380\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 1.0122 (1.0076)  Acc@1: 70.8333 (74.1870)  Acc@5: 91.6667 (93.9024)  Acc@task: 75.0000 (78.0488)  time: 0.1633  data: 0.0004  max mem: 1380\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 1.0721 (1.0477)  Acc@1: 70.8333 (73.6000)  Acc@5: 91.6667 (93.6000)  Acc@task: 75.0000 (77.7000)  time: 0.1608  data: 0.0003  max mem: 1380\n",
            "Test: [Task 5] Total time: 0:00:07 (0.1723 s / it)\n",
            "* Acc@task 77.700 Acc@1 73.600 Acc@5 93.600 loss 1.048\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:32  Loss: 0.5975 (0.5975)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  Acc@task: 91.6667 (91.6667)  time: 0.7840  data: 0.5883  max mem: 1380\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:07  Loss: 1.1643 (1.0965)  Acc@1: 75.0000 (73.4849)  Acc@5: 95.8333 (93.9394)  Acc@task: 79.1667 (79.1667)  time: 0.2207  data: 0.0544  max mem: 1380\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:04  Loss: 1.0346 (1.0910)  Acc@1: 70.8333 (72.4206)  Acc@5: 95.8333 (94.2460)  Acc@task: 79.1667 (78.7698)  time: 0.1647  data: 0.0007  max mem: 1380\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 1.0602 (1.1347)  Acc@1: 70.8333 (71.9086)  Acc@5: 95.8333 (94.2204)  Acc@task: 70.8333 (75.6720)  time: 0.1638  data: 0.0004  max mem: 1380\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.1326 (1.1484)  Acc@1: 70.8333 (72.3577)  Acc@5: 95.8333 (94.2073)  Acc@task: 70.8333 (74.7968)  time: 0.1628  data: 0.0003  max mem: 1380\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.2104 (1.1499)  Acc@1: 70.8333 (72.3000)  Acc@5: 95.8333 (94.2000)  Acc@task: 70.8333 (74.9000)  time: 0.1602  data: 0.0002  max mem: 1380\n",
            "Test: [Task 6] Total time: 0:00:07 (0.1793 s / it)\n",
            "* Acc@task 74.900 Acc@1 72.300 Acc@5 94.200 loss 1.150\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:23  Loss: 1.0865 (1.0865)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  Acc@task: 79.1667 (79.1667)  time: 0.5634  data: 0.3901  max mem: 1380\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:06  Loss: 1.0716 (0.9010)  Acc@1: 75.0000 (75.7576)  Acc@5: 95.8333 (93.9394)  Acc@task: 75.0000 (71.5909)  time: 0.1991  data: 0.0358  max mem: 1380\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:04  Loss: 1.0716 (1.0574)  Acc@1: 75.0000 (72.2222)  Acc@5: 91.6667 (92.6587)  Acc@task: 75.0000 (72.4206)  time: 0.1642  data: 0.0005  max mem: 1380\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:02  Loss: 1.2673 (1.1401)  Acc@1: 66.6667 (70.8333)  Acc@5: 91.6667 (92.0699)  Acc@task: 75.0000 (72.3118)  time: 0.1646  data: 0.0007  max mem: 1380\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 1.2554 (1.1459)  Acc@1: 70.8333 (70.9350)  Acc@5: 91.6667 (91.7683)  Acc@task: 75.0000 (72.4594)  time: 0.1631  data: 0.0004  max mem: 1380\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 1.1101 (1.1277)  Acc@1: 70.8333 (71.3000)  Acc@5: 91.6667 (91.9000)  Acc@task: 75.0000 (72.6000)  time: 0.1604  data: 0.0004  max mem: 1380\n",
            "Test: [Task 7] Total time: 0:00:07 (0.1752 s / it)\n",
            "* Acc@task 72.600 Acc@1 71.300 Acc@5 91.900 loss 1.128\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:27  Loss: 1.2563 (1.2563)  Acc@1: 75.0000 (75.0000)  Acc@5: 87.5000 (87.5000)  Acc@task: 79.1667 (79.1667)  time: 0.6628  data: 0.5071  max mem: 1380\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:06  Loss: 0.9934 (1.3110)  Acc@1: 70.8333 (69.6970)  Acc@5: 91.6667 (90.9091)  Acc@task: 70.8333 (71.2121)  time: 0.2078  data: 0.0465  max mem: 1380\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:04  Loss: 0.9184 (1.1876)  Acc@1: 70.8333 (71.2302)  Acc@5: 91.6667 (92.4603)  Acc@task: 70.8333 (71.4286)  time: 0.1640  data: 0.0005  max mem: 1380\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:02  Loss: 1.0552 (1.1459)  Acc@1: 75.0000 (71.2366)  Acc@5: 95.8333 (93.2796)  Acc@task: 70.8333 (71.5054)  time: 0.1649  data: 0.0005  max mem: 1380\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 1.2021 (1.1739)  Acc@1: 70.8333 (70.6301)  Acc@5: 91.6667 (92.6829)  Acc@task: 66.6667 (70.2236)  time: 0.1636  data: 0.0003  max mem: 1380\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 1.2021 (1.1695)  Acc@1: 70.8333 (70.6000)  Acc@5: 91.6667 (92.8000)  Acc@task: 66.6667 (70.3000)  time: 0.1611  data: 0.0002  max mem: 1380\n",
            "Test: [Task 8] Total time: 0:00:07 (0.1764 s / it)\n",
            "* Acc@task 70.300 Acc@1 70.600 Acc@5 92.800 loss 1.169\n",
            "Test: [Task 9]  [ 0/42]  eta: 0:00:24  Loss: 0.8349 (0.8349)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.5888  data: 0.4198  max mem: 1380\n",
            "Test: [Task 9]  [10/42]  eta: 0:00:06  Loss: 0.9443 (0.9873)  Acc@1: 79.1667 (76.8939)  Acc@5: 95.8333 (95.8333)  Acc@task: 70.8333 (76.1364)  time: 0.2018  data: 0.0384  max mem: 1380\n",
            "Test: [Task 9]  [20/42]  eta: 0:00:04  Loss: 0.8811 (0.8808)  Acc@1: 79.1667 (78.1746)  Acc@5: 95.8333 (95.6349)  Acc@task: 75.0000 (76.3889)  time: 0.1645  data: 0.0004  max mem: 1380\n",
            "Test: [Task 9]  [30/42]  eta: 0:00:02  Loss: 0.9560 (0.9924)  Acc@1: 75.0000 (75.4032)  Acc@5: 91.6667 (93.6828)  Acc@task: 75.0000 (75.9409)  time: 0.1643  data: 0.0009  max mem: 1380\n",
            "Test: [Task 9]  [40/42]  eta: 0:00:00  Loss: 0.8691 (0.9249)  Acc@1: 75.0000 (76.7276)  Acc@5: 91.6667 (94.4106)  Acc@task: 79.1667 (76.7276)  time: 0.1629  data: 0.0008  max mem: 1380\n",
            "Test: [Task 9]  [41/42]  eta: 0:00:00  Loss: 0.8691 (0.9129)  Acc@1: 75.0000 (76.9000)  Acc@5: 91.6667 (94.5000)  Acc@task: 79.1667 (77.0000)  time: 0.1604  data: 0.0008  max mem: 1380\n",
            "Test: [Task 9] Total time: 0:00:07 (0.1747 s / it)\n",
            "* Acc@task 77.000 Acc@1 76.900 Acc@5 94.500 loss 0.913\n",
            "Test: [Task 10]  [ 0/42]  eta: 0:00:17  Loss: 0.9798 (0.9798)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  Acc@task: 62.5000 (62.5000)  time: 0.4284  data: 0.2517  max mem: 1380\n",
            "Test: [Task 10]  [10/42]  eta: 0:00:06  Loss: 1.4944 (1.4145)  Acc@1: 62.5000 (64.3939)  Acc@5: 95.8333 (94.6970)  Acc@task: 58.3333 (56.8182)  time: 0.1917  data: 0.0284  max mem: 1380\n",
            "Test: [Task 10]  [20/42]  eta: 0:00:03  Loss: 1.4944 (1.4163)  Acc@1: 62.5000 (65.6746)  Acc@5: 91.6667 (94.0476)  Acc@task: 58.3333 (58.7302)  time: 0.1668  data: 0.0032  max mem: 1380\n",
            "Test: [Task 10]  [30/42]  eta: 0:00:02  Loss: 1.4768 (1.4759)  Acc@1: 58.3333 (63.7097)  Acc@5: 91.6667 (93.5484)  Acc@task: 58.3333 (58.6022)  time: 0.1648  data: 0.0003  max mem: 1380\n",
            "Test: [Task 10]  [40/42]  eta: 0:00:00  Loss: 1.4143 (1.4667)  Acc@1: 62.5000 (64.1260)  Acc@5: 91.6667 (93.2927)  Acc@task: 54.1667 (57.4187)  time: 0.1636  data: 0.0002  max mem: 1380\n",
            "Test: [Task 10]  [41/42]  eta: 0:00:00  Loss: 1.4459 (1.4666)  Acc@1: 62.5000 (64.2000)  Acc@5: 91.6667 (93.4000)  Acc@task: 54.1667 (57.7000)  time: 0.1611  data: 0.0002  max mem: 1380\n",
            "Test: [Task 10] Total time: 0:00:07 (0.1723 s / it)\n",
            "* Acc@task 57.700 Acc@1 64.200 Acc@5 93.400 loss 1.467\n",
            "[Average accuracy till task10]\tAcc@task: 74.2500\tAcc@1: 73.5300\tAcc@5: 93.8200\tLoss: 1.0834\tForgetting: 3.4556\tBackward: 3.6556\n",
            "Total training time: 0:17:29\n",
            "[rank0]:[W1006 11:14:30.637824411 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
          ]
        }
      ],
      "source": [
        "!torchrun --nproc_per_node=8 main.py cifar100_hideprompt_5e --original_model vit_small_patch16_224.dino --model vit_small_patch16_224.dino --batch-size 24 --epochs 10 --crct_epochs 10 --seed 20 --ca_lr 0.005 --prompt_momentum 0.1 --reg 0.1 --length 5 --larger_prompt_lr --data-path ./datasets/ --trained_original_model ./output/cifar100_full_dino_5epoch_10pct --output_dir ./output/cifar100_full_dino_5epoch_final_10pct --pct 0.10\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}