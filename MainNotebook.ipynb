{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1c65321c",
      "metadata": {
        "id": "1c65321c",
        "outputId": "9f0d9bc7-0e57-43e1-f57a-97189c2f0a3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32512"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import os\n",
        "os.system('cmd command')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4842b918",
      "metadata": {
        "id": "4842b918",
        "outputId": "5324f7e0-a765-44d3-9b10-fa1692bc47eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "31ecb10e",
      "metadata": {
        "id": "31ecb10e",
        "outputId": "91f01d33-228e-42a5-c277-2f40a917f856",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/HiDePrompt\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/HiDePrompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd565e2a",
      "metadata": {
        "id": "cd565e2a"
      },
      "outputs": [],
      "source": [
        "!pip install -q condacolab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a2473b5",
      "metadata": {
        "id": "6a2473b5",
        "outputId": "e435931a-a6cb-4f7a-f09c-fd79432ddd86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬ Downloading https://github.com/jaimergp/miniforge/releases/download/24.11.2-1_colab/Miniforge3-colab-24.11.2-1_colab-Linux-x86_64.sh...\n",
            "ğŸ“¦ Installing...\n",
            "ğŸ“Œ Adjusting configuration...\n",
            "ğŸ©¹ Patching environment...\n",
            "â² Done in 0:00:11\n",
            "ğŸ” Restarting kernel...\n"
          ]
        }
      ],
      "source": [
        "import condacolab\n",
        "condacolab.install()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/HiDePrompt/HiDePrompt"
      ],
      "metadata": {
        "id": "4aBTrmkLHGUx",
        "outputId": "e68209bf-eb84-4df7-808d-f98b45f0ab7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "4aBTrmkLHGUx",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59ddd81a",
      "metadata": {
        "id": "59ddd81a",
        "outputId": "2d958915-469d-49c8-90be-f3d2d67d4cfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm (from -r requirements.txt (line 1))\n",
            "  Downloading timm-1.0.20-py3-none-any.whl.metadata (61 kB)\n",
            "Collecting pillow (from -r requirements.txt (line 2))\n",
            "  Downloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting matplotlib (from -r requirements.txt (line 3))\n",
            "  Downloading matplotlib-3.10.6-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
            "Collecting torchprofile (from -r requirements.txt (line 4))\n",
            "  Downloading torchprofile-0.0.4-py3-none-any.whl.metadata (303 bytes)\n",
            "Collecting torch (from -r requirements.txt (line 5))\n",
            "  Downloading torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
            "Collecting torchvision (from -r requirements.txt (line 6))\n",
            "  Downloading torchvision-0.23.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (2.3.0)\n",
            "Collecting scipy (from -r requirements.txt (line 8))\n",
            "  Downloading scipy-1.16.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
            "Collecting scikit-learn (from -r requirements.txt (line 9))\n",
            "  Downloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
            "Collecting numpy (from -r requirements.txt (line 10))\n",
            "  Downloading numpy-2.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "Collecting pyyaml (from timm->-r requirements.txt (line 1))\n",
            "  Downloading pyyaml-6.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)\n",
            "Collecting huggingface_hub (from timm->-r requirements.txt (line 1))\n",
            "  Downloading huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting safetensors (from timm->-r requirements.txt (line 1))\n",
            "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib->-r requirements.txt (line 3))\n",
            "  Downloading contourpy-1.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 3))\n",
            "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib->-r requirements.txt (line 3))\n",
            "  Downloading fonttools-4.60.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (111 kB)\n",
            "Collecting kiwisolver>=1.3.1 (from matplotlib->-r requirements.txt (line 3))\n",
            "  Downloading kiwisolver-1.4.9-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 3)) (24.2)\n",
            "Collecting pyparsing>=2.3.1 (from matplotlib->-r requirements.txt (line 3))\n",
            "  Downloading pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting python-dateutil>=2.7 (from matplotlib->-r requirements.txt (line 3))\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting filelock (from torch->-r requirements.txt (line 5))\n",
            "  Downloading filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting typing-extensions>=4.10.0 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting sympy>=1.13.3 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting networkx (from torch->-r requirements.txt (line 5))\n",
            "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting jinja2 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting fsspec (from torch->-r requirements.txt (line 5))\n",
            "  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.9.90 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Collecting nvidia-nccl-cu12==2.27.3 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.8.90 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.4.0 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/site-packages (from triton==3.4.0->torch->-r requirements.txt (line 5)) (65.6.3)\n",
            "Collecting joblib>=1.2.0 (from scikit-learn->-r requirements.txt (line 9))\n",
            "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn->-r requirements.txt (line 9))\n",
            "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting six>=1.5 (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 3))\n",
            "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch->-r requirements.txt (line 5))\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/site-packages (from huggingface_hub->timm->-r requirements.txt (line 1)) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/site-packages (from huggingface_hub->timm->-r requirements.txt (line 1)) (4.67.1)\n",
            "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface_hub->timm->-r requirements.txt (line 1))\n",
            "  Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch->-r requirements.txt (line 5))\n",
            "  Downloading markupsafe-3.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 1)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 1)) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 1)) (2024.12.14)\n",
            "Downloading timm-1.0.20-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m141.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.10.6-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m179.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchprofile-0.0.4-py3-none-any.whl (7.7 kB)\n",
            "Downloading torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl (888.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m888.1/888.1 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m116.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "Downloading triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m155.5/155.5 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.23.0-cp311-cp311-manylinux_2_28_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m101.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.16.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m35.9/35.9 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m107.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m112.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading contourpy-1.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (355 kB)\n",
            "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading fonttools-4.60.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (5.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m96.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
            "Downloading kiwisolver-1.4.9-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
            "Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
            "Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\n",
            "Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
            "Downloading huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyyaml-6.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (806 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m806.6/806.6 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
            "Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markupsafe-3.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\n",
            "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: nvidia-cusparselt-cu12, mpmath, typing-extensions, triton, threadpoolctl, sympy, six, safetensors, pyyaml, pyparsing, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, kiwisolver, joblib, hf-xet, fsspec, fonttools, filelock, cycler, scipy, python-dateutil, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, jinja2, huggingface_hub, contourpy, scikit-learn, nvidia-cusolver-cu12, matplotlib, torch, torchvision, torchprofile, timm\n",
            "Successfully installed MarkupSafe-3.0.3 contourpy-1.3.3 cycler-0.12.1 filelock-3.19.1 fonttools-4.60.0 fsspec-2025.9.0 hf-xet-1.1.10 huggingface_hub-0.35.3 jinja2-3.1.6 joblib-1.5.2 kiwisolver-1.4.9 matplotlib-3.10.6 mpmath-1.3.0 networkx-3.5 numpy-2.3.3 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 pillow-11.3.0 pyparsing-3.2.5 python-dateutil-2.9.0.post0 pyyaml-6.0.3 safetensors-0.6.2 scikit-learn-1.7.2 scipy-1.16.2 six-1.17.0 sympy-1.14.0 threadpoolctl-3.6.0 timm-1.0.20 torch-2.8.0 torchprofile-0.0.4 torchvision-0.23.0 triton-3.4.0 typing-extensions-4.15.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "cycler",
                  "dateutil",
                  "kiwisolver",
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy",
                  "pyparsing",
                  "six"
                ]
              },
              "id": "5e9c917c5f81486e8bc6a6c737f1745b"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/HiDePrompt/HiDePrompt/"
      ],
      "metadata": {
        "id": "nTIVenS1Ih7Q",
        "outputId": "b594f79e-187a-4424-e403-475178ab4802",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "nTIVenS1Ih7Q",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "383f6206",
      "metadata": {
        "id": "383f6206",
        "outputId": "a606a7cc-f999-4b7a-863e-7a5c614c3ba4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(subparser_name='cifar100_hideprompt_5e', pct=0.5, batch_size=24, epochs=1, original_model='vit_small_patch16_224.dino', model='vit_small_patch16_224.dino', input_size=224, pretrained=True, drop=0.0, drop_path=0.0, opt='adam', opt_eps=1e-08, opt_betas=(0.9, 0.999), clip_grad=1.0, momentum=0.9, weight_decay=0.0, reinit_optimizer=True, sched='constant', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=0, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, unscale_lr=True, color_jitter=None, aa=None, smoothing=0.1, train_interpolation='bicubic', reprob=0.0, remode='pixel', recount=1, data_path='./datasets/', dataset='Split-CIFAR100', shuffle=False, output_dir='./output/cifar100_full_dino_1epoch_50pct', device='cuda', seed=20, eval=False, num_workers=4, pin_mem=True, world_size=1, dist_url='env://', num_tasks=10, train_mask=True, task_inc=False, use_g_prompt=False, g_prompt_length=5, g_prompt_layer_idx=[], use_prefix_tune_for_g_prompt=False, use_e_prompt=True, e_prompt_layer_idx=[0, 1, 2, 3, 4], use_prefix_tune_for_e_prompt=True, larger_prompt_lr=False, prompt_pool=True, size=10, length=20, top_k=1, initializer='uniform', prompt_key=False, prompt_key_init='uniform', use_prompt_mask=True, mask_first_epoch=False, shared_prompt_pool=True, shared_prompt_key=False, batchwise_prompt=False, embedding_key='cls', predefined_key='', pull_constraint=True, pull_constraint_coeff=1.0, same_key_value=False, global_pool='token', head_type='token', freeze=['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed'], crct_epochs=30, train_inference_task_only=True, original_model_mlp_structure=[2], ca_lr=0.005, milestones=[10], trained_original_model='', prompt_momentum=0.01, reg=0.01, not_train_ca=False, ca_epochs=30, ca_storage_efficient_method='multi-centroid', n_centroids=10, print_freq=10, config='cifar100_hideprompt_5e')\n",
            "| distributed init (rank 0): env://\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "[rank0]:[W1003 14:12:27.155542254 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/helpers.py:7: FutureWarning: Importing from timm.models.helpers is deprecated, please import via timm.models\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:886: UserWarning: Overwriting vit_tiny_patch16_224 in registry with vits.hide_prompt_vision_transformer.vit_tiny_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:895: UserWarning: Overwriting vit_tiny_patch16_384 in registry with vits.hide_prompt_vision_transformer.vit_tiny_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:904: UserWarning: Overwriting vit_small_patch32_224 in registry with vits.hide_prompt_vision_transformer.vit_small_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:913: UserWarning: Overwriting vit_small_patch32_384 in registry with vits.hide_prompt_vision_transformer.vit_small_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:922: UserWarning: Overwriting vit_small_patch16_224 in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:932: UserWarning: Overwriting vit_small_patch16_384 in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:942: UserWarning: Overwriting vit_base_patch32_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:952: UserWarning: Overwriting vit_base_patch32_384 in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:962: UserWarning: Overwriting vit_base_patch16_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:972: UserWarning: Overwriting vit_base_patch16_384 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:982: UserWarning: Overwriting vit_base_patch8_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:992: UserWarning: Overwriting vit_large_patch32_224 in registry with vits.hide_prompt_vision_transformer.vit_large_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1001: UserWarning: Overwriting vit_large_patch32_384 in registry with vits.hide_prompt_vision_transformer.vit_large_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1011: UserWarning: Overwriting vit_large_patch16_224 in registry with vits.hide_prompt_vision_transformer.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1021: UserWarning: Overwriting vit_large_patch16_384 in registry with vits.hide_prompt_vision_transformer.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1031: UserWarning: Overwriting vit_large_patch14_224 in registry with vits.hide_prompt_vision_transformer.vit_large_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1040: UserWarning: Overwriting vit_huge_patch14_224 in registry with vits.hide_prompt_vision_transformer.vit_huge_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1049: UserWarning: Overwriting vit_giant_patch14_224 in registry with vits.hide_prompt_vision_transformer.vit_giant_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1058: UserWarning: Overwriting vit_gigantic_patch14_224 in registry with vits.hide_prompt_vision_transformer.vit_gigantic_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1067: UserWarning: Overwriting vit_tiny_patch16_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_tiny_patch16_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1078: UserWarning: Overwriting vit_small_patch32_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_small_patch32_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1089: UserWarning: Overwriting vit_small_patch16_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1100: UserWarning: Overwriting vit_base_patch32_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1111: UserWarning: Overwriting vit_base_patch16_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1122: UserWarning: Overwriting vit_base_patch8_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_base_patch8_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1133: UserWarning: Overwriting vit_large_patch32_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_large_patch32_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1144: UserWarning: Overwriting vit_large_patch16_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_large_patch16_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1155: UserWarning: Overwriting vit_huge_patch14_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_huge_patch14_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1166: UserWarning: Overwriting vit_base_patch16_224_sam in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_sam. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1175: UserWarning: Overwriting vit_base_patch32_224_sam in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_224_sam. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1184: UserWarning: Overwriting vit_small_patch16_224_dino in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_224_dino. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1193: UserWarning: Overwriting vit_small_patch8_224_dino in registry with vits.hide_prompt_vision_transformer.vit_small_patch8_224_dino. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1211: UserWarning: Overwriting vit_base_patch8_224_dino in registry with vits.hide_prompt_vision_transformer.vit_base_patch8_224_dino. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1220: UserWarning: Overwriting vit_base_patch16_224_miil_in21k in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_miil_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1230: UserWarning: Overwriting vit_base_patch16_224_miil in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_miil. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1242: UserWarning: Overwriting vit_base_patch32_plus_256 in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_plus_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1251: UserWarning: Overwriting vit_base_patch16_plus_240 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_plus_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1260: UserWarning: Overwriting vit_base_patch16_rpn_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_rpn_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1271: UserWarning: Overwriting vit_small_patch16_36x1_224 in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_36x1_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1282: UserWarning: Overwriting vit_small_patch16_18x2_224 in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1294: UserWarning: Overwriting vit_base_patch16_18x2_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1331: UserWarning: Overwriting vit_base_patch16_224_dino in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_dino. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "Original train size:  50000\n",
            "Sampled train size:  25000\n",
            "Original train size:  50000\n",
            "Sampled train size:  25000\n",
            "100\n",
            "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, 13, 14, 15, 16, 17, 18, 19], [20, 21, 22, 23, 24, 25, 26, 27, 28, 29], [30, 31, 32, 33, 34, 35, 36, 37, 38, 39], [40, 41, 42, 43, 44, 45, 46, 47, 48, 49], [50, 51, 52, 53, 54, 55, 56, 57, 58, 59], [60, 61, 62, 63, 64, 65, 66, 67, 68, 69], [70, 71, 72, 73, 74, 75, 76, 77, 78, 79], [80, 81, 82, 83, 84, 85, 86, 87, 88, 89], [90, 91, 92, 93, 94, 95, 96, 97, 98, 99]]\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Creating original model: vit_small_patch16_224.dino\n",
            "[Sequential(\n",
            "  (0): Linear(in_features=384, out_features=768, bias=True)\n",
            "  (1): GELU(approximate='none')\n",
            "  (2): Dropout(p=0.0, inplace=False)\n",
            "), Sequential(\n",
            "  (0): Linear(in_features=768, out_features=384, bias=True)\n",
            "  (1): Dropout(p=0.0, inplace=False)\n",
            ")]\n",
            "model.safetensors: 100% 86.7M/86.7M [00:01<00:00, 49.6MB/s]\n",
            "Namespace(subparser_name='cifar100_hideprompt_5e', pct=0.5, batch_size=24, epochs=1, original_model='vit_small_patch16_224.dino', model='vit_small_patch16_224.dino', input_size=224, pretrained=True, drop=0.0, drop_path=0.0, opt='adam', opt_eps=1e-08, opt_betas=(0.9, 0.999), clip_grad=1.0, momentum=0.9, weight_decay=0.0, reinit_optimizer=True, sched='constant', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=0, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, unscale_lr=True, color_jitter=None, aa=None, smoothing=0.1, train_interpolation='bicubic', reprob=0.0, remode='pixel', recount=1, data_path='./datasets/', dataset='Split-CIFAR100', shuffle=False, output_dir='./output/cifar100_full_dino_1epoch_50pct', device='cuda', seed=20, eval=False, num_workers=4, pin_mem=True, world_size=1, dist_url='env://', num_tasks=10, train_mask=True, task_inc=False, use_g_prompt=False, g_prompt_length=5, g_prompt_layer_idx=[], use_prefix_tune_for_g_prompt=False, use_e_prompt=True, e_prompt_layer_idx=[0, 1, 2, 3, 4], use_prefix_tune_for_e_prompt=True, larger_prompt_lr=False, prompt_pool=True, size=10, length=20, top_k=1, initializer='uniform', prompt_key=False, prompt_key_init='uniform', use_prompt_mask=True, mask_first_epoch=False, shared_prompt_pool=True, shared_prompt_key=False, batchwise_prompt=False, embedding_key='cls', predefined_key='', pull_constraint=True, pull_constraint_coeff=1.0, same_key_value=False, global_pool='token', head_type='token', freeze=['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed'], crct_epochs=30, train_inference_task_only=True, original_model_mlp_structure=[2], ca_lr=0.005, milestones=[10], trained_original_model='', prompt_momentum=0.01, reg=0.01, not_train_ca=False, ca_epochs=30, ca_storage_efficient_method='multi-centroid', n_centroids=10, print_freq=10, config='cifar100_hideprompt_5e', rank=0, gpu=0, distributed=True, dist_backend='nccl', nb_classes=100)\n",
            "number of params: 630244\n",
            "Start training for 1 epochs\n",
            "Train: Epoch[1/1]  [  0/105]  eta: 0:05:37  Lr: 0.000047  Loss: 2.3336  Acc@1: 8.3333 (8.3333)  Acc@5: 58.3333 (58.3333)  time: 3.2142  data: 1.2197  max mem: 196\n",
            "Train: Epoch[1/1]  [ 10/105]  eta: 0:00:34  Lr: 0.000047  Loss: 2.2666  Acc@1: 8.3333 (10.2273)  Acc@5: 58.3333 (58.3333)  time: 0.3612  data: 0.1112  max mem: 217\n",
            "Train: Epoch[1/1]  [ 20/105]  eta: 0:00:19  Lr: 0.000047  Loss: 2.1160  Acc@1: 12.5000 (13.0952)  Acc@5: 62.5000 (61.5079)  time: 0.0801  data: 0.0010  max mem: 217\n",
            "Train: Epoch[1/1]  [ 30/105]  eta: 0:00:13  Lr: 0.000047  Loss: 2.1511  Acc@1: 12.5000 (15.4570)  Acc@5: 66.6667 (63.5753)  time: 0.0897  data: 0.0013  max mem: 217\n",
            "Train: Epoch[1/1]  [ 40/105]  eta: 0:00:10  Lr: 0.000047  Loss: 2.1316  Acc@1: 25.0000 (19.5122)  Acc@5: 70.8333 (66.0569)  time: 0.0951  data: 0.0010  max mem: 217\n",
            "Train: Epoch[1/1]  [ 50/105]  eta: 0:00:08  Lr: 0.000047  Loss: 2.0745  Acc@1: 33.3333 (22.7941)  Acc@5: 79.1667 (68.9543)  time: 0.0962  data: 0.0023  max mem: 217\n",
            "Train: Epoch[1/1]  [ 60/105]  eta: 0:00:06  Lr: 0.000047  Loss: 1.9161  Acc@1: 33.3333 (25.8880)  Acc@5: 83.3333 (71.7213)  time: 0.0972  data: 0.0029  max mem: 217\n",
            "Train: Epoch[1/1]  [ 70/105]  eta: 0:00:04  Lr: 0.000047  Loss: 1.8941  Acc@1: 45.8333 (29.5775)  Acc@5: 87.5000 (73.8850)  time: 0.0954  data: 0.0040  max mem: 217\n",
            "Train: Epoch[1/1]  [ 80/105]  eta: 0:00:03  Lr: 0.000047  Loss: 1.8156  Acc@1: 50.0000 (32.0473)  Acc@5: 87.5000 (75.5144)  time: 0.0867  data: 0.0039  max mem: 217\n",
            "Train: Epoch[1/1]  [ 90/105]  eta: 0:00:01  Lr: 0.000047  Loss: 1.6784  Acc@1: 50.0000 (34.7527)  Acc@5: 87.5000 (77.1520)  time: 0.0789  data: 0.0014  max mem: 217\n",
            "Train: Epoch[1/1]  [100/105]  eta: 0:00:00  Lr: 0.000047  Loss: 1.6553  Acc@1: 62.5000 (37.7475)  Acc@5: 91.6667 (78.3828)  time: 0.0779  data: 0.0006  max mem: 217\n",
            "Train: Epoch[1/1]  [104/105]  eta: 0:00:00  Lr: 0.000047  Loss: 1.5051  Acc@1: 66.6667 (38.8489)  Acc@5: 91.6667 (78.8169)  time: 0.0781  data: 0.0005  max mem: 217\n",
            "Train: Epoch[1/1] Total time: 0:00:12 (0.1177 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.5051  Acc@1: 66.6667 (38.8489)  Acc@5: 91.6667 (78.8169)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:12  Loss: 3.7163 (3.7163)  Acc@1: 45.8333 (45.8333)  Acc@5: 70.8333 (70.8333)  time: 0.3016  data: 0.2298  max mem: 217\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 3.7438 (3.7405)  Acc@1: 41.6667 (44.3182)  Acc@5: 62.5000 (65.1515)  time: 0.0982  data: 0.0241  max mem: 218\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:01  Loss: 3.7334 (3.7436)  Acc@1: 41.6667 (44.6429)  Acc@5: 62.5000 (65.6746)  time: 0.0772  data: 0.0022  max mem: 218\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 3.7216 (3.7234)  Acc@1: 50.0000 (46.5054)  Acc@5: 70.8333 (67.8763)  time: 0.0765  data: 0.0007  max mem: 218\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 3.7242 (3.7197)  Acc@1: 45.8333 (47.0528)  Acc@5: 70.8333 (68.9024)  time: 0.0760  data: 0.0004  max mem: 218\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 3.7228 (3.7176)  Acc@1: 45.8333 (47.0000)  Acc@5: 70.8333 (69.0000)  time: 0.0793  data: 0.0004  max mem: 218\n",
            "Test: [Task 1] Total time: 0:00:03 (0.0851 s / it)\n",
            "* Acc@1 47.000 Acc@5 69.000 loss 3.718\n",
            "[Average accuracy till task1]\tAcc@1: 47.0000\tAcc@5: 69.0000\tLoss: 3.7176\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:18  Loss: 3.7163 (3.7163)  Acc@1: 45.8333 (45.8333)  Acc@5: 70.8333 (70.8333)  time: 0.4373  data: 0.3582  max mem: 277\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 3.7438 (3.7405)  Acc@1: 41.6667 (44.3182)  Acc@5: 62.5000 (65.1515)  time: 0.1083  data: 0.0339  max mem: 277\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 3.7334 (3.7436)  Acc@1: 41.6667 (44.6429)  Acc@5: 62.5000 (65.6746)  time: 0.0764  data: 0.0011  max mem: 277\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 3.7216 (3.7234)  Acc@1: 50.0000 (46.5054)  Acc@5: 70.8333 (67.8763)  time: 0.0772  data: 0.0006  max mem: 277\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 3.7242 (3.7197)  Acc@1: 45.8333 (47.0528)  Acc@5: 70.8333 (68.9024)  time: 0.0765  data: 0.0005  max mem: 277\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 3.7228 (3.7176)  Acc@1: 45.8333 (47.0000)  Acc@5: 70.8333 (69.0000)  time: 0.0754  data: 0.0005  max mem: 277\n",
            "Test: [Task 1] Total time: 0:00:03 (0.0868 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@1 47.000 Acc@5 69.000 loss 3.718\n",
            "[Average accuracy till task1]\tAcc@1: 47.0000\tAcc@5: 69.0000\tLoss: 3.7176\n",
            "Train: Epoch[1/1]  [  0/105]  eta: 0:01:27  Lr: 0.000047  Loss: 2.1699  Acc@1: 29.1667 (29.1667)  Acc@5: 66.6667 (66.6667)  time: 0.8304  data: 0.7052  max mem: 277\n",
            "Train: Epoch[1/1]  [ 10/105]  eta: 0:00:16  Lr: 0.000047  Loss: 2.2801  Acc@1: 12.5000 (13.2576)  Acc@5: 54.1667 (56.8182)  time: 0.1728  data: 0.0749  max mem: 277\n",
            "Train: Epoch[1/1]  [ 20/105]  eta: 0:00:11  Lr: 0.000047  Loss: 2.2648  Acc@1: 12.5000 (13.8889)  Acc@5: 54.1667 (57.3413)  time: 0.1059  data: 0.0102  max mem: 277\n",
            "Train: Epoch[1/1]  [ 30/105]  eta: 0:00:09  Lr: 0.000047  Loss: 2.1591  Acc@1: 16.6667 (16.2634)  Acc@5: 62.5000 (62.0968)  time: 0.1019  data: 0.0048  max mem: 277\n",
            "Train: Epoch[1/1]  [ 40/105]  eta: 0:00:07  Lr: 0.000047  Loss: 2.1208  Acc@1: 29.1667 (19.3089)  Acc@5: 70.8333 (65.2439)  time: 0.0905  data: 0.0011  max mem: 277\n",
            "Train: Epoch[1/1]  [ 50/105]  eta: 0:00:06  Lr: 0.000047  Loss: 2.0428  Acc@1: 33.3333 (23.0392)  Acc@5: 79.1667 (69.0359)  time: 0.0832  data: 0.0015  max mem: 277\n",
            "Train: Epoch[1/1]  [ 60/105]  eta: 0:00:04  Lr: 0.000047  Loss: 2.0816  Acc@1: 37.5000 (25.4098)  Acc@5: 83.3333 (71.2432)  time: 0.0854  data: 0.0017  max mem: 277\n",
            "Train: Epoch[1/1]  [ 70/105]  eta: 0:00:03  Lr: 0.000047  Loss: 2.0529  Acc@1: 33.3333 (27.3474)  Acc@5: 83.3333 (73.0634)  time: 0.0842  data: 0.0017  max mem: 277\n",
            "Train: Epoch[1/1]  [ 80/105]  eta: 0:00:02  Lr: 0.000047  Loss: 1.7874  Acc@1: 45.8333 (30.7099)  Acc@5: 87.5000 (75.3601)  time: 0.0808  data: 0.0012  max mem: 277\n",
            "Train: Epoch[1/1]  [ 90/105]  eta: 0:00:01  Lr: 0.000047  Loss: 1.7239  Acc@1: 54.1667 (33.7912)  Acc@5: 91.6667 (77.2436)  time: 0.0800  data: 0.0008  max mem: 277\n",
            "Train: Epoch[1/1]  [100/105]  eta: 0:00:00  Lr: 0.000047  Loss: 1.6402  Acc@1: 54.1667 (36.3449)  Acc@5: 95.8333 (78.8366)  time: 0.0801  data: 0.0008  max mem: 277\n",
            "Train: Epoch[1/1]  [104/105]  eta: 0:00:00  Lr: 0.000047  Loss: 1.7646  Acc@1: 58.3333 (37.1954)  Acc@5: 95.8333 (79.1450)  time: 0.0792  data: 0.0007  max mem: 277\n",
            "Train: Epoch[1/1] Total time: 0:00:10 (0.0961 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.7646  Acc@1: 58.3333 (37.1954)  Acc@5: 95.8333 (79.1450)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:16  Loss: 3.5905 (3.5905)  Acc@1: 50.0000 (50.0000)  Acc@5: 79.1667 (79.1667)  time: 0.3831  data: 0.3131  max mem: 277\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 3.6259 (3.6203)  Acc@1: 45.8333 (48.8636)  Acc@5: 66.6667 (70.0758)  time: 0.1056  data: 0.0309  max mem: 277\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 3.6158 (3.6236)  Acc@1: 45.8333 (48.6111)  Acc@5: 66.6667 (71.0317)  time: 0.0781  data: 0.0015  max mem: 277\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 3.5945 (3.6021)  Acc@1: 54.1667 (51.3441)  Acc@5: 75.0000 (72.9839)  time: 0.0785  data: 0.0004  max mem: 277\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 3.6074 (3.5979)  Acc@1: 58.3333 (52.5407)  Acc@5: 79.1667 (74.2886)  time: 0.0783  data: 0.0003  max mem: 277\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 3.5841 (3.5954)  Acc@1: 58.3333 (52.4000)  Acc@5: 79.1667 (74.3000)  time: 0.0772  data: 0.0003  max mem: 277\n",
            "Test: [Task 1] Total time: 0:00:03 (0.0872 s / it)\n",
            "* Acc@1 52.400 Acc@5 74.300 loss 3.595\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:27  Loss: 3.7383 (3.7383)  Acc@1: 45.8333 (45.8333)  Acc@5: 75.0000 (75.0000)  time: 0.6535  data: 0.5208  max mem: 277\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 3.8099 (3.7858)  Acc@1: 45.8333 (44.6970)  Acc@5: 70.8333 (71.5909)  time: 0.1376  data: 0.0555  max mem: 277\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 3.7431 (3.7775)  Acc@1: 45.8333 (46.0317)  Acc@5: 70.8333 (72.4206)  time: 0.0904  data: 0.0125  max mem: 277\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 3.7559 (3.7751)  Acc@1: 45.8333 (44.8925)  Acc@5: 75.0000 (72.8495)  time: 0.0893  data: 0.0097  max mem: 277\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 3.7853 (3.7749)  Acc@1: 41.6667 (44.4106)  Acc@5: 66.6667 (72.3577)  time: 0.0816  data: 0.0022  max mem: 277\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 3.7442 (3.7710)  Acc@1: 45.8333 (44.6000)  Acc@5: 66.6667 (72.5000)  time: 0.0807  data: 0.0022  max mem: 277\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1009 s / it)\n",
            "* Acc@1 44.600 Acc@5 72.500 loss 3.771\n",
            "[Average accuracy till task2]\tAcc@1: 48.5000\tAcc@5: 73.4000\tLoss: 3.6832\tForgetting: 0.0000\tBackward: 5.4000\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([4776, 384])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 2.0421  Acc@1: 66.6667 (66.6667)  Acc@5: 95.8333 (93.7500)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000467  Loss: 1.8958  Acc@1: 62.5000 (64.1667)  Acc@5: 95.8333 (95.0000)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000464  Loss: 1.9888  Acc@1: 66.6667 (65.4167)  Acc@5: 91.6667 (91.6667)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 1.9602  Acc@1: 66.6667 (73.3333)  Acc@5: 95.8333 (95.0000)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000448  Loss: 1.7720  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (96.6667)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000437  Loss: 1.3939  Acc@1: 75.0000 (78.7500)  Acc@5: 100.0000 (98.3333)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 1.4909  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000409  Loss: 1.3544  Acc@1: 83.3333 (83.7500)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000391  Loss: 1.4836  Acc@1: 87.5000 (88.7500)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 1.4076  Acc@1: 87.5000 (86.2500)  Acc@5: 100.0000 (98.3333)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000352  Loss: 1.2494  Acc@1: 83.3333 (85.0000)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000330  Loss: 1.2453  Acc@1: 87.5000 (88.7500)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 1.2421  Acc@1: 83.3333 (87.5000)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000283  Loss: 1.1092  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000259  Loss: 1.1661  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 1.0621  Acc@1: 91.6667 (88.7500)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000210  Loss: 0.8990  Acc@1: 95.8333 (94.1667)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000186  Loss: 0.9987  Acc@1: 91.6667 (89.1667)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.8974  Acc@1: 87.5000 (88.7500)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000139  Loss: 1.0193  Acc@1: 83.3333 (86.6667)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000117  Loss: 1.0620  Acc@1: 87.5000 (89.1667)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 1.0863  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000078  Loss: 0.9427  Acc@1: 91.6667 (90.8333)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000060  Loss: 1.0501  Acc@1: 91.6667 (92.0833)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 0.9507  Acc@1: 91.6667 (89.5833)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000031  Loss: 1.0693  Acc@1: 87.5000 (88.7500)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000020  Loss: 1.0344  Acc@1: 91.6667 (90.0000)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.8977  Acc@1: 87.5000 (88.7500)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000005  Loss: 0.9984  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000001  Loss: 0.8710  Acc@1: 91.6667 (88.7500)  Acc@5: 100.0000 (98.3333)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:12  Loss: 1.8008 (1.8008)  Acc@1: 87.5000 (87.5000)  Acc@5: 95.8333 (95.8333)  time: 0.2980  data: 0.2306  max mem: 286\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 2.0036 (1.9518)  Acc@1: 87.5000 (81.4394)  Acc@5: 100.0000 (97.7273)  time: 0.1077  data: 0.0335  max mem: 286\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 2.0036 (1.9540)  Acc@1: 83.3333 (82.9365)  Acc@5: 100.0000 (97.8175)  time: 0.0835  data: 0.0072  max mem: 286\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 1.8905 (1.9169)  Acc@1: 87.5000 (84.6774)  Acc@5: 95.8333 (97.8495)  time: 0.0782  data: 0.0006  max mem: 286\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 1.8630 (1.8973)  Acc@1: 91.6667 (86.1789)  Acc@5: 100.0000 (97.9675)  time: 0.0779  data: 0.0004  max mem: 286\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 1.8531 (1.8918)  Acc@1: 91.6667 (86.3000)  Acc@5: 100.0000 (98.0000)  time: 0.0767  data: 0.0003  max mem: 286\n",
            "Test: [Task 1] Total time: 0:00:03 (0.0871 s / it)\n",
            "* Acc@1 86.300 Acc@5 98.000 loss 1.892\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:14  Loss: 2.1134 (2.1134)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.3570  data: 0.2864  max mem: 286\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:03  Loss: 2.1134 (2.1450)  Acc@1: 83.3333 (81.4394)  Acc@5: 95.8333 (95.4545)  time: 0.1062  data: 0.0289  max mem: 286\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 2.0910 (2.1482)  Acc@1: 83.3333 (82.3413)  Acc@5: 95.8333 (95.4365)  time: 0.0799  data: 0.0025  max mem: 286\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 2.1181 (2.1408)  Acc@1: 83.3333 (82.6613)  Acc@5: 95.8333 (95.4301)  time: 0.0789  data: 0.0011  max mem: 286\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 2.0692 (2.1353)  Acc@1: 83.3333 (82.7236)  Acc@5: 95.8333 (95.7317)  time: 0.0787  data: 0.0003  max mem: 286\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 2.0421 (2.1305)  Acc@1: 83.3333 (82.8000)  Acc@5: 95.8333 (95.8000)  time: 0.0775  data: 0.0003  max mem: 286\n",
            "Test: [Task 2] Total time: 0:00:03 (0.0870 s / it)\n",
            "* Acc@1 82.800 Acc@5 95.800 loss 2.131\n",
            "[Average accuracy till task2]\tAcc@1: 84.5500\tAcc@5: 96.9000\tLoss: 2.0112\tForgetting: 0.0000\tBackward: 39.3000\n",
            "Train: Epoch[1/1]  [  0/105]  eta: 0:00:57  Lr: 0.000047  Loss: 2.4238  Acc@1: 4.1667 (4.1667)  Acc@5: 50.0000 (50.0000)  time: 0.5433  data: 0.4570  max mem: 286\n",
            "Train: Epoch[1/1]  [ 10/105]  eta: 0:00:11  Lr: 0.000047  Loss: 2.3199  Acc@1: 12.5000 (12.5000)  Acc@5: 54.1667 (50.0000)  time: 0.1252  data: 0.0434  max mem: 286\n",
            "Train: Epoch[1/1]  [ 20/105]  eta: 0:00:08  Lr: 0.000047  Loss: 2.2353  Acc@1: 16.6667 (15.2778)  Acc@5: 58.3333 (56.5476)  time: 0.0828  data: 0.0014  max mem: 286\n",
            "Train: Epoch[1/1]  [ 30/105]  eta: 0:00:07  Lr: 0.000047  Loss: 2.1467  Acc@1: 20.8333 (18.8172)  Acc@5: 70.8333 (62.9032)  time: 0.0875  data: 0.0006  max mem: 286\n",
            "Train: Epoch[1/1]  [ 40/105]  eta: 0:00:06  Lr: 0.000047  Loss: 2.0494  Acc@1: 25.0000 (22.2561)  Acc@5: 79.1667 (66.6667)  time: 0.0956  data: 0.0008  max mem: 286\n",
            "Train: Epoch[1/1]  [ 50/105]  eta: 0:00:05  Lr: 0.000047  Loss: 2.0265  Acc@1: 37.5000 (26.4706)  Acc@5: 83.3333 (70.5882)  time: 0.0988  data: 0.0027  max mem: 286\n",
            "Train: Epoch[1/1]  [ 60/105]  eta: 0:00:04  Lr: 0.000047  Loss: 1.7686  Acc@1: 45.8333 (30.2596)  Acc@5: 87.5000 (72.7459)  time: 0.1043  data: 0.0060  max mem: 286\n",
            "Train: Epoch[1/1]  [ 70/105]  eta: 0:00:03  Lr: 0.000047  Loss: 1.6371  Acc@1: 50.0000 (33.6854)  Acc@5: 91.6667 (75.6455)  time: 0.1007  data: 0.0048  max mem: 286\n",
            "Train: Epoch[1/1]  [ 80/105]  eta: 0:00:02  Lr: 0.000047  Loss: 1.7511  Acc@1: 58.3333 (36.8827)  Acc@5: 91.6667 (77.7778)  time: 0.0877  data: 0.0016  max mem: 286\n",
            "Train: Epoch[1/1]  [ 90/105]  eta: 0:00:01  Lr: 0.000047  Loss: 1.5880  Acc@1: 62.5000 (39.9725)  Acc@5: 95.8333 (79.8535)  time: 0.0820  data: 0.0012  max mem: 286\n",
            "Train: Epoch[1/1]  [100/105]  eta: 0:00:00  Lr: 0.000047  Loss: 1.4863  Acc@1: 62.5000 (42.3680)  Acc@5: 95.8333 (81.0644)  time: 0.0806  data: 0.0012  max mem: 286\n",
            "Train: Epoch[1/1]  [104/105]  eta: 0:00:00  Lr: 0.000047  Loss: 1.4528  Acc@1: 66.6667 (43.2937)  Acc@5: 95.8333 (81.7064)  time: 0.0809  data: 0.0010  max mem: 286\n",
            "Train: Epoch[1/1] Total time: 0:00:09 (0.0952 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.4528  Acc@1: 66.6667 (43.2937)  Acc@5: 95.8333 (81.7064)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:25  Loss: 1.7220 (1.7220)  Acc@1: 87.5000 (87.5000)  Acc@5: 95.8333 (95.8333)  time: 0.6088  data: 0.5325  max mem: 286\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 1.9091 (1.8667)  Acc@1: 83.3333 (79.9242)  Acc@5: 100.0000 (97.7273)  time: 0.1280  data: 0.0503  max mem: 286\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 1.9091 (1.8715)  Acc@1: 83.3333 (83.5317)  Acc@5: 100.0000 (97.6190)  time: 0.0797  data: 0.0013  max mem: 286\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 1.8049 (1.8346)  Acc@1: 87.5000 (85.3495)  Acc@5: 95.8333 (97.5806)  time: 0.0797  data: 0.0004  max mem: 286\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 1.7995 (1.8164)  Acc@1: 87.5000 (86.4837)  Acc@5: 100.0000 (97.7642)  time: 0.0804  data: 0.0003  max mem: 286\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 1.7551 (1.8110)  Acc@1: 91.6667 (86.6000)  Acc@5: 100.0000 (97.8000)  time: 0.0792  data: 0.0003  max mem: 286\n",
            "Test: [Task 1] Total time: 0:00:03 (0.0938 s / it)\n",
            "* Acc@1 86.600 Acc@5 97.800 loss 1.811\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:18  Loss: 2.0358 (2.0358)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.4316  data: 0.3067  max mem: 286\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:03  Loss: 2.0358 (2.0390)  Acc@1: 79.1667 (80.3030)  Acc@5: 95.8333 (96.5909)  time: 0.1093  data: 0.0285  max mem: 286\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 1.9952 (2.0387)  Acc@1: 79.1667 (80.7540)  Acc@5: 95.8333 (96.2302)  time: 0.0790  data: 0.0006  max mem: 286\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 1.9935 (2.0321)  Acc@1: 83.3333 (81.4516)  Acc@5: 95.8333 (96.1022)  time: 0.0806  data: 0.0005  max mem: 286\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 1.9757 (2.0265)  Acc@1: 83.3333 (81.9106)  Acc@5: 95.8333 (96.4431)  time: 0.0807  data: 0.0005  max mem: 286\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 1.9621 (2.0210)  Acc@1: 83.3333 (82.0000)  Acc@5: 100.0000 (96.5000)  time: 0.0795  data: 0.0004  max mem: 286\n",
            "Test: [Task 2] Total time: 0:00:03 (0.0907 s / it)\n",
            "* Acc@1 82.000 Acc@5 96.500 loss 2.021\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:31  Loss: 3.4357 (3.4357)  Acc@1: 20.8333 (20.8333)  Acc@5: 70.8333 (70.8333)  time: 0.7466  data: 0.6815  max mem: 286\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:04  Loss: 3.4863 (3.4933)  Acc@1: 20.8333 (20.0758)  Acc@5: 70.8333 (70.4545)  time: 0.1420  data: 0.0622  max mem: 286\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 3.4898 (3.4938)  Acc@1: 16.6667 (19.2460)  Acc@5: 70.8333 (72.0238)  time: 0.0840  data: 0.0033  max mem: 286\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 3.4210 (3.4726)  Acc@1: 20.8333 (20.8333)  Acc@5: 70.8333 (72.1774)  time: 0.0844  data: 0.0035  max mem: 286\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 3.4414 (3.4913)  Acc@1: 20.8333 (19.9187)  Acc@5: 70.8333 (71.3415)  time: 0.0812  data: 0.0007  max mem: 286\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 3.4835 (3.4930)  Acc@1: 18.7500 (19.9000)  Acc@5: 70.8333 (71.4000)  time: 0.0795  data: 0.0006  max mem: 286\n",
            "Test: [Task 3] Total time: 0:00:04 (0.0998 s / it)\n",
            "* Acc@1 19.900 Acc@5 71.400 loss 3.493\n",
            "[Average accuracy till task3]\tAcc@1: 62.8333\tAcc@5: 88.5667\tLoss: 2.4417\tForgetting: 0.0000\tBackward: 38.5000\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([7152, 384])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 1.6099  Acc@1: 66.6667 (65.6250)  Acc@5: 95.8333 (93.7500)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000467  Loss: 1.4988  Acc@1: 75.0000 (74.7917)  Acc@5: 95.8333 (97.9167)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000464  Loss: 1.4861  Acc@1: 79.1667 (80.6250)  Acc@5: 100.0000 (98.3333)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 1.2737  Acc@1: 83.3333 (82.5000)  Acc@5: 100.0000 (99.3750)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000448  Loss: 1.0281  Acc@1: 83.3333 (83.9583)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000437  Loss: 0.8664  Acc@1: 83.3333 (85.8333)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 0.9454  Acc@1: 87.5000 (87.7083)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000409  Loss: 0.9244  Acc@1: 87.5000 (87.2917)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000391  Loss: 1.0804  Acc@1: 87.5000 (89.1667)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 0.8717  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000352  Loss: 1.0510  Acc@1: 91.6667 (91.2500)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000330  Loss: 0.8013  Acc@1: 87.5000 (88.3333)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 0.9034  Acc@1: 91.6667 (91.0417)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000283  Loss: 0.7191  Acc@1: 87.5000 (89.5833)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000259  Loss: 0.6868  Acc@1: 91.6667 (91.2500)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 0.6230  Acc@1: 91.6667 (92.0833)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000210  Loss: 0.7749  Acc@1: 87.5000 (88.1250)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000186  Loss: 0.6424  Acc@1: 91.6667 (90.8333)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.5572  Acc@1: 91.6667 (92.2917)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000139  Loss: 0.7447  Acc@1: 91.6667 (93.9583)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000117  Loss: 0.6975  Acc@1: 91.6667 (92.5000)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 0.6978  Acc@1: 91.6667 (92.5000)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000078  Loss: 0.6869  Acc@1: 91.6667 (91.8750)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000060  Loss: 0.6127  Acc@1: 91.6667 (91.4583)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 0.7343  Acc@1: 95.8333 (93.1250)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000031  Loss: 0.5861  Acc@1: 95.8333 (94.3750)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000020  Loss: 0.6352  Acc@1: 95.8333 (93.1250)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.5369  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000005  Loss: 0.5927  Acc@1: 91.6667 (93.1250)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000001  Loss: 0.7114  Acc@1: 95.8333 (93.3333)  Acc@5: 100.0000 (99.5833)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:16  Loss: 0.9005 (0.9005)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.4027  data: 0.3282  max mem: 291\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 1.1434 (1.1141)  Acc@1: 79.1667 (77.2727)  Acc@5: 100.0000 (99.2424)  time: 0.1101  data: 0.0312  max mem: 291\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 1.1350 (1.1048)  Acc@1: 79.1667 (80.5556)  Acc@5: 100.0000 (98.8095)  time: 0.0805  data: 0.0010  max mem: 291\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 1.0899 (1.0732)  Acc@1: 87.5000 (82.7957)  Acc@5: 100.0000 (98.7903)  time: 0.0806  data: 0.0004  max mem: 291\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 1.0096 (1.0514)  Acc@1: 87.5000 (83.7398)  Acc@5: 100.0000 (98.6789)  time: 0.0810  data: 0.0003  max mem: 291\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.9857 (1.0435)  Acc@1: 87.5000 (83.9000)  Acc@5: 100.0000 (98.7000)  time: 0.0797  data: 0.0003  max mem: 291\n",
            "Test: [Task 1] Total time: 0:00:03 (0.0910 s / it)\n",
            "* Acc@1 83.900 Acc@5 98.700 loss 1.044\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:31  Loss: 1.1995 (1.1995)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  time: 0.7565  data: 0.6665  max mem: 291\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 1.1846 (1.1699)  Acc@1: 83.3333 (81.8182)  Acc@5: 95.8333 (95.8333)  time: 0.1463  data: 0.0667  max mem: 291\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 1.1221 (1.1858)  Acc@1: 83.3333 (81.9444)  Acc@5: 95.8333 (96.0317)  time: 0.0840  data: 0.0039  max mem: 291\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 1.1012 (1.1745)  Acc@1: 83.3333 (82.3925)  Acc@5: 95.8333 (96.3710)  time: 0.0830  data: 0.0016  max mem: 291\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 1.0693 (1.1643)  Acc@1: 83.3333 (82.8252)  Acc@5: 100.0000 (96.9512)  time: 0.0824  data: 0.0015  max mem: 291\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 1.0656 (1.1574)  Acc@1: 83.3333 (83.0000)  Acc@5: 100.0000 (97.0000)  time: 0.0808  data: 0.0012  max mem: 291\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1008 s / it)\n",
            "* Acc@1 83.000 Acc@5 97.000 loss 1.157\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:21  Loss: 1.2156 (1.2156)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.5095  data: 0.4390  max mem: 291\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 1.3355 (1.3484)  Acc@1: 79.1667 (79.5455)  Acc@5: 95.8333 (97.7273)  time: 0.1180  data: 0.0402  max mem: 291\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 1.3100 (1.3044)  Acc@1: 79.1667 (79.7619)  Acc@5: 100.0000 (98.0159)  time: 0.0801  data: 0.0005  max mem: 291\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 1.2525 (1.2719)  Acc@1: 79.1667 (80.5108)  Acc@5: 100.0000 (97.9839)  time: 0.0820  data: 0.0006  max mem: 291\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 1.2580 (1.2913)  Acc@1: 79.1667 (80.6911)  Acc@5: 95.8333 (97.5610)  time: 0.0821  data: 0.0003  max mem: 291\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 1.2580 (1.2955)  Acc@1: 79.1667 (80.5000)  Acc@5: 95.8333 (97.6000)  time: 0.0807  data: 0.0003  max mem: 291\n",
            "Test: [Task 3] Total time: 0:00:03 (0.0925 s / it)\n",
            "* Acc@1 80.500 Acc@5 97.600 loss 1.295\n",
            "[Average accuracy till task3]\tAcc@1: 82.4667\tAcc@5: 97.7667\tLoss: 1.1655\tForgetting: 1.2000\tBackward: 18.5500\n",
            "Train: Epoch[1/1]  [  0/103]  eta: 0:00:42  Lr: 0.000047  Loss: 2.4407  Acc@1: 4.1667 (4.1667)  Acc@5: 45.8333 (45.8333)  time: 0.4101  data: 0.3088  max mem: 291\n",
            "Train: Epoch[1/1]  [ 10/103]  eta: 0:00:11  Lr: 0.000047  Loss: 2.2768  Acc@1: 8.3333 (8.7121)  Acc@5: 50.0000 (51.1364)  time: 0.1234  data: 0.0365  max mem: 291\n",
            "Train: Epoch[1/1]  [ 20/103]  eta: 0:00:08  Lr: 0.000047  Loss: 2.2171  Acc@1: 8.3333 (9.7222)  Acc@5: 54.1667 (53.9683)  time: 0.0888  data: 0.0051  max mem: 291\n",
            "Train: Epoch[1/1]  [ 30/103]  eta: 0:00:07  Lr: 0.000047  Loss: 2.2163  Acc@1: 16.6667 (12.5000)  Acc@5: 62.5000 (57.9301)  time: 0.0828  data: 0.0009  max mem: 291\n",
            "Train: Epoch[1/1]  [ 40/103]  eta: 0:00:05  Lr: 0.000047  Loss: 2.1261  Acc@1: 16.6667 (14.5325)  Acc@5: 66.6667 (60.1626)  time: 0.0829  data: 0.0011  max mem: 291\n",
            "Train: Epoch[1/1]  [ 50/103]  eta: 0:00:04  Lr: 0.000047  Loss: 1.8891  Acc@1: 25.0000 (17.6471)  Acc@5: 75.0000 (64.2157)  time: 0.0826  data: 0.0011  max mem: 291\n",
            "Train: Epoch[1/1]  [ 60/103]  eta: 0:00:03  Lr: 0.000047  Loss: 1.9644  Acc@1: 33.3333 (20.0137)  Acc@5: 79.1667 (66.9399)  time: 0.0850  data: 0.0012  max mem: 291\n",
            "Train: Epoch[1/1]  [ 70/103]  eta: 0:00:03  Lr: 0.000047  Loss: 1.8390  Acc@1: 33.3333 (23.0047)  Acc@5: 87.5000 (70.4225)  time: 0.0917  data: 0.0015  max mem: 291\n",
            "Train: Epoch[1/1]  [ 80/103]  eta: 0:00:02  Lr: 0.000047  Loss: 1.6734  Acc@1: 45.8333 (27.1091)  Acc@5: 91.6667 (72.9424)  time: 0.0973  data: 0.0009  max mem: 291\n",
            "Train: Epoch[1/1]  [ 90/103]  eta: 0:00:01  Lr: 0.000047  Loss: 1.6909  Acc@1: 58.3333 (30.9066)  Acc@5: 91.6667 (75.2289)  time: 0.0975  data: 0.0020  max mem: 291\n",
            "Train: Epoch[1/1]  [100/103]  eta: 0:00:00  Lr: 0.000047  Loss: 1.6301  Acc@1: 58.3333 (33.5396)  Acc@5: 95.8333 (77.1865)  time: 0.0968  data: 0.0037  max mem: 291\n",
            "Train: Epoch[1/1]  [102/103]  eta: 0:00:00  Lr: 0.000047  Loss: 1.7576  Acc@1: 58.3333 (33.8355)  Acc@5: 95.8333 (77.4023)  time: 0.0924  data: 0.0037  max mem: 291\n",
            "Train: Epoch[1/1] Total time: 0:00:09 (0.0936 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.7576  Acc@1: 58.3333 (33.8355)  Acc@5: 95.8333 (77.4023)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:22  Loss: 0.9297 (0.9297)  Acc@1: 87.5000 (87.5000)  Acc@5: 95.8333 (95.8333)  time: 0.5349  data: 0.4286  max mem: 291\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 1.1314 (1.1218)  Acc@1: 79.1667 (77.6515)  Acc@5: 100.0000 (98.4848)  time: 0.1244  data: 0.0412  max mem: 291\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 1.1314 (1.1169)  Acc@1: 79.1667 (80.9524)  Acc@5: 100.0000 (98.2143)  time: 0.0820  data: 0.0016  max mem: 291\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 1.0970 (1.0894)  Acc@1: 87.5000 (83.0645)  Acc@5: 100.0000 (98.1183)  time: 0.0818  data: 0.0007  max mem: 291\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 1.0412 (1.0689)  Acc@1: 87.5000 (84.3496)  Acc@5: 100.0000 (98.2724)  time: 0.0829  data: 0.0005  max mem: 291\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 1.0014 (1.0606)  Acc@1: 87.5000 (84.5000)  Acc@5: 100.0000 (98.3000)  time: 0.0813  data: 0.0004  max mem: 291\n",
            "Test: [Task 1] Total time: 0:00:03 (0.0946 s / it)\n",
            "* Acc@1 84.500 Acc@5 98.300 loss 1.061\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:17  Loss: 1.2379 (1.2379)  Acc@1: 83.3333 (83.3333)  Acc@5: 91.6667 (91.6667)  time: 0.4054  data: 0.3235  max mem: 291\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:03  Loss: 1.2182 (1.1997)  Acc@1: 83.3333 (81.4394)  Acc@5: 95.8333 (96.2121)  time: 0.1094  data: 0.0303  max mem: 291\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 1.1500 (1.2137)  Acc@1: 83.3333 (82.1429)  Acc@5: 95.8333 (96.2302)  time: 0.0813  data: 0.0008  max mem: 291\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 1.1268 (1.2013)  Acc@1: 83.3333 (82.3925)  Acc@5: 95.8333 (96.6398)  time: 0.0829  data: 0.0005  max mem: 291\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 1.1090 (1.1928)  Acc@1: 83.3333 (82.4187)  Acc@5: 95.8333 (96.9512)  time: 0.0830  data: 0.0003  max mem: 291\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 1.0987 (1.1855)  Acc@1: 83.3333 (82.6000)  Acc@5: 100.0000 (97.0000)  time: 0.0816  data: 0.0003  max mem: 291\n",
            "Test: [Task 2] Total time: 0:00:03 (0.0909 s / it)\n",
            "* Acc@1 82.600 Acc@5 97.000 loss 1.186\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:15  Loss: 1.2503 (1.2503)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.3770  data: 0.3021  max mem: 291\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 1.3701 (1.3828)  Acc@1: 79.1667 (79.5455)  Acc@5: 95.8333 (96.5909)  time: 0.1076  data: 0.0286  max mem: 291\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 1.3593 (1.3418)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (97.2222)  time: 0.0819  data: 0.0008  max mem: 291\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 1.2559 (1.3038)  Acc@1: 79.1667 (79.8387)  Acc@5: 100.0000 (97.5806)  time: 0.0830  data: 0.0007  max mem: 291\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 1.2846 (1.3237)  Acc@1: 79.1667 (79.9797)  Acc@5: 95.8333 (97.1545)  time: 0.0824  data: 0.0007  max mem: 291\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 1.3029 (1.3286)  Acc@1: 79.1667 (79.7000)  Acc@5: 95.8333 (97.2000)  time: 0.0812  data: 0.0007  max mem: 291\n",
            "Test: [Task 3] Total time: 0:00:03 (0.0919 s / it)\n",
            "* Acc@1 79.700 Acc@5 97.200 loss 1.329\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:31  Loss: 4.2208 (4.2208)  Acc@1: 4.1667 (4.1667)  Acc@5: 25.0000 (25.0000)  time: 0.7422  data: 0.6362  max mem: 291\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:04  Loss: 4.0698 (4.0491)  Acc@1: 4.1667 (2.6515)  Acc@5: 29.1667 (33.3333)  time: 0.1431  data: 0.0582  max mem: 291\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 4.0566 (4.0544)  Acc@1: 4.1667 (2.9762)  Acc@5: 33.3333 (34.3254)  time: 0.0830  data: 0.0009  max mem: 291\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 4.0521 (4.0437)  Acc@1: 0.0000 (2.5538)  Acc@5: 33.3333 (35.2151)  time: 0.0829  data: 0.0012  max mem: 291\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 3.9798 (4.0413)  Acc@1: 0.0000 (2.4390)  Acc@5: 37.5000 (34.6545)  time: 0.0830  data: 0.0007  max mem: 291\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 3.9798 (4.0376)  Acc@1: 0.0000 (2.5000)  Acc@5: 31.2500 (34.6000)  time: 0.0816  data: 0.0005  max mem: 291\n",
            "Test: [Task 4] Total time: 0:00:04 (0.1001 s / it)\n",
            "* Acc@1 2.500 Acc@5 34.600 loss 4.038\n",
            "[Average accuracy till task4]\tAcc@1: 62.3250\tAcc@5: 81.7750\tLoss: 1.9031\tForgetting: 0.7000\tBackward: 45.1000\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([9528, 384])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 1.1272  Acc@1: 70.8333 (72.6389)  Acc@5: 87.5000 (87.2222)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000467  Loss: 1.7750  Acc@1: 75.0000 (75.6944)  Acc@5: 95.8333 (94.3056)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000464  Loss: 0.8546  Acc@1: 75.0000 (76.2500)  Acc@5: 95.8333 (95.9722)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 1.0152  Acc@1: 83.3333 (82.6389)  Acc@5: 100.0000 (97.0833)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000448  Loss: 0.9091  Acc@1: 83.3333 (83.7500)  Acc@5: 100.0000 (98.1944)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000437  Loss: 0.4941  Acc@1: 83.3333 (86.8056)  Acc@5: 100.0000 (99.3056)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 0.9093  Acc@1: 91.6667 (87.9167)  Acc@5: 100.0000 (99.0278)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000409  Loss: 0.8382  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (99.0278)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000391  Loss: 0.7377  Acc@1: 91.6667 (89.5833)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 0.7532  Acc@1: 91.6667 (89.7222)  Acc@5: 100.0000 (99.3056)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000352  Loss: 0.5521  Acc@1: 91.6667 (91.5278)  Acc@5: 100.0000 (99.4444)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000330  Loss: 0.6410  Acc@1: 91.6667 (90.6944)  Acc@5: 100.0000 (99.7222)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 0.5710  Acc@1: 91.6667 (90.5556)  Acc@5: 100.0000 (99.4444)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000283  Loss: 0.7267  Acc@1: 91.6667 (91.2500)  Acc@5: 100.0000 (99.7222)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000259  Loss: 0.8463  Acc@1: 91.6667 (92.9167)  Acc@5: 100.0000 (99.8611)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 0.7175  Acc@1: 91.6667 (91.2500)  Acc@5: 100.0000 (99.7222)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000210  Loss: 0.6061  Acc@1: 95.8333 (93.1944)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000186  Loss: 0.7119  Acc@1: 91.6667 (92.6389)  Acc@5: 100.0000 (99.8611)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.5643  Acc@1: 91.6667 (94.1667)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000139  Loss: 0.5153  Acc@1: 91.6667 (93.1944)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000117  Loss: 0.5884  Acc@1: 95.8333 (93.3333)  Acc@5: 100.0000 (99.8611)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 0.6650  Acc@1: 91.6667 (92.6389)  Acc@5: 100.0000 (99.8611)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000078  Loss: 0.4484  Acc@1: 91.6667 (92.6389)  Acc@5: 100.0000 (99.8611)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000060  Loss: 0.5955  Acc@1: 91.6667 (92.9167)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 0.7155  Acc@1: 91.6667 (91.8056)  Acc@5: 100.0000 (99.8611)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000031  Loss: 0.5818  Acc@1: 91.6667 (93.7500)  Acc@5: 100.0000 (99.7222)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000020  Loss: 0.6016  Acc@1: 91.6667 (92.5000)  Acc@5: 100.0000 (99.8611)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.6378  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (99.7222)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000005  Loss: 0.5177  Acc@1: 91.6667 (92.5000)  Acc@5: 100.0000 (99.8611)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000001  Loss: 0.6232  Acc@1: 95.8333 (92.5000)  Acc@5: 100.0000 (99.7222)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:14  Loss: 0.6549 (0.6549)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  time: 0.3444  data: 0.2716  max mem: 292\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 0.8557 (0.8679)  Acc@1: 79.1667 (78.4091)  Acc@5: 100.0000 (97.3485)  time: 0.1104  data: 0.0313  max mem: 292\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.8567 (0.8513)  Acc@1: 79.1667 (81.1508)  Acc@5: 100.0000 (97.6190)  time: 0.0845  data: 0.0040  max mem: 292\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.7999 (0.8248)  Acc@1: 87.5000 (82.7957)  Acc@5: 100.0000 (97.8495)  time: 0.0827  data: 0.0005  max mem: 292\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.7530 (0.8020)  Acc@1: 87.5000 (83.5366)  Acc@5: 100.0000 (98.0691)  time: 0.0830  data: 0.0003  max mem: 292\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.7073 (0.7932)  Acc@1: 87.5000 (83.7000)  Acc@5: 100.0000 (98.1000)  time: 0.0815  data: 0.0003  max mem: 292\n",
            "Test: [Task 1] Total time: 0:00:03 (0.0911 s / it)\n",
            "* Acc@1 83.700 Acc@5 98.100 loss 0.793\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:17  Loss: 1.0208 (1.0208)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  time: 0.4118  data: 0.3379  max mem: 292\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:03  Loss: 0.9750 (0.9575)  Acc@1: 83.3333 (79.5455)  Acc@5: 95.8333 (96.5909)  time: 0.1131  data: 0.0315  max mem: 292\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 0.9452 (0.9848)  Acc@1: 79.1667 (79.5635)  Acc@5: 95.8333 (96.4286)  time: 0.0818  data: 0.0006  max mem: 292\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.9111 (0.9734)  Acc@1: 83.3333 (80.2419)  Acc@5: 95.8333 (96.5054)  time: 0.0819  data: 0.0006  max mem: 292\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.9071 (0.9569)  Acc@1: 83.3333 (80.4878)  Acc@5: 100.0000 (97.0528)  time: 0.0831  data: 0.0006  max mem: 292\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.8521 (0.9496)  Acc@1: 83.3333 (80.7000)  Acc@5: 100.0000 (97.1000)  time: 0.0816  data: 0.0006  max mem: 292\n",
            "Test: [Task 2] Total time: 0:00:03 (0.0927 s / it)\n",
            "* Acc@1 80.700 Acc@5 97.100 loss 0.950\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:31  Loss: 0.7801 (0.7801)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.7407  data: 0.6289  max mem: 292\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:04  Loss: 0.9605 (0.9651)  Acc@1: 83.3333 (81.4394)  Acc@5: 95.8333 (96.9697)  time: 0.1448  data: 0.0594  max mem: 292\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 0.9235 (0.9096)  Acc@1: 83.3333 (82.5397)  Acc@5: 95.8333 (97.6190)  time: 0.0854  data: 0.0018  max mem: 292\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.7818 (0.8751)  Acc@1: 83.3333 (82.9301)  Acc@5: 100.0000 (97.9839)  time: 0.0846  data: 0.0012  max mem: 292\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.8701 (0.8886)  Acc@1: 83.3333 (83.4350)  Acc@5: 95.8333 (97.3577)  time: 0.0828  data: 0.0007  max mem: 292\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.8769 (0.8951)  Acc@1: 83.3333 (83.2000)  Acc@5: 95.8333 (97.4000)  time: 0.0815  data: 0.0007  max mem: 292\n",
            "Test: [Task 3] Total time: 0:00:04 (0.1011 s / it)\n",
            "* Acc@1 83.200 Acc@5 97.400 loss 0.895\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:18  Loss: 1.5726 (1.5726)  Acc@1: 62.5000 (62.5000)  Acc@5: 91.6667 (91.6667)  time: 0.4436  data: 0.3660  max mem: 292\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:03  Loss: 1.1411 (1.1716)  Acc@1: 79.1667 (77.6515)  Acc@5: 100.0000 (98.1061)  time: 0.1158  data: 0.0344  max mem: 292\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 1.1537 (1.1827)  Acc@1: 75.0000 (75.3968)  Acc@5: 100.0000 (98.2143)  time: 0.0824  data: 0.0008  max mem: 292\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 1.1262 (1.1593)  Acc@1: 75.0000 (75.8065)  Acc@5: 100.0000 (98.1183)  time: 0.0824  data: 0.0005  max mem: 292\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 1.1262 (1.1709)  Acc@1: 79.1667 (75.8130)  Acc@5: 95.8333 (97.6626)  time: 0.0834  data: 0.0004  max mem: 292\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 1.1145 (1.1646)  Acc@1: 79.1667 (76.1000)  Acc@5: 100.0000 (97.7000)  time: 0.0821  data: 0.0004  max mem: 292\n",
            "Test: [Task 4] Total time: 0:00:03 (0.0932 s / it)\n",
            "* Acc@1 76.100 Acc@5 97.700 loss 1.165\n",
            "[Average accuracy till task4]\tAcc@1: 80.9250\tAcc@5: 97.5750\tLoss: 0.9506\tForgetting: 1.6333\tBackward: 12.4333\n",
            "Train: Epoch[1/1]  [  0/105]  eta: 0:00:44  Lr: 0.000047  Loss: 2.3057  Acc@1: 12.5000 (12.5000)  Acc@5: 62.5000 (62.5000)  time: 0.4285  data: 0.3330  max mem: 292\n",
            "Train: Epoch[1/1]  [ 10/105]  eta: 0:00:11  Lr: 0.000047  Loss: 2.1935  Acc@1: 16.6667 (15.9091)  Acc@5: 62.5000 (62.8788)  time: 0.1226  data: 0.0344  max mem: 292\n",
            "Train: Epoch[1/1]  [ 20/105]  eta: 0:00:08  Lr: 0.000047  Loss: 2.0227  Acc@1: 20.8333 (21.4286)  Acc@5: 70.8333 (68.4524)  time: 0.0876  data: 0.0026  max mem: 292\n",
            "Train: Epoch[1/1]  [ 30/105]  eta: 0:00:07  Lr: 0.000047  Loss: 2.0571  Acc@1: 25.0000 (24.4624)  Acc@5: 75.0000 (70.2957)  time: 0.0838  data: 0.0007  max mem: 292\n",
            "Train: Epoch[1/1]  [ 40/105]  eta: 0:00:06  Lr: 0.000047  Loss: 1.9023  Acc@1: 33.3333 (28.3537)  Acc@5: 79.1667 (73.5772)  time: 0.0858  data: 0.0009  max mem: 292\n",
            "Train: Epoch[1/1]  [ 50/105]  eta: 0:00:05  Lr: 0.000047  Loss: 1.9501  Acc@1: 41.6667 (30.7190)  Acc@5: 83.3333 (75.3268)  time: 0.0875  data: 0.0012  max mem: 292\n",
            "Train: Epoch[1/1]  [ 60/105]  eta: 0:00:04  Lr: 0.000047  Loss: 1.5590  Acc@1: 50.0000 (34.6311)  Acc@5: 87.5000 (77.8005)  time: 0.0927  data: 0.0010  max mem: 292\n",
            "Train: Epoch[1/1]  [ 70/105]  eta: 0:00:03  Lr: 0.000047  Loss: 1.5675  Acc@1: 54.1667 (37.1479)  Acc@5: 91.6667 (79.3427)  time: 0.0987  data: 0.0015  max mem: 292\n",
            "Train: Epoch[1/1]  [ 80/105]  eta: 0:00:02  Lr: 0.000047  Loss: 1.5997  Acc@1: 54.1667 (39.9177)  Acc@5: 91.6667 (80.7613)  time: 0.1048  data: 0.0060  max mem: 292\n",
            "Train: Epoch[1/1]  [ 90/105]  eta: 0:00:01  Lr: 0.000047  Loss: 1.4933  Acc@1: 58.3333 (42.0788)  Acc@5: 91.6667 (82.1429)  time: 0.1028  data: 0.0056  max mem: 292\n",
            "Train: Epoch[1/1]  [100/105]  eta: 0:00:00  Lr: 0.000047  Loss: 1.5393  Acc@1: 62.5000 (43.8119)  Acc@5: 91.6667 (83.1271)  time: 0.0904  data: 0.0010  max mem: 292\n",
            "Train: Epoch[1/1]  [104/105]  eta: 0:00:00  Lr: 0.000047  Loss: 1.1255  Acc@1: 62.5000 (44.4800)  Acc@5: 95.8333 (83.4800)  time: 0.0837  data: 0.0007  max mem: 292\n",
            "Train: Epoch[1/1] Total time: 0:00:10 (0.0955 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.1255  Acc@1: 62.5000 (44.4800)  Acc@5: 95.8333 (83.4800)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:17  Loss: 0.7410 (0.7410)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  time: 0.4254  data: 0.3416  max mem: 292\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 0.8810 (0.8860)  Acc@1: 79.1667 (78.0303)  Acc@5: 100.0000 (97.3485)  time: 0.1144  data: 0.0323  max mem: 292\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.8810 (0.8786)  Acc@1: 79.1667 (80.7540)  Acc@5: 100.0000 (97.6190)  time: 0.0828  data: 0.0013  max mem: 292\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.8255 (0.8565)  Acc@1: 83.3333 (82.1237)  Acc@5: 100.0000 (97.9839)  time: 0.0833  data: 0.0008  max mem: 292\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.7918 (0.8332)  Acc@1: 87.5000 (83.0285)  Acc@5: 100.0000 (98.2724)  time: 0.0841  data: 0.0003  max mem: 292\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.7308 (0.8243)  Acc@1: 87.5000 (83.1000)  Acc@5: 100.0000 (98.3000)  time: 0.0827  data: 0.0003  max mem: 292\n",
            "Test: [Task 1] Total time: 0:00:03 (0.0930 s / it)\n",
            "* Acc@1 83.100 Acc@5 98.300 loss 0.824\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:20  Loss: 1.0533 (1.0533)  Acc@1: 87.5000 (87.5000)  Acc@5: 91.6667 (91.6667)  time: 0.4896  data: 0.4070  max mem: 292\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:03  Loss: 0.9963 (0.9706)  Acc@1: 83.3333 (79.9242)  Acc@5: 95.8333 (96.2121)  time: 0.1201  data: 0.0379  max mem: 292\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 0.9528 (0.9985)  Acc@1: 83.3333 (80.7540)  Acc@5: 95.8333 (96.0317)  time: 0.0828  data: 0.0007  max mem: 292\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.9497 (0.9884)  Acc@1: 83.3333 (80.7796)  Acc@5: 95.8333 (96.5054)  time: 0.0838  data: 0.0005  max mem: 292\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.9147 (0.9751)  Acc@1: 83.3333 (80.7927)  Acc@5: 100.0000 (96.8496)  time: 0.0851  data: 0.0003  max mem: 292\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.8827 (0.9673)  Acc@1: 83.3333 (81.0000)  Acc@5: 100.0000 (96.9000)  time: 0.0833  data: 0.0003  max mem: 292\n",
            "Test: [Task 2] Total time: 0:00:03 (0.0947 s / it)\n",
            "* Acc@1 81.000 Acc@5 96.900 loss 0.967\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:18  Loss: 0.7751 (0.7751)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  time: 0.4298  data: 0.3529  max mem: 292\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 0.9750 (0.9813)  Acc@1: 83.3333 (82.1970)  Acc@5: 95.8333 (96.5909)  time: 0.1163  data: 0.0326  max mem: 292\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 0.9303 (0.9255)  Acc@1: 83.3333 (82.7381)  Acc@5: 95.8333 (97.6190)  time: 0.0847  data: 0.0008  max mem: 292\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.8146 (0.8902)  Acc@1: 83.3333 (83.1989)  Acc@5: 100.0000 (98.1183)  time: 0.0854  data: 0.0031  max mem: 292\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.8389 (0.9004)  Acc@1: 83.3333 (83.7398)  Acc@5: 95.8333 (97.4594)  time: 0.0872  data: 0.0067  max mem: 292\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.8668 (0.9080)  Acc@1: 83.3333 (83.4000)  Acc@5: 95.8333 (97.5000)  time: 0.0855  data: 0.0067  max mem: 292\n",
            "Test: [Task 3] Total time: 0:00:04 (0.0962 s / it)\n",
            "* Acc@1 83.400 Acc@5 97.500 loss 0.908\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:26  Loss: 1.7493 (1.7493)  Acc@1: 58.3333 (58.3333)  Acc@5: 87.5000 (87.5000)  time: 0.6314  data: 0.5436  max mem: 292\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:04  Loss: 1.2477 (1.2549)  Acc@1: 75.0000 (73.4849)  Acc@5: 95.8333 (94.6970)  time: 0.1336  data: 0.0501  max mem: 292\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 1.2477 (1.2728)  Acc@1: 70.8333 (71.4286)  Acc@5: 95.8333 (95.4365)  time: 0.0819  data: 0.0006  max mem: 292\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 1.2328 (1.2510)  Acc@1: 70.8333 (72.8495)  Acc@5: 95.8333 (95.5645)  time: 0.0816  data: 0.0004  max mem: 292\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 1.2307 (1.2609)  Acc@1: 75.0000 (73.0691)  Acc@5: 95.8333 (95.1220)  time: 0.0834  data: 0.0003  max mem: 292\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 1.1970 (1.2548)  Acc@1: 79.1667 (73.3000)  Acc@5: 95.8333 (95.1000)  time: 0.0816  data: 0.0003  max mem: 292\n",
            "Test: [Task 4] Total time: 0:00:04 (0.0970 s / it)\n",
            "* Acc@1 73.300 Acc@5 95.100 loss 1.255\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:14  Loss: 4.1069 (4.1069)  Acc@1: 0.0000 (0.0000)  Acc@5: 20.8333 (20.8333)  time: 0.3567  data: 0.2810  max mem: 292\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:03  Loss: 4.0916 (4.0649)  Acc@1: 0.0000 (0.0000)  Acc@5: 29.1667 (26.5152)  time: 0.1111  data: 0.0314  max mem: 292\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 4.0785 (4.0685)  Acc@1: 0.0000 (0.0000)  Acc@5: 20.8333 (25.7937)  time: 0.0845  data: 0.0035  max mem: 292\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 4.1131 (4.0922)  Acc@1: 0.0000 (0.0000)  Acc@5: 20.8333 (25.0000)  time: 0.0827  data: 0.0005  max mem: 292\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 4.1497 (4.1205)  Acc@1: 0.0000 (0.0000)  Acc@5: 20.8333 (24.0854)  time: 0.0829  data: 0.0003  max mem: 292\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 4.1564 (4.1235)  Acc@1: 0.0000 (0.0000)  Acc@5: 20.8333 (24.0000)  time: 0.0816  data: 0.0003  max mem: 292\n",
            "Test: [Task 5] Total time: 0:00:03 (0.0919 s / it)\n",
            "* Acc@1 0.000 Acc@5 24.000 loss 4.124\n",
            "[Average accuracy till task5]\tAcc@1: 64.1600\tAcc@5: 82.3600\tLoss: 1.6156\tForgetting: 1.2750\tBackward: 51.7000\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([11928, 384])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 0.9495  Acc@1: 79.1667 (74.6875)  Acc@5: 87.5000 (86.4583)\n",
            "torch.Size([11928, 384])\n",
            "Averaged stats: Lr: 0.000467  Loss: 0.5594  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (93.2292)\n",
            "torch.Size([11928, 384])\n",
            "Averaged stats: Lr: 0.000464  Loss: 0.7404  Acc@1: 79.1667 (79.6875)  Acc@5: 100.0000 (97.9167)\n",
            "torch.Size([11928, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 0.9419  Acc@1: 83.3333 (83.5417)  Acc@5: 100.0000 (98.0208)\n",
            "torch.Size([11928, 384])\n",
            "Averaged stats: Lr: 0.000448  Loss: 0.9679  Acc@1: 87.5000 (85.2083)  Acc@5: 100.0000 (99.3750)\n",
            "torch.Size([11928, 384])\n",
            "Averaged stats: Lr: 0.000437  Loss: 0.8554  Acc@1: 91.6667 (88.0208)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([11928, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 0.9064  Acc@1: 87.5000 (88.0208)  Acc@5: 100.0000 (99.6875)\n",
            "torch.Size([11928, 384])\n",
            "Averaged stats: Lr: 0.000409  Loss: 0.7808  Acc@1: 87.5000 (88.4375)  Acc@5: 100.0000 (99.6875)\n",
            "torch.Size([11928, 384])\n",
            "Averaged stats: Lr: 0.000391  Loss: 0.6295  Acc@1: 91.6667 (90.9375)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([11928, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 0.8914  Acc@1: 95.8333 (92.8125)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([11928, 384])\n",
            "Averaged stats: Lr: 0.000352  Loss: 0.5847  Acc@1: 91.6667 (92.2917)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([11928, 384])\n",
            "Averaged stats: Lr: 0.000330  Loss: 0.6269  Acc@1: 91.6667 (92.1875)  Acc@5: 100.0000 (99.4792)\n",
            "torch.Size([11928, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 0.6223  Acc@1: 91.6667 (92.0833)  Acc@5: 100.0000 (99.2708)\n",
            "torch.Size([11928, 384])\n",
            "Averaged stats: Lr: 0.000283  Loss: 0.6979  Acc@1: 87.5000 (91.7708)  Acc@5: 100.0000 (99.8958)\n",
            "torch.Size([11928, 384])\n",
            "Averaged stats: Lr: 0.000259  Loss: 0.4919  Acc@1: 91.6667 (90.6250)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([11928, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 0.4801  Acc@1: 95.8333 (92.5000)  Acc@5: 100.0000 (99.6875)\n",
            "torch.Size([11928, 384])\n",
            "Averaged stats: Lr: 0.000210  Loss: 0.4456  Acc@1: 95.8333 (93.6458)  Acc@5: 100.0000 (99.8958)\n",
            "torch.Size([11928, 384])\n",
            "Averaged stats: Lr: 0.000186  Loss: 0.4927  Acc@1: 95.8333 (93.3333)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([11928, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.4420  Acc@1: 95.8333 (93.5417)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([11928, 384])\n",
            "Averaged stats: Lr: 0.000139  Loss: 0.3360  Acc@1: 95.8333 (92.3958)  Acc@5: 100.0000 (99.6875)\n",
            "torch.Size([11928, 384])\n",
            "Averaged stats: Lr: 0.000117  Loss: 0.4046  Acc@1: 95.8333 (93.8542)  Acc@5: 100.0000 (99.6875)\n",
            "torch.Size([11928, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 0.4799  Acc@1: 95.8333 (93.1250)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([11928, 384])\n",
            "Averaged stats: Lr: 0.000078  Loss: 0.5266  Acc@1: 95.8333 (94.8958)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([11928, 384])\n",
            "Averaged stats: Lr: 0.000060  Loss: 0.4380  Acc@1: 91.6667 (93.9583)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([11928, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 0.6485  Acc@1: 95.8333 (94.4792)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([11928, 384])\n",
            "Averaged stats: Lr: 0.000031  Loss: 0.4461  Acc@1: 95.8333 (93.6458)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([11928, 384])\n",
            "Averaged stats: Lr: 0.000020  Loss: 0.4887  Acc@1: 91.6667 (93.5417)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([11928, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.6020  Acc@1: 91.6667 (94.3750)  Acc@5: 100.0000 (99.8958)\n",
            "torch.Size([11928, 384])\n",
            "Averaged stats: Lr: 0.000005  Loss: 0.4331  Acc@1: 95.8333 (93.6458)  Acc@5: 100.0000 (99.8958)\n",
            "torch.Size([11928, 384])\n",
            "Averaged stats: Lr: 0.000001  Loss: 0.4348  Acc@1: 91.6667 (93.3333)  Acc@5: 100.0000 (100.0000)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:15  Loss: 0.6889 (0.6889)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  time: 0.3762  data: 0.2981  max mem: 292\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 0.7456 (0.8038)  Acc@1: 79.1667 (77.2727)  Acc@5: 100.0000 (97.7273)  time: 0.1097  data: 0.0307  max mem: 292\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.7456 (0.7845)  Acc@1: 79.1667 (80.3571)  Acc@5: 100.0000 (97.8175)  time: 0.0816  data: 0.0023  max mem: 292\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.7270 (0.7603)  Acc@1: 83.3333 (81.5860)  Acc@5: 100.0000 (98.1183)  time: 0.0806  data: 0.0005  max mem: 292\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6673 (0.7360)  Acc@1: 83.3333 (82.3171)  Acc@5: 100.0000 (98.3740)  time: 0.0812  data: 0.0004  max mem: 292\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6549 (0.7270)  Acc@1: 83.3333 (82.5000)  Acc@5: 100.0000 (98.4000)  time: 0.0799  data: 0.0003  max mem: 292\n",
            "Test: [Task 1] Total time: 0:00:03 (0.0896 s / it)\n",
            "* Acc@1 82.500 Acc@5 98.400 loss 0.727\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:17  Loss: 0.9460 (0.9460)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  time: 0.4157  data: 0.3371  max mem: 292\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:03  Loss: 0.9078 (0.8619)  Acc@1: 83.3333 (79.9242)  Acc@5: 95.8333 (97.3485)  time: 0.1102  data: 0.0319  max mem: 292\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 0.8945 (0.9049)  Acc@1: 79.1667 (79.5635)  Acc@5: 95.8333 (96.6270)  time: 0.0805  data: 0.0008  max mem: 292\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.8518 (0.8975)  Acc@1: 79.1667 (79.8387)  Acc@5: 95.8333 (96.6398)  time: 0.0813  data: 0.0003  max mem: 292\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.7734 (0.8777)  Acc@1: 79.1667 (79.8781)  Acc@5: 100.0000 (97.0528)  time: 0.0812  data: 0.0003  max mem: 292\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.7696 (0.8709)  Acc@1: 83.3333 (80.1000)  Acc@5: 100.0000 (97.1000)  time: 0.0799  data: 0.0003  max mem: 292\n",
            "Test: [Task 2] Total time: 0:00:03 (0.0901 s / it)\n",
            "* Acc@1 80.100 Acc@5 97.100 loss 0.871\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:21  Loss: 0.6653 (0.6653)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.5052  data: 0.4273  max mem: 292\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 0.8343 (0.8586)  Acc@1: 79.1667 (81.0606)  Acc@5: 95.8333 (95.8333)  time: 0.1211  data: 0.0400  max mem: 292\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 0.8273 (0.8075)  Acc@1: 79.1667 (82.1429)  Acc@5: 95.8333 (96.8254)  time: 0.0823  data: 0.0010  max mem: 292\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.7318 (0.7787)  Acc@1: 83.3333 (82.5269)  Acc@5: 100.0000 (97.0430)  time: 0.0833  data: 0.0013  max mem: 292\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.7633 (0.7864)  Acc@1: 83.3333 (82.9268)  Acc@5: 95.8333 (96.7480)  time: 0.0838  data: 0.0020  max mem: 292\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.7633 (0.7915)  Acc@1: 83.3333 (82.6000)  Acc@5: 95.8333 (96.8000)  time: 0.0820  data: 0.0019  max mem: 292\n",
            "Test: [Task 3] Total time: 0:00:03 (0.0951 s / it)\n",
            "* Acc@1 82.600 Acc@5 96.800 loss 0.791\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:30  Loss: 1.2621 (1.2621)  Acc@1: 66.6667 (66.6667)  Acc@5: 87.5000 (87.5000)  time: 0.7365  data: 0.6314  max mem: 292\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:04  Loss: 0.8695 (0.8788)  Acc@1: 79.1667 (81.8182)  Acc@5: 95.8333 (96.5909)  time: 0.1442  data: 0.0594  max mem: 292\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 0.8599 (0.8767)  Acc@1: 79.1667 (81.1508)  Acc@5: 100.0000 (97.0238)  time: 0.0832  data: 0.0016  max mem: 292\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 0.8219 (0.8453)  Acc@1: 83.3333 (81.4516)  Acc@5: 100.0000 (97.3118)  time: 0.0814  data: 0.0010  max mem: 292\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7617 (0.8619)  Acc@1: 79.1667 (81.1992)  Acc@5: 95.8333 (97.1545)  time: 0.0812  data: 0.0006  max mem: 292\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.7545 (0.8554)  Acc@1: 83.3333 (81.4000)  Acc@5: 100.0000 (97.2000)  time: 0.0797  data: 0.0004  max mem: 292\n",
            "Test: [Task 4] Total time: 0:00:04 (0.0989 s / it)\n",
            "* Acc@1 81.400 Acc@5 97.200 loss 0.855\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:12  Loss: 0.9010 (0.9010)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.3057  data: 0.2341  max mem: 292\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:03  Loss: 1.0572 (1.0358)  Acc@1: 75.0000 (77.2727)  Acc@5: 100.0000 (97.7273)  time: 0.1155  data: 0.0376  max mem: 292\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 1.0716 (1.0625)  Acc@1: 75.0000 (76.3889)  Acc@5: 95.8333 (96.8254)  time: 0.0898  data: 0.0093  max mem: 292\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 1.0841 (1.0834)  Acc@1: 75.0000 (76.4785)  Acc@5: 95.8333 (96.5054)  time: 0.0831  data: 0.0006  max mem: 292\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 1.2216 (1.1362)  Acc@1: 75.0000 (74.2886)  Acc@5: 95.8333 (95.7317)  time: 0.0828  data: 0.0005  max mem: 292\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 1.2366 (1.1505)  Acc@1: 75.0000 (73.8000)  Acc@5: 95.8333 (95.5000)  time: 0.0816  data: 0.0005  max mem: 292\n",
            "Test: [Task 5] Total time: 0:00:03 (0.0930 s / it)\n",
            "* Acc@1 73.800 Acc@5 95.500 loss 1.150\n",
            "[Average accuracy till task5]\tAcc@1: 80.0800\tAcc@5: 97.0000\tLoss: 0.8790\tForgetting: 1.8250\tBackward: 10.0500\n",
            "Train: Epoch[1/1]  [  0/105]  eta: 0:00:54  Lr: 0.000047  Loss: 2.3123  Acc@1: 8.3333 (8.3333)  Acc@5: 54.1667 (54.1667)  time: 0.5197  data: 0.4132  max mem: 292\n",
            "Train: Epoch[1/1]  [ 10/105]  eta: 0:00:12  Lr: 0.000047  Loss: 2.4350  Acc@1: 8.3333 (8.7121)  Acc@5: 54.1667 (55.6818)  time: 0.1269  data: 0.0382  max mem: 292\n",
            "Train: Epoch[1/1]  [ 20/105]  eta: 0:00:09  Lr: 0.000047  Loss: 2.1706  Acc@1: 12.5000 (14.6825)  Acc@5: 62.5000 (61.1111)  time: 0.0864  data: 0.0009  max mem: 292\n",
            "Train: Epoch[1/1]  [ 30/105]  eta: 0:00:07  Lr: 0.000047  Loss: 2.0155  Acc@1: 20.8333 (17.2043)  Acc@5: 70.8333 (65.9946)  time: 0.0852  data: 0.0015  max mem: 292\n",
            "Train: Epoch[1/1]  [ 40/105]  eta: 0:00:06  Lr: 0.000047  Loss: 1.9436  Acc@1: 25.0000 (20.7317)  Acc@5: 79.1667 (70.1220)  time: 0.0912  data: 0.0017  max mem: 292\n",
            "Train: Epoch[1/1]  [ 50/105]  eta: 0:00:05  Lr: 0.000047  Loss: 1.9447  Acc@1: 33.3333 (24.0196)  Acc@5: 83.3333 (73.6111)  time: 0.0974  data: 0.0010  max mem: 292\n",
            "Train: Epoch[1/1]  [ 60/105]  eta: 0:00:04  Lr: 0.000047  Loss: 1.8709  Acc@1: 41.6667 (27.8005)  Acc@5: 87.5000 (75.9563)  time: 0.0996  data: 0.0013  max mem: 292\n",
            "Train: Epoch[1/1]  [ 70/105]  eta: 0:00:03  Lr: 0.000047  Loss: 1.6099  Acc@1: 50.0000 (31.6901)  Acc@5: 91.6667 (78.6385)  time: 0.1016  data: 0.0019  max mem: 292\n",
            "Train: Epoch[1/1]  [ 80/105]  eta: 0:00:02  Lr: 0.000047  Loss: 1.6992  Acc@1: 54.1667 (34.4650)  Acc@5: 95.8333 (80.6584)  time: 0.0949  data: 0.0016  max mem: 292\n",
            "Train: Epoch[1/1]  [ 90/105]  eta: 0:00:01  Lr: 0.000047  Loss: 1.5641  Acc@1: 54.1667 (36.7674)  Acc@5: 95.8333 (82.2344)  time: 0.0865  data: 0.0016  max mem: 292\n",
            "Train: Epoch[1/1]  [100/105]  eta: 0:00:00  Lr: 0.000047  Loss: 1.2995  Acc@1: 62.5000 (39.1914)  Acc@5: 95.8333 (83.4984)  time: 0.0843  data: 0.0012  max mem: 292\n",
            "Train: Epoch[1/1]  [104/105]  eta: 0:00:00  Lr: 0.000047  Loss: 1.4646  Acc@1: 62.5000 (40.1587)  Acc@5: 95.8333 (83.8889)  time: 0.0837  data: 0.0009  max mem: 292\n",
            "Train: Epoch[1/1] Total time: 0:00:10 (0.0959 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.4646  Acc@1: 62.5000 (40.1587)  Acc@5: 95.8333 (83.8889)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:21  Loss: 0.6827 (0.6827)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.5148  data: 0.4110  max mem: 292\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 0.7272 (0.8021)  Acc@1: 75.0000 (78.4091)  Acc@5: 100.0000 (98.1061)  time: 0.1301  data: 0.0490  max mem: 292\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.7358 (0.7869)  Acc@1: 79.1667 (81.3492)  Acc@5: 100.0000 (98.0159)  time: 0.0880  data: 0.0066  max mem: 292\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.7358 (0.7629)  Acc@1: 83.3333 (81.5860)  Acc@5: 100.0000 (98.1183)  time: 0.0839  data: 0.0004  max mem: 292\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6945 (0.7386)  Acc@1: 83.3333 (82.7236)  Acc@5: 100.0000 (98.2724)  time: 0.0834  data: 0.0003  max mem: 292\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6711 (0.7297)  Acc@1: 83.3333 (82.9000)  Acc@5: 100.0000 (98.3000)  time: 0.0819  data: 0.0003  max mem: 292\n",
            "Test: [Task 1] Total time: 0:00:04 (0.0972 s / it)\n",
            "* Acc@1 82.900 Acc@5 98.300 loss 0.730\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:16  Loss: 0.9615 (0.9615)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  time: 0.4039  data: 0.3227  max mem: 292\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:03  Loss: 0.9517 (0.8834)  Acc@1: 79.1667 (78.7879)  Acc@5: 95.8333 (96.5909)  time: 0.1143  data: 0.0306  max mem: 292\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 0.9229 (0.9297)  Acc@1: 79.1667 (78.3730)  Acc@5: 95.8333 (96.0317)  time: 0.0830  data: 0.0011  max mem: 292\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.8790 (0.9240)  Acc@1: 79.1667 (78.4946)  Acc@5: 95.8333 (96.2366)  time: 0.0822  data: 0.0005  max mem: 292\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.8061 (0.9071)  Acc@1: 79.1667 (78.2520)  Acc@5: 100.0000 (96.6463)  time: 0.0840  data: 0.0003  max mem: 292\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.7923 (0.9002)  Acc@1: 79.1667 (78.5000)  Acc@5: 100.0000 (96.7000)  time: 0.0825  data: 0.0003  max mem: 292\n",
            "Test: [Task 2] Total time: 0:00:03 (0.0930 s / it)\n",
            "* Acc@1 78.500 Acc@5 96.700 loss 0.900\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:22  Loss: 0.7166 (0.7166)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.5428  data: 0.4591  max mem: 292\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:04  Loss: 0.9061 (0.9075)  Acc@1: 79.1667 (81.0606)  Acc@5: 95.8333 (96.2121)  time: 0.1392  data: 0.0561  max mem: 292\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 0.8609 (0.8618)  Acc@1: 83.3333 (81.7460)  Acc@5: 95.8333 (96.6270)  time: 0.0947  data: 0.0108  max mem: 292\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.8169 (0.8321)  Acc@1: 79.1667 (81.0484)  Acc@5: 95.8333 (97.0430)  time: 0.0871  data: 0.0039  max mem: 292\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.8342 (0.8429)  Acc@1: 79.1667 (81.3008)  Acc@5: 95.8333 (96.7480)  time: 0.0837  data: 0.0014  max mem: 292\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.8342 (0.8489)  Acc@1: 79.1667 (81.1000)  Acc@5: 95.8333 (96.8000)  time: 0.0822  data: 0.0013  max mem: 292\n",
            "Test: [Task 3] Total time: 0:00:04 (0.1013 s / it)\n",
            "* Acc@1 81.100 Acc@5 96.800 loss 0.849\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:13  Loss: 1.3478 (1.3478)  Acc@1: 62.5000 (62.5000)  Acc@5: 87.5000 (87.5000)  time: 0.3270  data: 0.2489  max mem: 292\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:03  Loss: 0.8944 (0.9112)  Acc@1: 79.1667 (80.3030)  Acc@5: 95.8333 (96.9697)  time: 0.1113  data: 0.0324  max mem: 292\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 0.8944 (0.9083)  Acc@1: 79.1667 (80.7540)  Acc@5: 100.0000 (97.2222)  time: 0.0870  data: 0.0056  max mem: 292\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 0.8347 (0.8816)  Acc@1: 83.3333 (81.1828)  Acc@5: 95.8333 (97.1774)  time: 0.0838  data: 0.0005  max mem: 292\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.8104 (0.8983)  Acc@1: 79.1667 (80.7927)  Acc@5: 95.8333 (96.7480)  time: 0.0833  data: 0.0005  max mem: 292\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.8075 (0.8922)  Acc@1: 83.3333 (81.0000)  Acc@5: 95.8333 (96.8000)  time: 0.0819  data: 0.0005  max mem: 292\n",
            "Test: [Task 4] Total time: 0:00:03 (0.0919 s / it)\n",
            "* Acc@1 81.000 Acc@5 96.800 loss 0.892\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:21  Loss: 0.9310 (0.9310)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.5115  data: 0.4276  max mem: 292\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:03  Loss: 1.1019 (1.0770)  Acc@1: 70.8333 (73.8636)  Acc@5: 95.8333 (97.3485)  time: 0.1214  data: 0.0393  max mem: 292\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 1.1072 (1.0976)  Acc@1: 70.8333 (73.6111)  Acc@5: 95.8333 (96.6270)  time: 0.0827  data: 0.0004  max mem: 292\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 1.1133 (1.1154)  Acc@1: 75.0000 (73.7903)  Acc@5: 95.8333 (96.3710)  time: 0.0840  data: 0.0003  max mem: 292\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 1.2337 (1.1714)  Acc@1: 70.8333 (71.5447)  Acc@5: 95.8333 (95.5285)  time: 0.0848  data: 0.0003  max mem: 292\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 1.2506 (1.1857)  Acc@1: 66.6667 (71.2000)  Acc@5: 95.8333 (95.3000)  time: 0.0832  data: 0.0003  max mem: 292\n",
            "Test: [Task 5] Total time: 0:00:03 (0.0950 s / it)\n",
            "* Acc@1 71.200 Acc@5 95.300 loss 1.186\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:21  Loss: 4.7508 (4.7508)  Acc@1: 0.0000 (0.0000)  Acc@5: 4.1667 (4.1667)  time: 0.5183  data: 0.4412  max mem: 292\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:03  Loss: 4.8644 (4.8520)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.3788)  time: 0.1248  data: 0.0416  max mem: 292\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 4.8085 (4.8149)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (2.1825)  time: 0.0840  data: 0.0012  max mem: 292\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 4.7888 (4.8244)  Acc@1: 0.0000 (0.0000)  Acc@5: 4.1667 (2.4194)  time: 0.0841  data: 0.0008  max mem: 292\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 4.8159 (4.8261)  Acc@1: 0.0000 (0.0000)  Acc@5: 4.1667 (2.6423)  time: 0.0850  data: 0.0008  max mem: 292\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 4.7960 (4.8254)  Acc@1: 0.0000 (0.0000)  Acc@5: 4.1667 (2.7000)  time: 0.0831  data: 0.0008  max mem: 292\n",
            "Test: [Task 6] Total time: 0:00:04 (0.0969 s / it)\n",
            "* Acc@1 0.000 Acc@5 2.700 loss 4.825\n",
            "[Average accuracy till task6]\tAcc@1: 65.7833\tAcc@5: 81.1000\tLoss: 1.5637\tForgetting: 2.0200\tBackward: 56.1400\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([14328, 384])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 1.0509  Acc@1: 79.1667 (78.2500)  Acc@5: 83.3333 (84.6667)\n",
            "torch.Size([14328, 384])\n",
            "Averaged stats: Lr: 0.000467  Loss: 0.8511  Acc@1: 79.1667 (79.5000)  Acc@5: 91.6667 (93.0000)\n",
            "torch.Size([14328, 384])\n",
            "Averaged stats: Lr: 0.000464  Loss: 1.3225  Acc@1: 83.3333 (80.5833)  Acc@5: 100.0000 (96.1667)\n",
            "torch.Size([14328, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 0.7996  Acc@1: 87.5000 (84.3333)  Acc@5: 100.0000 (98.9167)\n",
            "torch.Size([14328, 384])\n",
            "Averaged stats: Lr: 0.000448  Loss: 0.9134  Acc@1: 83.3333 (84.6667)  Acc@5: 100.0000 (99.5000)\n",
            "torch.Size([14328, 384])\n",
            "Averaged stats: Lr: 0.000437  Loss: 0.6811  Acc@1: 87.5000 (87.5833)  Acc@5: 100.0000 (99.6667)\n",
            "torch.Size([14328, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 0.7451  Acc@1: 87.5000 (89.5833)  Acc@5: 100.0000 (99.7500)\n",
            "torch.Size([14328, 384])\n",
            "Averaged stats: Lr: 0.000409  Loss: 0.5929  Acc@1: 91.6667 (90.1667)  Acc@5: 100.0000 (99.8333)\n",
            "torch.Size([14328, 384])\n",
            "Averaged stats: Lr: 0.000391  Loss: 0.7771  Acc@1: 87.5000 (90.6667)  Acc@5: 100.0000 (99.9167)\n",
            "torch.Size([14328, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 0.6092  Acc@1: 91.6667 (89.8333)  Acc@5: 100.0000 (99.8333)\n",
            "torch.Size([14328, 384])\n",
            "Averaged stats: Lr: 0.000352  Loss: 0.4539  Acc@1: 91.6667 (91.5833)  Acc@5: 100.0000 (99.5000)\n",
            "torch.Size([14328, 384])\n",
            "Averaged stats: Lr: 0.000330  Loss: 0.5737  Acc@1: 87.5000 (90.8333)  Acc@5: 100.0000 (99.8333)\n",
            "torch.Size([14328, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 0.3973  Acc@1: 95.8333 (92.9167)  Acc@5: 100.0000 (99.9167)\n",
            "torch.Size([14328, 384])\n",
            "Averaged stats: Lr: 0.000283  Loss: 0.4633  Acc@1: 91.6667 (91.5000)  Acc@5: 100.0000 (99.9167)\n",
            "torch.Size([14328, 384])\n",
            "Averaged stats: Lr: 0.000259  Loss: 0.4249  Acc@1: 91.6667 (91.1667)  Acc@5: 100.0000 (99.9167)\n",
            "torch.Size([14328, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 0.3953  Acc@1: 95.8333 (92.0833)  Acc@5: 100.0000 (99.9167)\n",
            "torch.Size([14328, 384])\n",
            "Averaged stats: Lr: 0.000210  Loss: 0.3530  Acc@1: 91.6667 (92.2500)  Acc@5: 100.0000 (99.8333)\n",
            "torch.Size([14328, 384])\n",
            "Averaged stats: Lr: 0.000186  Loss: 0.3143  Acc@1: 91.6667 (92.0000)  Acc@5: 100.0000 (99.9167)\n",
            "torch.Size([14328, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.5993  Acc@1: 87.5000 (91.2500)  Acc@5: 100.0000 (99.8333)\n",
            "torch.Size([14328, 384])\n",
            "Averaged stats: Lr: 0.000139  Loss: 0.3936  Acc@1: 95.8333 (93.3333)  Acc@5: 100.0000 (99.9167)\n",
            "torch.Size([14328, 384])\n",
            "Averaged stats: Lr: 0.000117  Loss: 0.5359  Acc@1: 91.6667 (92.5000)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([14328, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 0.3270  Acc@1: 95.8333 (92.9167)  Acc@5: 100.0000 (99.9167)\n",
            "torch.Size([14328, 384])\n",
            "Averaged stats: Lr: 0.000078  Loss: 0.4606  Acc@1: 91.6667 (92.4167)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([14328, 384])\n",
            "Averaged stats: Lr: 0.000060  Loss: 0.3681  Acc@1: 91.6667 (91.4167)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([14328, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 0.4049  Acc@1: 95.8333 (93.1667)  Acc@5: 100.0000 (99.9167)\n",
            "torch.Size([14328, 384])\n",
            "Averaged stats: Lr: 0.000031  Loss: 0.4370  Acc@1: 91.6667 (93.8333)  Acc@5: 100.0000 (99.6667)\n",
            "torch.Size([14328, 384])\n",
            "Averaged stats: Lr: 0.000020  Loss: 0.3161  Acc@1: 91.6667 (92.4167)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([14328, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.4036  Acc@1: 91.6667 (92.3333)  Acc@5: 100.0000 (99.9167)\n",
            "torch.Size([14328, 384])\n",
            "Averaged stats: Lr: 0.000005  Loss: 0.6398  Acc@1: 95.8333 (93.0833)  Acc@5: 100.0000 (99.8333)\n",
            "torch.Size([14328, 384])\n",
            "Averaged stats: Lr: 0.000001  Loss: 0.5283  Acc@1: 95.8333 (94.1667)  Acc@5: 100.0000 (99.5000)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:12  Loss: 0.7831 (0.7831)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  time: 0.3077  data: 0.2378  max mem: 293\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 0.7831 (0.7804)  Acc@1: 79.1667 (78.0303)  Acc@5: 95.8333 (96.9697)  time: 0.1148  data: 0.0382  max mem: 293\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.7440 (0.7618)  Acc@1: 83.3333 (79.3651)  Acc@5: 95.8333 (96.8254)  time: 0.0892  data: 0.0094  max mem: 293\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.6658 (0.7323)  Acc@1: 83.3333 (80.7796)  Acc@5: 100.0000 (97.4462)  time: 0.0826  data: 0.0005  max mem: 293\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6588 (0.7110)  Acc@1: 83.3333 (81.6057)  Acc@5: 100.0000 (97.7642)  time: 0.0826  data: 0.0004  max mem: 293\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6170 (0.7021)  Acc@1: 83.3333 (81.8000)  Acc@5: 100.0000 (97.8000)  time: 0.0813  data: 0.0003  max mem: 293\n",
            "Test: [Task 1] Total time: 0:00:03 (0.0922 s / it)\n",
            "* Acc@1 81.800 Acc@5 97.800 loss 0.702\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:15  Loss: 0.8913 (0.8913)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  time: 0.3608  data: 0.2868  max mem: 293\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:03  Loss: 0.8913 (0.8408)  Acc@1: 79.1667 (78.4091)  Acc@5: 95.8333 (97.3485)  time: 0.1104  data: 0.0312  max mem: 293\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 0.8657 (0.8979)  Acc@1: 75.0000 (77.1825)  Acc@5: 95.8333 (96.0317)  time: 0.0835  data: 0.0031  max mem: 293\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.8614 (0.8929)  Acc@1: 75.0000 (77.2849)  Acc@5: 95.8333 (96.2366)  time: 0.0823  data: 0.0005  max mem: 293\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.7660 (0.8676)  Acc@1: 75.0000 (77.7439)  Acc@5: 95.8333 (96.4431)  time: 0.0829  data: 0.0004  max mem: 293\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.7432 (0.8617)  Acc@1: 79.1667 (78.0000)  Acc@5: 100.0000 (96.5000)  time: 0.0814  data: 0.0004  max mem: 293\n",
            "Test: [Task 2] Total time: 0:00:03 (0.0910 s / it)\n",
            "* Acc@1 78.000 Acc@5 96.500 loss 0.862\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:14  Loss: 0.5894 (0.5894)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.3563  data: 0.2834  max mem: 293\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 0.7774 (0.7859)  Acc@1: 79.1667 (80.3030)  Acc@5: 95.8333 (96.5909)  time: 0.1086  data: 0.0275  max mem: 293\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 0.7632 (0.7346)  Acc@1: 83.3333 (82.1429)  Acc@5: 95.8333 (96.8254)  time: 0.0826  data: 0.0012  max mem: 293\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.6763 (0.7107)  Acc@1: 83.3333 (81.5860)  Acc@5: 95.8333 (97.0430)  time: 0.0824  data: 0.0005  max mem: 293\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.6937 (0.7170)  Acc@1: 83.3333 (82.3171)  Acc@5: 95.8333 (96.7480)  time: 0.0828  data: 0.0004  max mem: 293\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.6937 (0.7215)  Acc@1: 83.3333 (82.0000)  Acc@5: 95.8333 (96.8000)  time: 0.0814  data: 0.0004  max mem: 293\n",
            "Test: [Task 3] Total time: 0:00:03 (0.0920 s / it)\n",
            "* Acc@1 82.000 Acc@5 96.800 loss 0.722\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:29  Loss: 1.2081 (1.2081)  Acc@1: 66.6667 (66.6667)  Acc@5: 87.5000 (87.5000)  time: 0.7143  data: 0.5950  max mem: 293\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:04  Loss: 0.8247 (0.8261)  Acc@1: 79.1667 (79.5455)  Acc@5: 95.8333 (96.5909)  time: 0.1404  data: 0.0559  max mem: 293\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 0.7976 (0.8108)  Acc@1: 79.1667 (79.5635)  Acc@5: 95.8333 (96.8254)  time: 0.0822  data: 0.0013  max mem: 293\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 0.7415 (0.7797)  Acc@1: 79.1667 (79.9731)  Acc@5: 95.8333 (96.9086)  time: 0.0817  data: 0.0005  max mem: 293\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.6942 (0.7955)  Acc@1: 79.1667 (79.6748)  Acc@5: 95.8333 (96.8496)  time: 0.0824  data: 0.0003  max mem: 293\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.6759 (0.7880)  Acc@1: 79.1667 (80.0000)  Acc@5: 95.8333 (96.9000)  time: 0.0808  data: 0.0002  max mem: 293\n",
            "Test: [Task 4] Total time: 0:00:04 (0.0985 s / it)\n",
            "* Acc@1 80.000 Acc@5 96.900 loss 0.788\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:14  Loss: 0.8031 (0.8031)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.3438  data: 0.2730  max mem: 293\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:03  Loss: 0.8896 (0.8940)  Acc@1: 75.0000 (77.6515)  Acc@5: 100.0000 (98.4848)  time: 0.1099  data: 0.0321  max mem: 293\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 0.9159 (0.9174)  Acc@1: 75.0000 (76.1905)  Acc@5: 95.8333 (97.2222)  time: 0.0844  data: 0.0043  max mem: 293\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 0.9159 (0.9233)  Acc@1: 79.1667 (77.4194)  Acc@5: 95.8333 (96.7742)  time: 0.0824  data: 0.0005  max mem: 293\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.9944 (0.9687)  Acc@1: 75.0000 (76.3211)  Acc@5: 95.8333 (96.2398)  time: 0.0828  data: 0.0003  max mem: 293\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 1.0094 (0.9823)  Acc@1: 75.0000 (76.2000)  Acc@5: 95.8333 (96.0000)  time: 0.0814  data: 0.0003  max mem: 293\n",
            "Test: [Task 5] Total time: 0:00:03 (0.0909 s / it)\n",
            "* Acc@1 76.200 Acc@5 96.000 loss 0.982\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:13  Loss: 0.9069 (0.9069)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.3291  data: 0.2607  max mem: 293\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:03  Loss: 1.1005 (1.1129)  Acc@1: 70.8333 (71.5909)  Acc@5: 95.8333 (95.8333)  time: 0.1123  data: 0.0341  max mem: 293\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 1.1061 (1.1390)  Acc@1: 70.8333 (69.2460)  Acc@5: 95.8333 (96.0317)  time: 0.0875  data: 0.0059  max mem: 293\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 1.1991 (1.1571)  Acc@1: 66.6667 (67.8763)  Acc@5: 95.8333 (96.3710)  time: 0.0840  data: 0.0004  max mem: 293\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.1753 (1.1629)  Acc@1: 66.6667 (67.9878)  Acc@5: 95.8333 (96.2398)  time: 0.0835  data: 0.0003  max mem: 293\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.1753 (1.1632)  Acc@1: 66.6667 (67.9000)  Acc@5: 95.8333 (96.3000)  time: 0.0821  data: 0.0003  max mem: 293\n",
            "Test: [Task 6] Total time: 0:00:03 (0.0929 s / it)\n",
            "* Acc@1 67.900 Acc@5 96.300 loss 1.163\n",
            "[Average accuracy till task6]\tAcc@1: 77.6500\tAcc@5: 96.7167\tLoss: 0.8698\tForgetting: 2.4200\tBackward: 7.5600\n",
            "Train: Epoch[1/1]  [  0/105]  eta: 0:01:30  Lr: 0.000047  Loss: 2.5292  Acc@1: 0.0000 (0.0000)  Acc@5: 54.1667 (54.1667)  time: 0.8572  data: 0.7438  max mem: 293\n",
            "Train: Epoch[1/1]  [ 10/105]  eta: 0:00:15  Lr: 0.000047  Loss: 2.3955  Acc@1: 8.3333 (8.3333)  Acc@5: 45.8333 (46.2121)  time: 0.1683  data: 0.0706  max mem: 293\n",
            "Train: Epoch[1/1]  [ 20/105]  eta: 0:00:12  Lr: 0.000047  Loss: 2.1925  Acc@1: 12.5000 (11.3095)  Acc@5: 45.8333 (48.8095)  time: 0.1065  data: 0.0078  max mem: 293\n",
            "Train: Epoch[1/1]  [ 30/105]  eta: 0:00:09  Lr: 0.000047  Loss: 2.1124  Acc@1: 16.6667 (16.3979)  Acc@5: 66.6667 (57.2581)  time: 0.1091  data: 0.0067  max mem: 293\n",
            "Train: Epoch[1/1]  [ 40/105]  eta: 0:00:07  Lr: 0.000047  Loss: 1.9120  Acc@1: 29.1667 (20.4268)  Acc@5: 75.0000 (62.1951)  time: 0.0942  data: 0.0011  max mem: 293\n",
            "Train: Epoch[1/1]  [ 50/105]  eta: 0:00:06  Lr: 0.000047  Loss: 1.9319  Acc@1: 29.1667 (23.1209)  Acc@5: 79.1667 (66.1765)  time: 0.0842  data: 0.0007  max mem: 293\n",
            "Train: Epoch[1/1]  [ 60/105]  eta: 0:00:04  Lr: 0.000047  Loss: 1.6510  Acc@1: 37.5000 (27.3224)  Acc@5: 87.5000 (69.8771)  time: 0.0852  data: 0.0007  max mem: 293\n",
            "Train: Epoch[1/1]  [ 70/105]  eta: 0:00:03  Lr: 0.000047  Loss: 1.8116  Acc@1: 45.8333 (29.4601)  Acc@5: 87.5000 (72.5352)  time: 0.0850  data: 0.0010  max mem: 293\n",
            "Train: Epoch[1/1]  [ 80/105]  eta: 0:00:02  Lr: 0.000047  Loss: 1.6886  Acc@1: 50.0000 (32.7161)  Acc@5: 91.6667 (75.3086)  time: 0.0847  data: 0.0012  max mem: 293\n",
            "Train: Epoch[1/1]  [ 90/105]  eta: 0:00:01  Lr: 0.000047  Loss: 1.6457  Acc@1: 58.3333 (35.3938)  Acc@5: 95.8333 (77.3810)  time: 0.0849  data: 0.0012  max mem: 293\n",
            "Train: Epoch[1/1]  [100/105]  eta: 0:00:00  Lr: 0.000047  Loss: 1.4670  Acc@1: 58.3333 (37.8713)  Acc@5: 95.8333 (79.2079)  time: 0.0848  data: 0.0010  max mem: 293\n",
            "Train: Epoch[1/1]  [104/105]  eta: 0:00:00  Lr: 0.000047  Loss: 2.3376  Acc@1: 58.3333 (38.5663)  Acc@5: 95.8333 (79.4553)  time: 0.0822  data: 0.0008  max mem: 293\n",
            "Train: Epoch[1/1] Total time: 0:00:10 (0.0990 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 2.3376  Acc@1: 58.3333 (38.5663)  Acc@5: 95.8333 (79.4553)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:16  Loss: 0.8297 (0.8297)  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.8333)  time: 0.3821  data: 0.3105  max mem: 293\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 0.8297 (0.8063)  Acc@1: 75.0000 (76.8939)  Acc@5: 95.8333 (96.9697)  time: 0.1269  data: 0.0473  max mem: 293\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.7499 (0.7906)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (96.8254)  time: 0.0928  data: 0.0106  max mem: 293\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.7187 (0.7637)  Acc@1: 83.3333 (80.3763)  Acc@5: 95.8333 (97.1774)  time: 0.0843  data: 0.0003  max mem: 293\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.7016 (0.7414)  Acc@1: 83.3333 (81.1992)  Acc@5: 100.0000 (97.7642)  time: 0.0837  data: 0.0004  max mem: 293\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6449 (0.7321)  Acc@1: 83.3333 (81.4000)  Acc@5: 100.0000 (97.8000)  time: 0.0824  data: 0.0004  max mem: 293\n",
            "Test: [Task 1] Total time: 0:00:04 (0.0975 s / it)\n",
            "* Acc@1 81.400 Acc@5 97.800 loss 0.732\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:29  Loss: 0.9050 (0.9050)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  time: 0.7048  data: 0.6063  max mem: 293\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 0.9050 (0.8748)  Acc@1: 79.1667 (77.6515)  Acc@5: 100.0000 (96.9697)  time: 0.1433  data: 0.0581  max mem: 293\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 0.8792 (0.9310)  Acc@1: 75.0000 (76.9841)  Acc@5: 95.8333 (95.4365)  time: 0.0887  data: 0.0061  max mem: 293\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.8792 (0.9274)  Acc@1: 75.0000 (76.7473)  Acc@5: 95.8333 (95.5645)  time: 0.0874  data: 0.0049  max mem: 293\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.8008 (0.9062)  Acc@1: 75.0000 (76.9309)  Acc@5: 95.8333 (95.7317)  time: 0.0846  data: 0.0007  max mem: 293\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.7850 (0.8995)  Acc@1: 79.1667 (77.2000)  Acc@5: 95.8333 (95.8000)  time: 0.0830  data: 0.0007  max mem: 293\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1026 s / it)\n",
            "* Acc@1 77.200 Acc@5 95.800 loss 0.899\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:16  Loss: 0.5937 (0.5937)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.3915  data: 0.3154  max mem: 293\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 0.7848 (0.8005)  Acc@1: 83.3333 (80.6818)  Acc@5: 95.8333 (96.9697)  time: 0.1133  data: 0.0299  max mem: 293\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 0.7848 (0.7618)  Acc@1: 79.1667 (81.3492)  Acc@5: 95.8333 (97.0238)  time: 0.0838  data: 0.0010  max mem: 293\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.7134 (0.7383)  Acc@1: 79.1667 (80.7796)  Acc@5: 95.8333 (97.3118)  time: 0.0836  data: 0.0005  max mem: 293\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.7271 (0.7445)  Acc@1: 83.3333 (81.7073)  Acc@5: 95.8333 (96.4431)  time: 0.0848  data: 0.0003  max mem: 293\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.7271 (0.7468)  Acc@1: 83.3333 (81.6000)  Acc@5: 95.8333 (96.5000)  time: 0.0829  data: 0.0003  max mem: 293\n",
            "Test: [Task 3] Total time: 0:00:03 (0.0928 s / it)\n",
            "* Acc@1 81.600 Acc@5 96.500 loss 0.747\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:12  Loss: 1.2780 (1.2780)  Acc@1: 62.5000 (62.5000)  Acc@5: 91.6667 (91.6667)  time: 0.3025  data: 0.2337  max mem: 293\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:03  Loss: 0.8255 (0.8312)  Acc@1: 79.1667 (79.5455)  Acc@5: 95.8333 (97.3485)  time: 0.1145  data: 0.0366  max mem: 293\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 0.8255 (0.8221)  Acc@1: 79.1667 (78.9683)  Acc@5: 95.8333 (96.8254)  time: 0.0906  data: 0.0092  max mem: 293\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 0.7432 (0.7934)  Acc@1: 79.1667 (79.7043)  Acc@5: 95.8333 (96.9086)  time: 0.0855  data: 0.0014  max mem: 293\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7432 (0.8104)  Acc@1: 79.1667 (79.4715)  Acc@5: 95.8333 (96.4431)  time: 0.0852  data: 0.0008  max mem: 293\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.6990 (0.8033)  Acc@1: 79.1667 (79.8000)  Acc@5: 95.8333 (96.5000)  time: 0.0837  data: 0.0005  max mem: 293\n",
            "Test: [Task 4] Total time: 0:00:03 (0.0952 s / it)\n",
            "* Acc@1 79.800 Acc@5 96.500 loss 0.803\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:25  Loss: 0.8206 (0.8206)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  time: 0.6099  data: 0.5184  max mem: 293\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:04  Loss: 0.9090 (0.8901)  Acc@1: 75.0000 (77.6515)  Acc@5: 100.0000 (98.4848)  time: 0.1373  data: 0.0484  max mem: 293\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 0.9218 (0.9170)  Acc@1: 70.8333 (75.3968)  Acc@5: 100.0000 (97.2222)  time: 0.0875  data: 0.0018  max mem: 293\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 0.9262 (0.9224)  Acc@1: 79.1667 (76.0753)  Acc@5: 95.8333 (97.1774)  time: 0.0850  data: 0.0013  max mem: 293\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.9919 (0.9648)  Acc@1: 75.0000 (75.3049)  Acc@5: 95.8333 (96.4431)  time: 0.0854  data: 0.0023  max mem: 293\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 1.0211 (0.9772)  Acc@1: 75.0000 (75.2000)  Acc@5: 95.8333 (96.3000)  time: 0.0840  data: 0.0023  max mem: 293\n",
            "Test: [Task 5] Total time: 0:00:04 (0.1013 s / it)\n",
            "* Acc@1 75.200 Acc@5 96.300 loss 0.977\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:27  Loss: 0.9303 (0.9303)  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  time: 0.6575  data: 0.5729  max mem: 293\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:04  Loss: 1.1993 (1.1974)  Acc@1: 66.6667 (67.8030)  Acc@5: 95.8333 (95.0758)  time: 0.1378  data: 0.0535  max mem: 293\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 1.1993 (1.2255)  Acc@1: 66.6667 (66.6667)  Acc@5: 95.8333 (95.0397)  time: 0.0839  data: 0.0010  max mem: 293\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 1.2671 (1.2463)  Acc@1: 62.5000 (64.6505)  Acc@5: 91.6667 (94.8925)  time: 0.0838  data: 0.0004  max mem: 293\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.2506 (1.2469)  Acc@1: 62.5000 (64.9390)  Acc@5: 95.8333 (95.2236)  time: 0.0856  data: 0.0004  max mem: 293\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.2428 (1.2468)  Acc@1: 62.5000 (64.9000)  Acc@5: 95.8333 (95.3000)  time: 0.0837  data: 0.0004  max mem: 293\n",
            "Test: [Task 6] Total time: 0:00:04 (0.0998 s / it)\n",
            "* Acc@1 64.900 Acc@5 95.300 loss 1.247\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:19  Loss: 5.0038 (5.0038)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  time: 0.4623  data: 0.3700  max mem: 293\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:03  Loss: 4.9228 (4.9285)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (1.5152)  time: 0.1179  data: 0.0341  max mem: 293\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:02  Loss: 4.8802 (4.9082)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (2.3810)  time: 0.0829  data: 0.0005  max mem: 293\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:01  Loss: 4.8468 (4.8951)  Acc@1: 0.0000 (0.0000)  Acc@5: 4.1667 (2.6882)  time: 0.0836  data: 0.0005  max mem: 293\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 4.8989 (4.9002)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (2.2358)  time: 0.0848  data: 0.0004  max mem: 293\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 4.8989 (4.8896)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (2.4000)  time: 0.0831  data: 0.0004  max mem: 293\n",
            "Test: [Task 7] Total time: 0:00:03 (0.0940 s / it)\n",
            "* Acc@1 0.000 Acc@5 2.400 loss 4.890\n",
            "[Average accuracy till task7]\tAcc@1: 65.7286\tAcc@5: 82.9429\tLoss: 1.4708\tForgetting: 2.2667\tBackward: 57.6833\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([16728, 384])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 0.8280  Acc@1: 83.3333 (80.5556)  Acc@5: 87.5000 (88.2639)\n",
            "torch.Size([16728, 384])\n",
            "Averaged stats: Lr: 0.000467  Loss: 1.0907  Acc@1: 79.1667 (79.6528)  Acc@5: 91.6667 (91.1111)\n",
            "torch.Size([16728, 384])\n",
            "Averaged stats: Lr: 0.000464  Loss: 0.8558  Acc@1: 83.3333 (81.8750)  Acc@5: 95.8333 (95.1389)\n",
            "torch.Size([16728, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 0.8297  Acc@1: 83.3333 (84.2361)  Acc@5: 100.0000 (97.7083)\n",
            "torch.Size([16728, 384])\n",
            "Averaged stats: Lr: 0.000448  Loss: 0.8527  Acc@1: 91.6667 (87.5000)  Acc@5: 100.0000 (98.5417)\n",
            "torch.Size([16728, 384])\n",
            "Averaged stats: Lr: 0.000437  Loss: 0.6737  Acc@1: 87.5000 (88.1250)  Acc@5: 100.0000 (99.3056)\n",
            "torch.Size([16728, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 0.7415  Acc@1: 91.6667 (89.0278)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([16728, 384])\n",
            "Averaged stats: Lr: 0.000409  Loss: 0.4307  Acc@1: 91.6667 (90.9722)  Acc@5: 100.0000 (99.2361)\n",
            "torch.Size([16728, 384])\n",
            "Averaged stats: Lr: 0.000391  Loss: 0.5448  Acc@1: 91.6667 (91.2500)  Acc@5: 100.0000 (99.2361)\n",
            "torch.Size([16728, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 0.4219  Acc@1: 91.6667 (91.5972)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([16728, 384])\n",
            "Averaged stats: Lr: 0.000352  Loss: 0.6190  Acc@1: 91.6667 (91.4583)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([16728, 384])\n",
            "Averaged stats: Lr: 0.000330  Loss: 0.6391  Acc@1: 91.6667 (92.4306)  Acc@5: 100.0000 (99.8611)\n",
            "torch.Size([16728, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 0.4028  Acc@1: 91.6667 (92.0139)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([16728, 384])\n",
            "Averaged stats: Lr: 0.000283  Loss: 0.4641  Acc@1: 91.6667 (91.8056)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([16728, 384])\n",
            "Averaged stats: Lr: 0.000259  Loss: 0.4628  Acc@1: 95.8333 (92.8472)  Acc@5: 100.0000 (99.4444)\n",
            "torch.Size([16728, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 0.4346  Acc@1: 95.8333 (92.4306)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([16728, 384])\n",
            "Averaged stats: Lr: 0.000210  Loss: 0.5137  Acc@1: 91.6667 (91.5972)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([16728, 384])\n",
            "Averaged stats: Lr: 0.000186  Loss: 0.3368  Acc@1: 91.6667 (93.0556)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([16728, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.3217  Acc@1: 95.8333 (94.3056)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([16728, 384])\n",
            "Averaged stats: Lr: 0.000139  Loss: 0.4632  Acc@1: 91.6667 (92.7083)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([16728, 384])\n",
            "Averaged stats: Lr: 0.000117  Loss: 0.3327  Acc@1: 91.6667 (93.1250)  Acc@5: 100.0000 (99.7222)\n",
            "torch.Size([16728, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 0.5059  Acc@1: 91.6667 (92.6389)  Acc@5: 100.0000 (99.6528)\n",
            "torch.Size([16728, 384])\n",
            "Averaged stats: Lr: 0.000078  Loss: 0.5349  Acc@1: 91.6667 (93.6806)  Acc@5: 100.0000 (99.6528)\n",
            "torch.Size([16728, 384])\n",
            "Averaged stats: Lr: 0.000060  Loss: 0.4956  Acc@1: 95.8333 (93.1944)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([16728, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 0.6184  Acc@1: 91.6667 (93.3333)  Acc@5: 100.0000 (99.9306)\n",
            "torch.Size([16728, 384])\n",
            "Averaged stats: Lr: 0.000031  Loss: 0.5280  Acc@1: 95.8333 (94.9306)  Acc@5: 100.0000 (99.6528)\n",
            "torch.Size([16728, 384])\n",
            "Averaged stats: Lr: 0.000020  Loss: 0.4717  Acc@1: 91.6667 (93.7500)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([16728, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.4946  Acc@1: 91.6667 (91.1806)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([16728, 384])\n",
            "Averaged stats: Lr: 0.000005  Loss: 0.5811  Acc@1: 95.8333 (93.7500)  Acc@5: 100.0000 (99.9306)\n",
            "torch.Size([16728, 384])\n",
            "Averaged stats: Lr: 0.000001  Loss: 0.3532  Acc@1: 95.8333 (94.0972)  Acc@5: 100.0000 (99.7222)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:14  Loss: 0.7944 (0.7944)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  time: 0.3349  data: 0.2644  max mem: 294\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 0.7912 (0.7777)  Acc@1: 75.0000 (76.1364)  Acc@5: 100.0000 (96.9697)  time: 0.1065  data: 0.0277  max mem: 294\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.7275 (0.7609)  Acc@1: 79.1667 (78.9683)  Acc@5: 95.8333 (96.4286)  time: 0.0824  data: 0.0023  max mem: 294\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.6692 (0.7284)  Acc@1: 83.3333 (79.7043)  Acc@5: 95.8333 (97.0430)  time: 0.0819  data: 0.0006  max mem: 294\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6264 (0.7049)  Acc@1: 83.3333 (80.7927)  Acc@5: 100.0000 (97.3577)  time: 0.0822  data: 0.0005  max mem: 294\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6058 (0.6958)  Acc@1: 83.3333 (81.0000)  Acc@5: 100.0000 (97.4000)  time: 0.0806  data: 0.0005  max mem: 294\n",
            "Test: [Task 1] Total time: 0:00:03 (0.0893 s / it)\n",
            "* Acc@1 81.000 Acc@5 97.400 loss 0.696\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:15  Loss: 0.8535 (0.8535)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  time: 0.3630  data: 0.2903  max mem: 294\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:03  Loss: 0.8535 (0.8445)  Acc@1: 75.0000 (76.8939)  Acc@5: 100.0000 (97.3485)  time: 0.1089  data: 0.0283  max mem: 294\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 0.8425 (0.8962)  Acc@1: 75.0000 (74.8016)  Acc@5: 95.8333 (95.6349)  time: 0.0821  data: 0.0014  max mem: 294\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.8425 (0.8937)  Acc@1: 75.0000 (74.8656)  Acc@5: 95.8333 (95.8333)  time: 0.0817  data: 0.0005  max mem: 294\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.7479 (0.8680)  Acc@1: 75.0000 (75.2033)  Acc@5: 95.8333 (96.0366)  time: 0.0827  data: 0.0003  max mem: 294\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.7397 (0.8614)  Acc@1: 75.0000 (75.4000)  Acc@5: 95.8333 (96.1000)  time: 0.0813  data: 0.0003  max mem: 294\n",
            "Test: [Task 2] Total time: 0:00:03 (0.0914 s / it)\n",
            "* Acc@1 75.400 Acc@5 96.100 loss 0.861\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:31  Loss: 0.6054 (0.6054)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.7415  data: 0.6413  max mem: 294\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:04  Loss: 0.7723 (0.7846)  Acc@1: 79.1667 (78.4091)  Acc@5: 95.8333 (96.2121)  time: 0.1440  data: 0.0597  max mem: 294\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 0.7670 (0.7303)  Acc@1: 79.1667 (80.7540)  Acc@5: 95.8333 (96.0317)  time: 0.0868  data: 0.0052  max mem: 294\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.6410 (0.7070)  Acc@1: 79.1667 (80.3763)  Acc@5: 95.8333 (96.6398)  time: 0.0877  data: 0.0067  max mem: 294\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.7005 (0.7153)  Acc@1: 83.3333 (81.0976)  Acc@5: 95.8333 (96.2398)  time: 0.0840  data: 0.0024  max mem: 294\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.7005 (0.7177)  Acc@1: 83.3333 (80.9000)  Acc@5: 95.8333 (96.3000)  time: 0.0824  data: 0.0022  max mem: 294\n",
            "Test: [Task 3] Total time: 0:00:04 (0.1023 s / it)\n",
            "* Acc@1 80.900 Acc@5 96.300 loss 0.718\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:16  Loss: 1.1713 (1.1713)  Acc@1: 66.6667 (66.6667)  Acc@5: 83.3333 (83.3333)  time: 0.3927  data: 0.3075  max mem: 294\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:03  Loss: 0.8204 (0.8201)  Acc@1: 75.0000 (78.0303)  Acc@5: 95.8333 (95.4545)  time: 0.1140  data: 0.0328  max mem: 294\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 0.7937 (0.8037)  Acc@1: 79.1667 (78.5714)  Acc@5: 95.8333 (95.8333)  time: 0.0838  data: 0.0029  max mem: 294\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 0.7181 (0.7638)  Acc@1: 79.1667 (79.5699)  Acc@5: 95.8333 (96.2366)  time: 0.0822  data: 0.0006  max mem: 294\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.6936 (0.7771)  Acc@1: 79.1667 (79.3699)  Acc@5: 95.8333 (96.3415)  time: 0.0832  data: 0.0004  max mem: 294\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.6642 (0.7702)  Acc@1: 79.1667 (79.6000)  Acc@5: 95.8333 (96.4000)  time: 0.0816  data: 0.0004  max mem: 294\n",
            "Test: [Task 4] Total time: 0:00:03 (0.0920 s / it)\n",
            "* Acc@1 79.600 Acc@5 96.400 loss 0.770\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:16  Loss: 0.6966 (0.6966)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.3897  data: 0.3119  max mem: 294\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:03  Loss: 0.7840 (0.7944)  Acc@1: 75.0000 (78.0303)  Acc@5: 100.0000 (98.4848)  time: 0.1125  data: 0.0300  max mem: 294\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 0.8154 (0.8164)  Acc@1: 75.0000 (76.5873)  Acc@5: 95.8333 (97.2222)  time: 0.0831  data: 0.0012  max mem: 294\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 0.8154 (0.8202)  Acc@1: 79.1667 (77.8226)  Acc@5: 95.8333 (96.9086)  time: 0.0824  data: 0.0006  max mem: 294\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.8865 (0.8622)  Acc@1: 79.1667 (76.8293)  Acc@5: 95.8333 (96.4431)  time: 0.0831  data: 0.0003  max mem: 294\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.8867 (0.8757)  Acc@1: 79.1667 (76.6000)  Acc@5: 95.8333 (96.3000)  time: 0.0816  data: 0.0003  max mem: 294\n",
            "Test: [Task 5] Total time: 0:00:03 (0.0915 s / it)\n",
            "* Acc@1 76.600 Acc@5 96.300 loss 0.876\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:17  Loss: 0.8454 (0.8454)  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  time: 0.4145  data: 0.3309  max mem: 294\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:03  Loss: 0.9226 (0.9614)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.4545)  time: 0.1141  data: 0.0314  max mem: 294\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 0.9351 (0.9801)  Acc@1: 75.0000 (73.6111)  Acc@5: 95.8333 (95.6349)  time: 0.0836  data: 0.0016  max mem: 294\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 1.0197 (0.9989)  Acc@1: 70.8333 (73.1183)  Acc@5: 95.8333 (95.8333)  time: 0.0836  data: 0.0014  max mem: 294\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.0367 (1.0087)  Acc@1: 70.8333 (73.2724)  Acc@5: 95.8333 (95.6301)  time: 0.0834  data: 0.0006  max mem: 294\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.0367 (1.0116)  Acc@1: 70.8333 (73.1000)  Acc@5: 95.8333 (95.6000)  time: 0.0818  data: 0.0005  max mem: 294\n",
            "Test: [Task 6] Total time: 0:00:03 (0.0940 s / it)\n",
            "* Acc@1 73.100 Acc@5 95.600 loss 1.012\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:25  Loss: 1.2391 (1.2391)  Acc@1: 70.8333 (70.8333)  Acc@5: 87.5000 (87.5000)  time: 0.6070  data: 0.5182  max mem: 294\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:04  Loss: 1.1211 (1.1766)  Acc@1: 66.6667 (65.5303)  Acc@5: 91.6667 (94.3182)  time: 0.1330  data: 0.0491  max mem: 294\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:02  Loss: 1.1192 (1.1983)  Acc@1: 66.6667 (65.2778)  Acc@5: 95.8333 (94.2460)  time: 0.0846  data: 0.0019  max mem: 294\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:01  Loss: 1.1967 (1.2245)  Acc@1: 66.6667 (65.0538)  Acc@5: 95.8333 (93.6828)  time: 0.0832  data: 0.0010  max mem: 294\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 1.1846 (1.2330)  Acc@1: 62.5000 (65.2439)  Acc@5: 91.6667 (93.4959)  time: 0.0833  data: 0.0003  max mem: 294\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 1.1636 (1.2312)  Acc@1: 62.5000 (65.3000)  Acc@5: 91.6667 (93.5000)  time: 0.0817  data: 0.0003  max mem: 294\n",
            "Test: [Task 7] Total time: 0:00:04 (0.0976 s / it)\n",
            "* Acc@1 65.300 Acc@5 93.500 loss 1.231\n",
            "[Average accuracy till task7]\tAcc@1: 75.9857\tAcc@5: 95.9429\tLoss: 0.8805\tForgetting: 2.8333\tBackward: 6.4167\n",
            "Train: Epoch[1/1]  [  0/106]  eta: 0:00:45  Lr: 0.000047  Loss: 2.2708  Acc@1: 8.3333 (8.3333)  Acc@5: 66.6667 (66.6667)  time: 0.4291  data: 0.3409  max mem: 294\n",
            "Train: Epoch[1/1]  [ 10/106]  eta: 0:00:11  Lr: 0.000047  Loss: 2.3020  Acc@1: 16.6667 (16.2879)  Acc@5: 62.5000 (60.9849)  time: 0.1242  data: 0.0362  max mem: 294\n",
            "Train: Epoch[1/1]  [ 20/106]  eta: 0:00:09  Lr: 0.000047  Loss: 2.1464  Acc@1: 20.8333 (19.4444)  Acc@5: 62.5000 (62.8968)  time: 0.0907  data: 0.0040  max mem: 294\n",
            "Train: Epoch[1/1]  [ 30/106]  eta: 0:00:07  Lr: 0.000047  Loss: 2.0135  Acc@1: 29.1667 (24.4624)  Acc@5: 75.0000 (68.6828)  time: 0.0866  data: 0.0017  max mem: 294\n",
            "Train: Epoch[1/1]  [ 40/106]  eta: 0:00:06  Lr: 0.000047  Loss: 1.9337  Acc@1: 37.5000 (28.5569)  Acc@5: 83.3333 (72.5610)  time: 0.0854  data: 0.0008  max mem: 294\n",
            "Train: Epoch[1/1]  [ 50/106]  eta: 0:00:05  Lr: 0.000047  Loss: 1.6844  Acc@1: 45.8333 (31.3725)  Acc@5: 87.5000 (75.4085)  time: 0.0847  data: 0.0007  max mem: 294\n",
            "Train: Epoch[1/1]  [ 60/106]  eta: 0:00:04  Lr: 0.000047  Loss: 1.7701  Acc@1: 45.8333 (34.6995)  Acc@5: 91.6667 (78.2104)  time: 0.0848  data: 0.0010  max mem: 294\n",
            "Train: Epoch[1/1]  [ 70/106]  eta: 0:00:03  Lr: 0.000047  Loss: 1.6120  Acc@1: 50.0000 (37.2066)  Acc@5: 91.6667 (80.1056)  time: 0.0850  data: 0.0010  max mem: 294\n",
            "Train: Epoch[1/1]  [ 80/106]  eta: 0:00:02  Lr: 0.000047  Loss: 1.4867  Acc@1: 62.5000 (40.5350)  Acc@5: 91.6667 (81.8930)  time: 0.0844  data: 0.0009  max mem: 294\n",
            "Train: Epoch[1/1]  [ 90/106]  eta: 0:00:01  Lr: 0.000047  Loss: 1.4531  Acc@1: 66.6667 (43.3150)  Acc@5: 95.8333 (83.3333)  time: 0.0882  data: 0.0014  max mem: 294\n",
            "Train: Epoch[1/1]  [100/106]  eta: 0:00:00  Lr: 0.000047  Loss: 1.1491  Acc@1: 66.6667 (45.4208)  Acc@5: 95.8333 (84.5710)  time: 0.0955  data: 0.0014  max mem: 294\n",
            "Train: Epoch[1/1]  [105/106]  eta: 0:00:00  Lr: 0.000047  Loss: 1.2007  Acc@1: 58.3333 (46.1965)  Acc@5: 95.8333 (85.0238)  time: 0.0903  data: 0.0011  max mem: 294\n",
            "Train: Epoch[1/1] Total time: 0:00:09 (0.0916 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.2007  Acc@1: 58.3333 (46.1965)  Acc@5: 95.8333 (85.0238)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:32  Loss: 0.8255 (0.8255)  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.8333)  time: 0.7825  data: 0.6512  max mem: 294\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 0.8154 (0.7909)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (96.5909)  time: 0.1532  data: 0.0636  max mem: 294\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.7982 (0.7720)  Acc@1: 79.1667 (78.9683)  Acc@5: 95.8333 (96.4286)  time: 0.0876  data: 0.0037  max mem: 294\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.7040 (0.7408)  Acc@1: 83.3333 (79.8387)  Acc@5: 95.8333 (96.7742)  time: 0.0843  data: 0.0016  max mem: 294\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6715 (0.7157)  Acc@1: 83.3333 (80.9959)  Acc@5: 100.0000 (97.0528)  time: 0.0838  data: 0.0005  max mem: 294\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6400 (0.7067)  Acc@1: 83.3333 (81.2000)  Acc@5: 100.0000 (97.1000)  time: 0.0825  data: 0.0005  max mem: 294\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1036 s / it)\n",
            "* Acc@1 81.200 Acc@5 97.100 loss 0.707\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:22  Loss: 0.8606 (0.8606)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  time: 0.5300  data: 0.4424  max mem: 294\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:03  Loss: 0.8606 (0.8914)  Acc@1: 75.0000 (74.6212)  Acc@5: 95.8333 (95.8333)  time: 0.1246  data: 0.0407  max mem: 294\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 0.8688 (0.9408)  Acc@1: 75.0000 (74.6032)  Acc@5: 95.8333 (94.6429)  time: 0.0828  data: 0.0005  max mem: 294\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.8914 (0.9413)  Acc@1: 75.0000 (74.4624)  Acc@5: 95.8333 (95.1613)  time: 0.0834  data: 0.0004  max mem: 294\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.8115 (0.9194)  Acc@1: 75.0000 (74.4919)  Acc@5: 95.8333 (95.4268)  time: 0.0850  data: 0.0003  max mem: 294\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.7987 (0.9125)  Acc@1: 75.0000 (74.7000)  Acc@5: 95.8333 (95.5000)  time: 0.0832  data: 0.0003  max mem: 294\n",
            "Test: [Task 2] Total time: 0:00:04 (0.0956 s / it)\n",
            "* Acc@1 74.700 Acc@5 95.500 loss 0.913\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:20  Loss: 0.6115 (0.6115)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  time: 0.4812  data: 0.4129  max mem: 294\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 0.7779 (0.7810)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  time: 0.1196  data: 0.0383  max mem: 294\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 0.7574 (0.7474)  Acc@1: 79.1667 (79.5635)  Acc@5: 95.8333 (95.6349)  time: 0.0830  data: 0.0007  max mem: 294\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.6785 (0.7255)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (96.1022)  time: 0.0836  data: 0.0004  max mem: 294\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.7119 (0.7366)  Acc@1: 79.1667 (80.0813)  Acc@5: 95.8333 (95.7317)  time: 0.0849  data: 0.0003  max mem: 294\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.7119 (0.7367)  Acc@1: 79.1667 (80.0000)  Acc@5: 95.8333 (95.8000)  time: 0.0831  data: 0.0003  max mem: 294\n",
            "Test: [Task 3] Total time: 0:00:03 (0.0951 s / it)\n",
            "* Acc@1 80.000 Acc@5 95.800 loss 0.737\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:30  Loss: 1.1972 (1.1972)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  time: 0.7304  data: 0.6352  max mem: 294\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:04  Loss: 0.8340 (0.8013)  Acc@1: 79.1667 (79.9242)  Acc@5: 95.8333 (96.9697)  time: 0.1455  data: 0.0621  max mem: 294\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 0.7733 (0.7885)  Acc@1: 79.1667 (79.3651)  Acc@5: 95.8333 (96.8254)  time: 0.0881  data: 0.0058  max mem: 294\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 0.7345 (0.7599)  Acc@1: 79.1667 (79.8387)  Acc@5: 95.8333 (96.7742)  time: 0.0875  data: 0.0051  max mem: 294\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7414 (0.7721)  Acc@1: 79.1667 (79.5732)  Acc@5: 95.8333 (96.5447)  time: 0.0854  data: 0.0020  max mem: 294\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.6550 (0.7651)  Acc@1: 79.1667 (79.8000)  Acc@5: 95.8333 (96.6000)  time: 0.0839  data: 0.0020  max mem: 294\n",
            "Test: [Task 4] Total time: 0:00:04 (0.1033 s / it)\n",
            "* Acc@1 79.800 Acc@5 96.600 loss 0.765\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:12  Loss: 0.7227 (0.7227)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  time: 0.2974  data: 0.2296  max mem: 294\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:03  Loss: 0.8025 (0.7932)  Acc@1: 79.1667 (78.4091)  Acc@5: 100.0000 (98.8636)  time: 0.1173  data: 0.0382  max mem: 294\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 0.8129 (0.8172)  Acc@1: 75.0000 (75.9921)  Acc@5: 100.0000 (97.6190)  time: 0.0920  data: 0.0098  max mem: 294\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 0.7966 (0.8169)  Acc@1: 79.1667 (77.2849)  Acc@5: 95.8333 (97.0430)  time: 0.0847  data: 0.0005  max mem: 294\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.8567 (0.8606)  Acc@1: 79.1667 (76.3211)  Acc@5: 95.8333 (96.5447)  time: 0.0846  data: 0.0004  max mem: 294\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.8628 (0.8735)  Acc@1: 79.1667 (76.1000)  Acc@5: 95.8333 (96.3000)  time: 0.0833  data: 0.0004  max mem: 294\n",
            "Test: [Task 5] Total time: 0:00:03 (0.0944 s / it)\n",
            "* Acc@1 76.100 Acc@5 96.300 loss 0.873\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:13  Loss: 0.8449 (0.8449)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  time: 0.3213  data: 0.2376  max mem: 294\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:03  Loss: 0.9746 (0.9980)  Acc@1: 70.8333 (73.4849)  Acc@5: 95.8333 (94.6970)  time: 0.1154  data: 0.0355  max mem: 294\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 0.9759 (1.0212)  Acc@1: 70.8333 (72.6191)  Acc@5: 95.8333 (95.0397)  time: 0.0897  data: 0.0078  max mem: 294\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 1.0743 (1.0457)  Acc@1: 70.8333 (71.1022)  Acc@5: 95.8333 (95.2957)  time: 0.0849  data: 0.0004  max mem: 294\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.0743 (1.0509)  Acc@1: 70.8333 (71.4431)  Acc@5: 95.8333 (95.2236)  time: 0.0851  data: 0.0004  max mem: 294\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.0743 (1.0533)  Acc@1: 70.8333 (71.2000)  Acc@5: 95.8333 (95.2000)  time: 0.0839  data: 0.0004  max mem: 294\n",
            "Test: [Task 6] Total time: 0:00:03 (0.0943 s / it)\n",
            "* Acc@1 71.200 Acc@5 95.200 loss 1.053\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:18  Loss: 1.4155 (1.4155)  Acc@1: 58.3333 (58.3333)  Acc@5: 87.5000 (87.5000)  time: 0.4481  data: 0.3688  max mem: 294\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:03  Loss: 1.2732 (1.3410)  Acc@1: 62.5000 (62.8788)  Acc@5: 91.6667 (93.1818)  time: 0.1184  data: 0.0349  max mem: 294\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:02  Loss: 1.2731 (1.3503)  Acc@1: 62.5000 (62.1032)  Acc@5: 91.6667 (91.6667)  time: 0.0847  data: 0.0019  max mem: 294\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:01  Loss: 1.3361 (1.3772)  Acc@1: 62.5000 (61.6936)  Acc@5: 91.6667 (91.3979)  time: 0.0851  data: 0.0022  max mem: 294\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 1.3242 (1.3868)  Acc@1: 62.5000 (62.0935)  Acc@5: 91.6667 (91.3618)  time: 0.0857  data: 0.0015  max mem: 294\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 1.3214 (1.3848)  Acc@1: 62.5000 (62.1000)  Acc@5: 91.6667 (91.3000)  time: 0.0839  data: 0.0013  max mem: 294\n",
            "Test: [Task 7] Total time: 0:00:04 (0.0964 s / it)\n",
            "* Acc@1 62.100 Acc@5 91.300 loss 1.385\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:25  Loss: 5.1174 (5.1174)  Acc@1: 0.0000 (0.0000)  Acc@5: 4.1667 (4.1667)  time: 0.6023  data: 0.5266  max mem: 294\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:04  Loss: 5.1174 (5.0838)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.7576)  time: 0.1334  data: 0.0497  max mem: 294\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:02  Loss: 5.0730 (5.0639)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.3968)  time: 0.0849  data: 0.0011  max mem: 294\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:01  Loss: 5.0456 (5.0637)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.4032)  time: 0.0840  data: 0.0004  max mem: 294\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 5.0825 (5.0699)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.7114)  time: 0.0849  data: 0.0004  max mem: 294\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 5.0825 (5.0698)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.7000)  time: 0.0832  data: 0.0004  max mem: 294\n",
            "Test: [Task 8] Total time: 0:00:04 (0.0983 s / it)\n",
            "* Acc@1 0.000 Acc@5 0.700 loss 5.070\n",
            "[Average accuracy till task8]\tAcc@1: 65.6375\tAcc@5: 83.5625\tLoss: 1.4378\tForgetting: 2.5571\tBackward: 58.7286\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([19128, 384])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 0.8301  Acc@1: 79.1667 (80.4167)  Acc@5: 87.5000 (88.0357)\n",
            "torch.Size([19128, 384])\n",
            "Averaged stats: Lr: 0.000467  Loss: 0.5831  Acc@1: 83.3333 (81.3095)  Acc@5: 95.8333 (92.0238)\n",
            "torch.Size([19128, 384])\n",
            "Averaged stats: Lr: 0.000464  Loss: 0.9545  Acc@1: 83.3333 (82.0238)  Acc@5: 95.8333 (96.2500)\n",
            "torch.Size([19128, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 0.5500  Acc@1: 87.5000 (84.4643)  Acc@5: 100.0000 (97.9762)\n",
            "torch.Size([19128, 384])\n",
            "Averaged stats: Lr: 0.000448  Loss: 0.6347  Acc@1: 91.6667 (86.6071)  Acc@5: 100.0000 (98.9286)\n",
            "torch.Size([19128, 384])\n",
            "Averaged stats: Lr: 0.000437  Loss: 0.6061  Acc@1: 87.5000 (89.3452)  Acc@5: 100.0000 (99.4643)\n",
            "torch.Size([19128, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 0.7825  Acc@1: 91.6667 (89.5238)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([19128, 384])\n",
            "Averaged stats: Lr: 0.000409  Loss: 0.5885  Acc@1: 91.6667 (90.1191)  Acc@5: 100.0000 (99.5238)\n",
            "torch.Size([19128, 384])\n",
            "Averaged stats: Lr: 0.000391  Loss: 0.7160  Acc@1: 91.6667 (91.0119)  Acc@5: 100.0000 (99.5238)\n",
            "torch.Size([19128, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 0.6051  Acc@1: 91.6667 (91.4286)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([19128, 384])\n",
            "Averaged stats: Lr: 0.000352  Loss: 0.5005  Acc@1: 95.8333 (92.3810)  Acc@5: 100.0000 (99.8214)\n",
            "torch.Size([19128, 384])\n",
            "Averaged stats: Lr: 0.000330  Loss: 0.3197  Acc@1: 91.6667 (91.9048)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([19128, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 0.3772  Acc@1: 91.6667 (92.9762)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([19128, 384])\n",
            "Averaged stats: Lr: 0.000283  Loss: 0.4197  Acc@1: 91.6667 (92.1429)  Acc@5: 100.0000 (99.7619)\n",
            "torch.Size([19128, 384])\n",
            "Averaged stats: Lr: 0.000259  Loss: 0.5483  Acc@1: 91.6667 (92.1429)  Acc@5: 100.0000 (99.8214)\n",
            "torch.Size([19128, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 0.5032  Acc@1: 91.6667 (92.2619)  Acc@5: 100.0000 (99.7024)\n",
            "torch.Size([19128, 384])\n",
            "Averaged stats: Lr: 0.000210  Loss: 0.5233  Acc@1: 91.6667 (92.2024)  Acc@5: 100.0000 (99.6429)\n",
            "torch.Size([19128, 384])\n",
            "Averaged stats: Lr: 0.000186  Loss: 0.4211  Acc@1: 91.6667 (92.6191)  Acc@5: 100.0000 (99.7619)\n",
            "torch.Size([19128, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.4517  Acc@1: 91.6667 (92.4405)  Acc@5: 100.0000 (99.9405)\n",
            "torch.Size([19128, 384])\n",
            "Averaged stats: Lr: 0.000139  Loss: 0.3384  Acc@1: 95.8333 (93.4524)  Acc@5: 100.0000 (99.8810)\n",
            "torch.Size([19128, 384])\n",
            "Averaged stats: Lr: 0.000117  Loss: 0.3631  Acc@1: 95.8333 (93.0952)  Acc@5: 100.0000 (99.6429)\n",
            "torch.Size([19128, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 0.4784  Acc@1: 95.8333 (93.2738)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([19128, 384])\n",
            "Averaged stats: Lr: 0.000078  Loss: 0.2556  Acc@1: 95.8333 (93.2738)  Acc@5: 100.0000 (99.8810)\n",
            "torch.Size([19128, 384])\n",
            "Averaged stats: Lr: 0.000060  Loss: 0.5444  Acc@1: 91.6667 (92.2619)  Acc@5: 100.0000 (99.7619)\n",
            "torch.Size([19128, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 0.3573  Acc@1: 91.6667 (92.3214)  Acc@5: 100.0000 (99.7024)\n",
            "torch.Size([19128, 384])\n",
            "Averaged stats: Lr: 0.000031  Loss: 0.4000  Acc@1: 95.8333 (94.4048)  Acc@5: 100.0000 (99.7024)\n",
            "torch.Size([19128, 384])\n",
            "Averaged stats: Lr: 0.000020  Loss: 0.5066  Acc@1: 95.8333 (93.3333)  Acc@5: 100.0000 (99.8214)\n",
            "torch.Size([19128, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.4381  Acc@1: 91.6667 (93.4524)  Acc@5: 100.0000 (99.8810)\n",
            "torch.Size([19128, 384])\n",
            "Averaged stats: Lr: 0.000005  Loss: 0.3491  Acc@1: 95.8333 (93.7500)  Acc@5: 100.0000 (99.8810)\n",
            "torch.Size([19128, 384])\n",
            "Averaged stats: Lr: 0.000001  Loss: 0.3735  Acc@1: 91.6667 (93.7500)  Acc@5: 100.0000 (99.5238)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:20  Loss: 0.7823 (0.7823)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  time: 0.4857  data: 0.4096  max mem: 295\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 0.7823 (0.7901)  Acc@1: 75.0000 (76.1364)  Acc@5: 95.8333 (96.5909)  time: 0.1182  data: 0.0384  max mem: 295\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.7688 (0.7703)  Acc@1: 79.1667 (78.7698)  Acc@5: 95.8333 (95.8333)  time: 0.0814  data: 0.0008  max mem: 295\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.6779 (0.7361)  Acc@1: 79.1667 (79.4355)  Acc@5: 95.8333 (96.3710)  time: 0.0814  data: 0.0004  max mem: 295\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6061 (0.7085)  Acc@1: 79.1667 (80.3862)  Acc@5: 100.0000 (96.8496)  time: 0.0814  data: 0.0004  max mem: 295\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.5915 (0.6990)  Acc@1: 79.1667 (80.6000)  Acc@5: 100.0000 (96.9000)  time: 0.0799  data: 0.0004  max mem: 295\n",
            "Test: [Task 1] Total time: 0:00:03 (0.0921 s / it)\n",
            "* Acc@1 80.600 Acc@5 96.900 loss 0.699\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:14  Loss: 0.8668 (0.8668)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  time: 0.3361  data: 0.2692  max mem: 295\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:03  Loss: 0.8665 (0.8439)  Acc@1: 70.8333 (72.7273)  Acc@5: 95.8333 (95.8333)  time: 0.1110  data: 0.0336  max mem: 295\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 0.8027 (0.8926)  Acc@1: 70.8333 (73.0159)  Acc@5: 95.8333 (94.8413)  time: 0.0858  data: 0.0056  max mem: 295\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.8331 (0.8874)  Acc@1: 75.0000 (73.5215)  Acc@5: 95.8333 (95.1613)  time: 0.0833  data: 0.0009  max mem: 295\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.7648 (0.8618)  Acc@1: 75.0000 (74.0854)  Acc@5: 95.8333 (95.5285)  time: 0.0825  data: 0.0004  max mem: 295\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.7585 (0.8553)  Acc@1: 75.0000 (74.3000)  Acc@5: 95.8333 (95.6000)  time: 0.0811  data: 0.0004  max mem: 295\n",
            "Test: [Task 2] Total time: 0:00:03 (0.0926 s / it)\n",
            "* Acc@1 74.300 Acc@5 95.600 loss 0.855\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:30  Loss: 0.6024 (0.6024)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.7241  data: 0.6398  max mem: 295\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:04  Loss: 0.7964 (0.7878)  Acc@1: 79.1667 (78.0303)  Acc@5: 95.8333 (95.4545)  time: 0.1499  data: 0.0685  max mem: 295\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 0.7781 (0.7356)  Acc@1: 79.1667 (79.5635)  Acc@5: 95.8333 (95.4365)  time: 0.0875  data: 0.0061  max mem: 295\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.6576 (0.7139)  Acc@1: 79.1667 (79.4355)  Acc@5: 95.8333 (95.9677)  time: 0.0829  data: 0.0006  max mem: 295\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.6950 (0.7264)  Acc@1: 79.1667 (79.8781)  Acc@5: 95.8333 (95.7317)  time: 0.0825  data: 0.0003  max mem: 295\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.6950 (0.7290)  Acc@1: 79.1667 (79.7000)  Acc@5: 95.8333 (95.8000)  time: 0.0814  data: 0.0003  max mem: 295\n",
            "Test: [Task 3] Total time: 0:00:04 (0.1015 s / it)\n",
            "* Acc@1 79.700 Acc@5 95.800 loss 0.729\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:19  Loss: 1.1312 (1.1312)  Acc@1: 66.6667 (66.6667)  Acc@5: 83.3333 (83.3333)  time: 0.4611  data: 0.3862  max mem: 295\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:03  Loss: 0.8725 (0.8420)  Acc@1: 75.0000 (73.8636)  Acc@5: 95.8333 (95.0758)  time: 0.1159  data: 0.0356  max mem: 295\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 0.7828 (0.8228)  Acc@1: 75.0000 (75.9921)  Acc@5: 95.8333 (95.4365)  time: 0.0816  data: 0.0004  max mem: 295\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 0.7142 (0.7762)  Acc@1: 79.1667 (77.1505)  Acc@5: 95.8333 (95.9677)  time: 0.0824  data: 0.0006  max mem: 295\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7142 (0.7852)  Acc@1: 79.1667 (77.2358)  Acc@5: 95.8333 (95.8333)  time: 0.0830  data: 0.0007  max mem: 295\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.7033 (0.7770)  Acc@1: 79.1667 (77.5000)  Acc@5: 95.8333 (95.9000)  time: 0.0815  data: 0.0006  max mem: 295\n",
            "Test: [Task 4] Total time: 0:00:03 (0.0925 s / it)\n",
            "* Acc@1 77.500 Acc@5 95.900 loss 0.777\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:16  Loss: 0.6350 (0.6350)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.4008  data: 0.3279  max mem: 295\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:03  Loss: 0.7062 (0.7377)  Acc@1: 75.0000 (77.6515)  Acc@5: 100.0000 (98.4848)  time: 0.1103  data: 0.0305  max mem: 295\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 0.7681 (0.7596)  Acc@1: 70.8333 (76.5873)  Acc@5: 95.8333 (97.2222)  time: 0.0815  data: 0.0005  max mem: 295\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 0.7621 (0.7617)  Acc@1: 79.1667 (78.2258)  Acc@5: 95.8333 (96.6398)  time: 0.0824  data: 0.0004  max mem: 295\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.7941 (0.8042)  Acc@1: 79.1667 (77.1341)  Acc@5: 95.8333 (96.2398)  time: 0.0829  data: 0.0004  max mem: 295\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.8313 (0.8182)  Acc@1: 79.1667 (76.9000)  Acc@5: 95.8333 (96.1000)  time: 0.0815  data: 0.0004  max mem: 295\n",
            "Test: [Task 5] Total time: 0:00:03 (0.0926 s / it)\n",
            "* Acc@1 76.900 Acc@5 96.100 loss 0.818\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:33  Loss: 0.8225 (0.8225)  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  time: 0.8055  data: 0.6956  max mem: 295\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:04  Loss: 0.8685 (0.9090)  Acc@1: 75.0000 (74.2424)  Acc@5: 95.8333 (95.4545)  time: 0.1502  data: 0.0654  max mem: 295\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 0.8938 (0.9280)  Acc@1: 75.0000 (73.4127)  Acc@5: 95.8333 (95.8333)  time: 0.0866  data: 0.0046  max mem: 295\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 0.9630 (0.9498)  Acc@1: 70.8333 (72.7151)  Acc@5: 95.8333 (95.4301)  time: 0.0864  data: 0.0042  max mem: 295\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 0.9910 (0.9591)  Acc@1: 75.0000 (73.1707)  Acc@5: 95.8333 (95.3252)  time: 0.0833  data: 0.0011  max mem: 295\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 0.9910 (0.9621)  Acc@1: 75.0000 (73.0000)  Acc@5: 95.8333 (95.3000)  time: 0.0821  data: 0.0011  max mem: 295\n",
            "Test: [Task 6] Total time: 0:00:04 (0.1036 s / it)\n",
            "* Acc@1 73.000 Acc@5 95.300 loss 0.962\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:19  Loss: 1.1482 (1.1482)  Acc@1: 70.8333 (70.8333)  Acc@5: 87.5000 (87.5000)  time: 0.4569  data: 0.3711  max mem: 295\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:03  Loss: 0.9387 (0.9920)  Acc@1: 70.8333 (71.2121)  Acc@5: 95.8333 (95.0758)  time: 0.1187  data: 0.0346  max mem: 295\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:02  Loss: 0.9387 (1.0224)  Acc@1: 75.0000 (71.4286)  Acc@5: 95.8333 (95.0397)  time: 0.0833  data: 0.0007  max mem: 295\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:01  Loss: 1.0674 (1.0637)  Acc@1: 70.8333 (70.0269)  Acc@5: 95.8333 (93.9516)  time: 0.0827  data: 0.0005  max mem: 295\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 1.0996 (1.0730)  Acc@1: 66.6667 (69.8171)  Acc@5: 95.8333 (93.9024)  time: 0.0835  data: 0.0005  max mem: 295\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 1.0706 (1.0696)  Acc@1: 66.6667 (69.9000)  Acc@5: 95.8333 (93.9000)  time: 0.0816  data: 0.0005  max mem: 295\n",
            "Test: [Task 7] Total time: 0:00:03 (0.0934 s / it)\n",
            "* Acc@1 69.900 Acc@5 93.900 loss 1.070\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:15  Loss: 1.1285 (1.1285)  Acc@1: 66.6667 (66.6667)  Acc@5: 95.8333 (95.8333)  time: 0.3665  data: 0.2905  max mem: 295\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:03  Loss: 1.2377 (1.3039)  Acc@1: 66.6667 (64.3939)  Acc@5: 91.6667 (92.0455)  time: 0.1102  data: 0.0298  max mem: 295\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:02  Loss: 1.2762 (1.2827)  Acc@1: 62.5000 (63.6905)  Acc@5: 91.6667 (92.6587)  time: 0.0828  data: 0.0021  max mem: 295\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:01  Loss: 1.2762 (1.2680)  Acc@1: 62.5000 (63.3065)  Acc@5: 95.8333 (93.2796)  time: 0.0819  data: 0.0004  max mem: 295\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 1.3206 (1.2856)  Acc@1: 62.5000 (62.1951)  Acc@5: 91.6667 (92.3781)  time: 0.0830  data: 0.0003  max mem: 295\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 1.3206 (1.2834)  Acc@1: 62.5000 (62.2000)  Acc@5: 91.6667 (92.4000)  time: 0.0815  data: 0.0003  max mem: 295\n",
            "Test: [Task 8] Total time: 0:00:03 (0.0910 s / it)\n",
            "* Acc@1 62.200 Acc@5 92.400 loss 1.283\n",
            "[Average accuracy till task8]\tAcc@1: 74.2625\tAcc@5: 95.2375\tLoss: 0.8992\tForgetting: 3.1286\tBackward: 5.5000\n",
            "Train: Epoch[1/1]  [  0/103]  eta: 0:00:43  Lr: 0.000047  Loss: 2.3929  Acc@1: 4.1667 (4.1667)  Acc@5: 50.0000 (50.0000)  time: 0.4202  data: 0.3278  max mem: 295\n",
            "Train: Epoch[1/1]  [ 10/103]  eta: 0:00:12  Lr: 0.000047  Loss: 2.3066  Acc@1: 12.5000 (11.3636)  Acc@5: 54.1667 (54.5455)  time: 0.1313  data: 0.0369  max mem: 295\n",
            "Train: Epoch[1/1]  [ 20/103]  eta: 0:00:09  Lr: 0.000047  Loss: 2.1057  Acc@1: 16.6667 (16.6667)  Acc@5: 62.5000 (62.1032)  time: 0.1003  data: 0.0052  max mem: 295\n",
            "Train: Epoch[1/1]  [ 30/103]  eta: 0:00:08  Lr: 0.000047  Loss: 1.9854  Acc@1: 20.8333 (18.4140)  Acc@5: 75.0000 (69.0860)  time: 0.1006  data: 0.0032  max mem: 295\n",
            "Train: Epoch[1/1]  [ 40/103]  eta: 0:00:07  Lr: 0.000047  Loss: 1.8468  Acc@1: 29.1667 (23.0691)  Acc@5: 87.5000 (73.1707)  time: 0.1087  data: 0.0095  max mem: 295\n",
            "Train: Epoch[1/1]  [ 50/103]  eta: 0:00:05  Lr: 0.000047  Loss: 1.7707  Acc@1: 37.5000 (25.6536)  Acc@5: 91.6667 (76.7974)  time: 0.1063  data: 0.0081  max mem: 295\n",
            "Train: Epoch[1/1]  [ 60/103]  eta: 0:00:04  Lr: 0.000047  Loss: 1.6065  Acc@1: 45.8333 (30.1230)  Acc@5: 91.6667 (79.5082)  time: 0.0930  data: 0.0013  max mem: 295\n",
            "Train: Epoch[1/1]  [ 70/103]  eta: 0:00:03  Lr: 0.000047  Loss: 1.6105  Acc@1: 54.1667 (33.6854)  Acc@5: 95.8333 (81.8075)  time: 0.0876  data: 0.0017  max mem: 295\n",
            "Train: Epoch[1/1]  [ 80/103]  eta: 0:00:02  Lr: 0.000047  Loss: 1.5496  Acc@1: 58.3333 (37.3971)  Acc@5: 95.8333 (83.6420)  time: 0.0861  data: 0.0011  max mem: 295\n",
            "Train: Epoch[1/1]  [ 90/103]  eta: 0:00:01  Lr: 0.000047  Loss: 1.5066  Acc@1: 66.6667 (40.6593)  Acc@5: 95.8333 (84.8901)  time: 0.0851  data: 0.0007  max mem: 295\n",
            "Train: Epoch[1/1]  [100/103]  eta: 0:00:00  Lr: 0.000047  Loss: 1.4136  Acc@1: 66.6667 (43.0693)  Acc@5: 95.8333 (86.1799)  time: 0.0846  data: 0.0009  max mem: 295\n",
            "Train: Epoch[1/1]  [102/103]  eta: 0:00:00  Lr: 0.000047  Loss: 1.5551  Acc@1: 62.5000 (43.3116)  Acc@5: 95.8333 (86.2561)  time: 0.0815  data: 0.0008  max mem: 295\n",
            "Train: Epoch[1/1] Total time: 0:00:10 (0.0980 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.5551  Acc@1: 62.5000 (43.3116)  Acc@5: 95.8333 (86.2561)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:26  Loss: 0.7996 (0.7996)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  time: 0.6365  data: 0.5447  max mem: 295\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 0.7996 (0.7962)  Acc@1: 75.0000 (77.6515)  Acc@5: 95.8333 (96.5909)  time: 0.1346  data: 0.0504  max mem: 295\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.7841 (0.7763)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (96.0317)  time: 0.0828  data: 0.0007  max mem: 295\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.7001 (0.7416)  Acc@1: 79.1667 (79.8387)  Acc@5: 95.8333 (96.5054)  time: 0.0826  data: 0.0006  max mem: 295\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6235 (0.7156)  Acc@1: 79.1667 (80.5894)  Acc@5: 95.8333 (96.8496)  time: 0.0839  data: 0.0004  max mem: 295\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6021 (0.7063)  Acc@1: 79.1667 (80.7000)  Acc@5: 100.0000 (96.9000)  time: 0.0824  data: 0.0004  max mem: 295\n",
            "Test: [Task 1] Total time: 0:00:04 (0.0978 s / it)\n",
            "* Acc@1 80.700 Acc@5 96.900 loss 0.706\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:18  Loss: 0.9360 (0.9360)  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.8333)  time: 0.4437  data: 0.3710  max mem: 295\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:03  Loss: 0.9360 (0.9279)  Acc@1: 70.8333 (71.2121)  Acc@5: 95.8333 (95.0758)  time: 0.1166  data: 0.0343  max mem: 295\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 0.8661 (0.9706)  Acc@1: 70.8333 (71.8254)  Acc@5: 95.8333 (94.0476)  time: 0.0842  data: 0.0016  max mem: 295\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.8858 (0.9658)  Acc@1: 70.8333 (72.1774)  Acc@5: 95.8333 (94.0860)  time: 0.0849  data: 0.0020  max mem: 295\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.8383 (0.9430)  Acc@1: 70.8333 (72.7642)  Acc@5: 95.8333 (94.5122)  time: 0.0853  data: 0.0011  max mem: 295\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.8236 (0.9360)  Acc@1: 70.8333 (73.0000)  Acc@5: 95.8333 (94.6000)  time: 0.0834  data: 0.0010  max mem: 295\n",
            "Test: [Task 2] Total time: 0:00:04 (0.0955 s / it)\n",
            "* Acc@1 73.000 Acc@5 94.600 loss 0.936\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:24  Loss: 0.6737 (0.6737)  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  time: 0.5866  data: 0.5037  max mem: 295\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:04  Loss: 0.8457 (0.8487)  Acc@1: 75.0000 (77.2727)  Acc@5: 95.8333 (95.4545)  time: 0.1294  data: 0.0474  max mem: 295\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 0.8222 (0.7941)  Acc@1: 79.1667 (78.7698)  Acc@5: 95.8333 (94.8413)  time: 0.0832  data: 0.0011  max mem: 295\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.7056 (0.7742)  Acc@1: 79.1667 (78.3602)  Acc@5: 95.8333 (95.2957)  time: 0.0832  data: 0.0003  max mem: 295\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.7593 (0.7867)  Acc@1: 79.1667 (78.9634)  Acc@5: 95.8333 (95.1220)  time: 0.0840  data: 0.0004  max mem: 295\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.7593 (0.7901)  Acc@1: 79.1667 (78.7000)  Acc@5: 95.8333 (95.2000)  time: 0.0823  data: 0.0004  max mem: 295\n",
            "Test: [Task 3] Total time: 0:00:04 (0.0967 s / it)\n",
            "* Acc@1 78.700 Acc@5 95.200 loss 0.790\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:20  Loss: 1.0726 (1.0726)  Acc@1: 66.6667 (66.6667)  Acc@5: 83.3333 (83.3333)  time: 0.4823  data: 0.3905  max mem: 295\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:03  Loss: 0.8586 (0.8222)  Acc@1: 75.0000 (75.7576)  Acc@5: 95.8333 (94.6970)  time: 0.1206  data: 0.0371  max mem: 295\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 0.7739 (0.8082)  Acc@1: 75.0000 (76.7857)  Acc@5: 95.8333 (95.2381)  time: 0.0833  data: 0.0011  max mem: 295\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 0.7054 (0.7661)  Acc@1: 79.1667 (78.6290)  Acc@5: 95.8333 (95.8333)  time: 0.0833  data: 0.0004  max mem: 295\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7207 (0.7759)  Acc@1: 79.1667 (78.3537)  Acc@5: 95.8333 (95.9350)  time: 0.0847  data: 0.0003  max mem: 295\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.7053 (0.7679)  Acc@1: 79.1667 (78.6000)  Acc@5: 95.8333 (96.0000)  time: 0.0828  data: 0.0003  max mem: 295\n",
            "Test: [Task 4] Total time: 0:00:03 (0.0946 s / it)\n",
            "* Acc@1 78.600 Acc@5 96.000 loss 0.768\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:12  Loss: 0.6155 (0.6155)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.3053  data: 0.2346  max mem: 295\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:03  Loss: 0.7214 (0.7428)  Acc@1: 79.1667 (80.3030)  Acc@5: 100.0000 (98.1061)  time: 0.1152  data: 0.0363  max mem: 295\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 0.7761 (0.7673)  Acc@1: 75.0000 (77.7778)  Acc@5: 95.8333 (97.0238)  time: 0.0904  data: 0.0084  max mem: 295\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 0.7712 (0.7683)  Acc@1: 79.1667 (79.3011)  Acc@5: 95.8333 (96.5054)  time: 0.0850  data: 0.0005  max mem: 295\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.8080 (0.8130)  Acc@1: 79.1667 (77.9472)  Acc@5: 95.8333 (95.9350)  time: 0.0850  data: 0.0006  max mem: 295\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.8740 (0.8276)  Acc@1: 79.1667 (77.7000)  Acc@5: 95.8333 (95.8000)  time: 0.0837  data: 0.0006  max mem: 295\n",
            "Test: [Task 5] Total time: 0:00:03 (0.0950 s / it)\n",
            "* Acc@1 77.700 Acc@5 95.800 loss 0.828\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:28  Loss: 0.7917 (0.7917)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  time: 0.6890  data: 0.5812  max mem: 295\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:04  Loss: 0.8375 (0.8530)  Acc@1: 79.1667 (76.5152)  Acc@5: 95.8333 (96.5909)  time: 0.1433  data: 0.0555  max mem: 295\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 0.8448 (0.8719)  Acc@1: 75.0000 (76.1905)  Acc@5: 95.8333 (96.4286)  time: 0.0869  data: 0.0018  max mem: 295\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 0.9202 (0.8935)  Acc@1: 75.0000 (75.1344)  Acc@5: 95.8333 (96.3710)  time: 0.0844  data: 0.0008  max mem: 295\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 0.9293 (0.9076)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (96.2398)  time: 0.0839  data: 0.0006  max mem: 295\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 0.9293 (0.9103)  Acc@1: 70.8333 (74.8000)  Acc@5: 95.8333 (96.2000)  time: 0.0825  data: 0.0006  max mem: 295\n",
            "Test: [Task 6] Total time: 0:00:04 (0.1012 s / it)\n",
            "* Acc@1 74.800 Acc@5 96.200 loss 0.910\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:19  Loss: 1.1320 (1.1320)  Acc@1: 75.0000 (75.0000)  Acc@5: 87.5000 (87.5000)  time: 0.4593  data: 0.3795  max mem: 295\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:03  Loss: 0.9465 (0.9914)  Acc@1: 70.8333 (71.2121)  Acc@5: 95.8333 (95.8333)  time: 0.1205  data: 0.0361  max mem: 295\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:02  Loss: 0.9488 (1.0230)  Acc@1: 75.0000 (71.8254)  Acc@5: 95.8333 (95.4365)  time: 0.0845  data: 0.0013  max mem: 295\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:01  Loss: 1.0836 (1.0635)  Acc@1: 66.6667 (70.2957)  Acc@5: 95.8333 (94.2204)  time: 0.0837  data: 0.0006  max mem: 295\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 1.1113 (1.0717)  Acc@1: 66.6667 (69.7154)  Acc@5: 95.8333 (94.0041)  time: 0.0849  data: 0.0005  max mem: 295\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 1.0969 (1.0686)  Acc@1: 66.6667 (69.7000)  Acc@5: 95.8333 (94.0000)  time: 0.0828  data: 0.0005  max mem: 295\n",
            "Test: [Task 7] Total time: 0:00:04 (0.0956 s / it)\n",
            "* Acc@1 69.700 Acc@5 94.000 loss 1.069\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:28  Loss: 1.1881 (1.1881)  Acc@1: 66.6667 (66.6667)  Acc@5: 95.8333 (95.8333)  time: 0.6772  data: 0.5423  max mem: 295\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:04  Loss: 1.2870 (1.3570)  Acc@1: 62.5000 (62.1212)  Acc@5: 91.6667 (90.9091)  time: 0.1391  data: 0.0510  max mem: 295\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:02  Loss: 1.3134 (1.3352)  Acc@1: 62.5000 (61.9048)  Acc@5: 91.6667 (91.8651)  time: 0.0895  data: 0.0071  max mem: 295\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:01  Loss: 1.3134 (1.3209)  Acc@1: 62.5000 (62.3656)  Acc@5: 95.8333 (92.3387)  time: 0.0904  data: 0.0073  max mem: 295\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 1.3409 (1.3350)  Acc@1: 58.3333 (61.3821)  Acc@5: 91.6667 (91.5650)  time: 0.0862  data: 0.0017  max mem: 295\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 1.3409 (1.3323)  Acc@1: 58.3333 (61.3000)  Acc@5: 91.6667 (91.6000)  time: 0.0849  data: 0.0017  max mem: 295\n",
            "Test: [Task 8] Total time: 0:00:04 (0.1037 s / it)\n",
            "* Acc@1 61.300 Acc@5 91.600 loss 1.332\n",
            "Test: [Task 9]  [ 0/42]  eta: 0:00:32  Loss: 6.1426 (6.1426)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  time: 0.7645  data: 0.6788  max mem: 295\n",
            "Test: [Task 9]  [10/42]  eta: 0:00:04  Loss: 5.8632 (5.8547)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  time: 0.1520  data: 0.0670  max mem: 295\n",
            "Test: [Task 9]  [20/42]  eta: 0:00:02  Loss: 5.8209 (5.8110)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  time: 0.0907  data: 0.0071  max mem: 295\n",
            "Test: [Task 9]  [30/42]  eta: 0:00:01  Loss: 5.7842 (5.8210)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  time: 0.0915  data: 0.0079  max mem: 295\n",
            "Test: [Task 9]  [40/42]  eta: 0:00:00  Loss: 5.7799 (5.8016)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  time: 0.0894  data: 0.0043  max mem: 295\n",
            "Test: [Task 9]  [41/42]  eta: 0:00:00  Loss: 5.7799 (5.7979)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  time: 0.0878  data: 0.0043  max mem: 295\n",
            "Test: [Task 9] Total time: 0:00:04 (0.1073 s / it)\n",
            "* Acc@1 0.000 Acc@5 0.000 loss 5.798\n",
            "[Average accuracy till task9]\tAcc@1: 66.0556\tAcc@5: 84.4778\tLoss: 1.4597\tForgetting: 2.8250\tBackward: 60.0625\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([21528, 384])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 0.6083  Acc@1: 83.3333 (83.9063)  Acc@5: 87.5000 (89.8958)\n",
            "torch.Size([21528, 384])\n",
            "Averaged stats: Lr: 0.000467  Loss: 0.5917  Acc@1: 83.3333 (83.1250)  Acc@5: 95.8333 (92.2396)\n",
            "torch.Size([21528, 384])\n",
            "Averaged stats: Lr: 0.000464  Loss: 0.5610  Acc@1: 83.3333 (83.1250)  Acc@5: 95.8333 (96.2500)\n",
            "torch.Size([21528, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 0.5623  Acc@1: 87.5000 (85.4688)  Acc@5: 100.0000 (98.2813)\n",
            "torch.Size([21528, 384])\n",
            "Averaged stats: Lr: 0.000448  Loss: 0.5574  Acc@1: 87.5000 (87.6042)  Acc@5: 100.0000 (99.2708)\n",
            "torch.Size([21528, 384])\n",
            "Averaged stats: Lr: 0.000437  Loss: 0.6143  Acc@1: 87.5000 (88.9583)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([21528, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 0.7395  Acc@1: 95.8333 (91.6667)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([21528, 384])\n",
            "Averaged stats: Lr: 0.000409  Loss: 0.4207  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (99.8438)\n",
            "torch.Size([21528, 384])\n",
            "Averaged stats: Lr: 0.000391  Loss: 0.4606  Acc@1: 91.6667 (91.6146)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([21528, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 0.6159  Acc@1: 87.5000 (91.6667)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([21528, 384])\n",
            "Averaged stats: Lr: 0.000352  Loss: 0.4566  Acc@1: 91.6667 (93.5417)  Acc@5: 100.0000 (99.8958)\n",
            "torch.Size([21528, 384])\n",
            "Averaged stats: Lr: 0.000330  Loss: 0.6106  Acc@1: 91.6667 (92.5521)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([21528, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 0.4943  Acc@1: 91.6667 (92.5521)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([21528, 384])\n",
            "Averaged stats: Lr: 0.000283  Loss: 0.4013  Acc@1: 95.8333 (92.8646)  Acc@5: 100.0000 (99.8438)\n",
            "torch.Size([21528, 384])\n",
            "Averaged stats: Lr: 0.000259  Loss: 0.3739  Acc@1: 91.6667 (93.2292)  Acc@5: 100.0000 (99.8438)\n",
            "torch.Size([21528, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 0.4065  Acc@1: 95.8333 (93.3333)  Acc@5: 100.0000 (99.6354)\n",
            "torch.Size([21528, 384])\n",
            "Averaged stats: Lr: 0.000210  Loss: 0.4262  Acc@1: 95.8333 (93.6979)  Acc@5: 100.0000 (99.9479)\n",
            "torch.Size([21528, 384])\n",
            "Averaged stats: Lr: 0.000186  Loss: 0.6411  Acc@1: 95.8333 (93.7500)  Acc@5: 100.0000 (99.8958)\n",
            "torch.Size([21528, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.4517  Acc@1: 95.8333 (94.2188)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([21528, 384])\n",
            "Averaged stats: Lr: 0.000139  Loss: 0.4664  Acc@1: 91.6667 (93.5938)  Acc@5: 100.0000 (99.8958)\n",
            "torch.Size([21528, 384])\n",
            "Averaged stats: Lr: 0.000117  Loss: 0.5548  Acc@1: 91.6667 (93.6458)  Acc@5: 100.0000 (99.8438)\n",
            "torch.Size([21528, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 0.4914  Acc@1: 95.8333 (93.7500)  Acc@5: 100.0000 (99.8958)\n",
            "torch.Size([21528, 384])\n",
            "Averaged stats: Lr: 0.000078  Loss: 0.3615  Acc@1: 95.8333 (95.5208)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([21528, 384])\n",
            "Averaged stats: Lr: 0.000060  Loss: 0.2711  Acc@1: 95.8333 (94.1667)  Acc@5: 100.0000 (99.8438)\n",
            "torch.Size([21528, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 0.3589  Acc@1: 91.6667 (93.7500)  Acc@5: 100.0000 (99.9479)\n",
            "torch.Size([21528, 384])\n",
            "Averaged stats: Lr: 0.000031  Loss: 0.5451  Acc@1: 95.8333 (94.9479)  Acc@5: 100.0000 (99.8438)\n",
            "torch.Size([21528, 384])\n",
            "Averaged stats: Lr: 0.000020  Loss: 0.2489  Acc@1: 95.8333 (94.4271)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([21528, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.2590  Acc@1: 91.6667 (94.7917)  Acc@5: 100.0000 (99.8958)\n",
            "torch.Size([21528, 384])\n",
            "Averaged stats: Lr: 0.000005  Loss: 0.4184  Acc@1: 95.8333 (93.4896)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([21528, 384])\n",
            "Averaged stats: Lr: 0.000001  Loss: 0.1875  Acc@1: 95.8333 (94.2708)  Acc@5: 100.0000 (99.9479)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:13  Loss: 0.8127 (0.8127)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  time: 0.3222  data: 0.2508  max mem: 295\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 0.8127 (0.8056)  Acc@1: 75.0000 (75.7576)  Acc@5: 95.8333 (94.6970)  time: 0.1121  data: 0.0281  max mem: 295\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.7803 (0.7832)  Acc@1: 79.1667 (77.9762)  Acc@5: 95.8333 (95.0397)  time: 0.0898  data: 0.0037  max mem: 295\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.7319 (0.7506)  Acc@1: 79.1667 (78.6290)  Acc@5: 95.8333 (95.8333)  time: 0.0883  data: 0.0015  max mem: 295\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6217 (0.7227)  Acc@1: 79.1667 (79.3699)  Acc@5: 100.0000 (96.3415)  time: 0.0883  data: 0.0008  max mem: 295\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.5850 (0.7127)  Acc@1: 79.1667 (79.6000)  Acc@5: 100.0000 (96.4000)  time: 0.0871  data: 0.0004  max mem: 295\n",
            "Test: [Task 1] Total time: 0:00:04 (0.0974 s / it)\n",
            "* Acc@1 79.600 Acc@5 96.400 loss 0.713\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:26  Loss: 0.8963 (0.8963)  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.8333)  time: 0.6226  data: 0.5071  max mem: 295\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 0.8963 (0.8727)  Acc@1: 70.8333 (72.3485)  Acc@5: 95.8333 (96.5909)  time: 0.1380  data: 0.0465  max mem: 295\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 0.8743 (0.9402)  Acc@1: 70.8333 (71.2302)  Acc@5: 95.8333 (94.8413)  time: 0.0886  data: 0.0009  max mem: 295\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.8743 (0.9344)  Acc@1: 70.8333 (71.6398)  Acc@5: 95.8333 (95.1613)  time: 0.0881  data: 0.0008  max mem: 295\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.7936 (0.9070)  Acc@1: 70.8333 (72.5610)  Acc@5: 95.8333 (95.3252)  time: 0.0894  data: 0.0002  max mem: 295\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.7789 (0.9015)  Acc@1: 70.8333 (72.8000)  Acc@5: 95.8333 (95.4000)  time: 0.0877  data: 0.0002  max mem: 295\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1030 s / it)\n",
            "* Acc@1 72.800 Acc@5 95.400 loss 0.902\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:13  Loss: 0.6078 (0.6078)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  time: 0.3249  data: 0.2521  max mem: 295\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 0.7930 (0.8048)  Acc@1: 79.1667 (78.4091)  Acc@5: 95.8333 (94.6970)  time: 0.1232  data: 0.0379  max mem: 295\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 0.7921 (0.7477)  Acc@1: 79.1667 (79.3651)  Acc@5: 95.8333 (95.4365)  time: 0.0960  data: 0.0085  max mem: 295\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.6687 (0.7244)  Acc@1: 83.3333 (79.4355)  Acc@5: 95.8333 (95.8333)  time: 0.0893  data: 0.0005  max mem: 295\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.7011 (0.7387)  Acc@1: 83.3333 (79.7764)  Acc@5: 95.8333 (95.2236)  time: 0.0901  data: 0.0005  max mem: 295\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.7011 (0.7395)  Acc@1: 83.3333 (79.6000)  Acc@5: 95.8333 (95.3000)  time: 0.0881  data: 0.0004  max mem: 295\n",
            "Test: [Task 3] Total time: 0:00:04 (0.0997 s / it)\n",
            "* Acc@1 79.600 Acc@5 95.300 loss 0.739\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:20  Loss: 1.2136 (1.2136)  Acc@1: 62.5000 (62.5000)  Acc@5: 83.3333 (83.3333)  time: 0.4850  data: 0.4037  max mem: 295\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:04  Loss: 0.8818 (0.8473)  Acc@1: 75.0000 (73.8636)  Acc@5: 95.8333 (93.9394)  time: 0.1257  data: 0.0381  max mem: 295\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 0.8149 (0.8250)  Acc@1: 75.0000 (76.1905)  Acc@5: 95.8333 (94.2460)  time: 0.0891  data: 0.0009  max mem: 295\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 0.7304 (0.7824)  Acc@1: 79.1667 (77.1505)  Acc@5: 95.8333 (94.8925)  time: 0.0895  data: 0.0004  max mem: 295\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7403 (0.7910)  Acc@1: 79.1667 (77.5407)  Acc@5: 95.8333 (94.9187)  time: 0.0903  data: 0.0004  max mem: 295\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.6931 (0.7827)  Acc@1: 79.1667 (77.8000)  Acc@5: 95.8333 (95.0000)  time: 0.0884  data: 0.0004  max mem: 295\n",
            "Test: [Task 4] Total time: 0:00:04 (0.1009 s / it)\n",
            "* Acc@1 77.800 Acc@5 95.000 loss 0.783\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:31  Loss: 0.6677 (0.6677)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.7575  data: 0.6658  max mem: 295\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:04  Loss: 0.7509 (0.7609)  Acc@1: 70.8333 (76.5152)  Acc@5: 100.0000 (98.4848)  time: 0.1520  data: 0.0615  max mem: 295\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 0.7738 (0.7892)  Acc@1: 70.8333 (76.1905)  Acc@5: 95.8333 (97.4206)  time: 0.0901  data: 0.0010  max mem: 295\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 0.7660 (0.7925)  Acc@1: 79.1667 (77.1505)  Acc@5: 95.8333 (96.5054)  time: 0.0891  data: 0.0006  max mem: 295\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.8321 (0.8405)  Acc@1: 79.1667 (75.8130)  Acc@5: 95.8333 (95.8333)  time: 0.0895  data: 0.0003  max mem: 295\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.8820 (0.8551)  Acc@1: 75.0000 (75.6000)  Acc@5: 95.8333 (95.7000)  time: 0.0878  data: 0.0003  max mem: 295\n",
            "Test: [Task 5] Total time: 0:00:04 (0.1072 s / it)\n",
            "* Acc@1 75.600 Acc@5 95.700 loss 0.855\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:16  Loss: 0.8192 (0.8192)  Acc@1: 83.3333 (83.3333)  Acc@5: 91.6667 (91.6667)  time: 0.3964  data: 0.3196  max mem: 295\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:03  Loss: 0.8435 (0.8709)  Acc@1: 79.1667 (76.8939)  Acc@5: 95.8333 (95.4545)  time: 0.1188  data: 0.0307  max mem: 295\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 0.8661 (0.9043)  Acc@1: 75.0000 (75.3968)  Acc@5: 95.8333 (95.6349)  time: 0.0902  data: 0.0011  max mem: 295\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 0.9310 (0.9226)  Acc@1: 70.8333 (73.7903)  Acc@5: 95.8333 (95.2957)  time: 0.0892  data: 0.0003  max mem: 295\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 0.9668 (0.9399)  Acc@1: 70.8333 (73.2724)  Acc@5: 95.8333 (95.2236)  time: 0.0896  data: 0.0003  max mem: 295\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 0.9668 (0.9431)  Acc@1: 70.8333 (73.2000)  Acc@5: 95.8333 (95.1000)  time: 0.0880  data: 0.0003  max mem: 295\n",
            "Test: [Task 6] Total time: 0:00:04 (0.0984 s / it)\n",
            "* Acc@1 73.200 Acc@5 95.100 loss 0.943\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:15  Loss: 1.1083 (1.1083)  Acc@1: 70.8333 (70.8333)  Acc@5: 87.5000 (87.5000)  time: 0.3686  data: 0.2934  max mem: 295\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:03  Loss: 0.8877 (0.9081)  Acc@1: 70.8333 (71.2121)  Acc@5: 95.8333 (95.4545)  time: 0.1175  data: 0.0309  max mem: 295\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:02  Loss: 0.8883 (0.9447)  Acc@1: 70.8333 (70.4365)  Acc@5: 95.8333 (95.6349)  time: 0.0908  data: 0.0025  max mem: 295\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:01  Loss: 0.9793 (0.9898)  Acc@1: 70.8333 (70.0269)  Acc@5: 95.8333 (94.4892)  time: 0.0895  data: 0.0004  max mem: 295\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 1.0220 (0.9949)  Acc@1: 70.8333 (70.3252)  Acc@5: 95.8333 (94.4106)  time: 0.0898  data: 0.0004  max mem: 295\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 1.0064 (0.9914)  Acc@1: 70.8333 (70.5000)  Acc@5: 95.8333 (94.4000)  time: 0.0881  data: 0.0003  max mem: 295\n",
            "Test: [Task 7] Total time: 0:00:04 (0.0993 s / it)\n",
            "* Acc@1 70.500 Acc@5 94.400 loss 0.991\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:28  Loss: 0.9595 (0.9595)  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.8333)  time: 0.6762  data: 0.5893  max mem: 295\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:04  Loss: 1.1162 (1.0851)  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (93.5606)  time: 0.1431  data: 0.0557  max mem: 295\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:02  Loss: 1.0198 (1.0547)  Acc@1: 66.6667 (68.8492)  Acc@5: 95.8333 (94.0476)  time: 0.0899  data: 0.0020  max mem: 295\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:01  Loss: 1.0013 (1.0371)  Acc@1: 66.6667 (69.0860)  Acc@5: 95.8333 (94.3548)  time: 0.0894  data: 0.0013  max mem: 295\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 1.0175 (1.0574)  Acc@1: 70.8333 (68.8008)  Acc@5: 91.6667 (93.3943)  time: 0.0896  data: 0.0007  max mem: 295\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 1.0175 (1.0564)  Acc@1: 66.6667 (68.7000)  Acc@5: 91.6667 (93.4000)  time: 0.0883  data: 0.0007  max mem: 295\n",
            "Test: [Task 8] Total time: 0:00:04 (0.1050 s / it)\n",
            "* Acc@1 68.700 Acc@5 93.400 loss 1.056\n",
            "Test: [Task 9]  [ 0/42]  eta: 0:00:20  Loss: 1.1259 (1.1259)  Acc@1: 54.1667 (54.1667)  Acc@5: 100.0000 (100.0000)  time: 0.4945  data: 0.4209  max mem: 295\n",
            "Test: [Task 9]  [10/42]  eta: 0:00:04  Loss: 1.1259 (1.1160)  Acc@1: 70.8333 (67.8030)  Acc@5: 95.8333 (94.3182)  time: 0.1284  data: 0.0389  max mem: 295\n",
            "Test: [Task 9]  [20/42]  eta: 0:00:02  Loss: 1.1334 (1.0963)  Acc@1: 66.6667 (68.6508)  Acc@5: 95.8333 (94.0476)  time: 0.0901  data: 0.0005  max mem: 295\n",
            "Test: [Task 9]  [30/42]  eta: 0:00:01  Loss: 1.1482 (1.1340)  Acc@1: 66.6667 (67.4731)  Acc@5: 95.8333 (94.3548)  time: 0.0886  data: 0.0005  max mem: 295\n",
            "Test: [Task 9]  [40/42]  eta: 0:00:00  Loss: 0.9853 (1.0922)  Acc@1: 66.6667 (68.5976)  Acc@5: 95.8333 (94.8171)  time: 0.0895  data: 0.0005  max mem: 295\n",
            "Test: [Task 9]  [41/42]  eta: 0:00:00  Loss: 0.9853 (1.0822)  Acc@1: 66.6667 (68.9000)  Acc@5: 95.8333 (94.9000)  time: 0.0878  data: 0.0004  max mem: 295\n",
            "Test: [Task 9] Total time: 0:00:04 (0.1006 s / it)\n",
            "* Acc@1 68.900 Acc@5 94.900 loss 1.082\n",
            "[Average accuracy till task9]\tAcc@1: 74.0778\tAcc@5: 95.0667\tLoss: 0.8961\tForgetting: 3.1750\tBackward: 5.2750\n",
            "Train: Epoch[1/1]  [  0/106]  eta: 0:01:02  Lr: 0.000047  Loss: 2.3341  Acc@1: 12.5000 (12.5000)  Acc@5: 54.1667 (54.1667)  time: 0.5880  data: 0.4912  max mem: 295\n",
            "Train: Epoch[1/1]  [ 10/106]  eta: 0:00:13  Lr: 0.000047  Loss: 2.2523  Acc@1: 8.3333 (7.5758)  Acc@5: 58.3333 (57.5758)  time: 0.1360  data: 0.0457  max mem: 295\n",
            "Train: Epoch[1/1]  [ 20/106]  eta: 0:00:09  Lr: 0.000047  Loss: 2.0269  Acc@1: 12.5000 (13.0952)  Acc@5: 62.5000 (65.8730)  time: 0.0902  data: 0.0012  max mem: 295\n",
            "Train: Epoch[1/1]  [ 30/106]  eta: 0:00:08  Lr: 0.000047  Loss: 1.9098  Acc@1: 20.8333 (16.6667)  Acc@5: 79.1667 (71.2366)  time: 0.0899  data: 0.0008  max mem: 295\n",
            "Train: Epoch[1/1]  [ 40/106]  eta: 0:00:06  Lr: 0.000047  Loss: 1.8397  Acc@1: 29.1667 (22.5610)  Acc@5: 83.3333 (74.2886)  time: 0.0929  data: 0.0009  max mem: 295\n",
            "Train: Epoch[1/1]  [ 50/106]  eta: 0:00:05  Lr: 0.000047  Loss: 1.7654  Acc@1: 45.8333 (28.7582)  Acc@5: 87.5000 (76.2255)  time: 0.1010  data: 0.0022  max mem: 295\n",
            "Train: Epoch[1/1]  [ 60/106]  eta: 0:00:04  Lr: 0.000047  Loss: 1.6392  Acc@1: 54.1667 (33.0601)  Acc@5: 87.5000 (79.0984)  time: 0.1058  data: 0.0018  max mem: 295\n",
            "Train: Epoch[1/1]  [ 70/106]  eta: 0:00:03  Lr: 0.000047  Loss: 1.4752  Acc@1: 58.3333 (37.7347)  Acc@5: 95.8333 (81.3967)  time: 0.1069  data: 0.0021  max mem: 295\n",
            "Train: Epoch[1/1]  [ 80/106]  eta: 0:00:02  Lr: 0.000047  Loss: 1.5160  Acc@1: 70.8333 (41.8724)  Acc@5: 95.8333 (83.0247)  time: 0.1090  data: 0.0034  max mem: 295\n",
            "Train: Epoch[1/1]  [ 90/106]  eta: 0:00:01  Lr: 0.000047  Loss: 1.6140  Acc@1: 70.8333 (45.1923)  Acc@5: 95.8333 (84.5696)  time: 0.0996  data: 0.0020  max mem: 295\n",
            "Train: Epoch[1/1]  [100/106]  eta: 0:00:00  Lr: 0.000047  Loss: 1.3434  Acc@1: 75.0000 (48.5149)  Acc@5: 95.8333 (85.6848)  time: 0.0898  data: 0.0009  max mem: 295\n",
            "Train: Epoch[1/1]  [105/106]  eta: 0:00:00  Lr: 0.000047  Loss: 1.2329  Acc@1: 75.0000 (49.4062)  Acc@5: 100.0000 (86.2629)  time: 0.0871  data: 0.0006  max mem: 295\n",
            "Train: Epoch[1/1] Total time: 0:00:10 (0.1022 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.2329  Acc@1: 75.0000 (49.4062)  Acc@5: 100.0000 (86.2629)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:16  Loss: 0.8448 (0.8448)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  time: 0.4000  data: 0.3225  max mem: 295\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 0.8418 (0.8266)  Acc@1: 75.0000 (75.3788)  Acc@5: 95.8333 (95.4545)  time: 0.1195  data: 0.0312  max mem: 295\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.8106 (0.8142)  Acc@1: 75.0000 (77.1825)  Acc@5: 95.8333 (94.8413)  time: 0.0897  data: 0.0016  max mem: 295\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.7957 (0.7791)  Acc@1: 79.1667 (78.2258)  Acc@5: 95.8333 (95.5645)  time: 0.0882  data: 0.0008  max mem: 295\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6126 (0.7499)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (96.0366)  time: 0.0889  data: 0.0004  max mem: 295\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.5839 (0.7399)  Acc@1: 83.3333 (79.4000)  Acc@5: 100.0000 (96.1000)  time: 0.0871  data: 0.0004  max mem: 295\n",
            "Test: [Task 1] Total time: 0:00:04 (0.0978 s / it)\n",
            "* Acc@1 79.400 Acc@5 96.100 loss 0.740\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:20  Loss: 0.9469 (0.9469)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  time: 0.4907  data: 0.4112  max mem: 295\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 0.9396 (0.9405)  Acc@1: 70.8333 (69.6970)  Acc@5: 95.8333 (95.4545)  time: 0.1265  data: 0.0387  max mem: 295\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 0.9394 (0.9964)  Acc@1: 66.6667 (69.6429)  Acc@5: 95.8333 (93.8492)  time: 0.0888  data: 0.0012  max mem: 295\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.9532 (0.9972)  Acc@1: 66.6667 (70.4301)  Acc@5: 95.8333 (94.2204)  time: 0.0882  data: 0.0007  max mem: 295\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.8801 (0.9786)  Acc@1: 70.8333 (70.7317)  Acc@5: 95.8333 (94.3089)  time: 0.0891  data: 0.0004  max mem: 295\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.8792 (0.9730)  Acc@1: 70.8333 (71.0000)  Acc@5: 95.8333 (94.3000)  time: 0.0871  data: 0.0004  max mem: 295\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1004 s / it)\n",
            "* Acc@1 71.000 Acc@5 94.300 loss 0.973\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:30  Loss: 0.6559 (0.6559)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  time: 0.7349  data: 0.6299  max mem: 295\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:04  Loss: 0.8180 (0.8386)  Acc@1: 75.0000 (76.5152)  Acc@5: 95.8333 (94.3182)  time: 0.1478  data: 0.0580  max mem: 295\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 0.8139 (0.7947)  Acc@1: 75.0000 (77.5794)  Acc@5: 95.8333 (95.0397)  time: 0.0884  data: 0.0012  max mem: 295\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.6860 (0.7734)  Acc@1: 79.1667 (77.9570)  Acc@5: 95.8333 (95.2957)  time: 0.0881  data: 0.0016  max mem: 295\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.7641 (0.7891)  Acc@1: 83.3333 (78.2520)  Acc@5: 95.8333 (94.8171)  time: 0.0887  data: 0.0009  max mem: 295\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.7641 (0.7894)  Acc@1: 83.3333 (78.2000)  Acc@5: 95.8333 (94.9000)  time: 0.0874  data: 0.0009  max mem: 295\n",
            "Test: [Task 3] Total time: 0:00:04 (0.1057 s / it)\n",
            "* Acc@1 78.200 Acc@5 94.900 loss 0.789\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:18  Loss: 1.2557 (1.2557)  Acc@1: 66.6667 (66.6667)  Acc@5: 79.1667 (79.1667)  time: 0.4496  data: 0.3765  max mem: 295\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:03  Loss: 0.8746 (0.8426)  Acc@1: 75.0000 (74.2424)  Acc@5: 95.8333 (93.9394)  time: 0.1214  data: 0.0351  max mem: 295\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 0.8321 (0.8136)  Acc@1: 75.0000 (76.5873)  Acc@5: 95.8333 (94.8413)  time: 0.0879  data: 0.0006  max mem: 295\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 0.7163 (0.7793)  Acc@1: 79.1667 (77.4194)  Acc@5: 95.8333 (95.0269)  time: 0.0880  data: 0.0003  max mem: 295\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7596 (0.7903)  Acc@1: 79.1667 (77.6423)  Acc@5: 95.8333 (95.2236)  time: 0.0892  data: 0.0003  max mem: 295\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.7076 (0.7813)  Acc@1: 79.1667 (77.9000)  Acc@5: 95.8333 (95.3000)  time: 0.0871  data: 0.0002  max mem: 295\n",
            "Test: [Task 4] Total time: 0:00:04 (0.0982 s / it)\n",
            "* Acc@1 77.900 Acc@5 95.300 loss 0.781\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:15  Loss: 0.6236 (0.6236)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.3708  data: 0.2899  max mem: 295\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:03  Loss: 0.7022 (0.7386)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (98.8636)  time: 0.1195  data: 0.0334  max mem: 295\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 0.7562 (0.7660)  Acc@1: 79.1667 (78.1746)  Acc@5: 100.0000 (97.6190)  time: 0.0913  data: 0.0043  max mem: 295\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 0.7422 (0.7661)  Acc@1: 79.1667 (79.3011)  Acc@5: 95.8333 (96.6398)  time: 0.0885  data: 0.0005  max mem: 295\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.8039 (0.8122)  Acc@1: 75.0000 (77.5407)  Acc@5: 95.8333 (96.3415)  time: 0.0890  data: 0.0002  max mem: 295\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.8736 (0.8265)  Acc@1: 75.0000 (77.4000)  Acc@5: 95.8333 (96.2000)  time: 0.0871  data: 0.0002  max mem: 295\n",
            "Test: [Task 5] Total time: 0:00:04 (0.0983 s / it)\n",
            "* Acc@1 77.400 Acc@5 96.200 loss 0.826\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:19  Loss: 0.8220 (0.8220)  Acc@1: 83.3333 (83.3333)  Acc@5: 91.6667 (91.6667)  time: 0.4580  data: 0.3733  max mem: 295\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:03  Loss: 0.8631 (0.8815)  Acc@1: 79.1667 (75.7576)  Acc@5: 95.8333 (94.6970)  time: 0.1218  data: 0.0342  max mem: 295\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 0.8838 (0.9113)  Acc@1: 75.0000 (74.4048)  Acc@5: 95.8333 (94.8413)  time: 0.0889  data: 0.0003  max mem: 295\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 0.9012 (0.9303)  Acc@1: 70.8333 (73.6559)  Acc@5: 95.8333 (94.7581)  time: 0.0894  data: 0.0014  max mem: 295\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 0.9198 (0.9463)  Acc@1: 70.8333 (73.2724)  Acc@5: 95.8333 (94.6138)  time: 0.0888  data: 0.0014  max mem: 295\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 0.9198 (0.9495)  Acc@1: 70.8333 (73.1000)  Acc@5: 91.6667 (94.5000)  time: 0.0873  data: 0.0014  max mem: 295\n",
            "Test: [Task 6] Total time: 0:00:04 (0.0997 s / it)\n",
            "* Acc@1 73.100 Acc@5 94.500 loss 0.950\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:15  Loss: 1.1433 (1.1433)  Acc@1: 70.8333 (70.8333)  Acc@5: 87.5000 (87.5000)  time: 0.3768  data: 0.3049  max mem: 295\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:03  Loss: 0.8817 (0.9095)  Acc@1: 70.8333 (73.1061)  Acc@5: 95.8333 (95.0758)  time: 0.1199  data: 0.0352  max mem: 295\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:02  Loss: 0.8817 (0.9400)  Acc@1: 70.8333 (72.8175)  Acc@5: 95.8333 (95.4365)  time: 0.0913  data: 0.0044  max mem: 295\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:01  Loss: 0.9919 (0.9845)  Acc@1: 70.8333 (71.9086)  Acc@5: 95.8333 (94.6237)  time: 0.0885  data: 0.0004  max mem: 295\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 1.0028 (0.9882)  Acc@1: 66.6667 (72.1545)  Acc@5: 95.8333 (94.4106)  time: 0.0889  data: 0.0003  max mem: 295\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 0.9919 (0.9839)  Acc@1: 70.8333 (72.3000)  Acc@5: 95.8333 (94.4000)  time: 0.0871  data: 0.0003  max mem: 295\n",
            "Test: [Task 7] Total time: 0:00:04 (0.0981 s / it)\n",
            "* Acc@1 72.300 Acc@5 94.400 loss 0.984\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:13  Loss: 1.0148 (1.0148)  Acc@1: 66.6667 (66.6667)  Acc@5: 95.8333 (95.8333)  time: 0.3284  data: 0.2384  max mem: 295\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:03  Loss: 1.1652 (1.1398)  Acc@1: 66.6667 (67.0455)  Acc@5: 91.6667 (92.0455)  time: 0.1220  data: 0.0375  max mem: 295\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:02  Loss: 1.0810 (1.1018)  Acc@1: 66.6667 (68.2540)  Acc@5: 91.6667 (92.6587)  time: 0.0954  data: 0.0088  max mem: 295\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:01  Loss: 1.0581 (1.0888)  Acc@1: 66.6667 (68.2796)  Acc@5: 95.8333 (93.2796)  time: 0.0895  data: 0.0003  max mem: 295\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 1.1366 (1.1100)  Acc@1: 66.6667 (67.8862)  Acc@5: 91.6667 (92.3781)  time: 0.0900  data: 0.0003  max mem: 295\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 1.1366 (1.1098)  Acc@1: 66.6667 (67.8000)  Acc@5: 91.6667 (92.4000)  time: 0.0879  data: 0.0003  max mem: 295\n",
            "Test: [Task 8] Total time: 0:00:04 (0.0993 s / it)\n",
            "* Acc@1 67.800 Acc@5 92.400 loss 1.110\n",
            "Test: [Task 9]  [ 0/42]  eta: 0:00:22  Loss: 1.1976 (1.1976)  Acc@1: 58.3333 (58.3333)  Acc@5: 100.0000 (100.0000)  time: 0.5340  data: 0.4593  max mem: 295\n",
            "Test: [Task 9]  [10/42]  eta: 0:00:04  Loss: 1.2013 (1.2112)  Acc@1: 66.6667 (65.1515)  Acc@5: 95.8333 (94.6970)  time: 0.1302  data: 0.0424  max mem: 295\n",
            "Test: [Task 9]  [20/42]  eta: 0:00:02  Loss: 1.2169 (1.1962)  Acc@1: 66.6667 (65.8730)  Acc@5: 91.6667 (93.8492)  time: 0.0885  data: 0.0005  max mem: 295\n",
            "Test: [Task 9]  [30/42]  eta: 0:00:01  Loss: 1.2367 (1.2359)  Acc@1: 66.6667 (64.5161)  Acc@5: 95.8333 (94.2204)  time: 0.0881  data: 0.0012  max mem: 295\n",
            "Test: [Task 9]  [40/42]  eta: 0:00:00  Loss: 1.0960 (1.1906)  Acc@1: 66.6667 (65.0407)  Acc@5: 95.8333 (94.5122)  time: 0.0892  data: 0.0013  max mem: 295\n",
            "Test: [Task 9]  [41/42]  eta: 0:00:00  Loss: 1.0960 (1.1801)  Acc@1: 66.6667 (65.3000)  Acc@5: 95.8333 (94.6000)  time: 0.0872  data: 0.0013  max mem: 295\n",
            "Test: [Task 9] Total time: 0:00:04 (0.1016 s / it)\n",
            "* Acc@1 65.300 Acc@5 94.600 loss 1.180\n",
            "Test: [Task 10]  [ 0/42]  eta: 0:00:31  Loss: 5.8164 (5.8164)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  time: 0.7552  data: 0.6587  max mem: 295\n",
            "Test: [Task 10]  [10/42]  eta: 0:00:04  Loss: 5.9775 (5.9532)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  time: 0.1506  data: 0.0610  max mem: 295\n",
            "Test: [Task 10]  [20/42]  eta: 0:00:02  Loss: 5.9775 (5.9392)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  time: 0.0892  data: 0.0018  max mem: 295\n",
            "Test: [Task 10]  [30/42]  eta: 0:00:01  Loss: 6.0038 (5.9592)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  time: 0.0886  data: 0.0014  max mem: 295\n",
            "Test: [Task 10]  [40/42]  eta: 0:00:00  Loss: 6.0289 (5.9596)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  time: 0.0894  data: 0.0003  max mem: 295\n",
            "Test: [Task 10]  [41/42]  eta: 0:00:00  Loss: 6.0110 (5.9504)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  time: 0.0875  data: 0.0003  max mem: 295\n",
            "Test: [Task 10] Total time: 0:00:04 (0.1065 s / it)\n",
            "* Acc@1 0.000 Acc@5 0.000 loss 5.950\n",
            "[Average accuracy till task10]\tAcc@1: 66.2400\tAcc@5: 85.2700\tLoss: 1.4284\tForgetting: 3.2333\tBackward: 60.9333\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([23928, 384])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 0.4276  Acc@1: 83.3333 (84.1667)  Acc@5: 87.5000 (89.6759)\n",
            "torch.Size([23928, 384])\n",
            "Averaged stats: Lr: 0.000467  Loss: 0.5786  Acc@1: 83.3333 (83.9815)  Acc@5: 91.6667 (92.1759)\n",
            "torch.Size([23928, 384])\n",
            "Averaged stats: Lr: 0.000464  Loss: 0.5664  Acc@1: 87.5000 (85.7870)  Acc@5: 100.0000 (96.7593)\n",
            "torch.Size([23928, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 0.8790  Acc@1: 87.5000 (86.8982)  Acc@5: 100.0000 (98.1944)\n",
            "torch.Size([23928, 384])\n",
            "Averaged stats: Lr: 0.000448  Loss: 0.5086  Acc@1: 87.5000 (87.4537)  Acc@5: 100.0000 (98.7500)\n",
            "torch.Size([23928, 384])\n",
            "Averaged stats: Lr: 0.000437  Loss: 0.6107  Acc@1: 87.5000 (89.4444)  Acc@5: 100.0000 (99.3519)\n",
            "torch.Size([23928, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 0.5298  Acc@1: 91.6667 (90.2778)  Acc@5: 100.0000 (99.3519)\n",
            "torch.Size([23928, 384])\n",
            "Averaged stats: Lr: 0.000409  Loss: 0.4386  Acc@1: 87.5000 (90.8333)  Acc@5: 100.0000 (99.7222)\n",
            "torch.Size([23928, 384])\n",
            "Averaged stats: Lr: 0.000391  Loss: 0.4479  Acc@1: 91.6667 (91.1111)  Acc@5: 100.0000 (99.5370)\n",
            "torch.Size([23928, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 0.3520  Acc@1: 91.6667 (91.9444)  Acc@5: 100.0000 (99.4907)\n",
            "torch.Size([23928, 384])\n",
            "Averaged stats: Lr: 0.000352  Loss: 0.4121  Acc@1: 91.6667 (91.2500)  Acc@5: 100.0000 (99.8611)\n",
            "torch.Size([23928, 384])\n",
            "Averaged stats: Lr: 0.000330  Loss: 0.3731  Acc@1: 91.6667 (91.4352)  Acc@5: 100.0000 (99.7685)\n",
            "torch.Size([23928, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 0.5217  Acc@1: 91.6667 (92.7778)  Acc@5: 100.0000 (99.6296)\n",
            "torch.Size([23928, 384])\n",
            "Averaged stats: Lr: 0.000283  Loss: 0.5604  Acc@1: 91.6667 (93.5185)  Acc@5: 100.0000 (99.7685)\n",
            "torch.Size([23928, 384])\n",
            "Averaged stats: Lr: 0.000259  Loss: 0.4429  Acc@1: 91.6667 (93.2407)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([23928, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 0.3411  Acc@1: 95.8333 (94.0278)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([23928, 384])\n",
            "Averaged stats: Lr: 0.000210  Loss: 0.4775  Acc@1: 95.8333 (93.2870)  Acc@5: 100.0000 (99.9074)\n",
            "torch.Size([23928, 384])\n",
            "Averaged stats: Lr: 0.000186  Loss: 0.4247  Acc@1: 91.6667 (91.8519)  Acc@5: 100.0000 (99.8148)\n",
            "torch.Size([23928, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.5383  Acc@1: 91.6667 (92.5463)  Acc@5: 100.0000 (99.6759)\n",
            "torch.Size([23928, 384])\n",
            "Averaged stats: Lr: 0.000139  Loss: 0.4719  Acc@1: 91.6667 (93.3333)  Acc@5: 100.0000 (99.9074)\n",
            "torch.Size([23928, 384])\n",
            "Averaged stats: Lr: 0.000117  Loss: 0.3312  Acc@1: 95.8333 (93.2870)  Acc@5: 100.0000 (99.8611)\n",
            "torch.Size([23928, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 0.3420  Acc@1: 91.6667 (92.5463)  Acc@5: 100.0000 (99.9074)\n",
            "torch.Size([23928, 384])\n",
            "Averaged stats: Lr: 0.000078  Loss: 0.3732  Acc@1: 91.6667 (91.9907)  Acc@5: 100.0000 (99.9074)\n",
            "torch.Size([23928, 384])\n",
            "Averaged stats: Lr: 0.000060  Loss: 0.3225  Acc@1: 91.6667 (93.6111)  Acc@5: 100.0000 (99.9074)\n",
            "torch.Size([23928, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 0.3710  Acc@1: 95.8333 (93.0093)  Acc@5: 100.0000 (99.7685)\n",
            "torch.Size([23928, 384])\n",
            "Averaged stats: Lr: 0.000031  Loss: 0.4284  Acc@1: 91.6667 (93.6111)  Acc@5: 100.0000 (99.8148)\n",
            "torch.Size([23928, 384])\n",
            "Averaged stats: Lr: 0.000020  Loss: 0.2989  Acc@1: 95.8333 (93.9352)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([23928, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.3088  Acc@1: 95.8333 (93.7963)  Acc@5: 100.0000 (99.8611)\n",
            "torch.Size([23928, 384])\n",
            "Averaged stats: Lr: 0.000005  Loss: 0.3648  Acc@1: 91.6667 (93.6574)  Acc@5: 100.0000 (99.9537)\n",
            "torch.Size([23928, 384])\n",
            "Averaged stats: Lr: 0.000001  Loss: 0.5923  Acc@1: 87.5000 (92.8704)  Acc@5: 100.0000 (99.8611)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:22  Loss: 0.8216 (0.8216)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  time: 0.5239  data: 0.4462  max mem: 296\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 0.8216 (0.8131)  Acc@1: 75.0000 (75.3788)  Acc@5: 95.8333 (94.3182)  time: 0.1302  data: 0.0416  max mem: 296\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.7661 (0.7867)  Acc@1: 79.1667 (77.9762)  Acc@5: 95.8333 (94.8413)  time: 0.0897  data: 0.0008  max mem: 296\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.7796 (0.7526)  Acc@1: 79.1667 (77.9570)  Acc@5: 95.8333 (95.4301)  time: 0.0888  data: 0.0007  max mem: 296\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.5860 (0.7246)  Acc@1: 79.1667 (78.7602)  Acc@5: 95.8333 (96.0366)  time: 0.0890  data: 0.0006  max mem: 296\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.5793 (0.7144)  Acc@1: 79.1667 (79.0000)  Acc@5: 100.0000 (96.1000)  time: 0.0875  data: 0.0006  max mem: 296\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1022 s / it)\n",
            "* Acc@1 79.000 Acc@5 96.100 loss 0.714\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:30  Loss: 0.8799 (0.8799)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  time: 0.7217  data: 0.6240  max mem: 296\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 0.8845 (0.8920)  Acc@1: 70.8333 (71.2121)  Acc@5: 95.8333 (96.2121)  time: 0.1473  data: 0.0570  max mem: 296\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 0.9031 (0.9650)  Acc@1: 70.8333 (71.0317)  Acc@5: 95.8333 (94.8413)  time: 0.0929  data: 0.0049  max mem: 296\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.9538 (0.9633)  Acc@1: 70.8333 (70.9677)  Acc@5: 95.8333 (94.8925)  time: 0.0931  data: 0.0052  max mem: 296\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.8240 (0.9363)  Acc@1: 70.8333 (71.4431)  Acc@5: 95.8333 (95.1220)  time: 0.0905  data: 0.0006  max mem: 296\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.8210 (0.9312)  Acc@1: 70.8333 (71.7000)  Acc@5: 95.8333 (95.1000)  time: 0.0892  data: 0.0005  max mem: 296\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1082 s / it)\n",
            "* Acc@1 71.700 Acc@5 95.100 loss 0.931\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:18  Loss: 0.5924 (0.5924)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.4523  data: 0.3751  max mem: 296\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 0.8166 (0.8070)  Acc@1: 79.1667 (77.6515)  Acc@5: 95.8333 (95.0758)  time: 0.1242  data: 0.0354  max mem: 296\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 0.7940 (0.7540)  Acc@1: 79.1667 (78.5714)  Acc@5: 95.8333 (95.4365)  time: 0.0906  data: 0.0010  max mem: 296\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.6649 (0.7331)  Acc@1: 79.1667 (78.2258)  Acc@5: 95.8333 (95.5645)  time: 0.0898  data: 0.0007  max mem: 296\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.6892 (0.7485)  Acc@1: 79.1667 (78.5569)  Acc@5: 95.8333 (95.0203)  time: 0.0901  data: 0.0006  max mem: 296\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.6892 (0.7490)  Acc@1: 79.1667 (78.4000)  Acc@5: 95.8333 (95.1000)  time: 0.0884  data: 0.0006  max mem: 296\n",
            "Test: [Task 3] Total time: 0:00:04 (0.1004 s / it)\n",
            "* Acc@1 78.400 Acc@5 95.100 loss 0.749\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:16  Loss: 1.2903 (1.2903)  Acc@1: 62.5000 (62.5000)  Acc@5: 83.3333 (83.3333)  time: 0.3861  data: 0.3084  max mem: 296\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:03  Loss: 0.9539 (0.9562)  Acc@1: 70.8333 (69.3182)  Acc@5: 95.8333 (94.6970)  time: 0.1217  data: 0.0331  max mem: 296\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 0.8657 (0.9171)  Acc@1: 75.0000 (72.0238)  Acc@5: 95.8333 (94.4444)  time: 0.0924  data: 0.0030  max mem: 296\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 0.8176 (0.8719)  Acc@1: 75.0000 (73.5215)  Acc@5: 95.8333 (94.8925)  time: 0.0901  data: 0.0006  max mem: 296\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.8214 (0.8866)  Acc@1: 75.0000 (73.2724)  Acc@5: 95.8333 (94.5122)  time: 0.0906  data: 0.0005  max mem: 296\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.8197 (0.8781)  Acc@1: 79.1667 (73.6000)  Acc@5: 95.8333 (94.5000)  time: 0.0889  data: 0.0005  max mem: 296\n",
            "Test: [Task 4] Total time: 0:00:04 (0.1017 s / it)\n",
            "* Acc@1 73.600 Acc@5 94.500 loss 0.878\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:31  Loss: 0.7227 (0.7227)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.7548  data: 0.6402  max mem: 296\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:05  Loss: 0.7320 (0.7900)  Acc@1: 75.0000 (76.5152)  Acc@5: 100.0000 (98.8636)  time: 0.1584  data: 0.0679  max mem: 296\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 0.8196 (0.8253)  Acc@1: 75.0000 (75.5952)  Acc@5: 100.0000 (97.8175)  time: 0.0958  data: 0.0074  max mem: 296\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 0.7959 (0.8182)  Acc@1: 79.1667 (76.8817)  Acc@5: 95.8333 (96.5054)  time: 0.0921  data: 0.0028  max mem: 296\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.8409 (0.8644)  Acc@1: 75.0000 (75.2033)  Acc@5: 95.8333 (95.8333)  time: 0.0911  data: 0.0010  max mem: 296\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.9001 (0.8799)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (95.7000)  time: 0.0897  data: 0.0008  max mem: 296\n",
            "Test: [Task 5] Total time: 0:00:04 (0.1106 s / it)\n",
            "* Acc@1 75.000 Acc@5 95.700 loss 0.880\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:20  Loss: 0.8016 (0.8016)  Acc@1: 83.3333 (83.3333)  Acc@5: 91.6667 (91.6667)  time: 0.4829  data: 0.3961  max mem: 296\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:04  Loss: 0.8360 (0.8669)  Acc@1: 79.1667 (76.8939)  Acc@5: 95.8333 (95.4545)  time: 0.1273  data: 0.0369  max mem: 296\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 0.8678 (0.9064)  Acc@1: 75.0000 (74.4048)  Acc@5: 95.8333 (95.2381)  time: 0.0907  data: 0.0008  max mem: 296\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 0.9633 (0.9299)  Acc@1: 70.8333 (72.8495)  Acc@5: 95.8333 (95.0269)  time: 0.0896  data: 0.0005  max mem: 296\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 0.9923 (0.9443)  Acc@1: 70.8333 (72.2561)  Acc@5: 95.8333 (94.7154)  time: 0.0901  data: 0.0004  max mem: 296\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 0.9923 (0.9471)  Acc@1: 68.7500 (72.2000)  Acc@5: 91.6667 (94.6000)  time: 0.0884  data: 0.0004  max mem: 296\n",
            "Test: [Task 6] Total time: 0:00:04 (0.1013 s / it)\n",
            "* Acc@1 72.200 Acc@5 94.600 loss 0.947\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:21  Loss: 1.1729 (1.1729)  Acc@1: 70.8333 (70.8333)  Acc@5: 83.3333 (83.3333)  time: 0.5109  data: 0.4211  max mem: 296\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:04  Loss: 0.8351 (0.8774)  Acc@1: 70.8333 (74.2424)  Acc@5: 95.8333 (94.6970)  time: 0.1293  data: 0.0388  max mem: 296\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:02  Loss: 0.8655 (0.9175)  Acc@1: 70.8333 (72.8175)  Acc@5: 95.8333 (94.8413)  time: 0.0906  data: 0.0010  max mem: 296\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:01  Loss: 1.0176 (0.9695)  Acc@1: 70.8333 (71.5054)  Acc@5: 95.8333 (93.9516)  time: 0.0905  data: 0.0011  max mem: 296\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 1.0176 (0.9743)  Acc@1: 70.8333 (71.6463)  Acc@5: 95.8333 (93.9024)  time: 0.0909  data: 0.0024  max mem: 296\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 1.0032 (0.9696)  Acc@1: 70.8333 (71.9000)  Acc@5: 95.8333 (93.9000)  time: 0.0890  data: 0.0022  max mem: 296\n",
            "Test: [Task 7] Total time: 0:00:04 (0.1031 s / it)\n",
            "* Acc@1 71.900 Acc@5 93.900 loss 0.970\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:31  Loss: 0.9504 (0.9504)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  time: 0.7542  data: 0.5812  max mem: 296\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:05  Loss: 1.1270 (1.0808)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (92.8030)  time: 0.1589  data: 0.0625  max mem: 296\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:02  Loss: 1.0493 (1.0429)  Acc@1: 70.8333 (69.8413)  Acc@5: 91.6667 (93.8492)  time: 0.0990  data: 0.0115  max mem: 296\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:01  Loss: 0.9198 (1.0225)  Acc@1: 70.8333 (70.0269)  Acc@5: 95.8333 (93.9516)  time: 0.1046  data: 0.0175  max mem: 296\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 1.0331 (1.0420)  Acc@1: 70.8333 (69.0041)  Acc@5: 91.6667 (93.2927)  time: 0.1001  data: 0.0116  max mem: 296\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 1.0369 (1.0418)  Acc@1: 68.7500 (69.0000)  Acc@5: 91.6667 (93.3000)  time: 0.0917  data: 0.0037  max mem: 296\n",
            "Test: [Task 8] Total time: 0:00:04 (0.1174 s / it)\n",
            "* Acc@1 69.000 Acc@5 93.300 loss 1.042\n",
            "Test: [Task 9]  [ 0/42]  eta: 0:00:23  Loss: 0.9007 (0.9007)  Acc@1: 70.8333 (70.8333)  Acc@5: 100.0000 (100.0000)  time: 0.5511  data: 0.4796  max mem: 296\n",
            "Test: [Task 9]  [10/42]  eta: 0:00:04  Loss: 0.9200 (0.9364)  Acc@1: 70.8333 (72.7273)  Acc@5: 95.8333 (94.6970)  time: 0.1320  data: 0.0446  max mem: 296\n",
            "Test: [Task 9]  [20/42]  eta: 0:00:02  Loss: 0.9200 (0.9084)  Acc@1: 70.8333 (75.0000)  Acc@5: 95.8333 (94.6429)  time: 0.0891  data: 0.0008  max mem: 296\n",
            "Test: [Task 9]  [30/42]  eta: 0:00:01  Loss: 0.9868 (0.9490)  Acc@1: 75.0000 (73.9247)  Acc@5: 95.8333 (94.8925)  time: 0.0885  data: 0.0005  max mem: 296\n",
            "Test: [Task 9]  [40/42]  eta: 0:00:00  Loss: 0.7622 (0.9002)  Acc@1: 79.1667 (75.6098)  Acc@5: 95.8333 (95.1220)  time: 0.0893  data: 0.0004  max mem: 296\n",
            "Test: [Task 9]  [41/42]  eta: 0:00:00  Loss: 0.7551 (0.8899)  Acc@1: 79.1667 (75.8000)  Acc@5: 95.8333 (95.2000)  time: 0.0874  data: 0.0004  max mem: 296\n",
            "Test: [Task 9] Total time: 0:00:04 (0.1015 s / it)\n",
            "* Acc@1 75.800 Acc@5 95.200 loss 0.890\n",
            "Test: [Task 10]  [ 0/42]  eta: 0:00:23  Loss: 1.0847 (1.0847)  Acc@1: 66.6667 (66.6667)  Acc@5: 95.8333 (95.8333)  time: 0.5638  data: 0.4882  max mem: 296\n",
            "Test: [Task 10]  [10/42]  eta: 0:00:04  Loss: 1.3898 (1.3566)  Acc@1: 62.5000 (61.3636)  Acc@5: 91.6667 (90.9091)  time: 0.1335  data: 0.0456  max mem: 296\n",
            "Test: [Task 10]  [20/42]  eta: 0:00:02  Loss: 1.3399 (1.3147)  Acc@1: 62.5000 (62.5000)  Acc@5: 91.6667 (92.2619)  time: 0.0891  data: 0.0009  max mem: 296\n",
            "Test: [Task 10]  [30/42]  eta: 0:00:01  Loss: 1.2901 (1.3071)  Acc@1: 58.3333 (61.5591)  Acc@5: 91.6667 (92.4731)  time: 0.0879  data: 0.0004  max mem: 296\n",
            "Test: [Task 10]  [40/42]  eta: 0:00:00  Loss: 1.2787 (1.3193)  Acc@1: 58.3333 (60.4675)  Acc@5: 91.6667 (92.6829)  time: 0.0889  data: 0.0004  max mem: 296\n",
            "Test: [Task 10]  [41/42]  eta: 0:00:00  Loss: 1.2581 (1.3108)  Acc@1: 58.3333 (60.7000)  Acc@5: 93.7500 (92.7000)  time: 0.0873  data: 0.0004  max mem: 296\n",
            "Test: [Task 10] Total time: 0:00:04 (0.1017 s / it)\n",
            "* Acc@1 60.700 Acc@5 92.700 loss 1.311\n",
            "[Average accuracy till task10]\tAcc@1: 72.7300\tAcc@5: 94.6200\tLoss: 0.9312\tForgetting: 3.7889\tBackward: 4.6778\n",
            "Total training time: 0:17:36\n",
            "[rank0]:[W1003 14:30:25.083115948 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
          ]
        }
      ],
      "source": [
        "!torchrun --nproc_per_node=1 main.py cifar100_hideprompt_5e --original_model vit_small_patch16_224.dino --model vit_small_patch16_224.dino --batch-size 24 --data-path ./datasets/ --output_dir ./output/cifar100_full_dino_1epoch_100pct --epochs 1 --sched constant --seed 20 --train_inference_task_only --lr 0.0005 --pct 1.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "902dd0fc",
      "metadata": {
        "id": "902dd0fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8e82de0-4f1e-4495-9be5-2bea8ef7dfc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(subparser_name='cifar100_hideprompt_5e', pct=0.5, batch_size=24, epochs=1, original_model='vit_small_patch16_224.dino', model='vit_small_patch16_224.dino', input_size=224, pretrained=True, drop=0.0, drop_path=0.0, opt='adam', opt_eps=1e-08, opt_betas=(0.9, 0.999), clip_grad=1.0, momentum=0.9, weight_decay=0.0, reinit_optimizer=True, sched='step', lr=0.03, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=0, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, unscale_lr=True, color_jitter=None, aa=None, smoothing=0.1, train_interpolation='bicubic', reprob=0.0, remode='pixel', recount=1, data_path='./datasets/', dataset='Split-CIFAR100', shuffle=False, output_dir='./output/cifar100_full_dino_1epoch_final_50pct', device='cuda', seed=20, eval=False, num_workers=4, pin_mem=True, world_size=1, dist_url='env://', num_tasks=10, train_mask=True, task_inc=False, use_g_prompt=False, g_prompt_length=5, g_prompt_layer_idx=[], use_prefix_tune_for_g_prompt=False, use_e_prompt=True, e_prompt_layer_idx=[0, 1, 2, 3, 4], use_prefix_tune_for_e_prompt=True, larger_prompt_lr=True, prompt_pool=True, size=10, length=5, top_k=1, initializer='uniform', prompt_key=False, prompt_key_init='uniform', use_prompt_mask=True, mask_first_epoch=False, shared_prompt_pool=True, shared_prompt_key=False, batchwise_prompt=False, embedding_key='cls', predefined_key='', pull_constraint=True, pull_constraint_coeff=1.0, same_key_value=False, global_pool='token', head_type='token', freeze=['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed'], crct_epochs=1, train_inference_task_only=False, original_model_mlp_structure=[2], ca_lr=0.005, milestones=[10], trained_original_model='./output/cifar100_full_dino_1epoch_50pct', prompt_momentum=0.1, reg=0.1, not_train_ca=False, ca_epochs=30, ca_storage_efficient_method='multi-centroid', n_centroids=10, print_freq=10, config='cifar100_hideprompt_5e')\n",
            "| distributed init (rank 0): env://\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "[rank0]:[W1003 14:33:16.448814567 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/helpers.py:7: FutureWarning: Importing from timm.models.helpers is deprecated, please import via timm.models\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:886: UserWarning: Overwriting vit_tiny_patch16_224 in registry with vits.hide_prompt_vision_transformer.vit_tiny_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:895: UserWarning: Overwriting vit_tiny_patch16_384 in registry with vits.hide_prompt_vision_transformer.vit_tiny_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:904: UserWarning: Overwriting vit_small_patch32_224 in registry with vits.hide_prompt_vision_transformer.vit_small_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:913: UserWarning: Overwriting vit_small_patch32_384 in registry with vits.hide_prompt_vision_transformer.vit_small_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:922: UserWarning: Overwriting vit_small_patch16_224 in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:932: UserWarning: Overwriting vit_small_patch16_384 in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:942: UserWarning: Overwriting vit_base_patch32_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:952: UserWarning: Overwriting vit_base_patch32_384 in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:962: UserWarning: Overwriting vit_base_patch16_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:972: UserWarning: Overwriting vit_base_patch16_384 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:982: UserWarning: Overwriting vit_base_patch8_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:992: UserWarning: Overwriting vit_large_patch32_224 in registry with vits.hide_prompt_vision_transformer.vit_large_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1001: UserWarning: Overwriting vit_large_patch32_384 in registry with vits.hide_prompt_vision_transformer.vit_large_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1011: UserWarning: Overwriting vit_large_patch16_224 in registry with vits.hide_prompt_vision_transformer.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1021: UserWarning: Overwriting vit_large_patch16_384 in registry with vits.hide_prompt_vision_transformer.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1031: UserWarning: Overwriting vit_large_patch14_224 in registry with vits.hide_prompt_vision_transformer.vit_large_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1040: UserWarning: Overwriting vit_huge_patch14_224 in registry with vits.hide_prompt_vision_transformer.vit_huge_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1049: UserWarning: Overwriting vit_giant_patch14_224 in registry with vits.hide_prompt_vision_transformer.vit_giant_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1058: UserWarning: Overwriting vit_gigantic_patch14_224 in registry with vits.hide_prompt_vision_transformer.vit_gigantic_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1067: UserWarning: Overwriting vit_tiny_patch16_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_tiny_patch16_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1078: UserWarning: Overwriting vit_small_patch32_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_small_patch32_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1089: UserWarning: Overwriting vit_small_patch16_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1100: UserWarning: Overwriting vit_base_patch32_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1111: UserWarning: Overwriting vit_base_patch16_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1122: UserWarning: Overwriting vit_base_patch8_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_base_patch8_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1133: UserWarning: Overwriting vit_large_patch32_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_large_patch32_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1144: UserWarning: Overwriting vit_large_patch16_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_large_patch16_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1155: UserWarning: Overwriting vit_huge_patch14_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_huge_patch14_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1166: UserWarning: Overwriting vit_base_patch16_224_sam in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_sam. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1175: UserWarning: Overwriting vit_base_patch32_224_sam in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_224_sam. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1184: UserWarning: Overwriting vit_small_patch16_224_dino in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_224_dino. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1193: UserWarning: Overwriting vit_small_patch8_224_dino in registry with vits.hide_prompt_vision_transformer.vit_small_patch8_224_dino. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1211: UserWarning: Overwriting vit_base_patch8_224_dino in registry with vits.hide_prompt_vision_transformer.vit_base_patch8_224_dino. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1220: UserWarning: Overwriting vit_base_patch16_224_miil_in21k in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_miil_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1230: UserWarning: Overwriting vit_base_patch16_224_miil in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_miil. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1242: UserWarning: Overwriting vit_base_patch32_plus_256 in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_plus_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1251: UserWarning: Overwriting vit_base_patch16_plus_240 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_plus_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1260: UserWarning: Overwriting vit_base_patch16_rpn_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_rpn_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1271: UserWarning: Overwriting vit_small_patch16_36x1_224 in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_36x1_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1282: UserWarning: Overwriting vit_small_patch16_18x2_224 in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1294: UserWarning: Overwriting vit_base_patch16_18x2_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1331: UserWarning: Overwriting vit_base_patch16_224_dino in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_dino. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "Original train size:  50000\n",
            "Sampled train size:  25000\n",
            "Original train size:  50000\n",
            "Sampled train size:  25000\n",
            "100\n",
            "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, 13, 14, 15, 16, 17, 18, 19], [20, 21, 22, 23, 24, 25, 26, 27, 28, 29], [30, 31, 32, 33, 34, 35, 36, 37, 38, 39], [40, 41, 42, 43, 44, 45, 46, 47, 48, 49], [50, 51, 52, 53, 54, 55, 56, 57, 58, 59], [60, 61, 62, 63, 64, 65, 66, 67, 68, 69], [70, 71, 72, 73, 74, 75, 76, 77, 78, 79], [80, 81, 82, 83, 84, 85, 86, 87, 88, 89], [90, 91, 92, 93, 94, 95, 96, 97, 98, 99]]\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Creating original model: vit_small_patch16_224.dino\n",
            "[Sequential(\n",
            "  (0): Linear(in_features=384, out_features=768, bias=True)\n",
            "  (1): GELU(approximate='none')\n",
            "  (2): Dropout(p=0.0, inplace=False)\n",
            "), Sequential(\n",
            "  (0): Linear(in_features=768, out_features=384, bias=True)\n",
            "  (1): Dropout(p=0.0, inplace=False)\n",
            ")]\n",
            "Creating model: vit_small_patch16_224.dino\n",
            "Namespace(subparser_name='cifar100_hideprompt_5e', pct=0.5, batch_size=24, epochs=1, original_model='vit_small_patch16_224.dino', model='vit_small_patch16_224.dino', input_size=224, pretrained=True, drop=0.0, drop_path=0.0, opt='adam', opt_eps=1e-08, opt_betas=(0.9, 0.999), clip_grad=1.0, momentum=0.9, weight_decay=0.0, reinit_optimizer=True, sched='step', lr=0.03, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=0, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, unscale_lr=True, color_jitter=None, aa=None, smoothing=0.1, train_interpolation='bicubic', reprob=0.0, remode='pixel', recount=1, data_path='./datasets/', dataset='Split-CIFAR100', shuffle=False, output_dir='./output/cifar100_full_dino_1epoch_final_50pct', device='cuda', seed=20, eval=False, num_workers=4, pin_mem=True, world_size=1, dist_url='env://', num_tasks=10, train_mask=True, task_inc=False, use_g_prompt=False, g_prompt_length=5, g_prompt_layer_idx=[], use_prefix_tune_for_g_prompt=False, use_e_prompt=True, e_prompt_layer_idx=[0, 1, 2, 3, 4], use_prefix_tune_for_e_prompt=True, larger_prompt_lr=True, prompt_pool=True, size=10, length=5, top_k=1, initializer='uniform', prompt_key=False, prompt_key_init='uniform', use_prompt_mask=True, mask_first_epoch=False, shared_prompt_pool=True, shared_prompt_key=False, batchwise_prompt=False, embedding_key='cls', predefined_key='', pull_constraint=True, pull_constraint_coeff=1.0, same_key_value=False, global_pool='token', head_type='token', freeze=['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed'], crct_epochs=1, train_inference_task_only=False, original_model_mlp_structure=[2], ca_lr=0.005, milestones=[10], trained_original_model='./output/cifar100_full_dino_1epoch_50pct', prompt_momentum=0.1, reg=0.1, not_train_ca=False, ca_epochs=30, ca_storage_efficient_method='multi-centroid', n_centroids=10, print_freq=10, config='cifar100_hideprompt_5e', rank=0, gpu=0, distributed=True, dist_backend='nccl', nb_classes=100)\n",
            "number of params: 230500\n",
            "Start training for 1 epochs\n",
            "Loading checkpoint from: ./output/cifar100_full_dino_1epoch_50pct/checkpoint/task1_checkpoint.pth\n",
            "[rank0]:[W1003 14:33:27.601466515 reducer.cpp:1457] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/engines/hide_promtp_wtp_and_tap_engine.py:545: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  loss = torch.nn.functional.cross_entropy(sim, torch.range(0, sim.shape[0] - 1).long().to(device))\n",
            "Train: Epoch[1/1]  [  0/105]  eta: 0:03:58  Lr: 0.002812  Loss: 2.7592  Acc@1: 12.5000 (12.5000)  Acc@5: 62.5000 (62.5000)  time: 2.2752  data: 1.0118  max mem: 1368\n",
            "Train: Epoch[1/1]  [ 10/105]  eta: 0:00:40  Lr: 0.002812  Loss: 2.6931  Acc@1: 12.5000 (12.1212)  Acc@5: 50.0000 (51.8939)  time: 0.4274  data: 0.0930  max mem: 1370\n",
            "Train: Epoch[1/1]  [ 20/105]  eta: 0:00:28  Lr: 0.002812  Loss: 2.3417  Acc@1: 16.6667 (16.2698)  Acc@5: 58.3333 (56.9444)  time: 0.2433  data: 0.0013  max mem: 1370\n",
            "Train: Epoch[1/1]  [ 30/105]  eta: 0:00:23  Lr: 0.002812  Loss: 1.7571  Acc@1: 25.0000 (20.9677)  Acc@5: 66.6667 (62.3656)  time: 0.2435  data: 0.0011  max mem: 1370\n",
            "Train: Epoch[1/1]  [ 40/105]  eta: 0:00:19  Lr: 0.002812  Loss: 1.6079  Acc@1: 33.3333 (25.4065)  Acc@5: 75.0000 (66.6667)  time: 0.2431  data: 0.0005  max mem: 1370\n",
            "Train: Epoch[1/1]  [ 50/105]  eta: 0:00:15  Lr: 0.002812  Loss: 1.3310  Acc@1: 45.8333 (29.6569)  Acc@5: 87.5000 (70.9967)  time: 0.2448  data: 0.0004  max mem: 1370\n",
            "Train: Epoch[1/1]  [ 60/105]  eta: 0:00:12  Lr: 0.002812  Loss: 1.5695  Acc@1: 50.0000 (32.7869)  Acc@5: 87.5000 (73.5656)  time: 0.2478  data: 0.0004  max mem: 1370\n",
            "Train: Epoch[1/1]  [ 70/105]  eta: 0:00:09  Lr: 0.002812  Loss: 1.1771  Acc@1: 50.0000 (36.0329)  Acc@5: 87.5000 (75.8216)  time: 0.2504  data: 0.0007  max mem: 1370\n",
            "Train: Epoch[1/1]  [ 80/105]  eta: 0:00:06  Lr: 0.002812  Loss: 1.0089  Acc@1: 58.3333 (39.2490)  Acc@5: 91.6667 (77.7778)  time: 0.2525  data: 0.0011  max mem: 1370\n",
            "Train: Epoch[1/1]  [ 90/105]  eta: 0:00:04  Lr: 0.002812  Loss: 1.4748  Acc@1: 66.6667 (42.3535)  Acc@5: 91.6667 (79.5788)  time: 0.2545  data: 0.0008  max mem: 1370\n",
            "Train: Epoch[1/1]  [100/105]  eta: 0:00:01  Lr: 0.002812  Loss: 0.7641  Acc@1: 66.6667 (44.7607)  Acc@5: 95.8333 (81.0644)  time: 0.2571  data: 0.0004  max mem: 1370\n",
            "Train: Epoch[1/1]  [104/105]  eta: 0:00:00  Lr: 0.002812  Loss: 0.7560  Acc@1: 66.6667 (45.5636)  Acc@5: 95.8333 (81.4948)  time: 0.2519  data: 0.0003  max mem: 1370\n",
            "Train: Epoch[1/1] Total time: 0:00:28 (0.2680 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.7560  Acc@1: 66.6667 (45.5636)  Acc@5: 95.8333 (81.4948)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:24  Loss: 2.1783 (2.1783)  Acc@1: 54.1667 (54.1667)  Acc@5: 66.6667 (66.6667)  Acc@task: 100.0000 (100.0000)  time: 0.5765  data: 0.3928  max mem: 1370\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 2.1783 (2.1785)  Acc@1: 50.0000 (48.8636)  Acc@5: 75.0000 (75.3788)  Acc@task: 100.0000 (100.0000)  time: 0.2085  data: 0.0364  max mem: 1370\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 2.1416 (2.1751)  Acc@1: 45.8333 (47.8175)  Acc@5: 75.0000 (75.1984)  Acc@task: 100.0000 (100.0000)  time: 0.1718  data: 0.0008  max mem: 1370\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 2.0726 (2.1358)  Acc@1: 45.8333 (49.5968)  Acc@5: 79.1667 (75.6720)  Acc@task: 100.0000 (100.0000)  time: 0.1726  data: 0.0013  max mem: 1370\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 1.9417 (2.1044)  Acc@1: 54.1667 (50.6098)  Acc@5: 79.1667 (76.6260)  Acc@task: 100.0000 (100.0000)  time: 0.1744  data: 0.0010  max mem: 1370\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 1.9174 (2.0901)  Acc@1: 54.1667 (50.8000)  Acc@5: 79.1667 (76.7000)  Acc@task: 100.0000 (100.0000)  time: 0.1764  data: 0.0010  max mem: 1370\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1858 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@task 100.000 Acc@1 50.800 Acc@5 76.700 loss 2.090\n",
            "[Average accuracy till task1]\tAcc@task: 100.0000\tAcc@1: 50.8000\tAcc@5: 76.7000\tLoss: 2.0901\n",
            "Loading checkpoint from: ./output/cifar100_full_dino_1epoch_50pct/checkpoint/task2_checkpoint.pth\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/engines/hide_promtp_wtp_and_tap_engine.py:540: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  loss = torch.nn.functional.cross_entropy(sim, torch.range(0, sim.shape[0] - 1).long().to(device))\n",
            "Train: Epoch[1/1]  [  0/105]  eta: 0:01:14  Lr: 0.002812  Loss: 5.2319  Acc@1: 16.6667 (16.6667)  Acc@5: 50.0000 (50.0000)  time: 0.7141  data: 0.2924  max mem: 1371\n",
            "Train: Epoch[1/1]  [ 10/105]  eta: 0:00:29  Lr: 0.002812  Loss: 2.9564  Acc@1: 12.5000 (12.8788)  Acc@5: 45.8333 (48.4848)  time: 0.3121  data: 0.0269  max mem: 1375\n",
            "Train: Epoch[1/1]  [ 20/105]  eta: 0:00:25  Lr: 0.002812  Loss: 3.2883  Acc@1: 16.6667 (14.6825)  Acc@5: 58.3333 (57.5397)  time: 0.2745  data: 0.0004  max mem: 1375\n",
            "Train: Epoch[1/1]  [ 30/105]  eta: 0:00:21  Lr: 0.002812  Loss: 2.4694  Acc@1: 16.6667 (18.5484)  Acc@5: 66.6667 (62.7688)  time: 0.2793  data: 0.0005  max mem: 1375\n",
            "Train: Epoch[1/1]  [ 40/105]  eta: 0:00:18  Lr: 0.002812  Loss: 2.0650  Acc@1: 29.1667 (22.1545)  Acc@5: 79.1667 (67.3781)  time: 0.2818  data: 0.0008  max mem: 1375\n",
            "Train: Epoch[1/1]  [ 50/105]  eta: 0:00:15  Lr: 0.002812  Loss: 1.6067  Acc@1: 37.5000 (26.4706)  Acc@5: 87.5000 (71.4052)  time: 0.2828  data: 0.0008  max mem: 1375\n",
            "Train: Epoch[1/1]  [ 60/105]  eta: 0:00:12  Lr: 0.002812  Loss: 1.4365  Acc@1: 45.8333 (30.2596)  Acc@5: 83.3333 (73.4973)  time: 0.2857  data: 0.0004  max mem: 1375\n",
            "Train: Epoch[1/1]  [ 70/105]  eta: 0:00:10  Lr: 0.002812  Loss: 1.7980  Acc@1: 50.0000 (34.2136)  Acc@5: 91.6667 (76.4085)  time: 0.2887  data: 0.0004  max mem: 1375\n",
            "Train: Epoch[1/1]  [ 80/105]  eta: 0:00:07  Lr: 0.002812  Loss: 1.0492  Acc@1: 58.3333 (37.6543)  Acc@5: 91.6667 (78.3436)  time: 0.2901  data: 0.0008  max mem: 1375\n",
            "Train: Epoch[1/1]  [ 90/105]  eta: 0:00:04  Lr: 0.002812  Loss: 1.1897  Acc@1: 62.5000 (40.3388)  Acc@5: 95.8333 (80.1740)  time: 0.2907  data: 0.0010  max mem: 1375\n",
            "Train: Epoch[1/1]  [100/105]  eta: 0:00:01  Lr: 0.002812  Loss: 0.9550  Acc@1: 66.6667 (43.3581)  Acc@5: 95.8333 (81.7244)  time: 0.2905  data: 0.0006  max mem: 1375\n",
            "Train: Epoch[1/1]  [104/105]  eta: 0:00:00  Lr: 0.002812  Loss: 1.4197  Acc@1: 66.6667 (44.0272)  Acc@5: 95.8333 (82.2213)  time: 0.2839  data: 0.0003  max mem: 1375\n",
            "Train: Epoch[1/1] Total time: 0:00:30 (0.2885 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.4197  Acc@1: 66.6667 (44.0272)  Acc@5: 95.8333 (82.2213)\n",
            "torch.Size([5, 2, 5, 6, 64])\n",
            "torch.Size([5, 2, 1, 5, 6, 64])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:19  Loss: 2.2198 (2.2198)  Acc@1: 54.1667 (54.1667)  Acc@5: 66.6667 (66.6667)  Acc@task: 95.8333 (95.8333)  time: 0.4546  data: 0.2903  max mem: 1375\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 2.2198 (2.2256)  Acc@1: 41.6667 (47.3485)  Acc@5: 75.0000 (75.0000)  Acc@task: 87.5000 (89.3939)  time: 0.2018  data: 0.0271  max mem: 1375\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 2.1940 (2.2019)  Acc@1: 41.6667 (46.8254)  Acc@5: 75.0000 (75.7937)  Acc@task: 87.5000 (90.4762)  time: 0.1764  data: 0.0006  max mem: 1375\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 2.1183 (2.1630)  Acc@1: 45.8333 (48.5215)  Acc@5: 79.1667 (76.0753)  Acc@task: 91.6667 (91.2634)  time: 0.1771  data: 0.0008  max mem: 1375\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 1.9706 (2.1301)  Acc@1: 50.0000 (49.7967)  Acc@5: 79.1667 (76.6260)  Acc@task: 95.8333 (92.3781)  time: 0.1778  data: 0.0010  max mem: 1375\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 1.9682 (2.1148)  Acc@1: 54.1667 (50.2000)  Acc@5: 79.1667 (76.8000)  Acc@task: 95.8333 (92.5000)  time: 0.1745  data: 0.0010  max mem: 1375\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1843 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@task 92.500 Acc@1 50.200 Acc@5 76.800 loss 2.115\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:19  Loss: 2.7036 (2.7036)  Acc@1: 37.5000 (37.5000)  Acc@5: 50.0000 (50.0000)  Acc@task: 95.8333 (95.8333)  time: 0.4689  data: 0.2928  max mem: 1375\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 2.5325 (2.5915)  Acc@1: 33.3333 (35.6061)  Acc@5: 66.6667 (65.9091)  Acc@task: 87.5000 (88.2576)  time: 0.2036  data: 0.0273  max mem: 1375\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 2.4279 (2.4880)  Acc@1: 37.5000 (37.6984)  Acc@5: 70.8333 (69.8413)  Acc@task: 87.5000 (88.8889)  time: 0.1765  data: 0.0006  max mem: 1375\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 2.4279 (2.4817)  Acc@1: 37.5000 (37.3656)  Acc@5: 70.8333 (70.4301)  Acc@task: 87.5000 (88.9785)  time: 0.1767  data: 0.0004  max mem: 1375\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 2.4735 (2.4548)  Acc@1: 37.5000 (37.9065)  Acc@5: 66.6667 (70.0203)  Acc@task: 87.5000 (88.5163)  time: 0.1777  data: 0.0003  max mem: 1375\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 2.4261 (2.4418)  Acc@1: 37.5000 (38.1000)  Acc@5: 66.6667 (70.0000)  Acc@task: 87.5000 (88.5000)  time: 0.1743  data: 0.0003  max mem: 1375\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1847 s / it)\n",
            "* Acc@task 88.500 Acc@1 38.100 Acc@5 70.000 loss 2.442\n",
            "[Average accuracy till task2]\tAcc@task: 90.5000\tAcc@1: 44.1500\tAcc@5: 73.4000\tLoss: 2.2783\tForgetting: 0.0000\tBackward: 50.2000\n",
            "torch.Size([24000, 384])\n",
            "Averaged stats: Lr: 0.005000  Loss: 0.3105  Acc@1: 75.0000 (77.0833)  Acc@5: 96.6667 (97.1667)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:17  Loss: 0.9803 (0.9803)  Acc@1: 75.0000 (75.0000)  Acc@5: 83.3333 (83.3333)  Acc@task: 95.8333 (95.8333)  time: 0.4159  data: 0.2445  max mem: 1375\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.9358 (1.0073)  Acc@1: 79.1667 (77.6515)  Acc@5: 91.6667 (90.5303)  Acc@task: 87.5000 (89.3939)  time: 0.1990  data: 0.0231  max mem: 1375\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.9358 (0.9880)  Acc@1: 75.0000 (76.3889)  Acc@5: 95.8333 (92.4603)  Acc@task: 87.5000 (90.4762)  time: 0.1769  data: 0.0011  max mem: 1375\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.9002 (0.9316)  Acc@1: 75.0000 (77.1505)  Acc@5: 95.8333 (93.1452)  Acc@task: 91.6667 (91.2634)  time: 0.1771  data: 0.0009  max mem: 1375\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.7863 (0.9032)  Acc@1: 83.3333 (78.2520)  Acc@5: 95.8333 (93.6992)  Acc@task: 95.8333 (92.3781)  time: 0.1776  data: 0.0004  max mem: 1375\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.7275 (0.8934)  Acc@1: 83.3333 (78.4000)  Acc@5: 95.8333 (93.8000)  Acc@task: 95.8333 (92.5000)  time: 0.1744  data: 0.0004  max mem: 1375\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1836 s / it)\n",
            "* Acc@task 92.500 Acc@1 78.400 Acc@5 93.800 loss 0.893\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:16  Loss: 1.0161 (1.0161)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  Acc@task: 95.8333 (95.8333)  time: 0.4044  data: 0.2374  max mem: 1375\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.8302 (0.8859)  Acc@1: 79.1667 (78.0303)  Acc@5: 95.8333 (94.3182)  Acc@task: 87.5000 (88.2576)  time: 0.2012  data: 0.0268  max mem: 1375\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.7917 (0.8521)  Acc@1: 79.1667 (78.9683)  Acc@5: 95.8333 (94.4444)  Acc@task: 87.5000 (88.8889)  time: 0.1792  data: 0.0031  max mem: 1375\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.8107 (0.8622)  Acc@1: 79.1667 (78.4946)  Acc@5: 95.8333 (94.0860)  Acc@task: 87.5000 (88.9785)  time: 0.1780  data: 0.0004  max mem: 1375\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.7963 (0.8280)  Acc@1: 79.1667 (79.2683)  Acc@5: 95.8333 (94.2073)  Acc@task: 87.5000 (88.5163)  time: 0.1783  data: 0.0003  max mem: 1375\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.7506 (0.8227)  Acc@1: 81.2500 (79.3000)  Acc@5: 95.8333 (94.2000)  Acc@task: 87.5000 (88.5000)  time: 0.1751  data: 0.0003  max mem: 1375\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1856 s / it)\n",
            "* Acc@task 88.500 Acc@1 79.300 Acc@5 94.200 loss 0.823\n",
            "[Average accuracy till task2]\tAcc@task: 90.5000\tAcc@1: 78.8500\tAcc@5: 94.0000\tLoss: 0.8580\tForgetting: 0.0000\tBackward: 27.6000\n",
            "Loading checkpoint from: ./output/cifar100_full_dino_1epoch_50pct/checkpoint/task3_checkpoint.pth\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/engines/hide_promtp_wtp_and_tap_engine.py:540: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  loss = torch.nn.functional.cross_entropy(sim, torch.range(0, sim.shape[0] - 1).long().to(device))\n",
            "Train: Epoch[1/1]  [  0/105]  eta: 0:01:57  Lr: 0.002812  Loss: 3.4828  Acc@1: 8.3333 (8.3333)  Acc@5: 50.0000 (50.0000)  time: 1.1230  data: 0.7039  max mem: 1375\n",
            "Train: Epoch[1/1]  [ 10/105]  eta: 0:00:33  Lr: 0.002812  Loss: 2.5754  Acc@1: 8.3333 (12.1212)  Acc@5: 58.3333 (57.9545)  time: 0.3519  data: 0.0645  max mem: 1375\n",
            "Train: Epoch[1/1]  [ 20/105]  eta: 0:00:26  Lr: 0.002812  Loss: 2.4427  Acc@1: 12.5000 (14.8810)  Acc@5: 62.5000 (62.3016)  time: 0.2755  data: 0.0006  max mem: 1375\n",
            "Train: Epoch[1/1]  [ 30/105]  eta: 0:00:22  Lr: 0.002812  Loss: 2.4002  Acc@1: 25.0000 (20.9677)  Acc@5: 70.8333 (65.9946)  time: 0.2760  data: 0.0005  max mem: 1375\n",
            "Train: Epoch[1/1]  [ 40/105]  eta: 0:00:19  Lr: 0.002812  Loss: 1.7415  Acc@1: 33.3333 (25.7114)  Acc@5: 79.1667 (70.1220)  time: 0.2765  data: 0.0004  max mem: 1375\n",
            "Train: Epoch[1/1]  [ 50/105]  eta: 0:00:16  Lr: 0.002812  Loss: 1.6148  Acc@1: 45.8333 (30.8007)  Acc@5: 83.3333 (73.2843)  time: 0.2770  data: 0.0007  max mem: 1375\n",
            "Train: Epoch[1/1]  [ 60/105]  eta: 0:00:13  Lr: 0.002812  Loss: 1.0122  Acc@1: 62.5000 (37.0902)  Acc@5: 91.6667 (76.6393)  time: 0.2774  data: 0.0012  max mem: 1375\n",
            "Train: Epoch[1/1]  [ 70/105]  eta: 0:00:10  Lr: 0.002812  Loss: 0.9336  Acc@1: 70.8333 (41.2559)  Acc@5: 91.6667 (78.9319)  time: 0.2780  data: 0.0009  max mem: 1375\n",
            "Train: Epoch[1/1]  [ 80/105]  eta: 0:00:07  Lr: 0.002812  Loss: 0.7818  Acc@1: 70.8333 (45.1646)  Acc@5: 91.6667 (80.9671)  time: 0.2782  data: 0.0005  max mem: 1375\n",
            "Train: Epoch[1/1]  [ 90/105]  eta: 0:00:04  Lr: 0.002812  Loss: 0.8821  Acc@1: 70.8333 (47.6648)  Acc@5: 95.8333 (82.6007)  time: 0.2784  data: 0.0005  max mem: 1375\n",
            "Train: Epoch[1/1]  [100/105]  eta: 0:00:01  Lr: 0.002812  Loss: 1.3208  Acc@1: 66.6667 (49.7525)  Acc@5: 95.8333 (83.8284)  time: 0.2786  data: 0.0003  max mem: 1375\n",
            "Train: Epoch[1/1]  [104/105]  eta: 0:00:00  Lr: 0.002812  Loss: 0.6869  Acc@1: 70.8333 (50.4762)  Acc@5: 95.8333 (84.2460)  time: 0.2788  data: 0.0003  max mem: 1375\n",
            "Train: Epoch[1/1] Total time: 0:00:30 (0.2867 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.6869  Acc@1: 70.8333 (50.4762)  Acc@5: 95.8333 (84.2460)\n",
            "torch.Size([5, 2, 5, 6, 64])\n",
            "torch.Size([5, 2, 1, 5, 6, 64])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:23  Loss: 0.9823 (0.9823)  Acc@1: 75.0000 (75.0000)  Acc@5: 83.3333 (83.3333)  Acc@task: 95.8333 (95.8333)  time: 0.5673  data: 0.3936  max mem: 1375\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.9823 (1.0524)  Acc@1: 75.0000 (75.3788)  Acc@5: 91.6667 (90.5303)  Acc@task: 83.3333 (82.1970)  time: 0.2116  data: 0.0361  max mem: 1375\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.9681 (1.0304)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (92.0635)  Acc@task: 83.3333 (84.3254)  time: 0.1766  data: 0.0004  max mem: 1375\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.8897 (0.9812)  Acc@1: 75.0000 (76.2097)  Acc@5: 91.6667 (92.2043)  Acc@task: 87.5000 (85.4839)  time: 0.1779  data: 0.0004  max mem: 1375\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.7676 (0.9460)  Acc@1: 79.1667 (77.3374)  Acc@5: 95.8333 (92.8862)  Acc@task: 91.6667 (86.5854)  time: 0.1788  data: 0.0004  max mem: 1375\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.7590 (0.9352)  Acc@1: 83.3333 (77.5000)  Acc@5: 95.8333 (93.0000)  Acc@task: 91.6667 (86.8000)  time: 0.1754  data: 0.0004  max mem: 1375\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1874 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@task 86.800 Acc@1 77.500 Acc@5 93.000 loss 0.935\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:24  Loss: 0.9794 (0.9794)  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  Acc@task: 87.5000 (87.5000)  time: 0.5808  data: 0.3996  max mem: 1375\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.7358 (0.8434)  Acc@1: 79.1667 (78.0303)  Acc@5: 95.8333 (96.2121)  Acc@task: 87.5000 (85.6061)  time: 0.2142  data: 0.0367  max mem: 1375\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.7358 (0.8148)  Acc@1: 79.1667 (79.3651)  Acc@5: 95.8333 (95.6349)  Acc@task: 87.5000 (86.7064)  time: 0.1777  data: 0.0008  max mem: 1375\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.8044 (0.8256)  Acc@1: 79.1667 (79.3011)  Acc@5: 95.8333 (94.8925)  Acc@task: 87.5000 (87.2312)  time: 0.1789  data: 0.0012  max mem: 1375\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.7129 (0.7892)  Acc@1: 83.3333 (80.2846)  Acc@5: 95.8333 (95.3252)  Acc@task: 87.5000 (87.3984)  time: 0.1798  data: 0.0007  max mem: 1375\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.7050 (0.7834)  Acc@1: 83.3333 (80.3000)  Acc@5: 95.8333 (95.4000)  Acc@task: 87.5000 (87.5000)  time: 0.1764  data: 0.0007  max mem: 1375\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1888 s / it)\n",
            "* Acc@task 87.500 Acc@1 80.300 Acc@5 95.400 loss 0.783\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:24  Loss: 2.7783 (2.7783)  Acc@1: 20.8333 (20.8333)  Acc@5: 70.8333 (70.8333)  Acc@task: 87.5000 (87.5000)  time: 0.5843  data: 0.4145  max mem: 1375\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 2.8984 (2.9882)  Acc@1: 20.8333 (21.2121)  Acc@5: 66.6667 (67.4242)  Acc@task: 83.3333 (84.4697)  time: 0.2162  data: 0.0382  max mem: 1375\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 2.8348 (2.8054)  Acc@1: 25.0000 (25.9921)  Acc@5: 66.6667 (69.2460)  Acc@task: 83.3333 (84.9206)  time: 0.1787  data: 0.0005  max mem: 1375\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 2.7559 (2.8268)  Acc@1: 33.3333 (27.4194)  Acc@5: 70.8333 (69.8925)  Acc@task: 83.3333 (85.3495)  time: 0.1791  data: 0.0004  max mem: 1375\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 2.8267 (2.8345)  Acc@1: 25.0000 (27.0325)  Acc@5: 66.6667 (69.0041)  Acc@task: 83.3333 (85.2642)  time: 0.1809  data: 0.0003  max mem: 1375\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 2.8267 (2.8345)  Acc@1: 25.0000 (27.0000)  Acc@5: 70.8333 (69.1000)  Acc@task: 83.3333 (85.1000)  time: 0.1772  data: 0.0003  max mem: 1375\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1900 s / it)\n",
            "* Acc@task 85.100 Acc@1 27.000 Acc@5 69.100 loss 2.835\n",
            "[Average accuracy till task3]\tAcc@task: 86.4667\tAcc@1: 61.6000\tAcc@5: 85.8333\tLoss: 1.5177\tForgetting: 0.0000\tBackward: 59.8500\n",
            "torch.Size([36000, 384])\n",
            "Averaged stats: Lr: 0.005000  Loss: 0.1953  Acc@1: 87.5000 (86.9583)  Acc@5: 98.3333 (98.4167)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:34  Loss: 0.3858 (0.3858)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  Acc@task: 95.8333 (95.8333)  time: 0.8115  data: 0.5780  max mem: 1375\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:07  Loss: 0.6594 (0.6492)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (82.1970)  time: 0.2369  data: 0.0536  max mem: 1375\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.5526 (0.5991)  Acc@1: 83.3333 (84.9206)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (84.3254)  time: 0.1793  data: 0.0007  max mem: 1375\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.5481 (0.5811)  Acc@1: 87.5000 (85.3495)  Acc@5: 100.0000 (96.5054)  Acc@task: 87.5000 (85.4839)  time: 0.1795  data: 0.0004  max mem: 1375\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.3802 (0.5431)  Acc@1: 91.6667 (86.6870)  Acc@5: 100.0000 (97.0528)  Acc@task: 91.6667 (86.5854)  time: 0.1802  data: 0.0004  max mem: 1375\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.3332 (0.5341)  Acc@1: 91.6667 (86.8000)  Acc@5: 100.0000 (97.1000)  Acc@task: 91.6667 (86.8000)  time: 0.1767  data: 0.0004  max mem: 1375\n",
            "Test: [Task 1] Total time: 0:00:08 (0.1956 s / it)\n",
            "* Acc@task 86.800 Acc@1 86.800 Acc@5 97.100 loss 0.534\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:19  Loss: 0.7140 (0.7140)  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.8333)  Acc@task: 87.5000 (87.5000)  time: 0.4557  data: 0.2888  max mem: 1375\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.7345 (0.7395)  Acc@1: 83.3333 (80.6818)  Acc@5: 95.8333 (96.9697)  Acc@task: 87.5000 (85.6061)  time: 0.2049  data: 0.0269  max mem: 1375\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.7410 (0.7859)  Acc@1: 83.3333 (80.7540)  Acc@5: 95.8333 (95.2381)  Acc@task: 87.5000 (86.7064)  time: 0.1794  data: 0.0005  max mem: 1375\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.7202 (0.7784)  Acc@1: 83.3333 (80.2419)  Acc@5: 95.8333 (94.8925)  Acc@task: 87.5000 (87.2312)  time: 0.1796  data: 0.0007  max mem: 1375\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.6904 (0.7674)  Acc@1: 79.1667 (80.2846)  Acc@5: 95.8333 (95.1220)  Acc@task: 87.5000 (87.3984)  time: 0.1800  data: 0.0007  max mem: 1375\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.6808 (0.7544)  Acc@1: 83.3333 (80.6000)  Acc@5: 95.8333 (95.2000)  Acc@task: 87.5000 (87.5000)  time: 0.1762  data: 0.0007  max mem: 1375\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1875 s / it)\n",
            "* Acc@task 87.500 Acc@1 80.600 Acc@5 95.200 loss 0.754\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:35  Loss: 0.8455 (0.8455)  Acc@1: 70.8333 (70.8333)  Acc@5: 100.0000 (100.0000)  Acc@task: 87.5000 (87.5000)  time: 0.8383  data: 0.6662  max mem: 1375\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:07  Loss: 0.7978 (0.7221)  Acc@1: 79.1667 (81.8182)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (84.4697)  time: 0.2393  data: 0.0614  max mem: 1375\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.5769 (0.6177)  Acc@1: 83.3333 (84.1270)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (84.9206)  time: 0.1788  data: 0.0007  max mem: 1375\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.3791 (0.5440)  Acc@1: 87.5000 (85.8871)  Acc@5: 95.8333 (96.5054)  Acc@task: 83.3333 (85.3495)  time: 0.1789  data: 0.0004  max mem: 1375\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.4574 (0.6100)  Acc@1: 87.5000 (84.7561)  Acc@5: 95.8333 (95.6301)  Acc@task: 83.3333 (85.2642)  time: 0.1798  data: 0.0003  max mem: 1375\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.4574 (0.6193)  Acc@1: 87.5000 (84.5000)  Acc@5: 95.8333 (95.6000)  Acc@task: 83.3333 (85.1000)  time: 0.1763  data: 0.0003  max mem: 1375\n",
            "Test: [Task 3] Total time: 0:00:08 (0.1958 s / it)\n",
            "* Acc@task 85.100 Acc@1 84.500 Acc@5 95.600 loss 0.619\n",
            "[Average accuracy till task3]\tAcc@task: 86.4667\tAcc@1: 83.9667\tAcc@5: 95.9667\tLoss: 0.6359\tForgetting: 0.0000\tBackward: 18.6500\n",
            "Loading checkpoint from: ./output/cifar100_full_dino_1epoch_50pct/checkpoint/task4_checkpoint.pth\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/engines/hide_promtp_wtp_and_tap_engine.py:540: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  loss = torch.nn.functional.cross_entropy(sim, torch.range(0, sim.shape[0] - 1).long().to(device))\n",
            "Train: Epoch[1/1]  [  0/103]  eta: 0:01:21  Lr: 0.002812  Loss: 3.1278  Acc@1: 20.8333 (20.8333)  Acc@5: 58.3333 (58.3333)  time: 0.7917  data: 0.5203  max mem: 1375\n",
            "Train: Epoch[1/1]  [ 10/103]  eta: 0:00:30  Lr: 0.002812  Loss: 2.3205  Acc@1: 20.8333 (19.3182)  Acc@5: 66.6667 (64.3939)  time: 0.3234  data: 0.0476  max mem: 1377\n",
            "Train: Epoch[1/1]  [ 20/103]  eta: 0:00:25  Lr: 0.002812  Loss: 2.4728  Acc@1: 20.8333 (21.2302)  Acc@5: 70.8333 (69.2460)  time: 0.2773  data: 0.0006  max mem: 1377\n",
            "Train: Epoch[1/1]  [ 30/103]  eta: 0:00:21  Lr: 0.002812  Loss: 2.3723  Acc@1: 29.1667 (25.2688)  Acc@5: 79.1667 (74.3280)  time: 0.2775  data: 0.0007  max mem: 1377\n",
            "Train: Epoch[1/1]  [ 40/103]  eta: 0:00:18  Lr: 0.002812  Loss: 1.5310  Acc@1: 33.3333 (29.4715)  Acc@5: 83.3333 (76.5244)  time: 0.2773  data: 0.0004  max mem: 1377\n",
            "Train: Epoch[1/1]  [ 50/103]  eta: 0:00:15  Lr: 0.002812  Loss: 1.5889  Acc@1: 54.1667 (35.2124)  Acc@5: 87.5000 (79.1667)  time: 0.2777  data: 0.0004  max mem: 1377\n",
            "Train: Epoch[1/1]  [ 60/103]  eta: 0:00:12  Lr: 0.002812  Loss: 1.3816  Acc@1: 58.3333 (39.1393)  Acc@5: 91.6667 (81.8306)  time: 0.2783  data: 0.0004  max mem: 1377\n",
            "Train: Epoch[1/1]  [ 70/103]  eta: 0:00:09  Lr: 0.002812  Loss: 1.3960  Acc@1: 62.5000 (42.9577)  Acc@5: 95.8333 (83.4507)  time: 0.2783  data: 0.0006  max mem: 1377\n",
            "Train: Epoch[1/1]  [ 80/103]  eta: 0:00:06  Lr: 0.002812  Loss: 1.1694  Acc@1: 66.6667 (46.2449)  Acc@5: 95.8333 (84.8765)  time: 0.2774  data: 0.0008  max mem: 1377\n",
            "Train: Epoch[1/1]  [ 90/103]  eta: 0:00:03  Lr: 0.002812  Loss: 1.3857  Acc@1: 66.6667 (48.3974)  Acc@5: 95.8333 (85.8517)  time: 0.2778  data: 0.0006  max mem: 1377\n",
            "Train: Epoch[1/1]  [100/103]  eta: 0:00:00  Lr: 0.002812  Loss: 1.1770  Acc@1: 66.6667 (50.4951)  Acc@5: 95.8333 (86.7162)  time: 0.2783  data: 0.0005  max mem: 1377\n",
            "Train: Epoch[1/1]  [102/103]  eta: 0:00:00  Lr: 0.002812  Loss: 0.9389  Acc@1: 66.6667 (50.6108)  Acc@5: 95.8333 (86.8078)  time: 0.2696  data: 0.0004  max mem: 1377\n",
            "Train: Epoch[1/1] Total time: 0:00:29 (0.2820 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.9389  Acc@1: 66.6667 (50.6108)  Acc@5: 95.8333 (86.8078)\n",
            "torch.Size([5, 2, 5, 6, 64])\n",
            "torch.Size([5, 2, 1, 5, 6, 64])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:19  Loss: 0.3846 (0.3846)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  Acc@task: 95.8333 (95.8333)  time: 0.4671  data: 0.2924  max mem: 1377\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.7373 (0.6915)  Acc@1: 83.3333 (83.7121)  Acc@5: 95.8333 (95.4545)  Acc@task: 87.5000 (83.7121)  time: 0.2077  data: 0.0327  max mem: 1377\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.5499 (0.6361)  Acc@1: 83.3333 (84.7222)  Acc@5: 95.8333 (95.2381)  Acc@task: 87.5000 (85.7143)  time: 0.1800  data: 0.0044  max mem: 1377\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.4907 (0.6140)  Acc@1: 87.5000 (84.9462)  Acc@5: 95.8333 (95.9677)  Acc@task: 87.5000 (86.2903)  time: 0.1783  data: 0.0014  max mem: 1377\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.3802 (0.5695)  Acc@1: 87.5000 (86.2805)  Acc@5: 100.0000 (96.6463)  Acc@task: 91.6667 (87.0935)  time: 0.1784  data: 0.0005  max mem: 1377\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.3381 (0.5599)  Acc@1: 91.6667 (86.4000)  Acc@5: 100.0000 (96.7000)  Acc@task: 91.6667 (87.3000)  time: 0.1755  data: 0.0005  max mem: 1377\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1869 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@task 87.300 Acc@1 86.400 Acc@5 96.700 loss 0.560\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:19  Loss: 0.6455 (0.6455)  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.8333)  Acc@task: 87.5000 (87.5000)  time: 0.4536  data: 0.2849  max mem: 1377\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.7730 (0.7576)  Acc@1: 83.3333 (79.5455)  Acc@5: 95.8333 (96.5909)  Acc@task: 83.3333 (82.5758)  time: 0.2048  data: 0.0285  max mem: 1377\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.7653 (0.7932)  Acc@1: 83.3333 (80.1587)  Acc@5: 95.8333 (95.0397)  Acc@task: 83.3333 (82.7381)  time: 0.1786  data: 0.0017  max mem: 1377\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.7200 (0.7817)  Acc@1: 83.3333 (79.8387)  Acc@5: 95.8333 (94.8925)  Acc@task: 83.3333 (83.4677)  time: 0.1784  data: 0.0005  max mem: 1377\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.6899 (0.7683)  Acc@1: 79.1667 (80.1829)  Acc@5: 95.8333 (95.2236)  Acc@task: 83.3333 (83.7398)  time: 0.1795  data: 0.0004  max mem: 1377\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.6586 (0.7557)  Acc@1: 83.3333 (80.5000)  Acc@5: 95.8333 (95.3000)  Acc@task: 83.3333 (83.9000)  time: 0.1762  data: 0.0004  max mem: 1377\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1875 s / it)\n",
            "* Acc@task 83.900 Acc@1 80.500 Acc@5 95.300 loss 0.756\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:35  Loss: 0.6978 (0.6978)  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  Acc@task: 91.6667 (91.6667)  time: 0.8398  data: 0.6365  max mem: 1377\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:07  Loss: 0.6932 (0.6739)  Acc@1: 83.3333 (81.8182)  Acc@5: 95.8333 (96.5909)  Acc@task: 87.5000 (85.2273)  time: 0.2403  data: 0.0593  max mem: 1377\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.5721 (0.5868)  Acc@1: 83.3333 (84.5238)  Acc@5: 95.8333 (96.8254)  Acc@task: 87.5000 (85.9127)  time: 0.1796  data: 0.0011  max mem: 1377\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.3785 (0.5202)  Acc@1: 87.5000 (86.0215)  Acc@5: 95.8333 (97.3118)  Acc@task: 87.5000 (85.8871)  time: 0.1793  data: 0.0006  max mem: 1377\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.4451 (0.5780)  Acc@1: 87.5000 (85.2642)  Acc@5: 95.8333 (96.4431)  Acc@task: 87.5000 (86.1789)  time: 0.1805  data: 0.0004  max mem: 1377\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.4451 (0.5867)  Acc@1: 87.5000 (85.2000)  Acc@5: 95.8333 (96.4000)  Acc@task: 87.5000 (85.9000)  time: 0.1774  data: 0.0003  max mem: 1377\n",
            "Test: [Task 3] Total time: 0:00:08 (0.1968 s / it)\n",
            "* Acc@task 85.900 Acc@1 85.200 Acc@5 96.400 loss 0.587\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:23  Loss: 3.7976 (3.7976)  Acc@1: 29.1667 (29.1667)  Acc@5: 50.0000 (50.0000)  Acc@task: 66.6667 (66.6667)  time: 0.5709  data: 0.3882  max mem: 1377\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:06  Loss: 3.4251 (3.5075)  Acc@1: 25.0000 (22.7273)  Acc@5: 62.5000 (62.5000)  Acc@task: 83.3333 (80.6818)  time: 0.2155  data: 0.0360  max mem: 1377\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 3.5001 (3.6110)  Acc@1: 16.6667 (19.6429)  Acc@5: 62.5000 (61.7064)  Acc@task: 79.1667 (78.5714)  time: 0.1792  data: 0.0006  max mem: 1377\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 3.5174 (3.5570)  Acc@1: 20.8333 (20.5645)  Acc@5: 66.6667 (65.0538)  Acc@task: 79.1667 (78.6290)  time: 0.1791  data: 0.0005  max mem: 1377\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 3.5779 (3.5856)  Acc@1: 20.8333 (20.6301)  Acc@5: 66.6667 (64.2276)  Acc@task: 79.1667 (78.6585)  time: 0.1799  data: 0.0005  max mem: 1377\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 3.5174 (3.5716)  Acc@1: 20.8333 (20.7000)  Acc@5: 66.6667 (64.2000)  Acc@task: 79.1667 (78.9000)  time: 0.1764  data: 0.0005  max mem: 1377\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1903 s / it)\n",
            "* Acc@task 78.900 Acc@1 20.700 Acc@5 64.200 loss 3.572\n",
            "[Average accuracy till task4]\tAcc@task: 84.0000\tAcc@1: 68.2000\tAcc@5: 88.1500\tLoss: 1.3685\tForgetting: 0.0000\tBackward: 62.3333\n",
            "torch.Size([48000, 384])\n",
            "Averaged stats: Lr: 0.005000  Loss: 0.1754  Acc@1: 94.1667 (90.5000)  Acc@5: 100.0000 (98.8056)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:23  Loss: 0.3498 (0.3498)  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)  Acc@task: 95.8333 (95.8333)  time: 0.5592  data: 0.3797  max mem: 1377\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.7580 (0.7810)  Acc@1: 79.1667 (79.5455)  Acc@5: 95.8333 (96.2121)  Acc@task: 87.5000 (83.7121)  time: 0.2132  data: 0.0351  max mem: 1377\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.7312 (0.7539)  Acc@1: 79.1667 (80.3571)  Acc@5: 95.8333 (96.2302)  Acc@task: 87.5000 (85.7143)  time: 0.1784  data: 0.0005  max mem: 1377\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.5245 (0.7186)  Acc@1: 83.3333 (81.0484)  Acc@5: 100.0000 (96.5054)  Acc@task: 87.5000 (86.2903)  time: 0.1789  data: 0.0004  max mem: 1377\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.4668 (0.6792)  Acc@1: 83.3333 (82.1138)  Acc@5: 100.0000 (96.8496)  Acc@task: 91.6667 (87.0935)  time: 0.1799  data: 0.0003  max mem: 1377\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.4476 (0.6706)  Acc@1: 87.5000 (82.2000)  Acc@5: 100.0000 (96.9000)  Acc@task: 91.6667 (87.3000)  time: 0.1763  data: 0.0003  max mem: 1377\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1889 s / it)\n",
            "* Acc@task 87.300 Acc@1 82.200 Acc@5 96.900 loss 0.671\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:26  Loss: 0.6623 (0.6623)  Acc@1: 83.3333 (83.3333)  Acc@5: 91.6667 (91.6667)  Acc@task: 87.5000 (87.5000)  time: 0.6391  data: 0.4688  max mem: 1377\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:07  Loss: 0.5993 (0.6972)  Acc@1: 83.3333 (81.4394)  Acc@5: 100.0000 (97.3485)  Acc@task: 83.3333 (82.5758)  time: 0.2191  data: 0.0437  max mem: 1377\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.5993 (0.6853)  Acc@1: 83.3333 (81.9444)  Acc@5: 95.8333 (96.8254)  Acc@task: 83.3333 (82.7381)  time: 0.1776  data: 0.0016  max mem: 1377\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.6610 (0.7055)  Acc@1: 75.0000 (80.7796)  Acc@5: 95.8333 (96.2366)  Acc@task: 83.3333 (83.4677)  time: 0.1787  data: 0.0016  max mem: 1377\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.6610 (0.6799)  Acc@1: 83.3333 (81.6057)  Acc@5: 95.8333 (96.4431)  Acc@task: 83.3333 (83.7398)  time: 0.1794  data: 0.0008  max mem: 1377\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.5954 (0.6742)  Acc@1: 83.3333 (81.6000)  Acc@5: 95.8333 (96.5000)  Acc@task: 83.3333 (83.9000)  time: 0.1763  data: 0.0007  max mem: 1377\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1902 s / it)\n",
            "* Acc@task 83.900 Acc@1 81.600 Acc@5 96.500 loss 0.674\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:24  Loss: 0.4271 (0.4271)  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)  Acc@task: 91.6667 (91.6667)  time: 0.5806  data: 0.4012  max mem: 1377\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 0.6414 (0.7004)  Acc@1: 83.3333 (83.7121)  Acc@5: 100.0000 (96.5909)  Acc@task: 87.5000 (85.2273)  time: 0.2155  data: 0.0368  max mem: 1377\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.6320 (0.6507)  Acc@1: 83.3333 (83.9286)  Acc@5: 95.8333 (96.0317)  Acc@task: 87.5000 (85.9127)  time: 0.1785  data: 0.0004  max mem: 1377\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.4599 (0.5808)  Acc@1: 83.3333 (84.6774)  Acc@5: 95.8333 (96.9086)  Acc@task: 87.5000 (85.8871)  time: 0.1787  data: 0.0005  max mem: 1377\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.4558 (0.6264)  Acc@1: 83.3333 (83.6382)  Acc@5: 95.8333 (96.3415)  Acc@task: 87.5000 (86.1789)  time: 0.1797  data: 0.0004  max mem: 1377\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.4558 (0.6311)  Acc@1: 83.3333 (83.2000)  Acc@5: 100.0000 (96.4000)  Acc@task: 87.5000 (85.9000)  time: 0.1761  data: 0.0004  max mem: 1377\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1902 s / it)\n",
            "* Acc@task 85.900 Acc@1 83.200 Acc@5 96.400 loss 0.631\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:39  Loss: 1.0520 (1.0520)  Acc@1: 66.6667 (66.6667)  Acc@5: 87.5000 (87.5000)  Acc@task: 66.6667 (66.6667)  time: 0.9513  data: 0.7726  max mem: 1377\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:07  Loss: 0.6551 (0.6501)  Acc@1: 87.5000 (81.4394)  Acc@5: 95.8333 (95.4545)  Acc@task: 83.3333 (80.6818)  time: 0.2485  data: 0.0716  max mem: 1377\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.7072 (0.7491)  Acc@1: 83.3333 (80.7540)  Acc@5: 95.8333 (95.2381)  Acc@task: 79.1667 (78.5714)  time: 0.1781  data: 0.0012  max mem: 1377\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.6953 (0.7386)  Acc@1: 83.3333 (81.4516)  Acc@5: 95.8333 (95.8333)  Acc@task: 79.1667 (78.6290)  time: 0.1789  data: 0.0007  max mem: 1377\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.5979 (0.7765)  Acc@1: 83.3333 (81.4024)  Acc@5: 95.8333 (95.4268)  Acc@task: 79.1667 (78.6585)  time: 0.1800  data: 0.0004  max mem: 1377\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.5471 (0.7646)  Acc@1: 83.3333 (81.6000)  Acc@5: 100.0000 (95.5000)  Acc@task: 79.1667 (78.9000)  time: 0.1769  data: 0.0004  max mem: 1377\n",
            "Test: [Task 4] Total time: 0:00:08 (0.1982 s / it)\n",
            "* Acc@task 78.900 Acc@1 81.600 Acc@5 95.500 loss 0.765\n",
            "[Average accuracy till task4]\tAcc@task: 84.0000\tAcc@1: 82.1500\tAcc@5: 96.3250\tLoss: 0.6851\tForgetting: 1.9667\tBackward: 10.8000\n",
            "Loading checkpoint from: ./output/cifar100_full_dino_1epoch_50pct/checkpoint/task5_checkpoint.pth\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/engines/hide_promtp_wtp_and_tap_engine.py:540: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  loss = torch.nn.functional.cross_entropy(sim, torch.range(0, sim.shape[0] - 1).long().to(device))\n",
            "Train: Epoch[1/1]  [  0/105]  eta: 0:01:18  Lr: 0.002812  Loss: 3.3877  Acc@1: 8.3333 (8.3333)  Acc@5: 58.3333 (58.3333)  time: 0.7505  data: 0.4821  max mem: 1377\n",
            "Train: Epoch[1/1]  [ 10/105]  eta: 0:00:30  Lr: 0.002812  Loss: 3.0351  Acc@1: 12.5000 (12.1212)  Acc@5: 58.3333 (55.6818)  time: 0.3210  data: 0.0443  max mem: 1377\n",
            "Train: Epoch[1/1]  [ 20/105]  eta: 0:00:25  Lr: 0.002812  Loss: 2.7688  Acc@1: 16.6667 (18.4524)  Acc@5: 62.5000 (61.7064)  time: 0.2785  data: 0.0004  max mem: 1377\n",
            "Train: Epoch[1/1]  [ 30/105]  eta: 0:00:22  Lr: 0.002812  Loss: 1.6786  Acc@1: 29.1667 (22.9839)  Acc@5: 75.0000 (66.8011)  time: 0.2790  data: 0.0008  max mem: 1377\n",
            "Train: Epoch[1/1]  [ 40/105]  eta: 0:00:18  Lr: 0.002812  Loss: 1.6485  Acc@1: 41.6667 (29.4715)  Acc@5: 83.3333 (71.8496)  time: 0.2790  data: 0.0008  max mem: 1377\n",
            "Train: Epoch[1/1]  [ 50/105]  eta: 0:00:15  Lr: 0.002812  Loss: 1.3660  Acc@1: 50.0000 (34.0686)  Acc@5: 87.5000 (74.6732)  time: 0.2795  data: 0.0004  max mem: 1377\n",
            "Train: Epoch[1/1]  [ 60/105]  eta: 0:00:12  Lr: 0.002812  Loss: 1.3263  Acc@1: 54.1667 (37.9781)  Acc@5: 87.5000 (77.3224)  time: 0.2797  data: 0.0005  max mem: 1377\n",
            "Train: Epoch[1/1]  [ 70/105]  eta: 0:00:10  Lr: 0.002812  Loss: 1.1037  Acc@1: 62.5000 (42.0775)  Acc@5: 91.6667 (79.3427)  time: 0.2796  data: 0.0004  max mem: 1377\n",
            "Train: Epoch[1/1]  [ 80/105]  eta: 0:00:07  Lr: 0.002812  Loss: 1.4461  Acc@1: 62.5000 (44.9588)  Acc@5: 91.6667 (81.1728)  time: 0.2798  data: 0.0009  max mem: 1377\n",
            "Train: Epoch[1/1]  [ 90/105]  eta: 0:00:04  Lr: 0.002812  Loss: 1.2277  Acc@1: 70.8333 (48.1227)  Acc@5: 95.8333 (82.6465)  time: 0.2801  data: 0.0009  max mem: 1377\n",
            "Train: Epoch[1/1]  [100/105]  eta: 0:00:01  Lr: 0.002812  Loss: 1.1899  Acc@1: 75.0000 (50.4125)  Acc@5: 95.8333 (83.8284)  time: 0.2800  data: 0.0004  max mem: 1377\n",
            "Train: Epoch[1/1]  [104/105]  eta: 0:00:00  Lr: 0.002812  Loss: 0.6382  Acc@1: 70.8333 (51.3600)  Acc@5: 95.8333 (84.3200)  time: 0.2690  data: 0.0003  max mem: 1377\n",
            "Train: Epoch[1/1] Total time: 0:00:29 (0.2830 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.6382  Acc@1: 70.8333 (51.3600)  Acc@5: 95.8333 (84.3200)\n",
            "torch.Size([5, 2, 5, 6, 64])\n",
            "torch.Size([5, 2, 1, 5, 6, 64])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:23  Loss: 0.3600 (0.3600)  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)  Acc@task: 91.6667 (91.6667)  time: 0.5577  data: 0.3755  max mem: 1377\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.7664 (0.7793)  Acc@1: 79.1667 (79.9242)  Acc@5: 95.8333 (96.2121)  Acc@task: 83.3333 (81.8182)  time: 0.2118  data: 0.0348  max mem: 1377\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.7204 (0.7528)  Acc@1: 79.1667 (80.5556)  Acc@5: 95.8333 (96.4286)  Acc@task: 83.3333 (84.3254)  time: 0.1765  data: 0.0010  max mem: 1377\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.5270 (0.7176)  Acc@1: 79.1667 (81.0484)  Acc@5: 100.0000 (96.6398)  Acc@task: 83.3333 (84.6774)  time: 0.1771  data: 0.0013  max mem: 1377\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.4807 (0.6807)  Acc@1: 83.3333 (82.0122)  Acc@5: 100.0000 (96.9512)  Acc@task: 87.5000 (85.4675)  time: 0.1787  data: 0.0008  max mem: 1377\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.4686 (0.6721)  Acc@1: 87.5000 (82.1000)  Acc@5: 100.0000 (97.0000)  Acc@task: 87.5000 (85.7000)  time: 0.1754  data: 0.0007  max mem: 1377\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1894 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@task 85.700 Acc@1 82.100 Acc@5 97.000 loss 0.672\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:39  Loss: 0.6915 (0.6915)  Acc@1: 83.3333 (83.3333)  Acc@5: 91.6667 (91.6667)  Acc@task: 87.5000 (87.5000)  time: 0.9303  data: 0.7325  max mem: 1377\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:07  Loss: 0.6383 (0.7030)  Acc@1: 83.3333 (81.8182)  Acc@5: 100.0000 (97.3485)  Acc@task: 83.3333 (82.5758)  time: 0.2465  data: 0.0678  max mem: 1377\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.6383 (0.6863)  Acc@1: 83.3333 (82.1429)  Acc@5: 95.8333 (96.8254)  Acc@task: 79.1667 (81.9444)  time: 0.1778  data: 0.0009  max mem: 1377\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.6752 (0.7005)  Acc@1: 75.0000 (80.9140)  Acc@5: 95.8333 (96.6398)  Acc@task: 83.3333 (82.3925)  time: 0.1784  data: 0.0005  max mem: 1377\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.6669 (0.6764)  Acc@1: 83.3333 (81.7073)  Acc@5: 95.8333 (96.7480)  Acc@task: 83.3333 (82.5203)  time: 0.1790  data: 0.0004  max mem: 1377\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.5808 (0.6707)  Acc@1: 83.3333 (81.7000)  Acc@5: 95.8333 (96.8000)  Acc@task: 83.3333 (82.7000)  time: 0.1756  data: 0.0003  max mem: 1377\n",
            "Test: [Task 2] Total time: 0:00:08 (0.1971 s / it)\n",
            "* Acc@task 82.700 Acc@1 81.700 Acc@5 96.800 loss 0.671\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:24  Loss: 0.4234 (0.4234)  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)  Acc@task: 87.5000 (87.5000)  time: 0.5892  data: 0.4061  max mem: 1377\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 0.6409 (0.7008)  Acc@1: 83.3333 (83.7121)  Acc@5: 100.0000 (96.5909)  Acc@task: 83.3333 (84.8485)  time: 0.2158  data: 0.0375  max mem: 1377\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.6256 (0.6393)  Acc@1: 83.3333 (83.9286)  Acc@5: 95.8333 (96.0317)  Acc@task: 83.3333 (85.3175)  time: 0.1782  data: 0.0012  max mem: 1377\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.4584 (0.5675)  Acc@1: 83.3333 (84.9462)  Acc@5: 95.8333 (96.9086)  Acc@task: 83.3333 (85.3495)  time: 0.1787  data: 0.0013  max mem: 1377\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.4552 (0.6139)  Acc@1: 83.3333 (83.9431)  Acc@5: 95.8333 (96.3415)  Acc@task: 83.3333 (85.3659)  time: 0.1797  data: 0.0006  max mem: 1377\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.4552 (0.6189)  Acc@1: 83.3333 (83.5000)  Acc@5: 100.0000 (96.4000)  Acc@task: 83.3333 (85.1000)  time: 0.1763  data: 0.0005  max mem: 1377\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1897 s / it)\n",
            "* Acc@task 85.100 Acc@1 83.500 Acc@5 96.400 loss 0.619\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:28  Loss: 1.0360 (1.0360)  Acc@1: 66.6667 (66.6667)  Acc@5: 87.5000 (87.5000)  Acc@task: 75.0000 (75.0000)  time: 0.6715  data: 0.5027  max mem: 1377\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:07  Loss: 0.7379 (0.6504)  Acc@1: 87.5000 (81.8182)  Acc@5: 95.8333 (95.4545)  Acc@task: 83.3333 (85.2273)  time: 0.2246  data: 0.0461  max mem: 1377\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.6886 (0.6910)  Acc@1: 83.3333 (81.7460)  Acc@5: 95.8333 (95.4365)  Acc@task: 83.3333 (84.1270)  time: 0.1793  data: 0.0004  max mem: 1377\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.5963 (0.6871)  Acc@1: 83.3333 (81.9892)  Acc@5: 95.8333 (95.9677)  Acc@task: 83.3333 (84.1398)  time: 0.1791  data: 0.0004  max mem: 1377\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.5963 (0.7249)  Acc@1: 83.3333 (81.9106)  Acc@5: 95.8333 (95.5285)  Acc@task: 83.3333 (83.9431)  time: 0.1802  data: 0.0004  max mem: 1377\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.5748 (0.7142)  Acc@1: 83.3333 (82.1000)  Acc@5: 100.0000 (95.6000)  Acc@task: 87.5000 (84.1000)  time: 0.1765  data: 0.0004  max mem: 1377\n",
            "Test: [Task 4] Total time: 0:00:08 (0.1920 s / it)\n",
            "* Acc@task 84.100 Acc@1 82.100 Acc@5 95.600 loss 0.714\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:27  Loss: 3.8827 (3.8827)  Acc@1: 0.0000 (0.0000)  Acc@5: 62.5000 (62.5000)  Acc@task: 83.3333 (83.3333)  time: 0.6566  data: 0.4777  max mem: 1377\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:07  Loss: 4.5836 (4.5821)  Acc@1: 8.3333 (9.8485)  Acc@5: 62.5000 (62.1212)  Acc@task: 75.0000 (78.7879)  time: 0.2228  data: 0.0440  max mem: 1377\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 4.5836 (4.6162)  Acc@1: 8.3333 (8.1349)  Acc@5: 62.5000 (61.3095)  Acc@task: 75.0000 (78.1746)  time: 0.1785  data: 0.0007  max mem: 1377\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 4.6495 (4.6179)  Acc@1: 8.3333 (8.6022)  Acc@5: 58.3333 (60.6183)  Acc@task: 79.1667 (78.6290)  time: 0.1785  data: 0.0008  max mem: 1377\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 4.7686 (4.6558)  Acc@1: 8.3333 (8.6382)  Acc@5: 58.3333 (59.6545)  Acc@task: 75.0000 (76.8293)  time: 0.1793  data: 0.0005  max mem: 1377\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 4.7686 (4.6660)  Acc@1: 8.3333 (8.6000)  Acc@5: 58.3333 (59.4000)  Acc@task: 75.0000 (76.5000)  time: 0.1755  data: 0.0004  max mem: 1377\n",
            "Test: [Task 5] Total time: 0:00:08 (0.1913 s / it)\n",
            "* Acc@task 76.500 Acc@1 8.600 Acc@5 59.400 loss 4.666\n",
            "[Average accuracy till task5]\tAcc@task: 82.8200\tAcc@1: 67.6000\tAcc@5: 89.0400\tLoss: 1.4684\tForgetting: 1.5000\tBackward: 60.9000\n",
            "torch.Size([60000, 384])\n",
            "Averaged stats: Lr: 0.005000  Loss: 0.0562  Acc@1: 95.8333 (92.0000)  Acc@5: 100.0000 (99.0417)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:23  Loss: 0.4443 (0.4443)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  Acc@task: 91.6667 (91.6667)  time: 0.5634  data: 0.3823  max mem: 1377\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.6145 (0.6647)  Acc@1: 83.3333 (83.7121)  Acc@5: 95.8333 (97.3485)  Acc@task: 83.3333 (81.8182)  time: 0.2137  data: 0.0352  max mem: 1377\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.6047 (0.6098)  Acc@1: 83.3333 (85.1191)  Acc@5: 95.8333 (97.0238)  Acc@task: 83.3333 (84.3254)  time: 0.1781  data: 0.0004  max mem: 1377\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.3997 (0.5724)  Acc@1: 83.3333 (85.7527)  Acc@5: 100.0000 (97.3118)  Acc@task: 83.3333 (84.6774)  time: 0.1781  data: 0.0004  max mem: 1377\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.3338 (0.5336)  Acc@1: 91.6667 (86.7886)  Acc@5: 100.0000 (97.5610)  Acc@task: 87.5000 (85.4675)  time: 0.1793  data: 0.0004  max mem: 1377\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.3277 (0.5261)  Acc@1: 91.6667 (86.9000)  Acc@5: 100.0000 (97.6000)  Acc@task: 87.5000 (85.7000)  time: 0.1756  data: 0.0003  max mem: 1377\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1896 s / it)\n",
            "* Acc@task 85.700 Acc@1 86.900 Acc@5 97.600 loss 0.526\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:35  Loss: 0.7512 (0.7512)  Acc@1: 83.3333 (83.3333)  Acc@5: 91.6667 (91.6667)  Acc@task: 87.5000 (87.5000)  time: 0.8534  data: 0.6668  max mem: 1377\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:07  Loss: 0.6638 (0.7586)  Acc@1: 83.3333 (80.6818)  Acc@5: 95.8333 (97.3485)  Acc@task: 83.3333 (82.5758)  time: 0.2395  data: 0.0611  max mem: 1377\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.6687 (0.7917)  Acc@1: 83.3333 (79.7619)  Acc@5: 95.8333 (96.2302)  Acc@task: 79.1667 (81.9444)  time: 0.1781  data: 0.0005  max mem: 1377\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.7741 (0.7980)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.9677)  Acc@task: 83.3333 (82.3925)  time: 0.1790  data: 0.0004  max mem: 1377\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.7034 (0.7656)  Acc@1: 79.1667 (79.3699)  Acc@5: 95.8333 (96.1382)  Acc@task: 83.3333 (82.5203)  time: 0.1803  data: 0.0003  max mem: 1377\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.6954 (0.7587)  Acc@1: 79.1667 (79.4000)  Acc@5: 95.8333 (96.2000)  Acc@task: 83.3333 (82.7000)  time: 0.1770  data: 0.0003  max mem: 1377\n",
            "Test: [Task 2] Total time: 0:00:08 (0.1961 s / it)\n",
            "* Acc@task 82.700 Acc@1 79.400 Acc@5 96.200 loss 0.759\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:19  Loss: 0.3762 (0.3762)  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)  Acc@task: 87.5000 (87.5000)  time: 0.4700  data: 0.2969  max mem: 1377\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 0.5847 (0.6529)  Acc@1: 83.3333 (84.4697)  Acc@5: 95.8333 (97.3485)  Acc@task: 83.3333 (84.8485)  time: 0.2072  data: 0.0294  max mem: 1377\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.5827 (0.6077)  Acc@1: 83.3333 (82.9365)  Acc@5: 95.8333 (97.2222)  Acc@task: 83.3333 (85.3175)  time: 0.1795  data: 0.0016  max mem: 1377\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.4337 (0.5326)  Acc@1: 83.3333 (84.5430)  Acc@5: 100.0000 (97.8495)  Acc@task: 83.3333 (85.3495)  time: 0.1790  data: 0.0010  max mem: 1377\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.4297 (0.5729)  Acc@1: 87.5000 (84.5528)  Acc@5: 100.0000 (97.3577)  Acc@task: 83.3333 (85.3659)  time: 0.1799  data: 0.0009  max mem: 1377\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.4497 (0.5763)  Acc@1: 87.5000 (84.4000)  Acc@5: 100.0000 (97.4000)  Acc@task: 83.3333 (85.1000)  time: 0.1765  data: 0.0009  max mem: 1377\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1876 s / it)\n",
            "* Acc@task 85.100 Acc@1 84.400 Acc@5 97.400 loss 0.576\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:24  Loss: 1.0176 (1.0176)  Acc@1: 70.8333 (70.8333)  Acc@5: 87.5000 (87.5000)  Acc@task: 75.0000 (75.0000)  time: 0.5743  data: 0.4017  max mem: 1377\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:06  Loss: 0.5956 (0.6073)  Acc@1: 83.3333 (84.4697)  Acc@5: 95.8333 (95.4545)  Acc@task: 83.3333 (85.2273)  time: 0.2143  data: 0.0371  max mem: 1377\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.6202 (0.6377)  Acc@1: 83.3333 (82.5397)  Acc@5: 95.8333 (95.4365)  Acc@task: 83.3333 (84.1270)  time: 0.1781  data: 0.0005  max mem: 1377\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.6551 (0.6374)  Acc@1: 79.1667 (82.7957)  Acc@5: 95.8333 (95.9677)  Acc@task: 83.3333 (84.1398)  time: 0.1791  data: 0.0004  max mem: 1377\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7613 (0.6826)  Acc@1: 79.1667 (82.1138)  Acc@5: 100.0000 (95.7317)  Acc@task: 83.3333 (83.9431)  time: 0.1805  data: 0.0003  max mem: 1377\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.6551 (0.6709)  Acc@1: 83.3333 (82.3000)  Acc@5: 100.0000 (95.8000)  Acc@task: 87.5000 (84.1000)  time: 0.1770  data: 0.0003  max mem: 1377\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1896 s / it)\n",
            "* Acc@task 84.100 Acc@1 82.300 Acc@5 95.800 loss 0.671\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:25  Loss: 0.3433 (0.3433)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.5978  data: 0.4328  max mem: 1377\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 0.7066 (0.7463)  Acc@1: 75.0000 (78.7879)  Acc@5: 100.0000 (97.3485)  Acc@task: 75.0000 (78.7879)  time: 0.2178  data: 0.0406  max mem: 1377\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.7908 (0.7819)  Acc@1: 75.0000 (78.1746)  Acc@5: 95.8333 (96.8254)  Acc@task: 75.0000 (78.1746)  time: 0.1793  data: 0.0016  max mem: 1377\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.7204 (0.7699)  Acc@1: 79.1667 (78.4946)  Acc@5: 95.8333 (96.1022)  Acc@task: 79.1667 (78.6290)  time: 0.1797  data: 0.0014  max mem: 1377\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.7811 (0.8074)  Acc@1: 79.1667 (78.1504)  Acc@5: 95.8333 (95.8333)  Acc@task: 75.0000 (76.8293)  time: 0.1808  data: 0.0006  max mem: 1377\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.8616 (0.8375)  Acc@1: 79.1667 (77.7000)  Acc@5: 95.8333 (95.6000)  Acc@task: 75.0000 (76.5000)  time: 0.1773  data: 0.0006  max mem: 1377\n",
            "Test: [Task 5] Total time: 0:00:08 (0.1906 s / it)\n",
            "* Acc@task 76.500 Acc@1 77.700 Acc@5 95.600 loss 0.838\n",
            "[Average accuracy till task5]\tAcc@task: 82.8200\tAcc@1: 82.1400\tAcc@5: 96.5200\tLoss: 0.6739\tForgetting: 0.5750\tBackward: 9.2000\n",
            "Loading checkpoint from: ./output/cifar100_full_dino_1epoch_50pct/checkpoint/task6_checkpoint.pth\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/engines/hide_promtp_wtp_and_tap_engine.py:540: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  loss = torch.nn.functional.cross_entropy(sim, torch.range(0, sim.shape[0] - 1).long().to(device))\n",
            "Train: Epoch[1/1]  [  0/105]  eta: 0:01:20  Lr: 0.002812  Loss: 3.7865  Acc@1: 4.1667 (4.1667)  Acc@5: 62.5000 (62.5000)  time: 0.7680  data: 0.4781  max mem: 1377\n",
            "Train: Epoch[1/1]  [ 10/105]  eta: 0:00:30  Lr: 0.002812  Loss: 3.1638  Acc@1: 12.5000 (12.1212)  Acc@5: 62.5000 (60.9849)  time: 0.3239  data: 0.0439  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 20/105]  eta: 0:00:25  Lr: 0.002812  Loss: 2.8719  Acc@1: 16.6667 (16.6667)  Acc@5: 66.6667 (64.2857)  time: 0.2793  data: 0.0004  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 30/105]  eta: 0:00:22  Lr: 0.002812  Loss: 2.3029  Acc@1: 20.8333 (20.1613)  Acc@5: 70.8333 (68.1452)  time: 0.2801  data: 0.0004  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 40/105]  eta: 0:00:19  Lr: 0.002812  Loss: 1.6700  Acc@1: 37.5000 (25.3049)  Acc@5: 83.3333 (73.3740)  time: 0.2818  data: 0.0006  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 50/105]  eta: 0:00:15  Lr: 0.002812  Loss: 1.4344  Acc@1: 45.8333 (30.3922)  Acc@5: 91.6667 (76.7974)  time: 0.2808  data: 0.0006  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 60/105]  eta: 0:00:12  Lr: 0.002812  Loss: 1.2592  Acc@1: 50.0000 (33.8798)  Acc@5: 91.6667 (79.6448)  time: 0.2788  data: 0.0005  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 70/105]  eta: 0:00:10  Lr: 0.002812  Loss: 1.1579  Acc@1: 54.1667 (37.5000)  Acc@5: 95.8333 (81.6315)  time: 0.2797  data: 0.0005  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 80/105]  eta: 0:00:07  Lr: 0.002812  Loss: 0.8980  Acc@1: 62.5000 (40.7407)  Acc@5: 95.8333 (83.3848)  time: 0.2802  data: 0.0005  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 90/105]  eta: 0:00:04  Lr: 0.002812  Loss: 1.6370  Acc@1: 62.5000 (43.2692)  Acc@5: 95.8333 (84.5238)  time: 0.2799  data: 0.0008  max mem: 1379\n",
            "Train: Epoch[1/1]  [100/105]  eta: 0:00:01  Lr: 0.002812  Loss: 0.7950  Acc@1: 62.5000 (45.7921)  Acc@5: 95.8333 (85.6848)  time: 0.2800  data: 0.0007  max mem: 1379\n",
            "Train: Epoch[1/1]  [104/105]  eta: 0:00:00  Lr: 0.002812  Loss: 1.3468  Acc@1: 62.5000 (46.3889)  Acc@5: 95.8333 (86.0317)  time: 0.2796  data: 0.0005  max mem: 1379\n",
            "Train: Epoch[1/1] Total time: 0:00:29 (0.2856 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.3468  Acc@1: 62.5000 (46.3889)  Acc@5: 95.8333 (86.0317)\n",
            "torch.Size([5, 2, 5, 6, 64])\n",
            "torch.Size([5, 2, 1, 5, 6, 64])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:20  Loss: 0.4456 (0.4456)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  Acc@task: 87.5000 (87.5000)  time: 0.4789  data: 0.3056  max mem: 1379\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.5938 (0.6247)  Acc@1: 83.3333 (84.8485)  Acc@5: 100.0000 (97.7273)  Acc@task: 83.3333 (81.4394)  time: 0.2062  data: 0.0294  max mem: 1379\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.5878 (0.6209)  Acc@1: 83.3333 (85.3175)  Acc@5: 95.8333 (97.0238)  Acc@task: 83.3333 (82.5397)  time: 0.1776  data: 0.0011  max mem: 1379\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.4127 (0.5817)  Acc@1: 83.3333 (85.8871)  Acc@5: 100.0000 (97.1774)  Acc@task: 83.3333 (83.1989)  time: 0.1772  data: 0.0006  max mem: 1379\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.3339 (0.5472)  Acc@1: 91.6667 (86.7886)  Acc@5: 100.0000 (97.3577)  Acc@task: 87.5000 (84.0447)  time: 0.1783  data: 0.0008  max mem: 1379\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.3264 (0.5394)  Acc@1: 91.6667 (86.9000)  Acc@5: 100.0000 (97.4000)  Acc@task: 87.5000 (84.3000)  time: 0.1749  data: 0.0008  max mem: 1379\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1870 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@task 84.300 Acc@1 86.900 Acc@5 97.400 loss 0.539\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:30  Loss: 0.7445 (0.7445)  Acc@1: 83.3333 (83.3333)  Acc@5: 91.6667 (91.6667)  Acc@task: 83.3333 (83.3333)  time: 0.7152  data: 0.5509  max mem: 1379\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:07  Loss: 0.6477 (0.7586)  Acc@1: 83.3333 (80.6818)  Acc@5: 95.8333 (97.3485)  Acc@task: 79.1667 (80.6818)  time: 0.2270  data: 0.0508  max mem: 1379\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.6659 (0.7960)  Acc@1: 83.3333 (79.7619)  Acc@5: 95.8333 (96.4286)  Acc@task: 75.0000 (79.7619)  time: 0.1781  data: 0.0007  max mem: 1379\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.7849 (0.8011)  Acc@1: 75.0000 (79.0323)  Acc@5: 95.8333 (95.9677)  Acc@task: 79.1667 (80.1075)  time: 0.1790  data: 0.0005  max mem: 1379\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.7028 (0.7668)  Acc@1: 75.0000 (79.1667)  Acc@5: 95.8333 (96.2398)  Acc@task: 83.3333 (80.2846)  time: 0.1801  data: 0.0004  max mem: 1379\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.6932 (0.7599)  Acc@1: 75.0000 (79.2000)  Acc@5: 95.8333 (96.3000)  Acc@task: 83.3333 (80.5000)  time: 0.1767  data: 0.0003  max mem: 1379\n",
            "Test: [Task 2] Total time: 0:00:08 (0.1927 s / it)\n",
            "* Acc@task 80.500 Acc@1 79.200 Acc@5 96.300 loss 0.760\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:25  Loss: 0.3763 (0.3763)  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)  Acc@task: 87.5000 (87.5000)  time: 0.5967  data: 0.4242  max mem: 1379\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 0.5803 (0.6345)  Acc@1: 83.3333 (84.4697)  Acc@5: 100.0000 (98.1061)  Acc@task: 83.3333 (84.0909)  time: 0.2181  data: 0.0389  max mem: 1379\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.4895 (0.5929)  Acc@1: 83.3333 (83.7302)  Acc@5: 95.8333 (97.6190)  Acc@task: 83.3333 (85.3175)  time: 0.1790  data: 0.0007  max mem: 1379\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.4268 (0.5230)  Acc@1: 83.3333 (85.2151)  Acc@5: 100.0000 (97.9839)  Acc@task: 83.3333 (84.1398)  time: 0.1786  data: 0.0012  max mem: 1379\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.4268 (0.5656)  Acc@1: 87.5000 (85.0610)  Acc@5: 95.8333 (97.4594)  Acc@task: 83.3333 (84.5528)  time: 0.1799  data: 0.0009  max mem: 1379\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.4771 (0.5694)  Acc@1: 87.5000 (84.9000)  Acc@5: 100.0000 (97.5000)  Acc@task: 83.3333 (84.3000)  time: 0.1763  data: 0.0009  max mem: 1379\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1901 s / it)\n",
            "* Acc@task 84.300 Acc@1 84.900 Acc@5 97.500 loss 0.569\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:26  Loss: 1.2324 (1.2324)  Acc@1: 62.5000 (62.5000)  Acc@5: 87.5000 (87.5000)  Acc@task: 66.6667 (66.6667)  time: 0.6297  data: 0.4568  max mem: 1379\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:07  Loss: 0.6179 (0.6363)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.4545)  Acc@task: 83.3333 (81.4394)  time: 0.2213  data: 0.0423  max mem: 1379\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.6133 (0.6426)  Acc@1: 83.3333 (81.7460)  Acc@5: 95.8333 (95.6349)  Acc@task: 83.3333 (81.9444)  time: 0.1795  data: 0.0007  max mem: 1379\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.6458 (0.6410)  Acc@1: 79.1667 (82.2581)  Acc@5: 95.8333 (96.1022)  Acc@task: 83.3333 (81.8548)  time: 0.1794  data: 0.0005  max mem: 1379\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7602 (0.6865)  Acc@1: 79.1667 (81.8089)  Acc@5: 95.8333 (95.8333)  Acc@task: 79.1667 (81.7073)  time: 0.1800  data: 0.0004  max mem: 1379\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.6540 (0.6748)  Acc@1: 83.3333 (82.0000)  Acc@5: 100.0000 (95.9000)  Acc@task: 79.1667 (82.0000)  time: 0.1763  data: 0.0004  max mem: 1379\n",
            "Test: [Task 4] Total time: 0:00:08 (0.1910 s / it)\n",
            "* Acc@task 82.000 Acc@1 82.000 Acc@5 95.900 loss 0.675\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:26  Loss: 0.3398 (0.3398)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  Acc@task: 79.1667 (79.1667)  time: 0.6214  data: 0.4378  max mem: 1379\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 0.7015 (0.7178)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (97.7273)  Acc@task: 79.1667 (79.9242)  time: 0.2185  data: 0.0415  max mem: 1379\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.7897 (0.7549)  Acc@1: 79.1667 (78.5714)  Acc@5: 95.8333 (96.8254)  Acc@task: 79.1667 (78.5714)  time: 0.1785  data: 0.0017  max mem: 1379\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.6868 (0.7421)  Acc@1: 79.1667 (78.6290)  Acc@5: 95.8333 (96.3710)  Acc@task: 79.1667 (79.7043)  time: 0.1791  data: 0.0009  max mem: 1379\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.7302 (0.7658)  Acc@1: 79.1667 (78.7602)  Acc@5: 95.8333 (96.1382)  Acc@task: 79.1667 (78.4553)  time: 0.1795  data: 0.0003  max mem: 1379\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.7467 (0.7914)  Acc@1: 79.1667 (78.4000)  Acc@5: 95.8333 (95.9000)  Acc@task: 79.1667 (78.5000)  time: 0.1763  data: 0.0003  max mem: 1379\n",
            "Test: [Task 5] Total time: 0:00:08 (0.1907 s / it)\n",
            "* Acc@task 78.500 Acc@1 78.400 Acc@5 95.900 loss 0.791\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:22  Loss: 5.7529 (5.7529)  Acc@1: 16.6667 (16.6667)  Acc@5: 58.3333 (58.3333)  Acc@task: 83.3333 (83.3333)  time: 0.5400  data: 0.3704  max mem: 1379\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:06  Loss: 5.7743 (5.8827)  Acc@1: 0.0000 (2.6515)  Acc@5: 54.1667 (53.0303)  Acc@task: 79.1667 (77.2727)  time: 0.2110  data: 0.0344  max mem: 1379\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:04  Loss: 5.7633 (5.7576)  Acc@1: 0.0000 (2.1825)  Acc@5: 50.0000 (49.0079)  Acc@task: 75.0000 (75.3968)  time: 0.1778  data: 0.0007  max mem: 1379\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 5.7806 (5.8410)  Acc@1: 0.0000 (2.5538)  Acc@5: 41.6667 (46.7742)  Acc@task: 75.0000 (74.7312)  time: 0.1783  data: 0.0005  max mem: 1379\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 5.7844 (5.8135)  Acc@1: 4.1667 (3.3537)  Acc@5: 41.6667 (46.8496)  Acc@task: 75.0000 (74.4919)  time: 0.1791  data: 0.0004  max mem: 1379\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 5.7844 (5.8311)  Acc@1: 4.1667 (3.3000)  Acc@5: 41.6667 (46.6000)  Acc@task: 75.0000 (74.7000)  time: 0.1756  data: 0.0004  max mem: 1379\n",
            "Test: [Task 6] Total time: 0:00:07 (0.1887 s / it)\n",
            "* Acc@task 74.700 Acc@1 3.300 Acc@5 46.600 loss 5.831\n",
            "[Average accuracy till task6]\tAcc@task: 80.7167\tAcc@1: 69.1167\tAcc@5: 88.2667\tLoss: 1.5277\tForgetting: 0.5800\tBackward: 63.4000\n",
            "torch.Size([72000, 384])\n",
            "Averaged stats: Lr: 0.005000  Loss: 0.1032  Acc@1: 96.6667 (93.0500)  Acc@5: 100.0000 (99.1500)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:26  Loss: 0.5269 (0.5269)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 87.5000 (87.5000)  time: 0.6314  data: 0.4615  max mem: 1379\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:07  Loss: 0.7271 (0.6851)  Acc@1: 83.3333 (82.9545)  Acc@5: 100.0000 (97.7273)  Acc@task: 83.3333 (81.4394)  time: 0.2188  data: 0.0423  max mem: 1379\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.5863 (0.6919)  Acc@1: 83.3333 (83.7302)  Acc@5: 95.8333 (96.4286)  Acc@task: 83.3333 (82.5397)  time: 0.1767  data: 0.0004  max mem: 1379\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.4748 (0.6577)  Acc@1: 83.3333 (84.0054)  Acc@5: 100.0000 (96.9086)  Acc@task: 83.3333 (83.1989)  time: 0.1775  data: 0.0004  max mem: 1379\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.4372 (0.6063)  Acc@1: 91.6667 (85.2642)  Acc@5: 100.0000 (97.4594)  Acc@task: 87.5000 (84.0447)  time: 0.1790  data: 0.0003  max mem: 1379\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.3665 (0.5980)  Acc@1: 91.6667 (85.4000)  Acc@5: 100.0000 (97.5000)  Acc@task: 87.5000 (84.3000)  time: 0.1753  data: 0.0003  max mem: 1379\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1894 s / it)\n",
            "* Acc@task 84.300 Acc@1 85.400 Acc@5 97.500 loss 0.598\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:19  Loss: 0.8238 (0.8238)  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  Acc@task: 83.3333 (83.3333)  time: 0.4630  data: 0.2922  max mem: 1379\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.7023 (0.7951)  Acc@1: 83.3333 (81.0606)  Acc@5: 95.8333 (96.5909)  Acc@task: 79.1667 (80.6818)  time: 0.2047  data: 0.0276  max mem: 1379\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.7023 (0.8413)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.6349)  Acc@task: 75.0000 (79.7619)  time: 0.1778  data: 0.0009  max mem: 1379\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.8460 (0.8542)  Acc@1: 79.1667 (79.4355)  Acc@5: 95.8333 (95.6989)  Acc@task: 79.1667 (80.1075)  time: 0.1780  data: 0.0010  max mem: 1379\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.7129 (0.8009)  Acc@1: 83.3333 (80.1829)  Acc@5: 95.8333 (95.9350)  Acc@task: 83.3333 (80.2846)  time: 0.1788  data: 0.0010  max mem: 1379\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.6231 (0.7921)  Acc@1: 83.3333 (80.4000)  Acc@5: 95.8333 (96.0000)  Acc@task: 83.3333 (80.5000)  time: 0.1750  data: 0.0010  max mem: 1379\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1860 s / it)\n",
            "* Acc@task 80.500 Acc@1 80.400 Acc@5 96.000 loss 0.792\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:24  Loss: 0.3120 (0.3120)  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)  Acc@task: 87.5000 (87.5000)  time: 0.5747  data: 0.4030  max mem: 1379\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 0.6301 (0.6076)  Acc@1: 87.5000 (85.6061)  Acc@5: 100.0000 (98.4848)  Acc@task: 83.3333 (84.0909)  time: 0.2128  data: 0.0372  max mem: 1379\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.5531 (0.5572)  Acc@1: 87.5000 (85.9127)  Acc@5: 95.8333 (97.0238)  Acc@task: 83.3333 (85.3175)  time: 0.1771  data: 0.0005  max mem: 1379\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.4211 (0.5155)  Acc@1: 87.5000 (86.0215)  Acc@5: 95.8333 (97.5806)  Acc@task: 83.3333 (84.1398)  time: 0.1788  data: 0.0004  max mem: 1379\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.4796 (0.5497)  Acc@1: 87.5000 (85.3659)  Acc@5: 95.8333 (96.8496)  Acc@task: 83.3333 (84.5528)  time: 0.1791  data: 0.0004  max mem: 1379\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.4796 (0.5519)  Acc@1: 87.5000 (85.2000)  Acc@5: 95.8333 (96.9000)  Acc@task: 83.3333 (84.3000)  time: 0.1760  data: 0.0003  max mem: 1379\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1884 s / it)\n",
            "* Acc@task 84.300 Acc@1 85.200 Acc@5 96.900 loss 0.552\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:21  Loss: 1.0709 (1.0709)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  Acc@task: 66.6667 (66.6667)  time: 0.5149  data: 0.3334  max mem: 1379\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:06  Loss: 0.7348 (0.7556)  Acc@1: 79.1667 (79.5455)  Acc@5: 95.8333 (94.6970)  Acc@task: 83.3333 (81.4394)  time: 0.2090  data: 0.0320  max mem: 1379\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.6764 (0.7542)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.2381)  Acc@task: 83.3333 (81.9444)  time: 0.1778  data: 0.0018  max mem: 1379\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.6068 (0.7429)  Acc@1: 79.1667 (79.9731)  Acc@5: 95.8333 (95.4301)  Acc@task: 83.3333 (81.8548)  time: 0.1778  data: 0.0013  max mem: 1379\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.6980 (0.8050)  Acc@1: 79.1667 (79.2683)  Acc@5: 95.8333 (94.9187)  Acc@task: 79.1667 (81.7073)  time: 0.1784  data: 0.0005  max mem: 1379\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.6209 (0.7952)  Acc@1: 79.1667 (79.4000)  Acc@5: 95.8333 (95.0000)  Acc@task: 79.1667 (82.0000)  time: 0.1750  data: 0.0004  max mem: 1379\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1868 s / it)\n",
            "* Acc@task 82.000 Acc@1 79.400 Acc@5 95.000 loss 0.795\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:20  Loss: 0.3596 (0.3596)  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)  Acc@task: 79.1667 (79.1667)  time: 0.4828  data: 0.3135  max mem: 1379\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 0.5702 (0.5944)  Acc@1: 83.3333 (82.9545)  Acc@5: 100.0000 (98.8636)  Acc@task: 79.1667 (79.9242)  time: 0.2065  data: 0.0294  max mem: 1379\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.6494 (0.6451)  Acc@1: 79.1667 (81.9444)  Acc@5: 100.0000 (98.2143)  Acc@task: 79.1667 (78.5714)  time: 0.1784  data: 0.0007  max mem: 1379\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.6420 (0.6394)  Acc@1: 83.3333 (82.2581)  Acc@5: 95.8333 (97.7151)  Acc@task: 79.1667 (79.7043)  time: 0.1788  data: 0.0004  max mem: 1379\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.6957 (0.6833)  Acc@1: 83.3333 (82.1138)  Acc@5: 95.8333 (96.8496)  Acc@task: 79.1667 (78.4553)  time: 0.1796  data: 0.0004  max mem: 1379\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.7140 (0.7073)  Acc@1: 83.3333 (81.8000)  Acc@5: 95.8333 (96.6000)  Acc@task: 79.1667 (78.5000)  time: 0.1761  data: 0.0004  max mem: 1379\n",
            "Test: [Task 5] Total time: 0:00:07 (0.1887 s / it)\n",
            "* Acc@task 78.500 Acc@1 81.800 Acc@5 96.600 loss 0.707\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:36  Loss: 0.4562 (0.4562)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.8645  data: 0.6660  max mem: 1379\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:07  Loss: 0.6657 (0.7659)  Acc@1: 83.3333 (78.7879)  Acc@5: 100.0000 (97.3485)  Acc@task: 79.1667 (77.2727)  time: 0.2421  data: 0.0636  max mem: 1379\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:04  Loss: 0.7917 (0.7940)  Acc@1: 79.1667 (77.5794)  Acc@5: 95.8333 (97.0238)  Acc@task: 75.0000 (75.3968)  time: 0.1793  data: 0.0020  max mem: 1379\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 0.7917 (0.8105)  Acc@1: 79.1667 (77.5538)  Acc@5: 95.8333 (96.7742)  Acc@task: 75.0000 (74.7312)  time: 0.1787  data: 0.0006  max mem: 1379\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 0.7057 (0.8280)  Acc@1: 75.0000 (77.1341)  Acc@5: 95.8333 (96.0366)  Acc@task: 75.0000 (74.4919)  time: 0.1792  data: 0.0004  max mem: 1379\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 0.7057 (0.8286)  Acc@1: 75.0000 (77.0000)  Acc@5: 95.8333 (96.1000)  Acc@task: 75.0000 (74.7000)  time: 0.1765  data: 0.0003  max mem: 1379\n",
            "Test: [Task 6] Total time: 0:00:08 (0.1966 s / it)\n",
            "* Acc@task 74.700 Acc@1 77.000 Acc@5 96.100 loss 0.829\n",
            "[Average accuracy till task6]\tAcc@task: 80.7167\tAcc@1: 81.5333\tAcc@5: 96.3500\tLoss: 0.7122\tForgetting: 1.1200\tBackward: 7.6600\n",
            "Loading checkpoint from: ./output/cifar100_full_dino_1epoch_50pct/checkpoint/task7_checkpoint.pth\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/engines/hide_promtp_wtp_and_tap_engine.py:540: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  loss = torch.nn.functional.cross_entropy(sim, torch.range(0, sim.shape[0] - 1).long().to(device))\n",
            "Train: Epoch[1/1]  [  0/105]  eta: 0:01:06  Lr: 0.002812  Loss: 3.9543  Acc@1: 4.1667 (4.1667)  Acc@5: 54.1667 (54.1667)  time: 0.6310  data: 0.3504  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 10/105]  eta: 0:00:29  Lr: 0.002812  Loss: 3.1779  Acc@1: 12.5000 (11.7424)  Acc@5: 50.0000 (52.2727)  time: 0.3112  data: 0.0322  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 20/105]  eta: 0:00:25  Lr: 0.002812  Loss: 2.0992  Acc@1: 16.6667 (16.6667)  Acc@5: 62.5000 (61.7064)  time: 0.2791  data: 0.0008  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 30/105]  eta: 0:00:21  Lr: 0.002812  Loss: 1.5874  Acc@1: 29.1667 (23.9247)  Acc@5: 79.1667 (68.5484)  time: 0.2799  data: 0.0012  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 40/105]  eta: 0:00:18  Lr: 0.002812  Loss: 1.7584  Acc@1: 41.6667 (28.2520)  Acc@5: 87.5000 (73.1707)  time: 0.2806  data: 0.0008  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 50/105]  eta: 0:00:15  Lr: 0.002812  Loss: 1.2010  Acc@1: 45.8333 (34.3954)  Acc@5: 87.5000 (76.7157)  time: 0.2805  data: 0.0004  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 60/105]  eta: 0:00:12  Lr: 0.002812  Loss: 1.2424  Acc@1: 58.3333 (37.7049)  Acc@5: 91.6667 (79.2350)  time: 0.2805  data: 0.0005  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 70/105]  eta: 0:00:09  Lr: 0.002812  Loss: 1.2919  Acc@1: 58.3333 (41.5493)  Acc@5: 91.6667 (81.1033)  time: 0.2806  data: 0.0006  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 80/105]  eta: 0:00:07  Lr: 0.002812  Loss: 0.9409  Acc@1: 62.5000 (44.4959)  Acc@5: 95.8333 (83.0247)  time: 0.2810  data: 0.0012  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 90/105]  eta: 0:00:04  Lr: 0.002812  Loss: 1.0297  Acc@1: 70.8333 (47.8480)  Acc@5: 95.8333 (84.3864)  time: 0.2814  data: 0.0012  max mem: 1379\n",
            "Train: Epoch[1/1]  [100/105]  eta: 0:00:01  Lr: 0.002812  Loss: 1.1153  Acc@1: 70.8333 (49.9587)  Acc@5: 95.8333 (85.4373)  time: 0.2806  data: 0.0005  max mem: 1379\n",
            "Train: Epoch[1/1]  [104/105]  eta: 0:00:00  Lr: 0.002812  Loss: 0.1579  Acc@1: 70.8333 (50.6207)  Acc@5: 95.8333 (85.7829)  time: 0.2687  data: 0.0004  max mem: 1379\n",
            "Train: Epoch[1/1] Total time: 0:00:29 (0.2824 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.1579  Acc@1: 70.8333 (50.6207)  Acc@5: 95.8333 (85.7829)\n",
            "torch.Size([5, 2, 5, 6, 64])\n",
            "torch.Size([5, 2, 1, 5, 6, 64])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:21  Loss: 0.5204 (0.5204)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 75.0000 (75.0000)  time: 0.5024  data: 0.3238  max mem: 1379\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.7867 (0.7038)  Acc@1: 83.3333 (82.1970)  Acc@5: 100.0000 (97.7273)  Acc@task: 79.1667 (79.5455)  time: 0.2051  data: 0.0300  max mem: 1379\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.5775 (0.6848)  Acc@1: 83.3333 (83.5317)  Acc@5: 95.8333 (96.6270)  Acc@task: 83.3333 (81.5476)  time: 0.1766  data: 0.0011  max mem: 1379\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.5040 (0.6562)  Acc@1: 79.1667 (83.7366)  Acc@5: 100.0000 (97.0430)  Acc@task: 83.3333 (81.7204)  time: 0.1785  data: 0.0015  max mem: 1379\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.4555 (0.6128)  Acc@1: 87.5000 (84.9594)  Acc@5: 100.0000 (97.5610)  Acc@task: 83.3333 (82.8252)  time: 0.1788  data: 0.0008  max mem: 1379\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.3661 (0.6043)  Acc@1: 91.6667 (85.1000)  Acc@5: 100.0000 (97.6000)  Acc@task: 83.3333 (83.1000)  time: 0.1760  data: 0.0007  max mem: 1379\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1864 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@task 83.100 Acc@1 85.100 Acc@5 97.600 loss 0.604\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:22  Loss: 0.8189 (0.8189)  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  Acc@task: 87.5000 (87.5000)  time: 0.5244  data: 0.3540  max mem: 1379\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.6972 (0.8073)  Acc@1: 83.3333 (81.0606)  Acc@5: 95.8333 (96.2121)  Acc@task: 79.1667 (78.4091)  time: 0.2095  data: 0.0327  max mem: 1379\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.6972 (0.8582)  Acc@1: 79.1667 (78.5714)  Acc@5: 95.8333 (95.4365)  Acc@task: 75.0000 (76.5873)  time: 0.1777  data: 0.0005  max mem: 1379\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.8696 (0.8719)  Acc@1: 79.1667 (79.0323)  Acc@5: 95.8333 (95.5645)  Acc@task: 75.0000 (77.0161)  time: 0.1780  data: 0.0004  max mem: 1379\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.7163 (0.8160)  Acc@1: 83.3333 (79.8781)  Acc@5: 95.8333 (95.8333)  Acc@task: 79.1667 (77.3374)  time: 0.1793  data: 0.0004  max mem: 1379\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.6614 (0.8069)  Acc@1: 83.3333 (80.1000)  Acc@5: 95.8333 (95.9000)  Acc@task: 79.1667 (77.5000)  time: 0.1761  data: 0.0003  max mem: 1379\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1876 s / it)\n",
            "* Acc@task 77.500 Acc@1 80.100 Acc@5 95.900 loss 0.807\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:29  Loss: 0.3122 (0.3122)  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.6980  data: 0.4880  max mem: 1379\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:07  Loss: 0.6300 (0.6136)  Acc@1: 87.5000 (85.2273)  Acc@5: 100.0000 (96.9697)  Acc@task: 83.3333 (82.1970)  time: 0.2271  data: 0.0463  max mem: 1379\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.5535 (0.5703)  Acc@1: 87.5000 (85.5159)  Acc@5: 95.8333 (96.8254)  Acc@task: 83.3333 (83.9286)  time: 0.1796  data: 0.0018  max mem: 1379\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.4087 (0.5277)  Acc@1: 87.5000 (85.7527)  Acc@5: 100.0000 (97.4462)  Acc@task: 83.3333 (82.9301)  time: 0.1791  data: 0.0011  max mem: 1379\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.4796 (0.5582)  Acc@1: 87.5000 (85.1626)  Acc@5: 95.8333 (96.7480)  Acc@task: 83.3333 (83.2317)  time: 0.1795  data: 0.0004  max mem: 1379\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.4796 (0.5600)  Acc@1: 87.5000 (85.0000)  Acc@5: 95.8333 (96.8000)  Acc@task: 83.3333 (83.1000)  time: 0.1763  data: 0.0004  max mem: 1379\n",
            "Test: [Task 3] Total time: 0:00:08 (0.1927 s / it)\n",
            "* Acc@task 83.100 Acc@1 85.000 Acc@5 96.800 loss 0.560\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:25  Loss: 0.7601 (0.7601)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  Acc@task: 66.6667 (66.6667)  time: 0.5956  data: 0.4184  max mem: 1379\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:06  Loss: 0.7558 (0.7309)  Acc@1: 79.1667 (79.9242)  Acc@5: 95.8333 (95.4545)  Acc@task: 79.1667 (79.5455)  time: 0.2167  data: 0.0389  max mem: 1379\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.6471 (0.7186)  Acc@1: 79.1667 (79.3651)  Acc@5: 95.8333 (95.8333)  Acc@task: 79.1667 (80.5556)  time: 0.1788  data: 0.0011  max mem: 1379\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.5829 (0.7149)  Acc@1: 79.1667 (80.5108)  Acc@5: 95.8333 (95.9677)  Acc@task: 79.1667 (81.4516)  time: 0.1797  data: 0.0010  max mem: 1379\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.6739 (0.7786)  Acc@1: 79.1667 (79.6748)  Acc@5: 95.8333 (95.2236)  Acc@task: 79.1667 (81.1992)  time: 0.1804  data: 0.0006  max mem: 1379\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.6228 (0.7695)  Acc@1: 79.1667 (79.8000)  Acc@5: 95.8333 (95.3000)  Acc@task: 79.1667 (81.4000)  time: 0.1771  data: 0.0005  max mem: 1379\n",
            "Test: [Task 4] Total time: 0:00:08 (0.1920 s / it)\n",
            "* Acc@task 81.400 Acc@1 79.800 Acc@5 95.300 loss 0.769\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:30  Loss: 0.3527 (0.3527)  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.7341  data: 0.5529  max mem: 1379\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:07  Loss: 0.5651 (0.6044)  Acc@1: 83.3333 (82.1970)  Acc@5: 100.0000 (99.2424)  Acc@task: 75.0000 (79.5455)  time: 0.2316  data: 0.0549  max mem: 1379\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.6477 (0.6460)  Acc@1: 79.1667 (82.1429)  Acc@5: 100.0000 (98.4127)  Acc@task: 75.0000 (78.3730)  time: 0.1806  data: 0.0029  max mem: 1379\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.6065 (0.6338)  Acc@1: 83.3333 (82.5269)  Acc@5: 95.8333 (97.8495)  Acc@task: 79.1667 (79.8387)  time: 0.1797  data: 0.0005  max mem: 1379\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.6703 (0.6748)  Acc@1: 83.3333 (82.4187)  Acc@5: 95.8333 (96.9512)  Acc@task: 79.1667 (78.8618)  time: 0.1797  data: 0.0003  max mem: 1379\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.7142 (0.6999)  Acc@1: 83.3333 (82.1000)  Acc@5: 95.8333 (96.7000)  Acc@task: 79.1667 (78.8000)  time: 0.1764  data: 0.0003  max mem: 1379\n",
            "Test: [Task 5] Total time: 0:00:08 (0.1942 s / it)\n",
            "* Acc@task 78.800 Acc@1 82.100 Acc@5 96.700 loss 0.700\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:19  Loss: 0.4561 (0.4561)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.4605  data: 0.2885  max mem: 1379\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:06  Loss: 0.6593 (0.7594)  Acc@1: 83.3333 (78.7879)  Acc@5: 100.0000 (97.3485)  Acc@task: 79.1667 (79.9242)  time: 0.2081  data: 0.0319  max mem: 1379\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:04  Loss: 0.6988 (0.7656)  Acc@1: 79.1667 (78.5714)  Acc@5: 95.8333 (97.0238)  Acc@task: 79.1667 (79.5635)  time: 0.1807  data: 0.0034  max mem: 1379\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 0.7231 (0.7762)  Acc@1: 79.1667 (78.2258)  Acc@5: 95.8333 (97.0430)  Acc@task: 79.1667 (79.0323)  time: 0.1794  data: 0.0005  max mem: 1379\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 0.7231 (0.7882)  Acc@1: 79.1667 (77.9472)  Acc@5: 95.8333 (96.4431)  Acc@task: 79.1667 (78.7602)  time: 0.1799  data: 0.0006  max mem: 1379\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 0.7231 (0.7897)  Acc@1: 79.1667 (77.8000)  Acc@5: 95.8333 (96.5000)  Acc@task: 79.1667 (78.8000)  time: 0.1768  data: 0.0006  max mem: 1379\n",
            "Test: [Task 6] Total time: 0:00:07 (0.1881 s / it)\n",
            "* Acc@task 78.800 Acc@1 77.800 Acc@5 96.500 loss 0.790\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:27  Loss: 5.5136 (5.5136)  Acc@1: 0.0000 (0.0000)  Acc@5: 41.6667 (41.6667)  Acc@task: 70.8333 (70.8333)  time: 0.6542  data: 0.4799  max mem: 1379\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:07  Loss: 5.8195 (5.7451)  Acc@1: 0.0000 (1.8939)  Acc@5: 41.6667 (43.1818)  Acc@task: 66.6667 (66.6667)  time: 0.2227  data: 0.0440  max mem: 1379\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:04  Loss: 5.8195 (5.7232)  Acc@1: 0.0000 (2.1825)  Acc@5: 41.6667 (42.0635)  Acc@task: 66.6667 (66.8651)  time: 0.1783  data: 0.0005  max mem: 1379\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:02  Loss: 5.8893 (5.8460)  Acc@1: 0.0000 (2.8226)  Acc@5: 37.5000 (40.1882)  Acc@task: 66.6667 (67.2043)  time: 0.1781  data: 0.0005  max mem: 1379\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 6.0487 (5.8522)  Acc@1: 0.0000 (2.5407)  Acc@5: 33.3333 (40.3455)  Acc@task: 66.6667 (67.5813)  time: 0.1791  data: 0.0004  max mem: 1379\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 6.0487 (5.8171)  Acc@1: 0.0000 (2.7000)  Acc@5: 37.5000 (40.4000)  Acc@task: 66.6667 (67.7000)  time: 0.1756  data: 0.0004  max mem: 1379\n",
            "Test: [Task 7] Total time: 0:00:08 (0.1907 s / it)\n",
            "* Acc@task 67.700 Acc@1 2.700 Acc@5 40.400 loss 5.817\n",
            "[Average accuracy till task7]\tAcc@task: 78.6286\tAcc@1: 70.3714\tAcc@5: 88.4571\tLoss: 1.4353\tForgetting: 0.9833\tBackward: 65.3667\n",
            "torch.Size([84000, 384])\n",
            "Averaged stats: Lr: 0.005000  Loss: 0.0528  Acc@1: 95.8333 (93.2917)  Acc@5: 100.0000 (99.1806)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:28  Loss: 0.4791 (0.4791)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  Acc@task: 75.0000 (75.0000)  time: 0.6904  data: 0.4769  max mem: 1379\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:07  Loss: 0.7895 (0.7955)  Acc@1: 83.3333 (81.8182)  Acc@5: 100.0000 (96.9697)  Acc@task: 79.1667 (79.5455)  time: 0.2255  data: 0.0458  max mem: 1379\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.7103 (0.7700)  Acc@1: 83.3333 (81.9444)  Acc@5: 95.8333 (96.4286)  Acc@task: 83.3333 (81.5476)  time: 0.1778  data: 0.0022  max mem: 1379\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.5619 (0.7274)  Acc@1: 83.3333 (82.7957)  Acc@5: 100.0000 (96.7742)  Acc@task: 83.3333 (81.7204)  time: 0.1775  data: 0.0011  max mem: 1379\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.4159 (0.6824)  Acc@1: 87.5000 (83.6382)  Acc@5: 100.0000 (97.0528)  Acc@task: 83.3333 (82.8252)  time: 0.1786  data: 0.0003  max mem: 1379\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.3739 (0.6718)  Acc@1: 87.5000 (83.8000)  Acc@5: 100.0000 (97.1000)  Acc@task: 83.3333 (83.1000)  time: 0.1751  data: 0.0003  max mem: 1379\n",
            "Test: [Task 1] Total time: 0:00:08 (0.1913 s / it)\n",
            "* Acc@task 83.100 Acc@1 83.800 Acc@5 97.100 loss 0.672\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:18  Loss: 0.9446 (0.9446)  Acc@1: 83.3333 (83.3333)  Acc@5: 91.6667 (91.6667)  Acc@task: 87.5000 (87.5000)  time: 0.4469  data: 0.2711  max mem: 1379\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.7490 (0.8550)  Acc@1: 83.3333 (78.7879)  Acc@5: 95.8333 (96.9697)  Acc@task: 79.1667 (78.4091)  time: 0.2034  data: 0.0279  max mem: 1379\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.7738 (0.9004)  Acc@1: 79.1667 (77.9762)  Acc@5: 95.8333 (96.2302)  Acc@task: 75.0000 (76.5873)  time: 0.1783  data: 0.0020  max mem: 1379\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.8745 (0.9212)  Acc@1: 79.1667 (77.0161)  Acc@5: 95.8333 (95.8333)  Acc@task: 75.0000 (77.0161)  time: 0.1781  data: 0.0004  max mem: 1379\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.7552 (0.8643)  Acc@1: 79.1667 (77.7439)  Acc@5: 95.8333 (96.2398)  Acc@task: 79.1667 (77.3374)  time: 0.1786  data: 0.0004  max mem: 1379\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.7552 (0.8660)  Acc@1: 79.1667 (77.8000)  Acc@5: 100.0000 (96.3000)  Acc@task: 79.1667 (77.5000)  time: 0.1754  data: 0.0004  max mem: 1379\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1870 s / it)\n",
            "* Acc@task 77.500 Acc@1 77.800 Acc@5 96.300 loss 0.866\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:37  Loss: 0.3859 (0.3859)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.8907  data: 0.7054  max mem: 1379\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:07  Loss: 0.6824 (0.7370)  Acc@1: 79.1667 (81.0606)  Acc@5: 100.0000 (96.5909)  Acc@task: 83.3333 (82.1970)  time: 0.2425  data: 0.0647  max mem: 1379\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.6434 (0.6673)  Acc@1: 83.3333 (82.5397)  Acc@5: 95.8333 (97.0238)  Acc@task: 83.3333 (83.9286)  time: 0.1774  data: 0.0006  max mem: 1379\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.4725 (0.6200)  Acc@1: 83.3333 (82.3925)  Acc@5: 100.0000 (97.4462)  Acc@task: 83.3333 (82.9301)  time: 0.1777  data: 0.0004  max mem: 1379\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.5102 (0.6275)  Acc@1: 83.3333 (83.2317)  Acc@5: 100.0000 (97.1545)  Acc@task: 83.3333 (83.2317)  time: 0.1782  data: 0.0003  max mem: 1379\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.4937 (0.6219)  Acc@1: 83.3333 (83.3000)  Acc@5: 100.0000 (97.2000)  Acc@task: 83.3333 (83.1000)  time: 0.1749  data: 0.0003  max mem: 1379\n",
            "Test: [Task 3] Total time: 0:00:08 (0.1957 s / it)\n",
            "* Acc@task 83.100 Acc@1 83.300 Acc@5 97.200 loss 0.622\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:21  Loss: 0.6804 (0.6804)  Acc@1: 83.3333 (83.3333)  Acc@5: 91.6667 (91.6667)  Acc@task: 66.6667 (66.6667)  time: 0.5179  data: 0.2937  max mem: 1379\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:06  Loss: 0.6804 (0.6126)  Acc@1: 83.3333 (82.5758)  Acc@5: 95.8333 (96.2121)  Acc@task: 79.1667 (79.5455)  time: 0.2096  data: 0.0273  max mem: 1379\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.5488 (0.5947)  Acc@1: 83.3333 (83.1349)  Acc@5: 95.8333 (96.4286)  Acc@task: 79.1667 (80.5556)  time: 0.1787  data: 0.0006  max mem: 1379\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.5969 (0.5937)  Acc@1: 83.3333 (84.1398)  Acc@5: 95.8333 (96.5054)  Acc@task: 79.1667 (81.4516)  time: 0.1787  data: 0.0007  max mem: 1379\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.6926 (0.6491)  Acc@1: 83.3333 (83.6382)  Acc@5: 95.8333 (95.9350)  Acc@task: 79.1667 (81.1992)  time: 0.1786  data: 0.0008  max mem: 1379\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.6146 (0.6417)  Acc@1: 83.3333 (83.8000)  Acc@5: 95.8333 (96.0000)  Acc@task: 79.1667 (81.4000)  time: 0.1757  data: 0.0008  max mem: 1379\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1877 s / it)\n",
            "* Acc@task 81.400 Acc@1 83.800 Acc@5 96.000 loss 0.642\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:25  Loss: 0.4057 (0.4057)  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.6080  data: 0.4305  max mem: 1379\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 0.4605 (0.5600)  Acc@1: 83.3333 (82.9545)  Acc@5: 100.0000 (98.4848)  Acc@task: 75.0000 (79.5455)  time: 0.2177  data: 0.0397  max mem: 1379\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.5076 (0.6148)  Acc@1: 83.3333 (83.1349)  Acc@5: 100.0000 (97.6190)  Acc@task: 75.0000 (78.3730)  time: 0.1775  data: 0.0005  max mem: 1379\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.5537 (0.6033)  Acc@1: 83.3333 (83.1989)  Acc@5: 95.8333 (97.3118)  Acc@task: 79.1667 (79.8387)  time: 0.1778  data: 0.0004  max mem: 1379\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.6299 (0.6403)  Acc@1: 83.3333 (83.1301)  Acc@5: 95.8333 (96.6463)  Acc@task: 79.1667 (78.8618)  time: 0.1791  data: 0.0003  max mem: 1379\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.6607 (0.6605)  Acc@1: 83.3333 (82.8000)  Acc@5: 95.8333 (96.5000)  Acc@task: 79.1667 (78.8000)  time: 0.1756  data: 0.0003  max mem: 1379\n",
            "Test: [Task 5] Total time: 0:00:07 (0.1893 s / it)\n",
            "* Acc@task 78.800 Acc@1 82.800 Acc@5 96.500 loss 0.661\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:22  Loss: 0.6497 (0.6497)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (83.3333)  time: 0.5394  data: 0.3460  max mem: 1379\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:06  Loss: 0.7554 (0.8696)  Acc@1: 83.3333 (78.4091)  Acc@5: 95.8333 (97.3485)  Acc@task: 79.1667 (79.9242)  time: 0.2103  data: 0.0325  max mem: 1379\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:04  Loss: 0.9315 (0.8889)  Acc@1: 79.1667 (77.5794)  Acc@5: 95.8333 (97.2222)  Acc@task: 79.1667 (79.5635)  time: 0.1775  data: 0.0015  max mem: 1379\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 0.9063 (0.9040)  Acc@1: 75.0000 (76.6129)  Acc@5: 95.8333 (96.6398)  Acc@task: 79.1667 (79.0323)  time: 0.1784  data: 0.0015  max mem: 1379\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 0.8977 (0.9168)  Acc@1: 75.0000 (76.1179)  Acc@5: 95.8333 (95.9350)  Acc@task: 79.1667 (78.7602)  time: 0.1787  data: 0.0007  max mem: 1379\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 0.8541 (0.9116)  Acc@1: 75.0000 (76.1000)  Acc@5: 95.8333 (95.9000)  Acc@task: 79.1667 (78.8000)  time: 0.1755  data: 0.0006  max mem: 1379\n",
            "Test: [Task 6] Total time: 0:00:07 (0.1877 s / it)\n",
            "* Acc@task 78.800 Acc@1 76.100 Acc@5 95.900 loss 0.912\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:19  Loss: 0.9145 (0.9145)  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  Acc@task: 70.8333 (70.8333)  time: 0.4717  data: 0.2987  max mem: 1379\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:06  Loss: 0.6752 (0.7526)  Acc@1: 79.1667 (78.7879)  Acc@5: 100.0000 (96.5909)  Acc@task: 66.6667 (66.6667)  time: 0.2054  data: 0.0281  max mem: 1379\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:04  Loss: 0.7686 (0.8411)  Acc@1: 79.1667 (76.1905)  Acc@5: 100.0000 (96.8254)  Acc@task: 66.6667 (66.8651)  time: 0.1777  data: 0.0007  max mem: 1379\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:02  Loss: 0.8127 (0.8764)  Acc@1: 75.0000 (76.3441)  Acc@5: 95.8333 (95.5645)  Acc@task: 66.6667 (67.2043)  time: 0.1778  data: 0.0004  max mem: 1379\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 0.8127 (0.9020)  Acc@1: 75.0000 (76.1179)  Acc@5: 95.8333 (95.3252)  Acc@task: 66.6667 (67.5813)  time: 0.1791  data: 0.0003  max mem: 1379\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 0.8060 (0.8948)  Acc@1: 75.0000 (76.2000)  Acc@5: 95.8333 (95.3000)  Acc@task: 66.6667 (67.7000)  time: 0.1756  data: 0.0003  max mem: 1379\n",
            "Test: [Task 7] Total time: 0:00:07 (0.1862 s / it)\n",
            "* Acc@task 67.700 Acc@1 76.200 Acc@5 95.300 loss 0.895\n",
            "[Average accuracy till task7]\tAcc@task: 78.6286\tAcc@1: 80.5429\tAcc@5: 96.3286\tLoss: 0.7526\tForgetting: 1.6167\tBackward: 6.1167\n",
            "Loading checkpoint from: ./output/cifar100_full_dino_1epoch_50pct/checkpoint/task8_checkpoint.pth\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/engines/hide_promtp_wtp_and_tap_engine.py:540: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  loss = torch.nn.functional.cross_entropy(sim, torch.range(0, sim.shape[0] - 1).long().to(device))\n",
            "Train: Epoch[1/1]  [  0/106]  eta: 0:02:10  Lr: 0.002812  Loss: 4.1115  Acc@1: 4.1667 (4.1667)  Acc@5: 54.1667 (54.1667)  time: 1.2321  data: 0.9043  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 10/106]  eta: 0:00:35  Lr: 0.002812  Loss: 3.1579  Acc@1: 4.1667 (6.0606)  Acc@5: 45.8333 (45.8333)  time: 0.3657  data: 0.0833  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 20/106]  eta: 0:00:27  Lr: 0.002812  Loss: 2.3936  Acc@1: 4.1667 (8.7302)  Acc@5: 54.1667 (52.9762)  time: 0.2793  data: 0.0009  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 30/106]  eta: 0:00:23  Lr: 0.002812  Loss: 2.1961  Acc@1: 16.6667 (15.7258)  Acc@5: 66.6667 (60.7527)  time: 0.2799  data: 0.0006  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 40/106]  eta: 0:00:20  Lr: 0.002812  Loss: 1.2754  Acc@1: 37.5000 (22.4594)  Acc@5: 79.1667 (66.2602)  time: 0.2807  data: 0.0004  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 50/106]  eta: 0:00:16  Lr: 0.002812  Loss: 1.5324  Acc@1: 45.8333 (27.6144)  Acc@5: 83.3333 (70.2614)  time: 0.2806  data: 0.0005  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 60/106]  eta: 0:00:13  Lr: 0.002812  Loss: 1.3109  Acc@1: 50.0000 (32.9918)  Acc@5: 87.5000 (73.5656)  time: 0.2806  data: 0.0007  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 70/106]  eta: 0:00:10  Lr: 0.002812  Loss: 1.3177  Acc@1: 62.5000 (37.2653)  Acc@5: 91.6667 (76.2911)  time: 0.2808  data: 0.0006  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 80/106]  eta: 0:00:07  Lr: 0.002812  Loss: 1.3193  Acc@1: 66.6667 (41.5638)  Acc@5: 95.8333 (78.7551)  time: 0.2810  data: 0.0004  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 90/106]  eta: 0:00:04  Lr: 0.002812  Loss: 1.4236  Acc@1: 70.8333 (44.9176)  Acc@5: 95.8333 (80.7692)  time: 0.2822  data: 0.0004  max mem: 1379\n",
            "Train: Epoch[1/1]  [100/106]  eta: 0:00:01  Lr: 0.002812  Loss: 1.1255  Acc@1: 75.0000 (48.1023)  Acc@5: 95.8333 (82.1782)  time: 0.2824  data: 0.0004  max mem: 1379\n",
            "Train: Epoch[1/1]  [105/106]  eta: 0:00:00  Lr: 0.002812  Loss: 0.2920  Acc@1: 70.8333 (49.0491)  Acc@5: 95.8333 (82.8447)  time: 0.2714  data: 0.0003  max mem: 1379\n",
            "Train: Epoch[1/1] Total time: 0:00:30 (0.2892 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.2920  Acc@1: 70.8333 (49.0491)  Acc@5: 95.8333 (82.8447)\n",
            "torch.Size([5, 2, 5, 6, 64])\n",
            "torch.Size([5, 2, 1, 5, 6, 64])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:20  Loss: 0.4785 (0.4785)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  Acc@task: 79.1667 (79.1667)  time: 0.4821  data: 0.3011  max mem: 1379\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.8378 (0.8199)  Acc@1: 83.3333 (80.6818)  Acc@5: 100.0000 (97.3485)  Acc@task: 79.1667 (79.5455)  time: 0.2061  data: 0.0307  max mem: 1379\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.7120 (0.8109)  Acc@1: 83.3333 (81.1508)  Acc@5: 95.8333 (96.2302)  Acc@task: 83.3333 (81.5476)  time: 0.1781  data: 0.0020  max mem: 1379\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.5475 (0.7430)  Acc@1: 83.3333 (82.3925)  Acc@5: 100.0000 (96.6398)  Acc@task: 83.3333 (81.5860)  time: 0.1783  data: 0.0004  max mem: 1379\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.4159 (0.6959)  Acc@1: 87.5000 (83.3333)  Acc@5: 100.0000 (96.9512)  Acc@task: 83.3333 (82.5203)  time: 0.1793  data: 0.0003  max mem: 1379\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.3584 (0.6849)  Acc@1: 87.5000 (83.5000)  Acc@5: 100.0000 (97.0000)  Acc@task: 83.3333 (82.8000)  time: 0.1762  data: 0.0003  max mem: 1379\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1869 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@task 82.800 Acc@1 83.500 Acc@5 97.000 loss 0.685\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:20  Loss: 0.9610 (0.9610)  Acc@1: 83.3333 (83.3333)  Acc@5: 91.6667 (91.6667)  Acc@task: 79.1667 (79.1667)  time: 0.4926  data: 0.3299  max mem: 1379\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.7466 (0.8804)  Acc@1: 83.3333 (78.4091)  Acc@5: 95.8333 (96.9697)  Acc@task: 70.8333 (74.2424)  time: 0.2076  data: 0.0309  max mem: 1379\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.7466 (0.9277)  Acc@1: 79.1667 (77.3810)  Acc@5: 95.8333 (96.2302)  Acc@task: 70.8333 (74.8016)  time: 0.1792  data: 0.0013  max mem: 1379\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.8867 (0.9349)  Acc@1: 79.1667 (76.6129)  Acc@5: 95.8333 (95.9677)  Acc@task: 75.0000 (75.4032)  time: 0.1797  data: 0.0013  max mem: 1379\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.7564 (0.8750)  Acc@1: 79.1667 (77.4390)  Acc@5: 95.8333 (96.3415)  Acc@task: 75.0000 (76.0163)  time: 0.1803  data: 0.0007  max mem: 1379\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.7564 (0.8764)  Acc@1: 79.1667 (77.5000)  Acc@5: 100.0000 (96.4000)  Acc@task: 75.0000 (76.2000)  time: 0.1770  data: 0.0007  max mem: 1379\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1880 s / it)\n",
            "* Acc@task 76.200 Acc@1 77.500 Acc@5 96.400 loss 0.876\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:20  Loss: 0.3858 (0.3858)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.4846  data: 0.2935  max mem: 1379\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 0.6444 (0.7357)  Acc@1: 83.3333 (81.4394)  Acc@5: 100.0000 (96.5909)  Acc@task: 83.3333 (81.8182)  time: 0.2111  data: 0.0308  max mem: 1379\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.6428 (0.6747)  Acc@1: 83.3333 (82.7381)  Acc@5: 95.8333 (97.0238)  Acc@task: 83.3333 (82.7381)  time: 0.1816  data: 0.0024  max mem: 1379\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.5298 (0.6288)  Acc@1: 83.3333 (82.5269)  Acc@5: 100.0000 (97.4462)  Acc@task: 83.3333 (82.1237)  time: 0.1800  data: 0.0005  max mem: 1379\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.5395 (0.6342)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (97.1545)  Acc@task: 83.3333 (82.1138)  time: 0.1808  data: 0.0004  max mem: 1379\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.4915 (0.6284)  Acc@1: 83.3333 (83.4000)  Acc@5: 100.0000 (97.2000)  Acc@task: 83.3333 (81.9000)  time: 0.1772  data: 0.0004  max mem: 1379\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1895 s / it)\n",
            "* Acc@task 81.900 Acc@1 83.400 Acc@5 97.200 loss 0.628\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:18  Loss: 0.8528 (0.8528)  Acc@1: 79.1667 (79.1667)  Acc@5: 87.5000 (87.5000)  Acc@task: 66.6667 (66.6667)  time: 0.4370  data: 0.2653  max mem: 1379\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:06  Loss: 0.6919 (0.6183)  Acc@1: 83.3333 (82.1970)  Acc@5: 95.8333 (95.8333)  Acc@task: 75.0000 (75.7576)  time: 0.2178  data: 0.0402  max mem: 1379\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.5482 (0.6016)  Acc@1: 83.3333 (82.9365)  Acc@5: 95.8333 (96.2302)  Acc@task: 79.1667 (77.7778)  time: 0.1886  data: 0.0097  max mem: 1379\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.5762 (0.5870)  Acc@1: 83.3333 (84.2742)  Acc@5: 95.8333 (96.5054)  Acc@task: 79.1667 (78.7634)  time: 0.1809  data: 0.0012  max mem: 1379\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.6122 (0.6415)  Acc@1: 83.3333 (83.8415)  Acc@5: 95.8333 (95.9350)  Acc@task: 79.1667 (78.8618)  time: 0.1804  data: 0.0004  max mem: 1379\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.6080 (0.6343)  Acc@1: 83.3333 (84.0000)  Acc@5: 95.8333 (96.0000)  Acc@task: 79.1667 (79.1000)  time: 0.1776  data: 0.0003  max mem: 1379\n",
            "Test: [Task 4] Total time: 0:00:08 (0.1912 s / it)\n",
            "* Acc@task 79.100 Acc@1 84.000 Acc@5 96.000 loss 0.634\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:23  Loss: 0.4052 (0.4052)  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.5535  data: 0.3721  max mem: 1379\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 0.4602 (0.5565)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (98.4848)  Acc@task: 75.0000 (79.1667)  time: 0.2138  data: 0.0343  max mem: 1379\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.5132 (0.6126)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (97.4206)  Acc@task: 75.0000 (78.1746)  time: 0.1788  data: 0.0005  max mem: 1379\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.5528 (0.6011)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (97.1774)  Acc@task: 83.3333 (80.1075)  time: 0.1786  data: 0.0004  max mem: 1379\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.6012 (0.6355)  Acc@1: 83.3333 (83.2317)  Acc@5: 95.8333 (96.5447)  Acc@task: 79.1667 (79.0650)  time: 0.1797  data: 0.0003  max mem: 1379\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.6585 (0.6564)  Acc@1: 83.3333 (82.8000)  Acc@5: 95.8333 (96.4000)  Acc@task: 79.1667 (79.0000)  time: 0.1762  data: 0.0003  max mem: 1379\n",
            "Test: [Task 5] Total time: 0:00:07 (0.1905 s / it)\n",
            "* Acc@task 79.000 Acc@1 82.800 Acc@5 96.400 loss 0.656\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:37  Loss: 0.6491 (0.6491)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (83.3333)  time: 0.8980  data: 0.7046  max mem: 1379\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:07  Loss: 0.7482 (0.8113)  Acc@1: 83.3333 (79.5455)  Acc@5: 95.8333 (97.3485)  Acc@task: 79.1667 (79.9242)  time: 0.2438  data: 0.0657  max mem: 1379\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:04  Loss: 0.7574 (0.8576)  Acc@1: 79.1667 (78.1746)  Acc@5: 95.8333 (97.2222)  Acc@task: 79.1667 (78.9683)  time: 0.1779  data: 0.0011  max mem: 1379\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 0.9309 (0.8763)  Acc@1: 75.0000 (76.8817)  Acc@5: 95.8333 (96.7742)  Acc@task: 79.1667 (78.4946)  time: 0.1779  data: 0.0004  max mem: 1379\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 0.8989 (0.8786)  Acc@1: 75.0000 (76.7276)  Acc@5: 95.8333 (96.2398)  Acc@task: 79.1667 (78.2520)  time: 0.1790  data: 0.0003  max mem: 1379\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 0.8216 (0.8739)  Acc@1: 75.0000 (76.7000)  Acc@5: 95.8333 (96.2000)  Acc@task: 79.1667 (78.3000)  time: 0.1757  data: 0.0003  max mem: 1379\n",
            "Test: [Task 6] Total time: 0:00:08 (0.1964 s / it)\n",
            "* Acc@task 78.300 Acc@1 76.700 Acc@5 96.200 loss 0.874\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:25  Loss: 0.8840 (0.8840)  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  Acc@task: 70.8333 (70.8333)  time: 0.5961  data: 0.4096  max mem: 1379\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:06  Loss: 0.6702 (0.6934)  Acc@1: 83.3333 (81.0606)  Acc@5: 100.0000 (97.3485)  Acc@task: 70.8333 (72.3485)  time: 0.2143  data: 0.0377  max mem: 1379\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:04  Loss: 0.7624 (0.7905)  Acc@1: 79.1667 (78.1746)  Acc@5: 100.0000 (97.2222)  Acc@task: 75.0000 (73.2143)  time: 0.1768  data: 0.0005  max mem: 1379\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:02  Loss: 0.7809 (0.8287)  Acc@1: 75.0000 (77.9570)  Acc@5: 95.8333 (95.6989)  Acc@task: 70.8333 (72.1774)  time: 0.1782  data: 0.0004  max mem: 1379\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 0.8060 (0.8420)  Acc@1: 75.0000 (77.5407)  Acc@5: 95.8333 (95.5285)  Acc@task: 70.8333 (72.0528)  time: 0.1789  data: 0.0006  max mem: 1379\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 0.8060 (0.8361)  Acc@1: 79.1667 (77.6000)  Acc@5: 95.8333 (95.5000)  Acc@task: 70.8333 (72.2000)  time: 0.1756  data: 0.0006  max mem: 1379\n",
            "Test: [Task 7] Total time: 0:00:07 (0.1885 s / it)\n",
            "* Acc@task 72.200 Acc@1 77.600 Acc@5 95.500 loss 0.836\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:23  Loss: 6.6452 (6.6452)  Acc@1: 0.0000 (0.0000)  Acc@5: 37.5000 (37.5000)  Acc@task: 66.6667 (66.6667)  time: 0.5627  data: 0.3859  max mem: 1379\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:06  Loss: 6.8203 (6.8377)  Acc@1: 0.0000 (0.7576)  Acc@5: 25.0000 (24.2424)  Acc@task: 66.6667 (66.2879)  time: 0.2116  data: 0.0354  max mem: 1379\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:04  Loss: 6.8203 (6.7815)  Acc@1: 0.0000 (1.3889)  Acc@5: 25.0000 (23.6111)  Acc@task: 66.6667 (65.4762)  time: 0.1769  data: 0.0004  max mem: 1379\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:02  Loss: 6.5602 (6.7859)  Acc@1: 0.0000 (1.6129)  Acc@5: 25.0000 (23.6559)  Acc@task: 62.5000 (64.7849)  time: 0.1781  data: 0.0004  max mem: 1379\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 6.8447 (6.7863)  Acc@1: 4.1667 (2.0325)  Acc@5: 25.0000 (24.5935)  Acc@task: 62.5000 (63.6179)  time: 0.1785  data: 0.0003  max mem: 1379\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 6.8971 (6.7890)  Acc@1: 4.1667 (2.1000)  Acc@5: 25.0000 (24.5000)  Acc@task: 62.5000 (63.7000)  time: 0.1750  data: 0.0003  max mem: 1379\n",
            "Test: [Task 8] Total time: 0:00:07 (0.1875 s / it)\n",
            "* Acc@task 63.700 Acc@1 2.100 Acc@5 24.500 loss 6.789\n",
            "[Average accuracy till task8]\tAcc@task: 76.6500\tAcc@1: 70.9500\tAcc@5: 87.4000\tLoss: 1.4974\tForgetting: 1.5000\tBackward: 66.4429\n",
            "torch.Size([96000, 384])\n",
            "Averaged stats: Lr: 0.005000  Loss: 0.1528  Acc@1: 96.6667 (94.0000)  Acc@5: 100.0000 (99.1071)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:29  Loss: 0.4334 (0.4334)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  Acc@task: 79.1667 (79.1667)  time: 0.6950  data: 0.4966  max mem: 1379\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:07  Loss: 0.6330 (0.6986)  Acc@1: 83.3333 (81.4394)  Acc@5: 100.0000 (97.3485)  Acc@task: 79.1667 (79.5455)  time: 0.2237  data: 0.0471  max mem: 1379\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.6330 (0.7348)  Acc@1: 83.3333 (82.7381)  Acc@5: 95.8333 (96.0317)  Acc@task: 83.3333 (81.5476)  time: 0.1764  data: 0.0018  max mem: 1379\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.4907 (0.6664)  Acc@1: 83.3333 (83.7366)  Acc@5: 95.8333 (96.6398)  Acc@task: 83.3333 (81.5860)  time: 0.1770  data: 0.0009  max mem: 1379\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.3441 (0.6260)  Acc@1: 87.5000 (84.7561)  Acc@5: 100.0000 (97.1545)  Acc@task: 83.3333 (82.5203)  time: 0.1785  data: 0.0004  max mem: 1379\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.3441 (0.6203)  Acc@1: 87.5000 (84.9000)  Acc@5: 100.0000 (97.2000)  Acc@task: 83.3333 (82.8000)  time: 0.1750  data: 0.0004  max mem: 1379\n",
            "Test: [Task 1] Total time: 0:00:08 (0.1906 s / it)\n",
            "* Acc@task 82.800 Acc@1 84.900 Acc@5 97.200 loss 0.620\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:25  Loss: 1.2563 (1.2563)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  Acc@task: 79.1667 (79.1667)  time: 0.6169  data: 0.4339  max mem: 1379\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.8261 (0.9461)  Acc@1: 79.1667 (77.6515)  Acc@5: 95.8333 (96.5909)  Acc@task: 70.8333 (74.2424)  time: 0.2159  data: 0.0398  max mem: 1379\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.8261 (0.9952)  Acc@1: 79.1667 (76.1905)  Acc@5: 95.8333 (96.0317)  Acc@task: 70.8333 (74.8016)  time: 0.1762  data: 0.0004  max mem: 1379\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.8967 (0.9693)  Acc@1: 75.0000 (76.3441)  Acc@5: 95.8333 (95.9677)  Acc@task: 75.0000 (75.4032)  time: 0.1777  data: 0.0004  max mem: 1379\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.7251 (0.8898)  Acc@1: 79.1667 (77.5407)  Acc@5: 100.0000 (96.4431)  Acc@task: 75.0000 (76.0163)  time: 0.1787  data: 0.0003  max mem: 1379\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.7251 (0.8912)  Acc@1: 79.1667 (77.5000)  Acc@5: 100.0000 (96.5000)  Acc@task: 75.0000 (76.2000)  time: 0.1751  data: 0.0003  max mem: 1379\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1896 s / it)\n",
            "* Acc@task 76.200 Acc@1 77.500 Acc@5 96.500 loss 0.891\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:38  Loss: 0.4141 (0.4141)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.9119  data: 0.7044  max mem: 1379\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:07  Loss: 0.6865 (0.7315)  Acc@1: 83.3333 (81.8182)  Acc@5: 100.0000 (98.1061)  Acc@task: 83.3333 (81.8182)  time: 0.2449  data: 0.0651  max mem: 1379\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.6222 (0.6560)  Acc@1: 83.3333 (83.7302)  Acc@5: 100.0000 (97.4206)  Acc@task: 83.3333 (82.7381)  time: 0.1775  data: 0.0008  max mem: 1379\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.5440 (0.6168)  Acc@1: 83.3333 (84.0054)  Acc@5: 95.8333 (97.1774)  Acc@task: 83.3333 (82.1237)  time: 0.1781  data: 0.0004  max mem: 1379\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.5718 (0.6344)  Acc@1: 83.3333 (84.3496)  Acc@5: 95.8333 (96.6463)  Acc@task: 83.3333 (82.1138)  time: 0.1793  data: 0.0003  max mem: 1379\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.5488 (0.6267)  Acc@1: 83.3333 (84.5000)  Acc@5: 95.8333 (96.7000)  Acc@task: 83.3333 (81.9000)  time: 0.1755  data: 0.0003  max mem: 1379\n",
            "Test: [Task 3] Total time: 0:00:08 (0.1970 s / it)\n",
            "* Acc@task 81.900 Acc@1 84.500 Acc@5 96.700 loss 0.627\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:24  Loss: 0.8838 (0.8838)  Acc@1: 75.0000 (75.0000)  Acc@5: 87.5000 (87.5000)  Acc@task: 66.6667 (66.6667)  time: 0.5949  data: 0.4040  max mem: 1379\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:06  Loss: 0.7346 (0.7145)  Acc@1: 83.3333 (81.8182)  Acc@5: 95.8333 (95.0758)  Acc@task: 75.0000 (75.7576)  time: 0.2164  data: 0.0375  max mem: 1379\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.5199 (0.6772)  Acc@1: 83.3333 (81.9444)  Acc@5: 95.8333 (96.0317)  Acc@task: 79.1667 (77.7778)  time: 0.1780  data: 0.0007  max mem: 1379\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.4744 (0.6475)  Acc@1: 83.3333 (82.7957)  Acc@5: 95.8333 (96.3710)  Acc@task: 79.1667 (78.7634)  time: 0.1783  data: 0.0006  max mem: 1379\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.6802 (0.7193)  Acc@1: 83.3333 (82.1138)  Acc@5: 95.8333 (95.6301)  Acc@task: 79.1667 (78.8618)  time: 0.1793  data: 0.0006  max mem: 1379\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.6171 (0.7097)  Acc@1: 83.3333 (82.3000)  Acc@5: 95.8333 (95.6000)  Acc@task: 79.1667 (79.1000)  time: 0.1754  data: 0.0006  max mem: 1379\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1894 s / it)\n",
            "* Acc@task 79.100 Acc@1 82.300 Acc@5 95.600 loss 0.710\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:24  Loss: 0.3688 (0.3688)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.5914  data: 0.4208  max mem: 1379\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 0.4791 (0.5917)  Acc@1: 83.3333 (83.7121)  Acc@5: 100.0000 (98.4848)  Acc@task: 75.0000 (79.1667)  time: 0.2158  data: 0.0387  max mem: 1379\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.5800 (0.6680)  Acc@1: 83.3333 (83.1349)  Acc@5: 95.8333 (97.4206)  Acc@task: 75.0000 (78.1746)  time: 0.1781  data: 0.0004  max mem: 1379\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.7118 (0.6757)  Acc@1: 83.3333 (81.9892)  Acc@5: 95.8333 (97.0430)  Acc@task: 83.3333 (80.1075)  time: 0.1790  data: 0.0004  max mem: 1379\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.7319 (0.7083)  Acc@1: 79.1667 (81.7073)  Acc@5: 95.8333 (96.9512)  Acc@task: 79.1667 (79.0650)  time: 0.1801  data: 0.0003  max mem: 1379\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.7381 (0.7352)  Acc@1: 79.1667 (81.3000)  Acc@5: 95.8333 (96.6000)  Acc@task: 79.1667 (79.0000)  time: 0.1764  data: 0.0003  max mem: 1379\n",
            "Test: [Task 5] Total time: 0:00:07 (0.1900 s / it)\n",
            "* Acc@task 79.000 Acc@1 81.300 Acc@5 96.600 loss 0.735\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:25  Loss: 0.6402 (0.6402)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (83.3333)  time: 0.6063  data: 0.4103  max mem: 1379\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:06  Loss: 0.8701 (0.9959)  Acc@1: 75.0000 (76.5152)  Acc@5: 95.8333 (95.8333)  Acc@task: 79.1667 (79.9242)  time: 0.2173  data: 0.0378  max mem: 1379\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:04  Loss: 0.9628 (1.0587)  Acc@1: 70.8333 (73.0159)  Acc@5: 95.8333 (95.6349)  Acc@task: 79.1667 (78.9683)  time: 0.1783  data: 0.0010  max mem: 1379\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 1.0498 (1.0607)  Acc@1: 66.6667 (72.3118)  Acc@5: 95.8333 (95.8333)  Acc@task: 79.1667 (78.4946)  time: 0.1782  data: 0.0014  max mem: 1379\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.0322 (1.0694)  Acc@1: 70.8333 (71.5447)  Acc@5: 95.8333 (95.3252)  Acc@task: 79.1667 (78.2520)  time: 0.1791  data: 0.0009  max mem: 1379\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.0231 (1.0637)  Acc@1: 70.8333 (71.5000)  Acc@5: 95.8333 (95.3000)  Acc@task: 79.1667 (78.3000)  time: 0.1759  data: 0.0007  max mem: 1379\n",
            "Test: [Task 6] Total time: 0:00:07 (0.1898 s / it)\n",
            "* Acc@task 78.300 Acc@1 71.500 Acc@5 95.300 loss 1.064\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:19  Loss: 0.8268 (0.8268)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  Acc@task: 70.8333 (70.8333)  time: 0.4688  data: 0.2923  max mem: 1379\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:06  Loss: 0.5977 (0.6998)  Acc@1: 79.1667 (79.9242)  Acc@5: 100.0000 (96.9697)  Acc@task: 70.8333 (72.3485)  time: 0.2076  data: 0.0297  max mem: 1379\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:04  Loss: 0.6790 (0.8044)  Acc@1: 79.1667 (77.5794)  Acc@5: 95.8333 (96.4286)  Acc@task: 75.0000 (73.2143)  time: 0.1796  data: 0.0019  max mem: 1379\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:02  Loss: 0.8666 (0.8492)  Acc@1: 75.0000 (77.8226)  Acc@5: 95.8333 (95.5645)  Acc@task: 70.8333 (72.1774)  time: 0.1786  data: 0.0005  max mem: 1379\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 0.9620 (0.8682)  Acc@1: 75.0000 (76.8293)  Acc@5: 95.8333 (95.5285)  Acc@task: 70.8333 (72.0528)  time: 0.1792  data: 0.0004  max mem: 1379\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 0.9620 (0.8586)  Acc@1: 75.0000 (76.9000)  Acc@5: 95.8333 (95.6000)  Acc@task: 70.8333 (72.2000)  time: 0.1759  data: 0.0004  max mem: 1379\n",
            "Test: [Task 7] Total time: 0:00:07 (0.1872 s / it)\n",
            "* Acc@task 72.200 Acc@1 76.900 Acc@5 95.600 loss 0.859\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:32  Loss: 1.1180 (1.1180)  Acc@1: 75.0000 (75.0000)  Acc@5: 87.5000 (87.5000)  Acc@task: 66.6667 (66.6667)  time: 0.7679  data: 0.5500  max mem: 1379\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:07  Loss: 0.8311 (0.9981)  Acc@1: 75.0000 (73.4849)  Acc@5: 91.6667 (92.8030)  Acc@task: 66.6667 (66.2879)  time: 0.2327  data: 0.0515  max mem: 1379\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:04  Loss: 0.7881 (0.9236)  Acc@1: 75.0000 (74.4048)  Acc@5: 95.8333 (94.6429)  Acc@task: 66.6667 (65.4762)  time: 0.1786  data: 0.0014  max mem: 1379\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:02  Loss: 0.7210 (0.9023)  Acc@1: 75.0000 (74.8656)  Acc@5: 95.8333 (94.6237)  Acc@task: 62.5000 (64.7849)  time: 0.1785  data: 0.0009  max mem: 1379\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 0.9611 (0.9331)  Acc@1: 75.0000 (74.3902)  Acc@5: 95.8333 (94.4106)  Acc@task: 62.5000 (63.6179)  time: 0.1794  data: 0.0004  max mem: 1379\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 0.9712 (0.9463)  Acc@1: 75.0000 (74.3000)  Acc@5: 95.8333 (94.5000)  Acc@task: 62.5000 (63.7000)  time: 0.1759  data: 0.0004  max mem: 1379\n",
            "Test: [Task 8] Total time: 0:00:08 (0.1940 s / it)\n",
            "* Acc@task 63.700 Acc@1 74.300 Acc@5 94.500 loss 0.946\n",
            "[Average accuracy till task8]\tAcc@task: 76.6500\tAcc@1: 79.1500\tAcc@5: 96.0000\tLoss: 0.8065\tForgetting: 2.1857\tBackward: 4.5429\n",
            "Loading checkpoint from: ./output/cifar100_full_dino_1epoch_50pct/checkpoint/task9_checkpoint.pth\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/engines/hide_promtp_wtp_and_tap_engine.py:540: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  loss = torch.nn.functional.cross_entropy(sim, torch.range(0, sim.shape[0] - 1).long().to(device))\n",
            "Train: Epoch[1/1]  [  0/103]  eta: 0:01:22  Lr: 0.002812  Loss: 3.6780  Acc@1: 16.6667 (16.6667)  Acc@5: 54.1667 (54.1667)  time: 0.8037  data: 0.5136  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 10/103]  eta: 0:00:30  Lr: 0.002812  Loss: 2.4517  Acc@1: 16.6667 (14.0152)  Acc@5: 58.3333 (58.3333)  time: 0.3290  data: 0.0473  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 20/103]  eta: 0:00:25  Lr: 0.002812  Loss: 2.3103  Acc@1: 16.6667 (19.8413)  Acc@5: 66.6667 (64.6825)  time: 0.2814  data: 0.0007  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 30/103]  eta: 0:00:21  Lr: 0.002812  Loss: 1.5523  Acc@1: 33.3333 (25.5376)  Acc@5: 79.1667 (70.8333)  time: 0.2819  data: 0.0010  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 40/103]  eta: 0:00:18  Lr: 0.002812  Loss: 1.6308  Acc@1: 45.8333 (32.0122)  Acc@5: 87.5000 (75.1016)  time: 0.2824  data: 0.0008  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 50/103]  eta: 0:00:15  Lr: 0.002812  Loss: 1.8409  Acc@1: 54.1667 (36.1928)  Acc@5: 91.6667 (78.3497)  time: 0.2819  data: 0.0005  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 60/103]  eta: 0:00:12  Lr: 0.002812  Loss: 1.3343  Acc@1: 54.1667 (39.8907)  Acc@5: 91.6667 (80.8060)  time: 0.2816  data: 0.0005  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 70/103]  eta: 0:00:09  Lr: 0.002812  Loss: 1.3301  Acc@1: 62.5000 (43.7793)  Acc@5: 91.6667 (82.5704)  time: 0.2825  data: 0.0005  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 80/103]  eta: 0:00:06  Lr: 0.002812  Loss: 0.8352  Acc@1: 70.8333 (47.2737)  Acc@5: 91.6667 (84.1564)  time: 0.2832  data: 0.0007  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 90/103]  eta: 0:00:03  Lr: 0.002812  Loss: 0.8796  Acc@1: 70.8333 (50.0000)  Acc@5: 95.8333 (85.4396)  time: 0.2825  data: 0.0007  max mem: 1379\n",
            "Train: Epoch[1/1]  [100/103]  eta: 0:00:00  Lr: 0.002812  Loss: 1.0334  Acc@1: 70.8333 (52.1040)  Acc@5: 95.8333 (86.5924)  time: 0.2822  data: 0.0004  max mem: 1379\n",
            "Train: Epoch[1/1]  [102/103]  eta: 0:00:00  Lr: 0.002812  Loss: 0.8903  Acc@1: 70.8333 (52.2431)  Acc@5: 95.8333 (86.7455)  time: 0.2716  data: 0.0003  max mem: 1379\n",
            "Train: Epoch[1/1] Total time: 0:00:29 (0.2867 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.8903  Acc@1: 70.8333 (52.2431)  Acc@5: 95.8333 (86.7455)\n",
            "torch.Size([5, 2, 5, 6, 64])\n",
            "torch.Size([5, 2, 1, 5, 6, 64])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:21  Loss: 0.5103 (0.5103)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 79.1667 (79.1667)  time: 0.5097  data: 0.3285  max mem: 1379\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.6331 (0.6635)  Acc@1: 83.3333 (81.0606)  Acc@5: 100.0000 (97.3485)  Acc@task: 79.1667 (79.1667)  time: 0.2071  data: 0.0305  max mem: 1379\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.6460 (0.7122)  Acc@1: 83.3333 (82.1429)  Acc@5: 95.8333 (96.2302)  Acc@task: 79.1667 (80.7540)  time: 0.1770  data: 0.0005  max mem: 1379\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.5358 (0.6538)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (96.7742)  Acc@task: 79.1667 (80.6452)  time: 0.1778  data: 0.0007  max mem: 1379\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.3446 (0.6204)  Acc@1: 87.5000 (84.2480)  Acc@5: 100.0000 (97.2561)  Acc@task: 83.3333 (81.4024)  time: 0.1788  data: 0.0007  max mem: 1379\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.3446 (0.6149)  Acc@1: 87.5000 (84.4000)  Acc@5: 100.0000 (97.3000)  Acc@task: 83.3333 (81.7000)  time: 0.1757  data: 0.0007  max mem: 1379\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1869 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@task 81.700 Acc@1 84.400 Acc@5 97.300 loss 0.615\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:23  Loss: 1.3044 (1.3044)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  Acc@task: 75.0000 (75.0000)  time: 0.5516  data: 0.3890  max mem: 1379\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.8503 (0.9627)  Acc@1: 79.1667 (77.2727)  Acc@5: 95.8333 (96.5909)  Acc@task: 70.8333 (73.8636)  time: 0.2125  data: 0.0360  max mem: 1379\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.8503 (1.0251)  Acc@1: 79.1667 (76.1905)  Acc@5: 95.8333 (96.0317)  Acc@task: 70.8333 (73.0159)  time: 0.1784  data: 0.0006  max mem: 1379\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.8839 (0.9926)  Acc@1: 75.0000 (76.2097)  Acc@5: 95.8333 (95.9677)  Acc@task: 70.8333 (73.3871)  time: 0.1792  data: 0.0004  max mem: 1379\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.7311 (0.9093)  Acc@1: 79.1667 (77.4390)  Acc@5: 100.0000 (96.4431)  Acc@task: 75.0000 (74.2886)  time: 0.1803  data: 0.0003  max mem: 1379\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.7311 (0.9103)  Acc@1: 79.1667 (77.4000)  Acc@5: 100.0000 (96.5000)  Acc@task: 75.0000 (74.5000)  time: 0.1769  data: 0.0003  max mem: 1379\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1888 s / it)\n",
            "* Acc@task 74.500 Acc@1 77.400 Acc@5 96.500 loss 0.910\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:18  Loss: 0.4136 (0.4136)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 79.1667 (79.1667)  time: 0.4509  data: 0.2819  max mem: 1379\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 0.6372 (0.7109)  Acc@1: 83.3333 (82.1970)  Acc@5: 100.0000 (98.1061)  Acc@task: 79.1667 (82.5758)  time: 0.2051  data: 0.0264  max mem: 1379\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.5893 (0.6455)  Acc@1: 83.3333 (83.7302)  Acc@5: 100.0000 (97.4206)  Acc@task: 83.3333 (82.3413)  time: 0.1802  data: 0.0012  max mem: 1379\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.5424 (0.6095)  Acc@1: 83.3333 (84.0054)  Acc@5: 95.8333 (97.1774)  Acc@task: 83.3333 (81.8548)  time: 0.1800  data: 0.0014  max mem: 1379\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.5725 (0.6284)  Acc@1: 83.3333 (84.3496)  Acc@5: 95.8333 (96.6463)  Acc@task: 83.3333 (81.8089)  time: 0.1809  data: 0.0008  max mem: 1379\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.5488 (0.6208)  Acc@1: 83.3333 (84.5000)  Acc@5: 95.8333 (96.7000)  Acc@task: 83.3333 (81.6000)  time: 0.1776  data: 0.0008  max mem: 1379\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1879 s / it)\n",
            "* Acc@task 81.600 Acc@1 84.500 Acc@5 96.700 loss 0.621\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:20  Loss: 0.7538 (0.7538)  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  Acc@task: 62.5000 (62.5000)  time: 0.4952  data: 0.3291  max mem: 1379\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:06  Loss: 0.7338 (0.7015)  Acc@1: 83.3333 (81.8182)  Acc@5: 95.8333 (95.4545)  Acc@task: 75.0000 (75.0000)  time: 0.2095  data: 0.0306  max mem: 1379\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.5212 (0.6765)  Acc@1: 83.3333 (81.3492)  Acc@5: 95.8333 (96.0317)  Acc@task: 75.0000 (77.1825)  time: 0.1800  data: 0.0006  max mem: 1379\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.4740 (0.6358)  Acc@1: 83.3333 (82.5269)  Acc@5: 95.8333 (96.5054)  Acc@task: 79.1667 (78.0914)  time: 0.1798  data: 0.0005  max mem: 1379\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.6471 (0.7001)  Acc@1: 83.3333 (82.1138)  Acc@5: 95.8333 (95.7317)  Acc@task: 83.3333 (78.6585)  time: 0.1810  data: 0.0004  max mem: 1379\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.5292 (0.6910)  Acc@1: 83.3333 (82.3000)  Acc@5: 95.8333 (95.7000)  Acc@task: 83.3333 (78.9000)  time: 0.1774  data: 0.0004  max mem: 1379\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1896 s / it)\n",
            "* Acc@task 78.900 Acc@1 82.300 Acc@5 95.700 loss 0.691\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:37  Loss: 0.4094 (0.4094)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.8838  data: 0.6545  max mem: 1379\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:07  Loss: 0.4791 (0.5903)  Acc@1: 83.3333 (84.0909)  Acc@5: 100.0000 (98.4848)  Acc@task: 75.0000 (78.0303)  time: 0.2433  data: 0.0608  max mem: 1379\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.5783 (0.6617)  Acc@1: 83.3333 (83.7302)  Acc@5: 95.8333 (97.4206)  Acc@task: 75.0000 (77.5794)  time: 0.1794  data: 0.0013  max mem: 1379\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.7117 (0.6794)  Acc@1: 83.3333 (82.5269)  Acc@5: 95.8333 (96.9086)  Acc@task: 79.1667 (78.4946)  time: 0.1796  data: 0.0007  max mem: 1379\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.7392 (0.7103)  Acc@1: 79.1667 (82.0122)  Acc@5: 95.8333 (96.8496)  Acc@task: 79.1667 (77.1341)  time: 0.1798  data: 0.0003  max mem: 1379\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.7713 (0.7371)  Acc@1: 79.1667 (81.6000)  Acc@5: 95.8333 (96.5000)  Acc@task: 75.0000 (77.1000)  time: 0.1764  data: 0.0003  max mem: 1379\n",
            "Test: [Task 5] Total time: 0:00:08 (0.1979 s / it)\n",
            "* Acc@task 77.100 Acc@1 81.600 Acc@5 96.500 loss 0.737\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:25  Loss: 0.6386 (0.6386)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  Acc@task: 87.5000 (87.5000)  time: 0.6105  data: 0.4408  max mem: 1379\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:06  Loss: 0.8638 (0.9819)  Acc@1: 79.1667 (77.2727)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (81.4394)  time: 0.2173  data: 0.0404  max mem: 1379\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:04  Loss: 0.9399 (1.0470)  Acc@1: 75.0000 (73.8095)  Acc@5: 95.8333 (95.8333)  Acc@task: 79.1667 (80.5556)  time: 0.1780  data: 0.0004  max mem: 1379\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 1.0515 (1.0507)  Acc@1: 70.8333 (72.8495)  Acc@5: 95.8333 (95.9677)  Acc@task: 79.1667 (79.1667)  time: 0.1786  data: 0.0005  max mem: 1379\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.0668 (1.0638)  Acc@1: 70.8333 (72.0528)  Acc@5: 95.8333 (95.5285)  Acc@task: 75.0000 (77.8455)  time: 0.1792  data: 0.0005  max mem: 1379\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.0226 (1.0578)  Acc@1: 70.8333 (72.0000)  Acc@5: 95.8333 (95.5000)  Acc@task: 79.1667 (78.0000)  time: 0.1758  data: 0.0005  max mem: 1379\n",
            "Test: [Task 6] Total time: 0:00:08 (0.1913 s / it)\n",
            "* Acc@task 78.000 Acc@1 72.000 Acc@5 95.500 loss 1.058\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:34  Loss: 0.7200 (0.7200)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  Acc@task: 70.8333 (70.8333)  time: 0.8151  data: 0.6270  max mem: 1379\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:07  Loss: 0.5875 (0.6832)  Acc@1: 79.1667 (80.3030)  Acc@5: 100.0000 (96.9697)  Acc@task: 70.8333 (72.7273)  time: 0.2352  data: 0.0577  max mem: 1379\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:04  Loss: 0.6961 (0.7814)  Acc@1: 79.1667 (78.1746)  Acc@5: 95.8333 (96.8254)  Acc@task: 70.8333 (72.6191)  time: 0.1770  data: 0.0006  max mem: 1379\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:02  Loss: 0.8543 (0.8129)  Acc@1: 75.0000 (78.6290)  Acc@5: 95.8333 (95.9677)  Acc@task: 70.8333 (72.7151)  time: 0.1777  data: 0.0005  max mem: 1379\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 0.8871 (0.8380)  Acc@1: 75.0000 (77.5407)  Acc@5: 95.8333 (95.8333)  Acc@task: 75.0000 (73.0691)  time: 0.1787  data: 0.0004  max mem: 1379\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 0.8871 (0.8308)  Acc@1: 75.0000 (77.6000)  Acc@5: 95.8333 (95.9000)  Acc@task: 75.0000 (73.2000)  time: 0.1753  data: 0.0004  max mem: 1379\n",
            "Test: [Task 7] Total time: 0:00:08 (0.1937 s / it)\n",
            "* Acc@task 73.200 Acc@1 77.600 Acc@5 95.900 loss 0.831\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:18  Loss: 1.1139 (1.1139)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  Acc@task: 70.8333 (70.8333)  time: 0.4417  data: 0.2719  max mem: 1379\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:06  Loss: 0.8211 (0.9341)  Acc@1: 79.1667 (75.0000)  Acc@5: 91.6667 (93.5606)  Acc@task: 75.0000 (72.3485)  time: 0.2023  data: 0.0252  max mem: 1379\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:04  Loss: 0.6954 (0.8799)  Acc@1: 75.0000 (75.3968)  Acc@5: 95.8333 (95.2381)  Acc@task: 75.0000 (70.8333)  time: 0.1775  data: 0.0007  max mem: 1379\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:02  Loss: 0.6954 (0.8626)  Acc@1: 75.0000 (75.6720)  Acc@5: 95.8333 (95.0269)  Acc@task: 66.6667 (71.1022)  time: 0.1775  data: 0.0009  max mem: 1379\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 0.8226 (0.8677)  Acc@1: 75.0000 (75.6098)  Acc@5: 95.8333 (95.0203)  Acc@task: 70.8333 (70.6301)  time: 0.1786  data: 0.0007  max mem: 1379\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 0.8910 (0.8683)  Acc@1: 75.0000 (75.7000)  Acc@5: 95.8333 (95.1000)  Acc@task: 68.7500 (70.6000)  time: 0.1753  data: 0.0007  max mem: 1379\n",
            "Test: [Task 8] Total time: 0:00:07 (0.1853 s / it)\n",
            "* Acc@task 70.600 Acc@1 75.700 Acc@5 95.100 loss 0.868\n",
            "Test: [Task 9]  [ 0/42]  eta: 0:00:24  Loss: 6.6275 (6.6275)  Acc@1: 0.0000 (0.0000)  Acc@5: 50.0000 (50.0000)  Acc@task: 62.5000 (62.5000)  time: 0.5884  data: 0.4150  max mem: 1379\n",
            "Test: [Task 9]  [10/42]  eta: 0:00:06  Loss: 6.8177 (6.6320)  Acc@1: 0.0000 (2.2727)  Acc@5: 45.8333 (45.0758)  Acc@task: 70.8333 (70.8333)  time: 0.2159  data: 0.0380  max mem: 1379\n",
            "Test: [Task 9]  [20/42]  eta: 0:00:04  Loss: 6.5866 (6.4656)  Acc@1: 0.0000 (2.1825)  Acc@5: 41.6667 (45.0397)  Acc@task: 70.8333 (70.6349)  time: 0.1774  data: 0.0003  max mem: 1379\n",
            "Test: [Task 9]  [30/42]  eta: 0:00:02  Loss: 6.2652 (6.5014)  Acc@1: 0.0000 (2.1505)  Acc@5: 41.6667 (42.6075)  Acc@task: 66.6667 (70.0269)  time: 0.1772  data: 0.0004  max mem: 1379\n",
            "Test: [Task 9]  [40/42]  eta: 0:00:00  Loss: 6.4877 (6.5112)  Acc@1: 0.0000 (1.8293)  Acc@5: 41.6667 (42.5813)  Acc@task: 70.8333 (70.7317)  time: 0.1785  data: 0.0003  max mem: 1379\n",
            "Test: [Task 9]  [41/42]  eta: 0:00:00  Loss: 6.4877 (6.5049)  Acc@1: 0.0000 (1.8000)  Acc@5: 41.6667 (43.0000)  Acc@task: 70.8333 (71.0000)  time: 0.1749  data: 0.0003  max mem: 1379\n",
            "Test: [Task 9] Total time: 0:00:07 (0.1887 s / it)\n",
            "* Acc@task 71.000 Acc@1 1.800 Acc@5 43.000 loss 6.505\n",
            "[Average accuracy till task9]\tAcc@task: 76.2889\tAcc@1: 70.8111\tAcc@5: 90.2444\tLoss: 1.4262\tForgetting: 2.0250\tBackward: 66.6250\n",
            "torch.Size([108000, 384])\n",
            "Averaged stats: Lr: 0.005000  Loss: 0.1234  Acc@1: 97.5000 (95.1458)  Acc@5: 100.0000 (99.5729)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:39  Loss: 0.6327 (0.6327)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 79.1667 (79.1667)  time: 0.9292  data: 0.7118  max mem: 1379\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:07  Loss: 0.6392 (0.6298)  Acc@1: 83.3333 (82.9545)  Acc@5: 100.0000 (97.3485)  Acc@task: 79.1667 (79.1667)  time: 0.2464  data: 0.0660  max mem: 1379\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.6392 (0.6895)  Acc@1: 83.3333 (83.1349)  Acc@5: 95.8333 (96.0317)  Acc@task: 79.1667 (80.7540)  time: 0.1777  data: 0.0009  max mem: 1379\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.4561 (0.6398)  Acc@1: 83.3333 (84.2742)  Acc@5: 95.8333 (96.2366)  Acc@task: 79.1667 (80.6452)  time: 0.1772  data: 0.0004  max mem: 1379\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.3574 (0.6099)  Acc@1: 91.6667 (85.3659)  Acc@5: 100.0000 (96.6463)  Acc@task: 83.3333 (81.4024)  time: 0.1774  data: 0.0003  max mem: 1379\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.3411 (0.6008)  Acc@1: 91.6667 (85.5000)  Acc@5: 100.0000 (96.7000)  Acc@task: 83.3333 (81.7000)  time: 0.1740  data: 0.0003  max mem: 1379\n",
            "Test: [Task 1] Total time: 0:00:08 (0.1964 s / it)\n",
            "* Acc@task 81.700 Acc@1 85.500 Acc@5 96.700 loss 0.601\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:27  Loss: 1.2377 (1.2377)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  Acc@task: 75.0000 (75.0000)  time: 0.6552  data: 0.4776  max mem: 1379\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.9561 (0.9649)  Acc@1: 75.0000 (77.6515)  Acc@5: 95.8333 (96.2121)  Acc@task: 70.8333 (73.8636)  time: 0.2183  data: 0.0437  max mem: 1379\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.8973 (1.0069)  Acc@1: 79.1667 (76.5873)  Acc@5: 95.8333 (95.4365)  Acc@task: 70.8333 (73.0159)  time: 0.1764  data: 0.0003  max mem: 1379\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.9764 (0.9839)  Acc@1: 79.1667 (76.8817)  Acc@5: 95.8333 (95.2957)  Acc@task: 70.8333 (73.3871)  time: 0.1782  data: 0.0004  max mem: 1379\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.7019 (0.9002)  Acc@1: 79.1667 (77.8455)  Acc@5: 95.8333 (95.8333)  Acc@task: 75.0000 (74.2886)  time: 0.1786  data: 0.0006  max mem: 1379\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.7019 (0.8981)  Acc@1: 79.1667 (77.7000)  Acc@5: 95.8333 (95.8000)  Acc@task: 75.0000 (74.5000)  time: 0.1756  data: 0.0006  max mem: 1379\n",
            "Test: [Task 2] Total time: 0:00:08 (0.1908 s / it)\n",
            "* Acc@task 74.500 Acc@1 77.700 Acc@5 95.800 loss 0.898\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:26  Loss: 0.5369 (0.5369)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  Acc@task: 79.1667 (79.1667)  time: 0.6289  data: 0.4593  max mem: 1379\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:07  Loss: 0.7050 (0.7449)  Acc@1: 83.3333 (82.9545)  Acc@5: 100.0000 (96.9697)  Acc@task: 79.1667 (82.5758)  time: 0.2189  data: 0.0422  max mem: 1379\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.6438 (0.6862)  Acc@1: 83.3333 (83.9286)  Acc@5: 95.8333 (96.8254)  Acc@task: 83.3333 (82.3413)  time: 0.1771  data: 0.0004  max mem: 1379\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.4991 (0.6533)  Acc@1: 83.3333 (84.0054)  Acc@5: 95.8333 (96.9086)  Acc@task: 83.3333 (81.8548)  time: 0.1776  data: 0.0004  max mem: 1379\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.6109 (0.6691)  Acc@1: 87.5000 (84.2480)  Acc@5: 95.8333 (96.2398)  Acc@task: 83.3333 (81.8089)  time: 0.1788  data: 0.0003  max mem: 1379\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.6109 (0.6682)  Acc@1: 87.5000 (84.3000)  Acc@5: 95.8333 (96.3000)  Acc@task: 83.3333 (81.6000)  time: 0.1751  data: 0.0003  max mem: 1379\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1894 s / it)\n",
            "* Acc@task 81.600 Acc@1 84.300 Acc@5 96.300 loss 0.668\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:25  Loss: 0.8614 (0.8614)  Acc@1: 83.3333 (83.3333)  Acc@5: 87.5000 (87.5000)  Acc@task: 62.5000 (62.5000)  time: 0.5993  data: 0.4377  max mem: 1379\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:06  Loss: 0.8674 (0.8431)  Acc@1: 83.3333 (79.9242)  Acc@5: 91.6667 (93.5606)  Acc@task: 75.0000 (75.0000)  time: 0.2167  data: 0.0403  max mem: 1379\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.7081 (0.7812)  Acc@1: 79.1667 (80.1587)  Acc@5: 95.8333 (94.4444)  Acc@task: 75.0000 (77.1825)  time: 0.1786  data: 0.0010  max mem: 1379\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.5698 (0.7298)  Acc@1: 83.3333 (81.3172)  Acc@5: 95.8333 (95.4301)  Acc@task: 79.1667 (78.0914)  time: 0.1792  data: 0.0014  max mem: 1379\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7520 (0.7938)  Acc@1: 83.3333 (80.6911)  Acc@5: 95.8333 (94.7154)  Acc@task: 83.3333 (78.6585)  time: 0.1796  data: 0.0008  max mem: 1379\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.7496 (0.7852)  Acc@1: 83.3333 (80.9000)  Acc@5: 95.8333 (94.8000)  Acc@task: 83.3333 (78.9000)  time: 0.1761  data: 0.0007  max mem: 1379\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1898 s / it)\n",
            "* Acc@task 78.900 Acc@1 80.900 Acc@5 94.800 loss 0.785\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:25  Loss: 0.5792 (0.5792)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.6168  data: 0.4402  max mem: 1379\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:07  Loss: 0.5246 (0.6174)  Acc@1: 79.1667 (82.9545)  Acc@5: 100.0000 (97.7273)  Acc@task: 75.0000 (78.0303)  time: 0.2193  data: 0.0403  max mem: 1379\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.5787 (0.6854)  Acc@1: 79.1667 (82.3413)  Acc@5: 95.8333 (96.8254)  Acc@task: 75.0000 (77.5794)  time: 0.1784  data: 0.0004  max mem: 1379\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.6629 (0.7121)  Acc@1: 79.1667 (81.9892)  Acc@5: 95.8333 (96.1022)  Acc@task: 79.1667 (78.4946)  time: 0.1784  data: 0.0004  max mem: 1379\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.8244 (0.7510)  Acc@1: 83.3333 (81.6057)  Acc@5: 95.8333 (95.9350)  Acc@task: 79.1667 (77.1341)  time: 0.1796  data: 0.0003  max mem: 1379\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.8340 (0.7726)  Acc@1: 83.3333 (81.3000)  Acc@5: 95.8333 (95.7000)  Acc@task: 75.0000 (77.1000)  time: 0.1760  data: 0.0003  max mem: 1379\n",
            "Test: [Task 5] Total time: 0:00:07 (0.1901 s / it)\n",
            "* Acc@task 77.100 Acc@1 81.300 Acc@5 95.700 loss 0.773\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:27  Loss: 0.6159 (0.6159)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 87.5000 (87.5000)  time: 0.6646  data: 0.4133  max mem: 1379\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:07  Loss: 0.7266 (0.8434)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (96.9697)  Acc@task: 83.3333 (81.4394)  time: 0.2232  data: 0.0393  max mem: 1379\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:04  Loss: 0.8857 (0.9150)  Acc@1: 79.1667 (77.7778)  Acc@5: 95.8333 (95.8333)  Acc@task: 79.1667 (80.5556)  time: 0.1788  data: 0.0019  max mem: 1379\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 0.9424 (0.9131)  Acc@1: 75.0000 (77.0161)  Acc@5: 95.8333 (95.5645)  Acc@task: 79.1667 (79.1667)  time: 0.1789  data: 0.0013  max mem: 1379\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 0.8665 (0.9426)  Acc@1: 75.0000 (76.0163)  Acc@5: 95.8333 (95.0203)  Acc@task: 75.0000 (77.8455)  time: 0.1796  data: 0.0005  max mem: 1379\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 0.7764 (0.9382)  Acc@1: 75.0000 (76.0000)  Acc@5: 95.8333 (95.0000)  Acc@task: 79.1667 (78.0000)  time: 0.1766  data: 0.0005  max mem: 1379\n",
            "Test: [Task 6] Total time: 0:00:08 (0.1917 s / it)\n",
            "* Acc@task 78.000 Acc@1 76.000 Acc@5 95.000 loss 0.938\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:24  Loss: 0.8111 (0.8111)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  Acc@task: 70.8333 (70.8333)  time: 0.5722  data: 0.4051  max mem: 1379\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:06  Loss: 0.6709 (0.7441)  Acc@1: 79.1667 (78.0303)  Acc@5: 100.0000 (96.9697)  Acc@task: 70.8333 (72.7273)  time: 0.2127  data: 0.0374  max mem: 1379\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:04  Loss: 0.7409 (0.8609)  Acc@1: 79.1667 (76.1905)  Acc@5: 100.0000 (96.4286)  Acc@task: 70.8333 (72.6191)  time: 0.1771  data: 0.0005  max mem: 1379\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:02  Loss: 0.9045 (0.8961)  Acc@1: 75.0000 (76.8817)  Acc@5: 95.8333 (95.4301)  Acc@task: 70.8333 (72.7151)  time: 0.1785  data: 0.0004  max mem: 1379\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 0.9045 (0.9220)  Acc@1: 79.1667 (76.4228)  Acc@5: 95.8333 (95.4268)  Acc@task: 75.0000 (73.0691)  time: 0.1795  data: 0.0003  max mem: 1379\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 0.9045 (0.9095)  Acc@1: 79.1667 (76.6000)  Acc@5: 95.8333 (95.4000)  Acc@task: 75.0000 (73.2000)  time: 0.1760  data: 0.0003  max mem: 1379\n",
            "Test: [Task 7] Total time: 0:00:07 (0.1893 s / it)\n",
            "* Acc@task 73.200 Acc@1 76.600 Acc@5 95.400 loss 0.909\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:38  Loss: 1.2195 (1.2195)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  Acc@task: 70.8333 (70.8333)  time: 0.9242  data: 0.6842  max mem: 1379\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:07  Loss: 0.7919 (1.0252)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (92.8030)  Acc@task: 75.0000 (72.3485)  time: 0.2451  data: 0.0631  max mem: 1379\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:04  Loss: 0.7515 (0.9402)  Acc@1: 75.0000 (75.3968)  Acc@5: 95.8333 (94.6429)  Acc@task: 75.0000 (70.8333)  time: 0.1773  data: 0.0007  max mem: 1379\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:02  Loss: 0.8494 (0.9160)  Acc@1: 75.0000 (76.6129)  Acc@5: 95.8333 (94.7581)  Acc@task: 66.6667 (71.1022)  time: 0.1784  data: 0.0004  max mem: 1379\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 0.8835 (0.9094)  Acc@1: 79.1667 (76.2195)  Acc@5: 95.8333 (94.7154)  Acc@task: 70.8333 (70.6301)  time: 0.1790  data: 0.0003  max mem: 1379\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 0.8835 (0.9059)  Acc@1: 79.1667 (76.3000)  Acc@5: 95.8333 (94.8000)  Acc@task: 68.7500 (70.6000)  time: 0.1757  data: 0.0003  max mem: 1379\n",
            "Test: [Task 8] Total time: 0:00:08 (0.1968 s / it)\n",
            "* Acc@task 70.600 Acc@1 76.300 Acc@5 94.800 loss 0.906\n",
            "Test: [Task 9]  [ 0/42]  eta: 0:00:17  Loss: 0.7805 (0.7805)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 62.5000 (62.5000)  time: 0.4250  data: 0.2561  max mem: 1379\n",
            "Test: [Task 9]  [10/42]  eta: 0:00:06  Loss: 0.9251 (0.9365)  Acc@1: 75.0000 (74.6212)  Acc@5: 95.8333 (96.9697)  Acc@task: 70.8333 (70.8333)  time: 0.2078  data: 0.0334  max mem: 1379\n",
            "Test: [Task 9]  [20/42]  eta: 0:00:04  Loss: 0.7551 (0.8236)  Acc@1: 79.1667 (76.9841)  Acc@5: 95.8333 (96.6270)  Acc@task: 70.8333 (70.6349)  time: 0.1829  data: 0.0057  max mem: 1379\n",
            "Test: [Task 9]  [30/42]  eta: 0:00:02  Loss: 0.7937 (0.8798)  Acc@1: 79.1667 (75.9409)  Acc@5: 95.8333 (96.2366)  Acc@task: 66.6667 (70.0269)  time: 0.1794  data: 0.0005  max mem: 1379\n",
            "Test: [Task 9]  [40/42]  eta: 0:00:00  Loss: 0.8459 (0.8482)  Acc@1: 75.0000 (76.1179)  Acc@5: 95.8333 (96.3415)  Acc@task: 70.8333 (70.7317)  time: 0.1794  data: 0.0007  max mem: 1379\n",
            "Test: [Task 9]  [41/42]  eta: 0:00:00  Loss: 0.8459 (0.8358)  Acc@1: 75.0000 (76.3000)  Acc@5: 95.8333 (96.4000)  Acc@task: 70.8333 (71.0000)  time: 0.1768  data: 0.0007  max mem: 1379\n",
            "Test: [Task 9] Total time: 0:00:07 (0.1876 s / it)\n",
            "* Acc@task 71.000 Acc@1 76.300 Acc@5 96.400 loss 0.836\n",
            "[Average accuracy till task9]\tAcc@task: 76.2889\tAcc@1: 79.4333\tAcc@5: 95.6556\tLoss: 0.8127\tForgetting: 1.4875\tBackward: 4.6500\n",
            "Loading checkpoint from: ./output/cifar100_full_dino_1epoch_50pct/checkpoint/task10_checkpoint.pth\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/engines/hide_promtp_wtp_and_tap_engine.py:540: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  loss = torch.nn.functional.cross_entropy(sim, torch.range(0, sim.shape[0] - 1).long().to(device))\n",
            "Train: Epoch[1/1]  [  0/106]  eta: 0:01:25  Lr: 0.002812  Loss: 4.0244  Acc@1: 8.3333 (8.3333)  Acc@5: 58.3333 (58.3333)  time: 0.8037  data: 0.5165  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 10/106]  eta: 0:00:31  Lr: 0.002812  Loss: 3.4497  Acc@1: 8.3333 (9.8485)  Acc@5: 50.0000 (53.4091)  time: 0.3321  data: 0.0473  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 20/106]  eta: 0:00:26  Lr: 0.002812  Loss: 2.6527  Acc@1: 12.5000 (15.6746)  Acc@5: 62.5000 (61.5079)  time: 0.2843  data: 0.0005  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 30/106]  eta: 0:00:22  Lr: 0.002812  Loss: 1.4428  Acc@1: 29.1667 (23.1183)  Acc@5: 75.0000 (68.2796)  time: 0.2835  data: 0.0006  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 40/106]  eta: 0:00:19  Lr: 0.002812  Loss: 1.7018  Acc@1: 41.6667 (28.8618)  Acc@5: 87.5000 (73.6789)  time: 0.2858  data: 0.0007  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 50/106]  eta: 0:00:16  Lr: 0.002812  Loss: 1.3436  Acc@1: 54.1667 (34.9673)  Acc@5: 91.6667 (77.4510)  time: 0.2866  data: 0.0009  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 60/106]  eta: 0:00:13  Lr: 0.002812  Loss: 1.6643  Acc@1: 62.5000 (40.3005)  Acc@5: 95.8333 (80.2596)  time: 0.2845  data: 0.0008  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 70/106]  eta: 0:00:10  Lr: 0.002812  Loss: 1.2135  Acc@1: 70.8333 (45.0704)  Acc@5: 95.8333 (82.4531)  time: 0.2843  data: 0.0006  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 80/106]  eta: 0:00:07  Lr: 0.002812  Loss: 1.3672  Acc@1: 75.0000 (48.9712)  Acc@5: 95.8333 (84.0021)  time: 0.2841  data: 0.0005  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 90/106]  eta: 0:00:04  Lr: 0.002812  Loss: 0.7555  Acc@1: 79.1667 (52.3352)  Acc@5: 95.8333 (85.5311)  time: 0.2848  data: 0.0010  max mem: 1379\n",
            "Train: Epoch[1/1]  [100/106]  eta: 0:00:01  Lr: 0.002812  Loss: 1.1315  Acc@1: 79.1667 (54.9918)  Acc@5: 100.0000 (86.6337)  time: 0.2853  data: 0.0013  max mem: 1379\n",
            "Train: Epoch[1/1]  [105/106]  eta: 0:00:00  Lr: 0.002812  Loss: 1.0568  Acc@1: 79.1667 (55.9382)  Acc@5: 100.0000 (87.0546)  time: 0.2751  data: 0.0009  max mem: 1379\n",
            "Train: Epoch[1/1] Total time: 0:00:30 (0.2888 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.0568  Acc@1: 79.1667 (55.9382)  Acc@5: 100.0000 (87.0546)\n",
            "torch.Size([5, 2, 5, 6, 64])\n",
            "torch.Size([5, 2, 1, 5, 6, 64])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:20  Loss: 0.6328 (0.6328)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 79.1667 (79.1667)  time: 0.4828  data: 0.3103  max mem: 1379\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.6392 (0.6536)  Acc@1: 83.3333 (82.5758)  Acc@5: 100.0000 (97.3485)  Acc@task: 79.1667 (78.4091)  time: 0.2051  data: 0.0297  max mem: 1379\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.5678 (0.6925)  Acc@1: 83.3333 (82.7381)  Acc@5: 95.8333 (96.4286)  Acc@task: 79.1667 (80.5556)  time: 0.1770  data: 0.0010  max mem: 1379\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.4486 (0.6475)  Acc@1: 87.5000 (83.8710)  Acc@5: 95.8333 (96.5054)  Acc@task: 79.1667 (79.8387)  time: 0.1776  data: 0.0004  max mem: 1379\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.3574 (0.6133)  Acc@1: 91.6667 (85.0610)  Acc@5: 100.0000 (96.9512)  Acc@task: 79.1667 (80.6911)  time: 0.1783  data: 0.0003  max mem: 1379\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.3408 (0.6041)  Acc@1: 91.6667 (85.2000)  Acc@5: 100.0000 (97.0000)  Acc@task: 83.3333 (81.0000)  time: 0.1752  data: 0.0003  max mem: 1379\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1870 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@task 81.000 Acc@1 85.200 Acc@5 97.000 loss 0.604\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:34  Loss: 1.1621 (1.1621)  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  Acc@task: 75.0000 (75.0000)  time: 0.8121  data: 0.6075  max mem: 1379\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:07  Loss: 1.0046 (0.9916)  Acc@1: 79.1667 (77.2727)  Acc@5: 95.8333 (96.2121)  Acc@task: 70.8333 (72.3485)  time: 0.2363  data: 0.0570  max mem: 1379\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.8987 (1.0215)  Acc@1: 79.1667 (76.1905)  Acc@5: 95.8333 (95.4365)  Acc@task: 70.8333 (72.2222)  time: 0.1783  data: 0.0012  max mem: 1379\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.9765 (0.9859)  Acc@1: 79.1667 (76.6129)  Acc@5: 95.8333 (95.2957)  Acc@task: 70.8333 (72.5806)  time: 0.1785  data: 0.0004  max mem: 1379\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.7301 (0.9086)  Acc@1: 79.1667 (77.5407)  Acc@5: 95.8333 (95.8333)  Acc@task: 70.8333 (72.9675)  time: 0.1791  data: 0.0003  max mem: 1379\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.7301 (0.9063)  Acc@1: 79.1667 (77.4000)  Acc@5: 95.8333 (95.8000)  Acc@task: 70.8333 (73.2000)  time: 0.1758  data: 0.0003  max mem: 1379\n",
            "Test: [Task 2] Total time: 0:00:08 (0.1948 s / it)\n",
            "* Acc@task 73.200 Acc@1 77.400 Acc@5 95.800 loss 0.906\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:23  Loss: 0.5374 (0.5374)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.5484  data: 0.3693  max mem: 1379\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 0.7051 (0.7661)  Acc@1: 83.3333 (82.1970)  Acc@5: 100.0000 (96.9697)  Acc@task: 83.3333 (81.0606)  time: 0.2130  data: 0.0341  max mem: 1379\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.6438 (0.6967)  Acc@1: 83.3333 (83.5317)  Acc@5: 95.8333 (96.8254)  Acc@task: 83.3333 (81.3492)  time: 0.1789  data: 0.0005  max mem: 1379\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.4979 (0.6616)  Acc@1: 83.3333 (83.7366)  Acc@5: 95.8333 (96.9086)  Acc@task: 79.1667 (80.5108)  time: 0.1793  data: 0.0005  max mem: 1379\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.5753 (0.6738)  Acc@1: 87.5000 (84.3496)  Acc@5: 95.8333 (96.2398)  Acc@task: 79.1667 (80.4878)  time: 0.1802  data: 0.0007  max mem: 1379\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.5753 (0.6727)  Acc@1: 87.5000 (84.4000)  Acc@5: 95.8333 (96.3000)  Acc@task: 79.1667 (80.3000)  time: 0.1768  data: 0.0007  max mem: 1379\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1891 s / it)\n",
            "* Acc@task 80.300 Acc@1 84.400 Acc@5 96.300 loss 0.673\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:24  Loss: 0.8542 (0.8542)  Acc@1: 83.3333 (83.3333)  Acc@5: 87.5000 (87.5000)  Acc@task: 62.5000 (62.5000)  time: 0.5807  data: 0.3881  max mem: 1379\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:06  Loss: 0.8675 (0.8612)  Acc@1: 83.3333 (79.5455)  Acc@5: 91.6667 (93.1818)  Acc@task: 70.8333 (71.2121)  time: 0.2154  data: 0.0356  max mem: 1379\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.6255 (0.8035)  Acc@1: 79.1667 (79.5635)  Acc@5: 95.8333 (94.2460)  Acc@task: 75.0000 (73.4127)  time: 0.1790  data: 0.0004  max mem: 1379\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.6100 (0.7489)  Acc@1: 79.1667 (80.5108)  Acc@5: 95.8333 (95.2957)  Acc@task: 75.0000 (74.8656)  time: 0.1797  data: 0.0004  max mem: 1379\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7781 (0.8269)  Acc@1: 79.1667 (79.8781)  Acc@5: 95.8333 (94.6138)  Acc@task: 75.0000 (74.5935)  time: 0.1806  data: 0.0004  max mem: 1379\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.7547 (0.8175)  Acc@1: 79.1667 (80.1000)  Acc@5: 95.8333 (94.7000)  Acc@task: 79.1667 (74.9000)  time: 0.1770  data: 0.0003  max mem: 1379\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1905 s / it)\n",
            "* Acc@task 74.900 Acc@1 80.100 Acc@5 94.700 loss 0.818\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:19  Loss: 0.5791 (0.5791)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  Acc@task: 79.1667 (79.1667)  time: 0.4742  data: 0.3026  max mem: 1379\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 0.5245 (0.6108)  Acc@1: 79.1667 (82.9545)  Acc@5: 100.0000 (97.7273)  Acc@task: 75.0000 (78.0303)  time: 0.2065  data: 0.0290  max mem: 1379\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.5768 (0.6819)  Acc@1: 79.1667 (82.3413)  Acc@5: 95.8333 (96.8254)  Acc@task: 79.1667 (77.1825)  time: 0.1794  data: 0.0014  max mem: 1379\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.6631 (0.7050)  Acc@1: 79.1667 (81.9892)  Acc@5: 95.8333 (96.2366)  Acc@task: 83.3333 (78.4946)  time: 0.1792  data: 0.0011  max mem: 1379\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.7482 (0.7390)  Acc@1: 83.3333 (81.7073)  Acc@5: 95.8333 (96.0366)  Acc@task: 75.0000 (76.7276)  time: 0.1800  data: 0.0007  max mem: 1379\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.8245 (0.7582)  Acc@1: 83.3333 (81.5000)  Acc@5: 95.8333 (95.8000)  Acc@task: 75.0000 (76.7000)  time: 0.1769  data: 0.0007  max mem: 1379\n",
            "Test: [Task 5] Total time: 0:00:07 (0.1878 s / it)\n",
            "* Acc@task 76.700 Acc@1 81.500 Acc@5 95.800 loss 0.758\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:22  Loss: 0.5992 (0.5992)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  Acc@task: 87.5000 (87.5000)  time: 0.5413  data: 0.3323  max mem: 1379\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:06  Loss: 0.7172 (0.8396)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (96.9697)  Acc@task: 83.3333 (80.6818)  time: 0.2121  data: 0.0308  max mem: 1379\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:04  Loss: 0.8691 (0.9064)  Acc@1: 79.1667 (77.7778)  Acc@5: 95.8333 (96.0317)  Acc@task: 79.1667 (78.9683)  time: 0.1788  data: 0.0005  max mem: 1379\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 0.9426 (0.9051)  Acc@1: 75.0000 (77.1505)  Acc@5: 95.8333 (95.9677)  Acc@task: 75.0000 (77.8226)  time: 0.1788  data: 0.0004  max mem: 1379\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 0.7761 (0.9357)  Acc@1: 75.0000 (76.1179)  Acc@5: 95.8333 (95.4268)  Acc@task: 75.0000 (76.5244)  time: 0.1796  data: 0.0004  max mem: 1379\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 0.7761 (0.9315)  Acc@1: 75.0000 (76.1000)  Acc@5: 95.8333 (95.4000)  Acc@task: 75.0000 (76.7000)  time: 0.1760  data: 0.0004  max mem: 1379\n",
            "Test: [Task 6] Total time: 0:00:07 (0.1902 s / it)\n",
            "* Acc@task 76.700 Acc@1 76.100 Acc@5 95.400 loss 0.932\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:34  Loss: 0.7668 (0.7668)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  Acc@task: 70.8333 (70.8333)  time: 0.8253  data: 0.6040  max mem: 1379\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:07  Loss: 0.6709 (0.7225)  Acc@1: 79.1667 (78.4091)  Acc@5: 100.0000 (96.9697)  Acc@task: 75.0000 (75.7576)  time: 0.2373  data: 0.0566  max mem: 1379\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:04  Loss: 0.7409 (0.8363)  Acc@1: 79.1667 (76.5873)  Acc@5: 100.0000 (96.2302)  Acc@task: 75.0000 (74.8016)  time: 0.1782  data: 0.0016  max mem: 1379\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:02  Loss: 0.8431 (0.8805)  Acc@1: 75.0000 (77.1505)  Acc@5: 95.8333 (95.4301)  Acc@task: 75.0000 (74.1936)  time: 0.1787  data: 0.0008  max mem: 1379\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 0.9105 (0.9103)  Acc@1: 79.1667 (76.6260)  Acc@5: 95.8333 (95.4268)  Acc@task: 75.0000 (74.3902)  time: 0.1788  data: 0.0003  max mem: 1379\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 0.9105 (0.8981)  Acc@1: 79.1667 (76.8000)  Acc@5: 95.8333 (95.4000)  Acc@task: 75.0000 (74.6000)  time: 0.1755  data: 0.0003  max mem: 1379\n",
            "Test: [Task 7] Total time: 0:00:08 (0.1949 s / it)\n",
            "* Acc@task 74.600 Acc@1 76.800 Acc@5 95.400 loss 0.898\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:21  Loss: 1.2188 (1.2188)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  Acc@task: 70.8333 (70.8333)  time: 0.5095  data: 0.2949  max mem: 1379\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:06  Loss: 0.8438 (0.9936)  Acc@1: 75.0000 (76.1364)  Acc@5: 91.6667 (93.1818)  Acc@task: 75.0000 (72.3485)  time: 0.2076  data: 0.0274  max mem: 1379\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:04  Loss: 0.6696 (0.8893)  Acc@1: 75.0000 (76.7857)  Acc@5: 95.8333 (94.8413)  Acc@task: 70.8333 (71.2302)  time: 0.1775  data: 0.0005  max mem: 1379\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:02  Loss: 0.7483 (0.8761)  Acc@1: 79.1667 (77.8226)  Acc@5: 95.8333 (95.0269)  Acc@task: 70.8333 (71.2366)  time: 0.1783  data: 0.0004  max mem: 1379\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 0.8779 (0.8836)  Acc@1: 79.1667 (77.1341)  Acc@5: 95.8333 (95.0203)  Acc@task: 70.8333 (70.3252)  time: 0.1790  data: 0.0005  max mem: 1379\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 0.8779 (0.8807)  Acc@1: 79.1667 (77.2000)  Acc@5: 95.8333 (95.1000)  Acc@task: 70.8333 (70.4000)  time: 0.1757  data: 0.0005  max mem: 1379\n",
            "Test: [Task 8] Total time: 0:00:07 (0.1882 s / it)\n",
            "* Acc@task 70.400 Acc@1 77.200 Acc@5 95.100 loss 0.881\n",
            "Test: [Task 9]  [ 0/42]  eta: 0:00:35  Loss: 0.7759 (0.7759)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 79.1667 (79.1667)  time: 0.8377  data: 0.6480  max mem: 1379\n",
            "Test: [Task 9]  [10/42]  eta: 0:00:07  Loss: 0.9104 (0.9213)  Acc@1: 79.1667 (75.0000)  Acc@5: 95.8333 (96.9697)  Acc@task: 75.0000 (75.3788)  time: 0.2384  data: 0.0598  max mem: 1379\n",
            "Test: [Task 9]  [20/42]  eta: 0:00:04  Loss: 0.6604 (0.7843)  Acc@1: 79.1667 (77.9762)  Acc@5: 95.8333 (97.0238)  Acc@task: 75.0000 (76.7857)  time: 0.1776  data: 0.0009  max mem: 1379\n",
            "Test: [Task 9]  [30/42]  eta: 0:00:02  Loss: 0.7195 (0.8128)  Acc@1: 79.1667 (77.0161)  Acc@5: 95.8333 (96.7742)  Acc@task: 79.1667 (76.2097)  time: 0.1778  data: 0.0006  max mem: 1379\n",
            "Test: [Task 9]  [40/42]  eta: 0:00:00  Loss: 0.7639 (0.7782)  Acc@1: 75.0000 (77.5407)  Acc@5: 95.8333 (96.8496)  Acc@task: 79.1667 (77.5407)  time: 0.1786  data: 0.0004  max mem: 1379\n",
            "Test: [Task 9]  [41/42]  eta: 0:00:00  Loss: 0.7639 (0.7676)  Acc@1: 75.0000 (77.7000)  Acc@5: 95.8333 (96.9000)  Acc@task: 79.1667 (77.7000)  time: 0.1751  data: 0.0004  max mem: 1379\n",
            "Test: [Task 9] Total time: 0:00:08 (0.1945 s / it)\n",
            "* Acc@task 77.700 Acc@1 77.700 Acc@5 96.900 loss 0.768\n",
            "Test: [Task 10]  [ 0/42]  eta: 0:00:28  Loss: 6.7162 (6.7162)  Acc@1: 0.0000 (0.0000)  Acc@5: 16.6667 (16.6667)  Acc@task: 66.6667 (66.6667)  time: 0.6805  data: 0.5165  max mem: 1379\n",
            "Test: [Task 10]  [10/42]  eta: 0:00:07  Loss: 7.1309 (7.1090)  Acc@1: 0.0000 (0.7576)  Acc@5: 25.0000 (26.1364)  Acc@task: 62.5000 (61.3636)  time: 0.2242  data: 0.0473  max mem: 1379\n",
            "Test: [Task 10]  [20/42]  eta: 0:00:04  Loss: 7.1209 (7.0534)  Acc@1: 0.0000 (1.5873)  Acc@5: 33.3333 (31.7460)  Acc@task: 62.5000 (62.5000)  time: 0.1768  data: 0.0008  max mem: 1379\n",
            "Test: [Task 10]  [30/42]  eta: 0:00:02  Loss: 6.9207 (7.0777)  Acc@1: 0.0000 (1.4785)  Acc@5: 33.3333 (32.6613)  Acc@task: 58.3333 (61.9624)  time: 0.1767  data: 0.0015  max mem: 1379\n",
            "Test: [Task 10]  [40/42]  eta: 0:00:00  Loss: 7.0160 (7.0650)  Acc@1: 0.0000 (1.3211)  Acc@5: 33.3333 (32.7236)  Acc@task: 58.3333 (60.8740)  time: 0.1786  data: 0.0011  max mem: 1379\n",
            "Test: [Task 10]  [41/42]  eta: 0:00:00  Loss: 7.0160 (7.0597)  Acc@1: 0.0000 (1.3000)  Acc@5: 33.3333 (33.2000)  Acc@task: 58.3333 (61.1000)  time: 0.1752  data: 0.0009  max mem: 1379\n",
            "Test: [Task 10] Total time: 0:00:07 (0.1904 s / it)\n",
            "* Acc@task 61.100 Acc@1 1.300 Acc@5 33.200 loss 7.060\n",
            "[Average accuracy till task10]\tAcc@task: 74.6600\tAcc@1: 71.7700\tAcc@5: 89.5600\tLoss: 1.4297\tForgetting: 1.6111\tBackward: 68.0111\n",
            "torch.Size([120000, 384])\n",
            "Averaged stats: Lr: 0.005000  Loss: 0.0589  Acc@1: 97.5000 (95.0556)  Acc@5: 100.0000 (99.5556)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:41  Loss: 0.6544 (0.6544)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 79.1667 (79.1667)  time: 0.9902  data: 0.8197  max mem: 1379\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:07  Loss: 0.6697 (0.7109)  Acc@1: 83.3333 (82.5758)  Acc@5: 100.0000 (97.7273)  Acc@task: 79.1667 (78.4091)  time: 0.2493  data: 0.0753  max mem: 1379\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.6107 (0.7522)  Acc@1: 83.3333 (82.5397)  Acc@5: 95.8333 (96.4286)  Acc@task: 79.1667 (80.5556)  time: 0.1754  data: 0.0008  max mem: 1379\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.4594 (0.6978)  Acc@1: 83.3333 (83.6022)  Acc@5: 95.8333 (96.5054)  Acc@task: 79.1667 (79.8387)  time: 0.1766  data: 0.0007  max mem: 1379\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.3949 (0.6576)  Acc@1: 87.5000 (84.5528)  Acc@5: 95.8333 (96.9512)  Acc@task: 79.1667 (80.6911)  time: 0.1775  data: 0.0004  max mem: 1379\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.3469 (0.6497)  Acc@1: 87.5000 (84.7000)  Acc@5: 100.0000 (97.0000)  Acc@task: 83.3333 (81.0000)  time: 0.1741  data: 0.0004  max mem: 1379\n",
            "Test: [Task 1] Total time: 0:00:08 (0.1977 s / it)\n",
            "* Acc@task 81.000 Acc@1 84.700 Acc@5 97.000 loss 0.650\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:35  Loss: 1.2654 (1.2654)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  Acc@task: 75.0000 (75.0000)  time: 0.8419  data: 0.6510  max mem: 1379\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:07  Loss: 0.9289 (1.0334)  Acc@1: 75.0000 (76.1364)  Acc@5: 95.8333 (94.6970)  Acc@task: 70.8333 (72.3485)  time: 0.2366  data: 0.0603  max mem: 1379\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.8789 (1.1002)  Acc@1: 75.0000 (74.6032)  Acc@5: 95.8333 (94.6429)  Acc@task: 70.8333 (72.2222)  time: 0.1767  data: 0.0008  max mem: 1379\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.9938 (1.0499)  Acc@1: 75.0000 (74.7312)  Acc@5: 95.8333 (94.8925)  Acc@task: 70.8333 (72.5806)  time: 0.1777  data: 0.0005  max mem: 1379\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.7652 (0.9646)  Acc@1: 75.0000 (75.7114)  Acc@5: 95.8333 (95.3252)  Acc@task: 70.8333 (72.9675)  time: 0.1782  data: 0.0004  max mem: 1379\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.7595 (0.9598)  Acc@1: 75.0000 (75.7000)  Acc@5: 95.8333 (95.3000)  Acc@task: 70.8333 (73.2000)  time: 0.1749  data: 0.0004  max mem: 1379\n",
            "Test: [Task 2] Total time: 0:00:08 (0.1942 s / it)\n",
            "* Acc@task 73.200 Acc@1 75.700 Acc@5 95.300 loss 0.960\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:24  Loss: 0.4634 (0.4634)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.5838  data: 0.3906  max mem: 1379\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 0.7795 (0.7594)  Acc@1: 83.3333 (81.4394)  Acc@5: 100.0000 (96.5909)  Acc@task: 83.3333 (81.0606)  time: 0.2150  data: 0.0358  max mem: 1379\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.6463 (0.6830)  Acc@1: 83.3333 (83.5317)  Acc@5: 95.8333 (96.8254)  Acc@task: 83.3333 (81.3492)  time: 0.1774  data: 0.0004  max mem: 1379\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.5231 (0.6506)  Acc@1: 83.3333 (83.6022)  Acc@5: 95.8333 (96.9086)  Acc@task: 79.1667 (80.5108)  time: 0.1780  data: 0.0009  max mem: 1379\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.5864 (0.6661)  Acc@1: 83.3333 (84.2480)  Acc@5: 95.8333 (96.3415)  Acc@task: 79.1667 (80.4878)  time: 0.1790  data: 0.0008  max mem: 1379\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.5792 (0.6640)  Acc@1: 83.3333 (84.3000)  Acc@5: 95.8333 (96.3000)  Acc@task: 79.1667 (80.3000)  time: 0.1757  data: 0.0008  max mem: 1379\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1899 s / it)\n",
            "* Acc@task 80.300 Acc@1 84.300 Acc@5 96.300 loss 0.664\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:25  Loss: 0.7146 (0.7146)  Acc@1: 87.5000 (87.5000)  Acc@5: 87.5000 (87.5000)  Acc@task: 62.5000 (62.5000)  time: 0.6088  data: 0.4163  max mem: 1379\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:06  Loss: 0.7677 (0.8181)  Acc@1: 79.1667 (79.5455)  Acc@5: 91.6667 (93.5606)  Acc@task: 70.8333 (71.2121)  time: 0.2166  data: 0.0384  max mem: 1379\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.5670 (0.7482)  Acc@1: 79.1667 (80.5556)  Acc@5: 95.8333 (95.0397)  Acc@task: 75.0000 (73.4127)  time: 0.1775  data: 0.0005  max mem: 1379\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.5271 (0.7208)  Acc@1: 83.3333 (81.1828)  Acc@5: 95.8333 (95.8333)  Acc@task: 75.0000 (74.8656)  time: 0.1785  data: 0.0005  max mem: 1379\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7382 (0.7876)  Acc@1: 79.1667 (80.3862)  Acc@5: 95.8333 (94.9187)  Acc@task: 75.0000 (74.5935)  time: 0.1797  data: 0.0004  max mem: 1379\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.7304 (0.7789)  Acc@1: 79.1667 (80.5000)  Acc@5: 95.8333 (95.0000)  Acc@task: 79.1667 (74.9000)  time: 0.1763  data: 0.0003  max mem: 1379\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1902 s / it)\n",
            "* Acc@task 74.900 Acc@1 80.500 Acc@5 95.000 loss 0.779\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:20  Loss: 0.6275 (0.6275)  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  Acc@task: 79.1667 (79.1667)  time: 0.4889  data: 0.3115  max mem: 1379\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 0.6095 (0.6464)  Acc@1: 83.3333 (82.1970)  Acc@5: 100.0000 (97.7273)  Acc@task: 75.0000 (78.0303)  time: 0.2068  data: 0.0298  max mem: 1379\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.8178 (0.7790)  Acc@1: 79.1667 (80.5556)  Acc@5: 95.8333 (96.8254)  Acc@task: 79.1667 (77.1825)  time: 0.1787  data: 0.0013  max mem: 1379\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.8830 (0.8044)  Acc@1: 75.0000 (79.8387)  Acc@5: 95.8333 (96.2366)  Acc@task: 83.3333 (78.4946)  time: 0.1793  data: 0.0013  max mem: 1379\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.8830 (0.8442)  Acc@1: 75.0000 (79.2683)  Acc@5: 95.8333 (95.9350)  Acc@task: 75.0000 (76.7276)  time: 0.1798  data: 0.0010  max mem: 1379\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.9082 (0.8640)  Acc@1: 75.0000 (79.0000)  Acc@5: 95.8333 (95.6000)  Acc@task: 75.0000 (76.7000)  time: 0.1764  data: 0.0009  max mem: 1379\n",
            "Test: [Task 5] Total time: 0:00:07 (0.1874 s / it)\n",
            "* Acc@task 76.700 Acc@1 79.000 Acc@5 95.600 loss 0.864\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:18  Loss: 0.5048 (0.5048)  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)  Acc@task: 87.5000 (87.5000)  time: 0.4383  data: 0.2687  max mem: 1379\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:06  Loss: 0.6384 (0.7872)  Acc@1: 83.3333 (80.6818)  Acc@5: 100.0000 (96.9697)  Acc@task: 83.3333 (80.6818)  time: 0.2078  data: 0.0333  max mem: 1379\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:04  Loss: 0.7663 (0.8634)  Acc@1: 79.1667 (79.3651)  Acc@5: 95.8333 (96.6270)  Acc@task: 79.1667 (78.9683)  time: 0.1826  data: 0.0051  max mem: 1379\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 0.8841 (0.8594)  Acc@1: 79.1667 (78.6290)  Acc@5: 95.8333 (96.5054)  Acc@task: 75.0000 (77.8226)  time: 0.1802  data: 0.0004  max mem: 1379\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 0.8841 (0.8848)  Acc@1: 75.0000 (77.4390)  Acc@5: 95.8333 (95.6301)  Acc@task: 75.0000 (76.5244)  time: 0.1799  data: 0.0005  max mem: 1379\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 0.8653 (0.8820)  Acc@1: 75.0000 (77.5000)  Acc@5: 95.8333 (95.6000)  Acc@task: 75.0000 (76.7000)  time: 0.1769  data: 0.0004  max mem: 1379\n",
            "Test: [Task 6] Total time: 0:00:07 (0.1881 s / it)\n",
            "* Acc@task 76.700 Acc@1 77.500 Acc@5 95.600 loss 0.882\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:29  Loss: 0.7805 (0.7805)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 70.8333 (70.8333)  time: 0.6915  data: 0.5255  max mem: 1379\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:07  Loss: 0.7373 (0.7371)  Acc@1: 79.1667 (78.4091)  Acc@5: 100.0000 (96.2121)  Acc@task: 75.0000 (75.7576)  time: 0.2247  data: 0.0487  max mem: 1379\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:04  Loss: 0.8775 (0.8704)  Acc@1: 79.1667 (76.1905)  Acc@5: 95.8333 (95.2381)  Acc@task: 75.0000 (74.8016)  time: 0.1780  data: 0.0012  max mem: 1379\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:02  Loss: 0.9693 (0.9087)  Acc@1: 75.0000 (75.9409)  Acc@5: 95.8333 (94.6237)  Acc@task: 75.0000 (74.1936)  time: 0.1787  data: 0.0009  max mem: 1379\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 0.9438 (0.9211)  Acc@1: 75.0000 (75.7114)  Acc@5: 95.8333 (94.6138)  Acc@task: 75.0000 (74.3902)  time: 0.1797  data: 0.0003  max mem: 1379\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 0.9438 (0.9077)  Acc@1: 75.0000 (76.0000)  Acc@5: 95.8333 (94.6000)  Acc@task: 75.0000 (74.6000)  time: 0.1761  data: 0.0003  max mem: 1379\n",
            "Test: [Task 7] Total time: 0:00:08 (0.1919 s / it)\n",
            "* Acc@task 74.600 Acc@1 76.000 Acc@5 94.600 loss 0.908\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:22  Loss: 1.2516 (1.2516)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  Acc@task: 70.8333 (70.8333)  time: 0.5394  data: 0.3518  max mem: 1379\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:06  Loss: 1.2516 (1.1991)  Acc@1: 75.0000 (71.2121)  Acc@5: 91.6667 (92.4242)  Acc@task: 75.0000 (72.3485)  time: 0.2112  data: 0.0323  max mem: 1379\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:04  Loss: 0.9502 (1.0906)  Acc@1: 75.0000 (72.8175)  Acc@5: 95.8333 (93.6508)  Acc@task: 70.8333 (71.2302)  time: 0.1779  data: 0.0004  max mem: 1379\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:02  Loss: 0.9709 (1.0800)  Acc@1: 75.0000 (73.3871)  Acc@5: 95.8333 (93.4140)  Acc@task: 70.8333 (71.2366)  time: 0.1783  data: 0.0004  max mem: 1379\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 1.2112 (1.0990)  Acc@1: 70.8333 (72.4594)  Acc@5: 91.6667 (93.2927)  Acc@task: 70.8333 (70.3252)  time: 0.1792  data: 0.0003  max mem: 1379\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 1.2112 (1.0988)  Acc@1: 70.8333 (72.4000)  Acc@5: 91.6667 (93.3000)  Acc@task: 70.8333 (70.4000)  time: 0.1757  data: 0.0003  max mem: 1379\n",
            "Test: [Task 8] Total time: 0:00:07 (0.1889 s / it)\n",
            "* Acc@task 70.400 Acc@1 72.400 Acc@5 93.300 loss 1.099\n",
            "Test: [Task 9]  [ 0/42]  eta: 0:00:41  Loss: 0.7842 (0.7842)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  Acc@task: 79.1667 (79.1667)  time: 0.9993  data: 0.8139  max mem: 1379\n",
            "Test: [Task 9]  [10/42]  eta: 0:00:08  Loss: 0.9984 (0.9226)  Acc@1: 79.1667 (75.7576)  Acc@5: 95.8333 (95.4545)  Acc@task: 75.0000 (75.3788)  time: 0.2534  data: 0.0757  max mem: 1379\n",
            "Test: [Task 9]  [20/42]  eta: 0:00:04  Loss: 0.6972 (0.7858)  Acc@1: 79.1667 (79.3651)  Acc@5: 95.8333 (96.2302)  Acc@task: 75.0000 (76.7857)  time: 0.1781  data: 0.0011  max mem: 1379\n",
            "Test: [Task 9]  [30/42]  eta: 0:00:02  Loss: 0.6972 (0.8190)  Acc@1: 79.1667 (77.1505)  Acc@5: 95.8333 (95.8333)  Acc@task: 79.1667 (76.2097)  time: 0.1784  data: 0.0004  max mem: 1379\n",
            "Test: [Task 9]  [40/42]  eta: 0:00:00  Loss: 0.6791 (0.7778)  Acc@1: 79.1667 (78.0488)  Acc@5: 95.8333 (96.0366)  Acc@task: 79.1667 (77.5407)  time: 0.1789  data: 0.0003  max mem: 1379\n",
            "Test: [Task 9]  [41/42]  eta: 0:00:00  Loss: 0.6791 (0.7663)  Acc@1: 79.1667 (78.3000)  Acc@5: 95.8333 (96.1000)  Acc@task: 79.1667 (77.7000)  time: 0.1753  data: 0.0003  max mem: 1379\n",
            "Test: [Task 9] Total time: 0:00:08 (0.1988 s / it)\n",
            "* Acc@task 77.700 Acc@1 78.300 Acc@5 96.100 loss 0.766\n",
            "Test: [Task 10]  [ 0/42]  eta: 0:00:21  Loss: 0.8188 (0.8188)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  Acc@task: 66.6667 (66.6667)  time: 0.5227  data: 0.3338  max mem: 1379\n",
            "Test: [Task 10]  [10/42]  eta: 0:00:06  Loss: 1.0310 (1.0354)  Acc@1: 75.0000 (73.8636)  Acc@5: 91.6667 (93.5606)  Acc@task: 62.5000 (61.3636)  time: 0.2097  data: 0.0317  max mem: 1379\n",
            "Test: [Task 10]  [20/42]  eta: 0:00:04  Loss: 1.0682 (1.0625)  Acc@1: 75.0000 (73.0159)  Acc@5: 95.8333 (94.2460)  Acc@task: 62.5000 (62.5000)  time: 0.1781  data: 0.0009  max mem: 1379\n",
            "Test: [Task 10]  [30/42]  eta: 0:00:02  Loss: 1.1433 (1.1326)  Acc@1: 70.8333 (71.9086)  Acc@5: 95.8333 (93.9516)  Acc@task: 58.3333 (61.9624)  time: 0.1782  data: 0.0005  max mem: 1379\n",
            "Test: [Task 10]  [40/42]  eta: 0:00:00  Loss: 1.0817 (1.1255)  Acc@1: 70.8333 (71.2398)  Acc@5: 95.8333 (94.3089)  Acc@task: 58.3333 (60.8740)  time: 0.1785  data: 0.0005  max mem: 1379\n",
            "Test: [Task 10]  [41/42]  eta: 0:00:00  Loss: 1.0071 (1.1130)  Acc@1: 70.8333 (71.5000)  Acc@5: 95.8333 (94.4000)  Acc@task: 58.3333 (61.1000)  time: 0.1749  data: 0.0005  max mem: 1379\n",
            "Test: [Task 10] Total time: 0:00:07 (0.1885 s / it)\n",
            "* Acc@task 61.100 Acc@1 71.500 Acc@5 94.400 loss 1.113\n",
            "[Average accuracy till task10]\tAcc@task: 74.6600\tAcc@1: 77.9900\tAcc@5: 95.3200\tLoss: 0.8684\tForgetting: 2.3222\tBackward: 3.4111\n",
            "Total training time: 0:22:16\n",
            "[rank0]:[W1003 14:55:42.463229911 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
          ]
        }
      ],
      "source": [
        "!torchrun --nproc_per_node=1 main.py cifar100_hideprompt_5e --original_model vit_small_patch16_224.dino --model vit_small_patch16_224.dino --batch-size 24 --epochs 1 --seed 20 --ca_lr 0.005 --crct_epochs 1 --prompt_momentum 0.1 --reg 0.1 --length 5 --larger_prompt_lr --data-path ./datasets/ --trained_original_model ./output/cifar100_full_dino_1epoch_100pct --output_dir ./output/cifar100_full_dino_1epoch_final_50pct --pct 1.0\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}