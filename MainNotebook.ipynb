{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1c65321c",
      "metadata": {
        "id": "1c65321c",
        "outputId": "94a7b7fc-b9f5-458e-d920-34d02996df7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32512"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import os\n",
        "os.system('cmd command')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4842b918",
      "metadata": {
        "id": "4842b918",
        "outputId": "b977b391-af4f-469f-c9db-5ed3911f9329",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "31ecb10e",
      "metadata": {
        "id": "31ecb10e",
        "outputId": "004e0bf6-1cff-45e0-c283-af304cc53cf3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/HiDePrompt\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/HiDePrompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd565e2a",
      "metadata": {
        "id": "cd565e2a"
      },
      "outputs": [],
      "source": [
        "!pip install -q condacolab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a2473b5",
      "metadata": {
        "id": "6a2473b5",
        "outputId": "e435931a-a6cb-4f7a-f09c-fd79432ddd86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬ Downloading https://github.com/jaimergp/miniforge/releases/download/24.11.2-1_colab/Miniforge3-colab-24.11.2-1_colab-Linux-x86_64.sh...\n",
            "ğŸ“¦ Installing...\n",
            "ğŸ“Œ Adjusting configuration...\n",
            "ğŸ©¹ Patching environment...\n",
            "â² Done in 0:00:11\n",
            "ğŸ” Restarting kernel...\n"
          ]
        }
      ],
      "source": [
        "import condacolab\n",
        "condacolab.install()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/HiDePrompt/HiDePrompt"
      ],
      "metadata": {
        "id": "4aBTrmkLHGUx",
        "outputId": "444c2c2e-9e6b-4d89-dabc-91daa1677078",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "4aBTrmkLHGUx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59ddd81a",
      "metadata": {
        "id": "59ddd81a",
        "outputId": "2d958915-469d-49c8-90be-f3d2d67d4cfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm (from -r requirements.txt (line 1))\n",
            "  Downloading timm-1.0.20-py3-none-any.whl.metadata (61 kB)\n",
            "Collecting pillow (from -r requirements.txt (line 2))\n",
            "  Downloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting matplotlib (from -r requirements.txt (line 3))\n",
            "  Downloading matplotlib-3.10.6-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
            "Collecting torchprofile (from -r requirements.txt (line 4))\n",
            "  Downloading torchprofile-0.0.4-py3-none-any.whl.metadata (303 bytes)\n",
            "Collecting torch (from -r requirements.txt (line 5))\n",
            "  Downloading torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
            "Collecting torchvision (from -r requirements.txt (line 6))\n",
            "  Downloading torchvision-0.23.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (2.3.0)\n",
            "Collecting scipy (from -r requirements.txt (line 8))\n",
            "  Downloading scipy-1.16.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
            "Collecting scikit-learn (from -r requirements.txt (line 9))\n",
            "  Downloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
            "Collecting numpy (from -r requirements.txt (line 10))\n",
            "  Downloading numpy-2.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "Collecting pyyaml (from timm->-r requirements.txt (line 1))\n",
            "  Downloading pyyaml-6.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)\n",
            "Collecting huggingface_hub (from timm->-r requirements.txt (line 1))\n",
            "  Downloading huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting safetensors (from timm->-r requirements.txt (line 1))\n",
            "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib->-r requirements.txt (line 3))\n",
            "  Downloading contourpy-1.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 3))\n",
            "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib->-r requirements.txt (line 3))\n",
            "  Downloading fonttools-4.60.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (111 kB)\n",
            "Collecting kiwisolver>=1.3.1 (from matplotlib->-r requirements.txt (line 3))\n",
            "  Downloading kiwisolver-1.4.9-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 3)) (24.2)\n",
            "Collecting pyparsing>=2.3.1 (from matplotlib->-r requirements.txt (line 3))\n",
            "  Downloading pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting python-dateutil>=2.7 (from matplotlib->-r requirements.txt (line 3))\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting filelock (from torch->-r requirements.txt (line 5))\n",
            "  Downloading filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting typing-extensions>=4.10.0 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting sympy>=1.13.3 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting networkx (from torch->-r requirements.txt (line 5))\n",
            "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting jinja2 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting fsspec (from torch->-r requirements.txt (line 5))\n",
            "  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.9.90 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Collecting nvidia-nccl-cu12==2.27.3 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.8.90 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.4.0 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/site-packages (from triton==3.4.0->torch->-r requirements.txt (line 5)) (65.6.3)\n",
            "Collecting joblib>=1.2.0 (from scikit-learn->-r requirements.txt (line 9))\n",
            "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn->-r requirements.txt (line 9))\n",
            "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting six>=1.5 (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 3))\n",
            "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch->-r requirements.txt (line 5))\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/site-packages (from huggingface_hub->timm->-r requirements.txt (line 1)) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/site-packages (from huggingface_hub->timm->-r requirements.txt (line 1)) (4.67.1)\n",
            "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface_hub->timm->-r requirements.txt (line 1))\n",
            "  Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch->-r requirements.txt (line 5))\n",
            "  Downloading markupsafe-3.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 1)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 1)) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 1)) (2024.12.14)\n",
            "Downloading timm-1.0.20-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m141.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.10.6-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m179.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchprofile-0.0.4-py3-none-any.whl (7.7 kB)\n",
            "Downloading torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl (888.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m888.1/888.1 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m116.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "Downloading triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m155.5/155.5 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.23.0-cp311-cp311-manylinux_2_28_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m101.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.16.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m35.9/35.9 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m107.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m112.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading contourpy-1.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (355 kB)\n",
            "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading fonttools-4.60.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (5.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m96.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
            "Downloading kiwisolver-1.4.9-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
            "Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
            "Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\n",
            "Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
            "Downloading huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyyaml-6.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (806 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m806.6/806.6 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
            "Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markupsafe-3.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\n",
            "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: nvidia-cusparselt-cu12, mpmath, typing-extensions, triton, threadpoolctl, sympy, six, safetensors, pyyaml, pyparsing, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, kiwisolver, joblib, hf-xet, fsspec, fonttools, filelock, cycler, scipy, python-dateutil, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, jinja2, huggingface_hub, contourpy, scikit-learn, nvidia-cusolver-cu12, matplotlib, torch, torchvision, torchprofile, timm\n",
            "Successfully installed MarkupSafe-3.0.3 contourpy-1.3.3 cycler-0.12.1 filelock-3.19.1 fonttools-4.60.0 fsspec-2025.9.0 hf-xet-1.1.10 huggingface_hub-0.35.3 jinja2-3.1.6 joblib-1.5.2 kiwisolver-1.4.9 matplotlib-3.10.6 mpmath-1.3.0 networkx-3.5 numpy-2.3.3 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 pillow-11.3.0 pyparsing-3.2.5 python-dateutil-2.9.0.post0 pyyaml-6.0.3 safetensors-0.6.2 scikit-learn-1.7.2 scipy-1.16.2 six-1.17.0 sympy-1.14.0 threadpoolctl-3.6.0 timm-1.0.20 torch-2.8.0 torchprofile-0.0.4 torchvision-0.23.0 triton-3.4.0 typing-extensions-4.15.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "cycler",
                  "dateutil",
                  "kiwisolver",
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy",
                  "pyparsing",
                  "six"
                ]
              },
              "id": "5e9c917c5f81486e8bc6a6c737f1745b"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/HiDePrompt/HiDePrompt/"
      ],
      "metadata": {
        "id": "nTIVenS1Ih7Q",
        "outputId": "7a8a3d9a-5fdc-4cc1-e864-d35fc92ee4ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "nTIVenS1Ih7Q",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "383f6206",
      "metadata": {
        "id": "383f6206",
        "outputId": "887b9b8c-d03c-4914-e0cd-8eeb8ae0f9ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(subparser_name='cifar100_hideprompt_5e', pct=0.1, batch_size=24, epochs=10, original_model='vit_small_patch16_224.dino', model='vit_small_patch16_224.dino', input_size=224, pretrained=True, drop=0.0, drop_path=0.0, opt='adam', opt_eps=1e-08, opt_betas=(0.9, 0.999), clip_grad=1.0, momentum=0.9, weight_decay=0.0, reinit_optimizer=True, sched='constant', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=0, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, unscale_lr=True, color_jitter=None, aa=None, smoothing=0.1, train_interpolation='bicubic', reprob=0.0, remode='pixel', recount=1, data_path='./datasets/', dataset='Split-CIFAR100', shuffle=False, output_dir='./output/cifar100_full_dino_5epoch_10pct', device='cuda', seed=20, eval=False, num_workers=4, pin_mem=True, world_size=1, dist_url='env://', num_tasks=10, train_mask=True, task_inc=False, use_g_prompt=False, g_prompt_length=5, g_prompt_layer_idx=[], use_prefix_tune_for_g_prompt=False, use_e_prompt=True, e_prompt_layer_idx=[0, 1, 2, 3, 4], use_prefix_tune_for_e_prompt=True, larger_prompt_lr=False, prompt_pool=True, size=10, length=20, top_k=1, initializer='uniform', prompt_key=False, prompt_key_init='uniform', use_prompt_mask=True, mask_first_epoch=False, shared_prompt_pool=True, shared_prompt_key=False, batchwise_prompt=False, embedding_key='cls', predefined_key='', pull_constraint=True, pull_constraint_coeff=1.0, same_key_value=False, global_pool='token', head_type='token', freeze=['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed'], crct_epochs=10, train_inference_task_only=True, original_model_mlp_structure=[2], ca_lr=0.005, milestones=[10], trained_original_model='', prompt_momentum=0.01, reg=0.01, not_train_ca=False, ca_epochs=30, ca_storage_efficient_method='multi-centroid', n_centroids=10, print_freq=10, config='cifar100_hideprompt_5e')\n",
            "| distributed init (rank 0): env://\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "[rank0]:[W1008 14:06:38.521404037 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/helpers.py:7: FutureWarning: Importing from timm.models.helpers is deprecated, please import via timm.models\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:886: UserWarning: Overwriting vit_tiny_patch16_224 in registry with vits.hide_prompt_vision_transformer.vit_tiny_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:895: UserWarning: Overwriting vit_tiny_patch16_384 in registry with vits.hide_prompt_vision_transformer.vit_tiny_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:904: UserWarning: Overwriting vit_small_patch32_224 in registry with vits.hide_prompt_vision_transformer.vit_small_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:913: UserWarning: Overwriting vit_small_patch32_384 in registry with vits.hide_prompt_vision_transformer.vit_small_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:922: UserWarning: Overwriting vit_small_patch16_224 in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:932: UserWarning: Overwriting vit_small_patch16_384 in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:942: UserWarning: Overwriting vit_base_patch32_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:952: UserWarning: Overwriting vit_base_patch32_384 in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:962: UserWarning: Overwriting vit_base_patch16_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:972: UserWarning: Overwriting vit_base_patch16_384 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:982: UserWarning: Overwriting vit_base_patch8_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:992: UserWarning: Overwriting vit_large_patch32_224 in registry with vits.hide_prompt_vision_transformer.vit_large_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1001: UserWarning: Overwriting vit_large_patch32_384 in registry with vits.hide_prompt_vision_transformer.vit_large_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1011: UserWarning: Overwriting vit_large_patch16_224 in registry with vits.hide_prompt_vision_transformer.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1021: UserWarning: Overwriting vit_large_patch16_384 in registry with vits.hide_prompt_vision_transformer.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1031: UserWarning: Overwriting vit_large_patch14_224 in registry with vits.hide_prompt_vision_transformer.vit_large_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1040: UserWarning: Overwriting vit_huge_patch14_224 in registry with vits.hide_prompt_vision_transformer.vit_huge_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1049: UserWarning: Overwriting vit_giant_patch14_224 in registry with vits.hide_prompt_vision_transformer.vit_giant_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1058: UserWarning: Overwriting vit_gigantic_patch14_224 in registry with vits.hide_prompt_vision_transformer.vit_gigantic_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1067: UserWarning: Overwriting vit_tiny_patch16_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_tiny_patch16_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1078: UserWarning: Overwriting vit_small_patch32_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_small_patch32_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1089: UserWarning: Overwriting vit_small_patch16_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1100: UserWarning: Overwriting vit_base_patch32_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1111: UserWarning: Overwriting vit_base_patch16_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1122: UserWarning: Overwriting vit_base_patch8_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_base_patch8_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1133: UserWarning: Overwriting vit_large_patch32_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_large_patch32_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1144: UserWarning: Overwriting vit_large_patch16_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_large_patch16_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1155: UserWarning: Overwriting vit_huge_patch14_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_huge_patch14_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1166: UserWarning: Overwriting vit_base_patch16_224_sam in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_sam. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1175: UserWarning: Overwriting vit_base_patch32_224_sam in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_224_sam. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1184: UserWarning: Overwriting vit_small_patch16_224_dino in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_224_dino. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1193: UserWarning: Overwriting vit_small_patch8_224_dino in registry with vits.hide_prompt_vision_transformer.vit_small_patch8_224_dino. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1211: UserWarning: Overwriting vit_base_patch8_224_dino in registry with vits.hide_prompt_vision_transformer.vit_base_patch8_224_dino. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1220: UserWarning: Overwriting vit_base_patch16_224_miil_in21k in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_miil_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1230: UserWarning: Overwriting vit_base_patch16_224_miil in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_miil. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1242: UserWarning: Overwriting vit_base_patch32_plus_256 in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_plus_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1251: UserWarning: Overwriting vit_base_patch16_plus_240 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_plus_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1260: UserWarning: Overwriting vit_base_patch16_rpn_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_rpn_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1271: UserWarning: Overwriting vit_small_patch16_36x1_224 in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_36x1_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1282: UserWarning: Overwriting vit_small_patch16_18x2_224 in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1294: UserWarning: Overwriting vit_base_patch16_18x2_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1331: UserWarning: Overwriting vit_base_patch16_224_dino in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_dino. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "Original train size:  50000\n",
            "Sampled train size:  5000\n",
            "Original train size:  50000\n",
            "Sampled train size:  5000\n",
            "100\n",
            "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, 13, 14, 15, 16, 17, 18, 19], [20, 21, 22, 23, 24, 25, 26, 27, 28, 29], [30, 31, 32, 33, 34, 35, 36, 37, 38, 39], [40, 41, 42, 43, 44, 45, 46, 47, 48, 49], [50, 51, 52, 53, 54, 55, 56, 57, 58, 59], [60, 61, 62, 63, 64, 65, 66, 67, 68, 69], [70, 71, 72, 73, 74, 75, 76, 77, 78, 79], [80, 81, 82, 83, 84, 85, 86, 87, 88, 89], [90, 91, 92, 93, 94, 95, 96, 97, 98, 99]]\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Creating original model: vit_small_patch16_224.dino\n",
            "[Sequential(\n",
            "  (0): Linear(in_features=384, out_features=768, bias=True)\n",
            "  (1): GELU(approximate='none')\n",
            "  (2): Dropout(p=0.0, inplace=False)\n",
            "), Sequential(\n",
            "  (0): Linear(in_features=768, out_features=384, bias=True)\n",
            "  (1): Dropout(p=0.0, inplace=False)\n",
            ")]\n",
            "model.safetensors: 100% 86.7M/86.7M [00:01<00:00, 51.9MB/s]\n",
            "Namespace(subparser_name='cifar100_hideprompt_5e', pct=0.1, batch_size=24, epochs=10, original_model='vit_small_patch16_224.dino', model='vit_small_patch16_224.dino', input_size=224, pretrained=True, drop=0.0, drop_path=0.0, opt='adam', opt_eps=1e-08, opt_betas=(0.9, 0.999), clip_grad=1.0, momentum=0.9, weight_decay=0.0, reinit_optimizer=True, sched='constant', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=0, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, unscale_lr=True, color_jitter=None, aa=None, smoothing=0.1, train_interpolation='bicubic', reprob=0.0, remode='pixel', recount=1, data_path='./datasets/', dataset='Split-CIFAR100', shuffle=False, output_dir='./output/cifar100_full_dino_5epoch_10pct', device='cuda', seed=20, eval=False, num_workers=4, pin_mem=True, world_size=1, dist_url='env://', num_tasks=10, train_mask=True, task_inc=False, use_g_prompt=False, g_prompt_length=5, g_prompt_layer_idx=[], use_prefix_tune_for_g_prompt=False, use_e_prompt=True, e_prompt_layer_idx=[0, 1, 2, 3, 4], use_prefix_tune_for_e_prompt=True, larger_prompt_lr=False, prompt_pool=True, size=10, length=20, top_k=1, initializer='uniform', prompt_key=False, prompt_key_init='uniform', use_prompt_mask=True, mask_first_epoch=False, shared_prompt_pool=True, shared_prompt_key=False, batchwise_prompt=False, embedding_key='cls', predefined_key='', pull_constraint=True, pull_constraint_coeff=1.0, same_key_value=False, global_pool='token', head_type='token', freeze=['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed'], crct_epochs=10, train_inference_task_only=True, original_model_mlp_structure=[2], ca_lr=0.005, milestones=[10], trained_original_model='', prompt_momentum=0.01, reg=0.01, not_train_ca=False, ca_epochs=30, ca_storage_efficient_method='multi-centroid', n_centroids=10, print_freq=10, config='cifar100_hideprompt_5e', rank=0, gpu=0, distributed=True, dist_backend='nccl', nb_classes=100)\n",
            "number of params: 630244\n",
            "Start training for 10 epochs\n",
            "Train: Epoch[ 1/10]  [ 0/23]  eta: 0:01:09  Lr: 0.000047  Loss: 2.4446  Acc@1: 4.1667 (4.1667)  Acc@5: 45.8333 (45.8333)  time: 3.0238  data: 1.2893  max mem: 196\n",
            "Train: Epoch[ 1/10]  [10/23]  eta: 0:00:04  Lr: 0.000047  Loss: 2.2694  Acc@1: 12.5000 (9.4697)  Acc@5: 54.1667 (53.7879)  time: 0.3482  data: 0.1182  max mem: 217\n",
            "Train: Epoch[ 1/10]  [20/23]  eta: 0:00:00  Lr: 0.000047  Loss: 2.1486  Acc@1: 16.6667 (14.2857)  Acc@5: 62.5000 (60.7143)  time: 0.0808  data: 0.0008  max mem: 217\n",
            "Train: Epoch[ 1/10]  [22/23]  eta: 0:00:00  Lr: 0.000047  Loss: 2.2717  Acc@1: 16.6667 (14.5522)  Acc@5: 62.5000 (61.0075)  time: 0.0816  data: 0.0007  max mem: 217\n",
            "Train: Epoch[ 1/10] Total time: 0:00:04 (0.2116 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000047  Loss: 2.2717  Acc@1: 16.6667 (14.5522)  Acc@5: 62.5000 (61.0075)\n",
            "Train: Epoch[ 2/10]  [ 0/23]  eta: 0:00:08  Lr: 0.000047  Loss: 2.2233  Acc@1: 12.5000 (12.5000)  Acc@5: 62.5000 (62.5000)  time: 0.3491  data: 0.2695  max mem: 217\n",
            "Train: Epoch[ 2/10]  [10/23]  eta: 0:00:01  Lr: 0.000047  Loss: 2.1777  Acc@1: 29.1667 (26.5152)  Acc@5: 75.0000 (75.0000)  time: 0.1190  data: 0.0323  max mem: 218\n",
            "Train: Epoch[ 2/10]  [20/23]  eta: 0:00:00  Lr: 0.000047  Loss: 2.1104  Acc@1: 29.1667 (28.7698)  Acc@5: 79.1667 (77.7778)  time: 0.0912  data: 0.0045  max mem: 218\n",
            "Train: Epoch[ 2/10]  [22/23]  eta: 0:00:00  Lr: 0.000047  Loss: 2.1508  Acc@1: 29.1667 (28.5448)  Acc@5: 79.1667 (78.1716)  time: 0.0846  data: 0.0007  max mem: 218\n",
            "Train: Epoch[ 2/10] Total time: 0:00:02 (0.1050 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 2.1508  Acc@1: 29.1667 (28.5448)  Acc@5: 79.1667 (78.1716)\n",
            "Train: Epoch[ 3/10]  [ 0/23]  eta: 0:00:13  Lr: 0.000047  Loss: 2.0407  Acc@1: 37.5000 (37.5000)  Acc@5: 83.3333 (83.3333)  time: 0.6002  data: 0.5019  max mem: 218\n",
            "Train: Epoch[ 3/10]  [10/23]  eta: 0:00:01  Lr: 0.000047  Loss: 1.9859  Acc@1: 41.6667 (42.8030)  Acc@5: 83.3333 (85.6061)  time: 0.1479  data: 0.0556  max mem: 218\n",
            "Train: Epoch[ 3/10]  [20/23]  eta: 0:00:00  Lr: 0.000047  Loss: 1.9601  Acc@1: 45.8333 (46.4286)  Acc@5: 87.5000 (86.9048)  time: 0.1000  data: 0.0079  max mem: 218\n",
            "Train: Epoch[ 3/10]  [22/23]  eta: 0:00:00  Lr: 0.000047  Loss: 1.6656  Acc@1: 50.0000 (47.5746)  Acc@5: 91.6667 (87.5000)  time: 0.0961  data: 0.0077  max mem: 218\n",
            "Train: Epoch[ 3/10] Total time: 0:00:02 (0.1212 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.6656  Acc@1: 50.0000 (47.5746)  Acc@5: 91.6667 (87.5000)\n",
            "Train: Epoch[ 4/10]  [ 0/23]  eta: 0:00:09  Lr: 0.000047  Loss: 1.8733  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  time: 0.4282  data: 0.2861  max mem: 218\n",
            "Train: Epoch[ 4/10]  [10/23]  eta: 0:00:01  Lr: 0.000047  Loss: 1.9630  Acc@1: 54.1667 (54.9242)  Acc@5: 91.6667 (93.1818)  time: 0.1291  data: 0.0309  max mem: 218\n",
            "Train: Epoch[ 4/10]  [20/23]  eta: 0:00:00  Lr: 0.000047  Loss: 1.7245  Acc@1: 58.3333 (57.5397)  Acc@5: 91.6667 (90.2778)  time: 0.0897  data: 0.0033  max mem: 218\n",
            "Train: Epoch[ 4/10]  [22/23]  eta: 0:00:00  Lr: 0.000047  Loss: 1.7460  Acc@1: 58.3333 (57.6493)  Acc@5: 87.5000 (89.5522)  time: 0.0842  data: 0.0010  max mem: 218\n",
            "Train: Epoch[ 4/10] Total time: 0:00:02 (0.1049 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.7460  Acc@1: 58.3333 (57.6493)  Acc@5: 87.5000 (89.5522)\n",
            "Train: Epoch[ 5/10]  [ 0/23]  eta: 0:00:07  Lr: 0.000047  Loss: 1.7427  Acc@1: 58.3333 (58.3333)  Acc@5: 91.6667 (91.6667)  time: 0.3320  data: 0.2426  max mem: 218\n",
            "Train: Epoch[ 5/10]  [10/23]  eta: 0:00:01  Lr: 0.000047  Loss: 1.6558  Acc@1: 62.5000 (62.5000)  Acc@5: 91.6667 (92.8030)  time: 0.1222  data: 0.0370  max mem: 218\n",
            "Train: Epoch[ 5/10]  [20/23]  eta: 0:00:00  Lr: 0.000047  Loss: 1.3737  Acc@1: 66.6667 (65.6746)  Acc@5: 95.8333 (94.0476)  time: 0.0916  data: 0.0083  max mem: 218\n",
            "Train: Epoch[ 5/10]  [22/23]  eta: 0:00:00  Lr: 0.000047  Loss: 1.4343  Acc@1: 66.6667 (66.6045)  Acc@5: 95.8333 (94.0299)  time: 0.0813  data: 0.0006  max mem: 218\n",
            "Train: Epoch[ 5/10] Total time: 0:00:02 (0.1020 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.4343  Acc@1: 66.6667 (66.6045)  Acc@5: 95.8333 (94.0299)\n",
            "Train: Epoch[ 6/10]  [ 0/23]  eta: 0:00:08  Lr: 0.000047  Loss: 1.6509  Acc@1: 58.3333 (58.3333)  Acc@5: 100.0000 (100.0000)  time: 0.3609  data: 0.2645  max mem: 218\n",
            "Train: Epoch[ 6/10]  [10/23]  eta: 0:00:01  Lr: 0.000047  Loss: 1.3734  Acc@1: 70.8333 (70.0758)  Acc@5: 95.8333 (95.8333)  time: 0.1170  data: 0.0286  max mem: 218\n",
            "Train: Epoch[ 6/10]  [20/23]  eta: 0:00:00  Lr: 0.000047  Loss: 1.3177  Acc@1: 70.8333 (71.4286)  Acc@5: 95.8333 (95.2381)  time: 0.0869  data: 0.0027  max mem: 218\n",
            "Train: Epoch[ 6/10]  [22/23]  eta: 0:00:00  Lr: 0.000047  Loss: 1.4211  Acc@1: 70.8333 (71.2687)  Acc@5: 95.8333 (95.1493)  time: 0.0818  data: 0.0003  max mem: 218\n",
            "Train: Epoch[ 6/10] Total time: 0:00:02 (0.0993 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.4211  Acc@1: 70.8333 (71.2687)  Acc@5: 95.8333 (95.1493)\n",
            "Train: Epoch[ 7/10]  [ 0/23]  eta: 0:00:10  Lr: 0.000047  Loss: 1.2684  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.4544  data: 0.3643  max mem: 218\n",
            "Train: Epoch[ 7/10]  [10/23]  eta: 0:00:01  Lr: 0.000047  Loss: 1.1345  Acc@1: 75.0000 (76.1364)  Acc@5: 95.8333 (96.2121)  time: 0.1196  data: 0.0339  max mem: 218\n",
            "Train: Epoch[ 7/10]  [20/23]  eta: 0:00:00  Lr: 0.000047  Loss: 1.0185  Acc@1: 75.0000 (78.1746)  Acc@5: 95.8333 (96.8254)  time: 0.0836  data: 0.0005  max mem: 218\n",
            "Train: Epoch[ 7/10]  [22/23]  eta: 0:00:00  Lr: 0.000047  Loss: 1.0452  Acc@1: 75.0000 (77.6119)  Acc@5: 95.8333 (96.4552)  time: 0.0806  data: 0.0005  max mem: 218\n",
            "Train: Epoch[ 7/10] Total time: 0:00:02 (0.1024 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.0452  Acc@1: 75.0000 (77.6119)  Acc@5: 95.8333 (96.4552)\n",
            "Train: Epoch[ 8/10]  [ 0/23]  eta: 0:00:18  Lr: 0.000047  Loss: 1.3136  Acc@1: 62.5000 (62.5000)  Acc@5: 91.6667 (91.6667)  time: 0.7847  data: 0.6088  max mem: 218\n",
            "Train: Epoch[ 8/10]  [10/23]  eta: 0:00:02  Lr: 0.000047  Loss: 0.8666  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (96.2121)  time: 0.1775  data: 0.0606  max mem: 218\n",
            "Train: Epoch[ 8/10]  [20/23]  eta: 0:00:00  Lr: 0.000047  Loss: 0.9131  Acc@1: 79.1667 (76.9841)  Acc@5: 95.8333 (96.4286)  time: 0.1113  data: 0.0094  max mem: 218\n",
            "Train: Epoch[ 8/10]  [22/23]  eta: 0:00:00  Lr: 0.000047  Loss: 1.0799  Acc@1: 79.1667 (77.6119)  Acc@5: 95.8333 (96.4552)  time: 0.1066  data: 0.0094  max mem: 218\n",
            "Train: Epoch[ 8/10] Total time: 0:00:03 (0.1420 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.0799  Acc@1: 79.1667 (77.6119)  Acc@5: 95.8333 (96.4552)\n",
            "Train: Epoch[ 9/10]  [ 0/23]  eta: 0:00:20  Lr: 0.000047  Loss: 1.0929  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  time: 0.8704  data: 0.7360  max mem: 218\n",
            "Train: Epoch[ 9/10]  [10/23]  eta: 0:00:02  Lr: 0.000047  Loss: 0.6721  Acc@1: 79.1667 (78.0303)  Acc@5: 100.0000 (98.1061)  time: 0.1817  data: 0.0681  max mem: 218\n",
            "Train: Epoch[ 9/10]  [20/23]  eta: 0:00:00  Lr: 0.000047  Loss: 0.7630  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (98.4127)  time: 0.1078  data: 0.0020  max mem: 218\n",
            "Train: Epoch[ 9/10]  [22/23]  eta: 0:00:00  Lr: 0.000047  Loss: 1.2575  Acc@1: 79.1667 (78.9179)  Acc@5: 100.0000 (98.5075)  time: 0.1029  data: 0.0018  max mem: 218\n",
            "Train: Epoch[ 9/10] Total time: 0:00:03 (0.1414 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.2575  Acc@1: 79.1667 (78.9179)  Acc@5: 100.0000 (98.5075)\n",
            "Train: Epoch[10/10]  [ 0/23]  eta: 0:00:14  Lr: 0.000047  Loss: 0.9354  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  time: 0.6254  data: 0.5042  max mem: 218\n",
            "Train: Epoch[10/10]  [10/23]  eta: 0:00:01  Lr: 0.000047  Loss: 0.8938  Acc@1: 79.1667 (80.6818)  Acc@5: 100.0000 (98.4848)  time: 0.1375  data: 0.0473  max mem: 218\n",
            "Train: Epoch[10/10]  [20/23]  eta: 0:00:00  Lr: 0.000047  Loss: 0.8502  Acc@1: 83.3333 (82.5397)  Acc@5: 100.0000 (98.4127)  time: 0.0860  data: 0.0012  max mem: 218\n",
            "Train: Epoch[10/10]  [22/23]  eta: 0:00:00  Lr: 0.000047  Loss: 0.6239  Acc@1: 83.3333 (81.7164)  Acc@5: 100.0000 (98.5075)  time: 0.0816  data: 0.0009  max mem: 218\n",
            "Train: Epoch[10/10] Total time: 0:00:02 (0.1105 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.6239  Acc@1: 83.3333 (81.7164)  Acc@5: 100.0000 (98.5075)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:12  Loss: 1.9617 (1.9617)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  time: 0.3067  data: 0.2209  max mem: 218\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 2.0206 (2.0027)  Acc@1: 87.5000 (89.3939)  Acc@5: 100.0000 (98.4848)  time: 0.1075  data: 0.0280  max mem: 218\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 2.0206 (2.0216)  Acc@1: 87.5000 (88.8889)  Acc@5: 100.0000 (98.2143)  time: 0.0854  data: 0.0045  max mem: 218\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 1.9738 (1.9915)  Acc@1: 87.5000 (90.0538)  Acc@5: 95.8333 (97.8495)  time: 0.0839  data: 0.0005  max mem: 218\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 1.9190 (1.9833)  Acc@1: 91.6667 (90.1423)  Acc@5: 100.0000 (98.0691)  time: 0.0845  data: 0.0004  max mem: 218\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 1.9004 (1.9801)  Acc@1: 91.6667 (90.1000)  Acc@5: 100.0000 (98.1000)  time: 0.0878  data: 0.0004  max mem: 218\n",
            "Test: [Task 1] Total time: 0:00:03 (0.0935 s / it)\n",
            "* Acc@1 90.100 Acc@5 98.100 loss 1.980\n",
            "[Average accuracy till task1]\tAcc@1: 90.1000\tAcc@5: 98.1000\tLoss: 1.9801\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:16  Loss: 1.9617 (1.9617)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  time: 0.3874  data: 0.3001  max mem: 218\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 2.0206 (2.0027)  Acc@1: 87.5000 (89.3939)  Acc@5: 100.0000 (98.4848)  time: 0.1131  data: 0.0289  max mem: 218\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 2.0206 (2.0216)  Acc@1: 87.5000 (88.8889)  Acc@5: 100.0000 (98.2143)  time: 0.0851  data: 0.0011  max mem: 218\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 1.9738 (1.9915)  Acc@1: 87.5000 (90.0538)  Acc@5: 95.8333 (97.8495)  time: 0.0853  data: 0.0004  max mem: 218\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 1.9190 (1.9833)  Acc@1: 91.6667 (90.1423)  Acc@5: 100.0000 (98.0691)  time: 0.0865  data: 0.0004  max mem: 218\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 1.9004 (1.9801)  Acc@1: 91.6667 (90.1000)  Acc@5: 100.0000 (98.1000)  time: 0.0846  data: 0.0003  max mem: 218\n",
            "Test: [Task 1] Total time: 0:00:03 (0.0944 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@1 90.100 Acc@5 98.100 loss 1.980\n",
            "[Average accuracy till task1]\tAcc@1: 90.1000\tAcc@5: 98.1000\tLoss: 1.9801\n",
            "Train: Epoch[ 1/10]  [ 0/20]  eta: 0:00:08  Lr: 0.000047  Loss: 2.4646  Acc@1: 8.3333 (8.3333)  Acc@5: 33.3333 (33.3333)  time: 0.4121  data: 0.2962  max mem: 218\n",
            "Train: Epoch[ 1/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 2.3387  Acc@1: 16.6667 (15.1515)  Acc@5: 58.3333 (56.0606)  time: 0.1243  data: 0.0310  max mem: 223\n",
            "Train: Epoch[ 1/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 2.1848  Acc@1: 16.6667 (15.8664)  Acc@5: 62.5000 (62.0042)  time: 0.1072  data: 0.0172  max mem: 223\n",
            "Train: Epoch[ 1/10] Total time: 0:00:02 (0.1108 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 2.1848  Acc@1: 16.6667 (15.8664)  Acc@5: 62.5000 (62.0042)\n",
            "Train: Epoch[ 2/10]  [ 0/20]  eta: 0:00:10  Lr: 0.000047  Loss: 2.2291  Acc@1: 16.6667 (16.6667)  Acc@5: 58.3333 (58.3333)  time: 0.5004  data: 0.3968  max mem: 223\n",
            "Train: Epoch[ 2/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 1.9894  Acc@1: 25.0000 (25.0000)  Acc@5: 70.8333 (68.9394)  time: 0.1271  data: 0.0371  max mem: 223\n",
            "Train: Epoch[ 2/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 2.0952  Acc@1: 29.1667 (27.9749)  Acc@5: 75.0000 (72.4426)  time: 0.1094  data: 0.0205  max mem: 223\n",
            "Train: Epoch[ 2/10] Total time: 0:00:02 (0.1131 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 2.0952  Acc@1: 29.1667 (27.9749)  Acc@5: 75.0000 (72.4426)\n",
            "Train: Epoch[ 3/10]  [ 0/20]  eta: 0:00:09  Lr: 0.000047  Loss: 2.1092  Acc@1: 25.0000 (25.0000)  Acc@5: 66.6667 (66.6667)  time: 0.4727  data: 0.3687  max mem: 223\n",
            "Train: Epoch[ 3/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 2.0034  Acc@1: 41.6667 (40.5303)  Acc@5: 83.3333 (83.7121)  time: 0.1351  data: 0.0342  max mem: 223\n",
            "Train: Epoch[ 3/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 1.8779  Acc@1: 41.6667 (42.5887)  Acc@5: 87.5000 (86.2213)  time: 0.1147  data: 0.0190  max mem: 223\n",
            "Train: Epoch[ 3/10] Total time: 0:00:02 (0.1240 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.8779  Acc@1: 41.6667 (42.5887)  Acc@5: 87.5000 (86.2213)\n",
            "Train: Epoch[ 4/10]  [ 0/20]  eta: 0:00:13  Lr: 0.000047  Loss: 1.8656  Acc@1: 45.8333 (45.8333)  Acc@5: 91.6667 (91.6667)  time: 0.6783  data: 0.5581  max mem: 223\n",
            "Train: Epoch[ 4/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 1.7567  Acc@1: 50.0000 (50.0000)  Acc@5: 91.6667 (90.9091)  time: 0.1613  data: 0.0564  max mem: 223\n",
            "Train: Epoch[ 4/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 1.6614  Acc@1: 50.0000 (50.5219)  Acc@5: 91.6667 (90.8142)  time: 0.1315  data: 0.0312  max mem: 223\n",
            "Train: Epoch[ 4/10] Total time: 0:00:02 (0.1368 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.6614  Acc@1: 50.0000 (50.5219)  Acc@5: 91.6667 (90.8142)\n",
            "Train: Epoch[ 5/10]  [ 0/20]  eta: 0:00:08  Lr: 0.000047  Loss: 1.4961  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.8333)  time: 0.4291  data: 0.3226  max mem: 223\n",
            "Train: Epoch[ 5/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 1.4410  Acc@1: 54.1667 (57.9545)  Acc@5: 91.6667 (93.5606)  time: 0.1224  data: 0.0307  max mem: 223\n",
            "Train: Epoch[ 5/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 1.6391  Acc@1: 58.3333 (60.3340)  Acc@5: 95.8333 (95.8246)  time: 0.1079  data: 0.0170  max mem: 223\n",
            "Train: Epoch[ 5/10] Total time: 0:00:02 (0.1117 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.6391  Acc@1: 58.3333 (60.3340)  Acc@5: 95.8333 (95.8246)\n",
            "Train: Epoch[ 6/10]  [ 0/20]  eta: 0:00:07  Lr: 0.000047  Loss: 1.4564  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.8333)  time: 0.3515  data: 0.2546  max mem: 223\n",
            "Train: Epoch[ 6/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 1.7823  Acc@1: 66.6667 (65.1515)  Acc@5: 95.8333 (95.0758)  time: 0.1218  data: 0.0283  max mem: 223\n",
            "Train: Epoch[ 6/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 1.3874  Acc@1: 70.8333 (66.3883)  Acc@5: 95.8333 (95.4071)  time: 0.1072  data: 0.0156  max mem: 223\n",
            "Train: Epoch[ 6/10] Total time: 0:00:02 (0.1111 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.3874  Acc@1: 70.8333 (66.3883)  Acc@5: 95.8333 (95.4071)\n",
            "Train: Epoch[ 7/10]  [ 0/20]  eta: 0:00:09  Lr: 0.000047  Loss: 1.3684  Acc@1: 62.5000 (62.5000)  Acc@5: 95.8333 (95.8333)  time: 0.4545  data: 0.3460  max mem: 223\n",
            "Train: Epoch[ 7/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 1.0280  Acc@1: 66.6667 (67.4242)  Acc@5: 95.8333 (96.2121)  time: 0.1270  data: 0.0328  max mem: 223\n",
            "Train: Epoch[ 7/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 1.3645  Acc@1: 66.6667 (67.8497)  Acc@5: 95.8333 (96.0334)  time: 0.1110  data: 0.0182  max mem: 223\n",
            "Train: Epoch[ 7/10] Total time: 0:00:02 (0.1145 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.3645  Acc@1: 66.6667 (67.8497)  Acc@5: 95.8333 (96.0334)\n",
            "Train: Epoch[ 8/10]  [ 0/20]  eta: 0:00:09  Lr: 0.000047  Loss: 1.0192  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.4653  data: 0.3525  max mem: 223\n",
            "Train: Epoch[ 8/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 1.1572  Acc@1: 79.1667 (76.8939)  Acc@5: 100.0000 (97.3485)  time: 0.1282  data: 0.0339  max mem: 223\n",
            "Train: Epoch[ 8/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 1.1571  Acc@1: 73.9130 (74.1127)  Acc@5: 95.8333 (97.2860)  time: 0.1117  data: 0.0187  max mem: 223\n",
            "Train: Epoch[ 8/10] Total time: 0:00:02 (0.1155 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.1571  Acc@1: 73.9130 (74.1127)  Acc@5: 95.8333 (97.2860)\n",
            "Train: Epoch[ 9/10]  [ 0/20]  eta: 0:00:07  Lr: 0.000047  Loss: 1.1392  Acc@1: 83.3333 (83.3333)  Acc@5: 91.6667 (91.6667)  time: 0.3681  data: 0.2585  max mem: 223\n",
            "Train: Epoch[ 9/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 1.1026  Acc@1: 75.0000 (73.8636)  Acc@5: 95.8333 (96.5909)  time: 0.1335  data: 0.0355  max mem: 223\n",
            "Train: Epoch[ 9/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 0.9543  Acc@1: 75.0000 (74.3215)  Acc@5: 95.8333 (96.4509)  time: 0.1152  data: 0.0200  max mem: 223\n",
            "Train: Epoch[ 9/10] Total time: 0:00:02 (0.1204 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.9543  Acc@1: 75.0000 (74.3215)  Acc@5: 95.8333 (96.4509)\n",
            "Train: Epoch[10/10]  [ 0/20]  eta: 0:00:18  Lr: 0.000047  Loss: 1.0956  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  time: 0.9089  data: 0.7957  max mem: 223\n",
            "Train: Epoch[10/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 1.0666  Acc@1: 75.0000 (74.2424)  Acc@5: 95.8333 (97.3485)  time: 0.1802  data: 0.0740  max mem: 223\n",
            "Train: Epoch[10/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 0.8274  Acc@1: 75.0000 (75.3653)  Acc@5: 95.8333 (97.4948)  time: 0.1407  data: 0.0409  max mem: 223\n",
            "Train: Epoch[10/10] Total time: 0:00:02 (0.1449 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.8274  Acc@1: 75.0000 (75.3653)  Acc@5: 95.8333 (97.4948)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:13  Loss: 2.0127 (2.0127)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.3261  data: 0.2495  max mem: 223\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 2.0735 (2.0714)  Acc@1: 83.3333 (82.5758)  Acc@5: 100.0000 (98.1061)  time: 0.1185  data: 0.0264  max mem: 223\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 2.0735 (2.0872)  Acc@1: 79.1667 (81.7460)  Acc@5: 100.0000 (97.8175)  time: 0.0957  data: 0.0023  max mem: 223\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 2.0212 (2.0613)  Acc@1: 83.3333 (83.4677)  Acc@5: 95.8333 (97.5806)  time: 0.0938  data: 0.0006  max mem: 223\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 1.9980 (2.0497)  Acc@1: 83.3333 (84.0447)  Acc@5: 95.8333 (97.6626)  time: 0.0944  data: 0.0004  max mem: 223\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 1.9843 (2.0455)  Acc@1: 87.5000 (84.1000)  Acc@5: 95.8333 (97.6000)  time: 0.0931  data: 0.0004  max mem: 223\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1017 s / it)\n",
            "* Acc@1 84.100 Acc@5 97.600 loss 2.045\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:13  Loss: 2.6762 (2.6762)  Acc@1: 66.6667 (66.6667)  Acc@5: 91.6667 (91.6667)  time: 0.3128  data: 0.2359  max mem: 223\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:03  Loss: 2.7366 (2.7501)  Acc@1: 66.6667 (68.5606)  Acc@5: 91.6667 (92.4242)  time: 0.1178  data: 0.0274  max mem: 223\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 2.7204 (2.7219)  Acc@1: 70.8333 (69.2460)  Acc@5: 91.6667 (92.8571)  time: 0.0960  data: 0.0034  max mem: 223\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 2.6647 (2.7101)  Acc@1: 70.8333 (69.3548)  Acc@5: 95.8333 (93.0108)  time: 0.0940  data: 0.0003  max mem: 223\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 2.6763 (2.7108)  Acc@1: 70.8333 (69.0041)  Acc@5: 95.8333 (93.6992)  time: 0.0946  data: 0.0003  max mem: 223\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 2.6647 (2.7039)  Acc@1: 70.8333 (69.2000)  Acc@5: 95.8333 (93.8000)  time: 0.0931  data: 0.0003  max mem: 223\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1015 s / it)\n",
            "* Acc@1 69.200 Acc@5 93.800 loss 2.704\n",
            "[Average accuracy till task2]\tAcc@1: 76.6500\tAcc@5: 95.7000\tLoss: 2.3747\tForgetting: 6.0000\tBackward: -6.0000\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([4152, 384])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 1.2002  Acc@1: 83.3333 (81.2500)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([4152, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 1.1574  Acc@1: 79.1667 (80.8333)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([4152, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 1.1876  Acc@1: 87.5000 (85.4167)  Acc@5: 100.0000 (98.3333)\n",
            "torch.Size([4152, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 1.0846  Acc@1: 87.5000 (88.7500)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([4152, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 0.7919  Acc@1: 87.5000 (89.5833)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([4152, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 0.9053  Acc@1: 83.3333 (87.0833)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([4152, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.9937  Acc@1: 83.3333 (85.8333)  Acc@5: 100.0000 (98.7500)\n",
            "torch.Size([4152, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 0.7244  Acc@1: 87.5000 (87.9167)  Acc@5: 100.0000 (98.3333)\n",
            "torch.Size([4152, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 1.1298  Acc@1: 87.5000 (88.7500)  Acc@5: 100.0000 (98.7500)\n",
            "torch.Size([4152, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.9839  Acc@1: 87.5000 (88.7500)  Acc@5: 100.0000 (99.5833)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:19  Loss: 1.6020 (1.6020)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.4603  data: 0.3600  max mem: 224\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 1.7424 (1.7318)  Acc@1: 83.3333 (81.0606)  Acc@5: 100.0000 (98.4848)  time: 0.1248  data: 0.0336  max mem: 224\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 1.7424 (1.7342)  Acc@1: 83.3333 (81.5476)  Acc@5: 100.0000 (98.2143)  time: 0.0906  data: 0.0007  max mem: 224\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 1.6941 (1.7054)  Acc@1: 83.3333 (83.6022)  Acc@5: 95.8333 (98.1183)  time: 0.0901  data: 0.0005  max mem: 224\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 1.6257 (1.6865)  Acc@1: 87.5000 (84.1463)  Acc@5: 100.0000 (98.2724)  time: 0.0906  data: 0.0004  max mem: 224\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 1.6252 (1.6829)  Acc@1: 87.5000 (84.2000)  Acc@5: 100.0000 (98.3000)  time: 0.0888  data: 0.0004  max mem: 224\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1005 s / it)\n",
            "* Acc@1 84.200 Acc@5 98.300 loss 1.683\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:14  Loss: 2.1041 (2.1041)  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  time: 0.3516  data: 0.2609  max mem: 224\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:03  Loss: 2.1571 (2.1362)  Acc@1: 79.1667 (77.6515)  Acc@5: 95.8333 (96.5909)  time: 0.1224  data: 0.0314  max mem: 224\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 2.1337 (2.1148)  Acc@1: 79.1667 (79.9603)  Acc@5: 95.8333 (96.0317)  time: 0.0964  data: 0.0051  max mem: 224\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 2.0270 (2.1053)  Acc@1: 83.3333 (80.3763)  Acc@5: 95.8333 (95.9677)  time: 0.0924  data: 0.0015  max mem: 224\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 2.0365 (2.1014)  Acc@1: 83.3333 (80.7927)  Acc@5: 100.0000 (96.3415)  time: 0.0916  data: 0.0011  max mem: 224\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 2.0204 (2.0950)  Acc@1: 83.3333 (80.9000)  Acc@5: 100.0000 (96.4000)  time: 0.0900  data: 0.0010  max mem: 224\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1015 s / it)\n",
            "* Acc@1 80.900 Acc@5 96.400 loss 2.095\n",
            "[Average accuracy till task2]\tAcc@1: 82.5500\tAcc@5: 97.3500\tLoss: 1.8889\tForgetting: 5.9000\tBackward: -5.9000\n",
            "Train: Epoch[ 1/10]  [ 0/21]  eta: 0:00:08  Lr: 0.000047  Loss: 2.3818  Acc@1: 8.3333 (8.3333)  Acc@5: 37.5000 (37.5000)  time: 0.3915  data: 0.2870  max mem: 224\n",
            "Train: Epoch[ 1/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 2.3121  Acc@1: 12.5000 (12.5000)  Acc@5: 62.5000 (56.0606)  time: 0.1321  data: 0.0367  max mem: 229\n",
            "Train: Epoch[ 1/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 2.0049  Acc@1: 12.5000 (15.2918)  Acc@5: 62.5000 (59.7586)  time: 0.1026  data: 0.0059  max mem: 229\n",
            "Train: Epoch[ 1/10] Total time: 0:00:02 (0.1230 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 2.0049  Acc@1: 12.5000 (15.2918)  Acc@5: 62.5000 (59.7586)\n",
            "Train: Epoch[ 2/10]  [ 0/21]  eta: 0:00:17  Lr: 0.000047  Loss: 2.1931  Acc@1: 20.8333 (20.8333)  Acc@5: 62.5000 (62.5000)  time: 0.8452  data: 0.7030  max mem: 229\n",
            "Train: Epoch[ 2/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 2.1113  Acc@1: 29.1667 (28.4091)  Acc@5: 79.1667 (79.1667)  time: 0.1764  data: 0.0664  max mem: 229\n",
            "Train: Epoch[ 2/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 1.8010  Acc@1: 29.1667 (29.7787)  Acc@5: 79.1667 (79.4769)  time: 0.0990  data: 0.0015  max mem: 229\n",
            "Train: Epoch[ 2/10] Total time: 0:00:02 (0.1387 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.8010  Acc@1: 29.1667 (29.7787)  Acc@5: 79.1667 (79.4769)\n",
            "Train: Epoch[ 3/10]  [ 0/21]  eta: 0:00:10  Lr: 0.000047  Loss: 1.9434  Acc@1: 33.3333 (33.3333)  Acc@5: 87.5000 (87.5000)  time: 0.5074  data: 0.3814  max mem: 229\n",
            "Train: Epoch[ 3/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.7444  Acc@1: 50.0000 (46.9697)  Acc@5: 95.8333 (93.1818)  time: 0.1353  data: 0.0369  max mem: 229\n",
            "Train: Epoch[ 3/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 2.0863  Acc@1: 50.0000 (49.2958)  Acc@5: 91.6667 (91.7505)  time: 0.0923  data: 0.0013  max mem: 229\n",
            "Train: Epoch[ 3/10] Total time: 0:00:02 (0.1166 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 2.0863  Acc@1: 50.0000 (49.2958)  Acc@5: 91.6667 (91.7505)\n",
            "Train: Epoch[ 4/10]  [ 0/21]  eta: 0:00:09  Lr: 0.000047  Loss: 1.6885  Acc@1: 58.3333 (58.3333)  Acc@5: 91.6667 (91.6667)  time: 0.4458  data: 0.3253  max mem: 229\n",
            "Train: Epoch[ 4/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.5813  Acc@1: 54.1667 (54.9242)  Acc@5: 95.8333 (95.0758)  time: 0.1269  data: 0.0342  max mem: 229\n",
            "Train: Epoch[ 4/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 1.4324  Acc@1: 62.5000 (60.9658)  Acc@5: 95.8333 (95.5734)  time: 0.0912  data: 0.0028  max mem: 229\n",
            "Train: Epoch[ 4/10] Total time: 0:00:02 (0.1159 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.4324  Acc@1: 62.5000 (60.9658)  Acc@5: 95.8333 (95.5734)\n",
            "Train: Epoch[ 5/10]  [ 0/21]  eta: 0:00:21  Lr: 0.000047  Loss: 1.4022  Acc@1: 66.6667 (66.6667)  Acc@5: 100.0000 (100.0000)  time: 1.0404  data: 0.8715  max mem: 229\n",
            "Train: Epoch[ 5/10]  [10/21]  eta: 0:00:02  Lr: 0.000047  Loss: 1.2034  Acc@1: 66.6667 (67.0455)  Acc@5: 100.0000 (96.9697)  time: 0.1976  data: 0.0838  max mem: 229\n",
            "Train: Epoch[ 5/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 1.3227  Acc@1: 66.6667 (67.0020)  Acc@5: 95.8333 (96.5795)  time: 0.1014  data: 0.0026  max mem: 229\n",
            "Train: Epoch[ 5/10] Total time: 0:00:03 (0.1499 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.3227  Acc@1: 66.6667 (67.0020)  Acc@5: 95.8333 (96.5795)\n",
            "Train: Epoch[ 6/10]  [ 0/21]  eta: 0:00:10  Lr: 0.000047  Loss: 1.2931  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  time: 0.5035  data: 0.3867  max mem: 229\n",
            "Train: Epoch[ 6/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.1904  Acc@1: 75.0000 (72.7273)  Acc@5: 95.8333 (96.9697)  time: 0.1278  data: 0.0361  max mem: 229\n",
            "Train: Epoch[ 6/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 1.2570  Acc@1: 70.8333 (72.4346)  Acc@5: 95.8333 (96.9819)  time: 0.0886  data: 0.0006  max mem: 229\n",
            "Train: Epoch[ 6/10] Total time: 0:00:02 (0.1122 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.2570  Acc@1: 70.8333 (72.4346)  Acc@5: 95.8333 (96.9819)\n",
            "Train: Epoch[ 7/10]  [ 0/21]  eta: 0:00:10  Lr: 0.000047  Loss: 1.3821  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  time: 0.5000  data: 0.3936  max mem: 229\n",
            "Train: Epoch[ 7/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.0405  Acc@1: 75.0000 (77.2727)  Acc@5: 100.0000 (97.3485)  time: 0.1286  data: 0.0364  max mem: 229\n",
            "Train: Epoch[ 7/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 1.1173  Acc@1: 75.0000 (77.0624)  Acc@5: 100.0000 (97.5855)  time: 0.0888  data: 0.0005  max mem: 229\n",
            "Train: Epoch[ 7/10] Total time: 0:00:02 (0.1118 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.1173  Acc@1: 75.0000 (77.0624)  Acc@5: 100.0000 (97.5855)\n",
            "Train: Epoch[ 8/10]  [ 0/21]  eta: 0:00:08  Lr: 0.000047  Loss: 1.1415  Acc@1: 66.6667 (66.6667)  Acc@5: 100.0000 (100.0000)  time: 0.3909  data: 0.2835  max mem: 229\n",
            "Train: Epoch[ 8/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.0417  Acc@1: 75.0000 (78.4091)  Acc@5: 100.0000 (97.7273)  time: 0.1265  data: 0.0346  max mem: 229\n",
            "Train: Epoch[ 8/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 0.9007  Acc@1: 75.0000 (78.0684)  Acc@5: 100.0000 (97.1831)  time: 0.0927  data: 0.0050  max mem: 229\n",
            "Train: Epoch[ 8/10] Total time: 0:00:02 (0.1106 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.9007  Acc@1: 75.0000 (78.0684)  Acc@5: 100.0000 (97.1831)\n",
            "Train: Epoch[ 9/10]  [ 0/21]  eta: 0:00:07  Lr: 0.000047  Loss: 0.8909  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  time: 0.3487  data: 0.2620  max mem: 229\n",
            "Train: Epoch[ 9/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 0.8045  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (96.5909)  time: 0.1272  data: 0.0382  max mem: 229\n",
            "Train: Epoch[ 9/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 0.8086  Acc@1: 79.1667 (79.8793)  Acc@5: 100.0000 (96.7807)  time: 0.0956  data: 0.0080  max mem: 229\n",
            "Train: Epoch[ 9/10] Total time: 0:00:02 (0.1113 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.8086  Acc@1: 79.1667 (79.8793)  Acc@5: 100.0000 (96.7807)\n",
            "Train: Epoch[10/10]  [ 0/21]  eta: 0:00:10  Lr: 0.000047  Loss: 0.8771  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.5013  data: 0.3520  max mem: 229\n",
            "Train: Epoch[10/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 0.8118  Acc@1: 83.3333 (84.4697)  Acc@5: 100.0000 (98.8636)  time: 0.1480  data: 0.0459  max mem: 229\n",
            "Train: Epoch[10/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 1.1597  Acc@1: 79.1667 (81.4889)  Acc@5: 100.0000 (98.5916)  time: 0.1030  data: 0.0098  max mem: 229\n",
            "Train: Epoch[10/10] Total time: 0:00:02 (0.1289 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.1597  Acc@1: 79.1667 (81.4889)  Acc@5: 100.0000 (98.5916)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:27  Loss: 1.7046 (1.7046)  Acc@1: 87.5000 (87.5000)  Acc@5: 95.8333 (95.8333)  time: 0.6593  data: 0.5842  max mem: 229\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 1.8285 (1.8034)  Acc@1: 79.1667 (75.3788)  Acc@5: 100.0000 (98.1061)  time: 0.1418  data: 0.0539  max mem: 229\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 1.8285 (1.8112)  Acc@1: 79.1667 (76.3889)  Acc@5: 100.0000 (98.2143)  time: 0.0890  data: 0.0009  max mem: 229\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 1.7564 (1.7840)  Acc@1: 79.1667 (77.8226)  Acc@5: 100.0000 (97.9839)  time: 0.0883  data: 0.0006  max mem: 229\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 1.7064 (1.7670)  Acc@1: 83.3333 (79.6748)  Acc@5: 95.8333 (97.9675)  time: 0.0889  data: 0.0002  max mem: 229\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 1.6667 (1.7633)  Acc@1: 83.3333 (79.8000)  Acc@5: 95.8333 (97.9000)  time: 0.0869  data: 0.0002  max mem: 229\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1038 s / it)\n",
            "* Acc@1 79.800 Acc@5 97.900 loss 1.763\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:12  Loss: 2.2082 (2.2082)  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.8333)  time: 0.2914  data: 0.2179  max mem: 229\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:03  Loss: 2.2511 (2.2180)  Acc@1: 75.0000 (72.7273)  Acc@5: 95.8333 (94.6970)  time: 0.1186  data: 0.0335  max mem: 229\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 2.1686 (2.1929)  Acc@1: 70.8333 (73.2143)  Acc@5: 95.8333 (94.6429)  time: 0.0948  data: 0.0077  max mem: 229\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 2.0941 (2.1816)  Acc@1: 70.8333 (73.6559)  Acc@5: 95.8333 (95.0269)  time: 0.0893  data: 0.0004  max mem: 229\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 2.1074 (2.1761)  Acc@1: 75.0000 (73.9837)  Acc@5: 95.8333 (95.5285)  time: 0.0902  data: 0.0003  max mem: 229\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 2.1061 (2.1685)  Acc@1: 75.0000 (74.2000)  Acc@5: 100.0000 (95.6000)  time: 0.0877  data: 0.0003  max mem: 229\n",
            "Test: [Task 2] Total time: 0:00:04 (0.0982 s / it)\n",
            "* Acc@1 74.200 Acc@5 95.600 loss 2.168\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:14  Loss: 2.3298 (2.3298)  Acc@1: 50.0000 (50.0000)  Acc@5: 100.0000 (100.0000)  time: 0.3398  data: 0.2526  max mem: 229\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 2.4050 (2.3968)  Acc@1: 58.3333 (58.7121)  Acc@5: 95.8333 (95.4545)  time: 0.1134  data: 0.0270  max mem: 229\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 2.4043 (2.3923)  Acc@1: 58.3333 (60.1190)  Acc@5: 95.8333 (95.0397)  time: 0.0900  data: 0.0024  max mem: 229\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 2.3248 (2.3657)  Acc@1: 62.5000 (62.7688)  Acc@5: 95.8333 (94.6237)  time: 0.0895  data: 0.0003  max mem: 229\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 2.3511 (2.3891)  Acc@1: 66.6667 (62.6016)  Acc@5: 95.8333 (93.9024)  time: 0.0897  data: 0.0003  max mem: 229\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 2.3691 (2.3934)  Acc@1: 66.6667 (62.4000)  Acc@5: 95.8333 (93.8000)  time: 0.0879  data: 0.0003  max mem: 229\n",
            "Test: [Task 3] Total time: 0:00:04 (0.0987 s / it)\n",
            "* Acc@1 62.400 Acc@5 93.800 loss 2.393\n",
            "[Average accuracy till task3]\tAcc@1: 72.1333\tAcc@5: 95.7667\tLoss: 2.1084\tForgetting: 5.1500\tBackward: -2.6500\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([6312, 384])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 1.3075  Acc@1: 75.0000 (77.7083)  Acc@5: 100.0000 (98.1250)\n",
            "torch.Size([6312, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 0.9994  Acc@1: 83.3333 (81.0417)  Acc@5: 100.0000 (98.9583)\n",
            "torch.Size([6312, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 1.2063  Acc@1: 83.3333 (82.9167)  Acc@5: 100.0000 (98.5417)\n",
            "torch.Size([6312, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 1.2003  Acc@1: 87.5000 (84.7917)  Acc@5: 100.0000 (98.7500)\n",
            "torch.Size([6312, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 0.9329  Acc@1: 83.3333 (86.2500)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([6312, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 1.0929  Acc@1: 87.5000 (88.1250)  Acc@5: 100.0000 (98.9583)\n",
            "torch.Size([6312, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.9606  Acc@1: 83.3333 (84.5833)  Acc@5: 100.0000 (99.3750)\n",
            "torch.Size([6312, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 0.8918  Acc@1: 87.5000 (88.3333)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([6312, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 0.9278  Acc@1: 87.5000 (86.6667)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([6312, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.9084  Acc@1: 87.5000 (87.0833)  Acc@5: 100.0000 (99.7917)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:21  Loss: 1.2300 (1.2300)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.5232  data: 0.4459  max mem: 230\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 1.4302 (1.4075)  Acc@1: 70.8333 (74.2424)  Acc@5: 100.0000 (98.8636)  time: 0.1298  data: 0.0414  max mem: 230\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 1.3925 (1.4006)  Acc@1: 75.0000 (76.7857)  Acc@5: 100.0000 (98.8095)  time: 0.0892  data: 0.0009  max mem: 230\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 1.3777 (1.3789)  Acc@1: 79.1667 (78.0914)  Acc@5: 100.0000 (98.5215)  time: 0.0884  data: 0.0008  max mem: 230\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 1.3066 (1.3549)  Acc@1: 83.3333 (80.0813)  Acc@5: 100.0000 (98.7805)  time: 0.0895  data: 0.0007  max mem: 230\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 1.3032 (1.3500)  Acc@1: 83.3333 (80.2000)  Acc@5: 100.0000 (98.8000)  time: 0.0880  data: 0.0007  max mem: 230\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1027 s / it)\n",
            "* Acc@1 80.200 Acc@5 98.800 loss 1.350\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:26  Loss: 1.6811 (1.6811)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  time: 0.6386  data: 0.5481  max mem: 230\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 1.7308 (1.7275)  Acc@1: 75.0000 (75.7576)  Acc@5: 95.8333 (95.8333)  time: 0.1406  data: 0.0501  max mem: 230\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 1.7166 (1.7031)  Acc@1: 79.1667 (78.3730)  Acc@5: 95.8333 (95.6349)  time: 0.0899  data: 0.0003  max mem: 230\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 1.6011 (1.6914)  Acc@1: 79.1667 (78.7634)  Acc@5: 95.8333 (95.6989)  time: 0.0889  data: 0.0003  max mem: 230\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 1.6156 (1.6847)  Acc@1: 79.1667 (78.7602)  Acc@5: 95.8333 (96.2398)  time: 0.0893  data: 0.0002  max mem: 230\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 1.5965 (1.6772)  Acc@1: 79.1667 (78.9000)  Acc@5: 100.0000 (96.3000)  time: 0.0874  data: 0.0002  max mem: 230\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1044 s / it)\n",
            "* Acc@1 78.900 Acc@5 96.300 loss 1.677\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:19  Loss: 1.5094 (1.5094)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.4529  data: 0.3768  max mem: 230\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 1.6313 (1.6558)  Acc@1: 79.1667 (76.8939)  Acc@5: 100.0000 (97.3485)  time: 0.1242  data: 0.0350  max mem: 230\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 1.6313 (1.6364)  Acc@1: 75.0000 (76.1905)  Acc@5: 95.8333 (97.2222)  time: 0.0904  data: 0.0007  max mem: 230\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 1.5505 (1.6084)  Acc@1: 75.0000 (77.0161)  Acc@5: 95.8333 (97.1774)  time: 0.0896  data: 0.0004  max mem: 230\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 1.6034 (1.6294)  Acc@1: 75.0000 (77.1341)  Acc@5: 95.8333 (97.0528)  time: 0.0904  data: 0.0002  max mem: 230\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 1.6309 (1.6347)  Acc@1: 75.0000 (77.1000)  Acc@5: 95.8333 (97.1000)  time: 0.0888  data: 0.0002  max mem: 230\n",
            "Test: [Task 3] Total time: 0:00:04 (0.1006 s / it)\n",
            "* Acc@1 77.100 Acc@5 97.100 loss 1.635\n",
            "[Average accuracy till task3]\tAcc@1: 78.7333\tAcc@5: 97.4000\tLoss: 1.5540\tForgetting: 5.9500\tBackward: -5.9500\n",
            "Train: Epoch[ 1/10]  [ 0/20]  eta: 0:00:07  Lr: 0.000047  Loss: 2.3116  Acc@1: 8.3333 (8.3333)  Acc@5: 58.3333 (58.3333)  time: 0.3942  data: 0.2753  max mem: 230\n",
            "Train: Epoch[ 1/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 2.3757  Acc@1: 8.3333 (8.7121)  Acc@5: 41.6667 (46.2121)  time: 0.1288  data: 0.0346  max mem: 230\n",
            "Train: Epoch[ 1/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 2.1850  Acc@1: 8.3333 (9.3418)  Acc@5: 50.0000 (51.5924)  time: 0.1101  data: 0.0191  max mem: 230\n",
            "Train: Epoch[ 1/10] Total time: 0:00:02 (0.1154 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 2.1850  Acc@1: 8.3333 (9.3418)  Acc@5: 50.0000 (51.5924)\n",
            "Train: Epoch[ 2/10]  [ 0/20]  eta: 0:00:15  Lr: 0.000047  Loss: 2.1878  Acc@1: 16.6667 (16.6667)  Acc@5: 70.8333 (70.8333)  time: 0.7645  data: 0.6163  max mem: 230\n",
            "Train: Epoch[ 2/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 2.2032  Acc@1: 20.8333 (22.3485)  Acc@5: 75.0000 (74.6212)  time: 0.1666  data: 0.0569  max mem: 230\n",
            "Train: Epoch[ 2/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 2.2316  Acc@1: 25.0000 (24.4161)  Acc@5: 75.0000 (76.8578)  time: 0.1328  data: 0.0313  max mem: 230\n",
            "Train: Epoch[ 2/10] Total time: 0:00:02 (0.1418 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 2.2316  Acc@1: 25.0000 (24.4161)  Acc@5: 75.0000 (76.8578)\n",
            "Train: Epoch[ 3/10]  [ 0/20]  eta: 0:00:13  Lr: 0.000047  Loss: 1.8133  Acc@1: 45.8333 (45.8333)  Acc@5: 95.8333 (95.8333)  time: 0.6531  data: 0.5531  max mem: 230\n",
            "Train: Epoch[ 3/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 2.0103  Acc@1: 37.5000 (35.9848)  Acc@5: 91.6667 (88.6364)  time: 0.1437  data: 0.0514  max mem: 230\n",
            "Train: Epoch[ 3/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 1.8163  Acc@1: 37.5000 (40.9766)  Acc@5: 91.6667 (89.3843)  time: 0.1182  data: 0.0283  max mem: 230\n",
            "Train: Epoch[ 3/10] Total time: 0:00:02 (0.1219 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.8163  Acc@1: 37.5000 (40.9766)  Acc@5: 91.6667 (89.3843)\n",
            "Train: Epoch[ 4/10]  [ 0/20]  eta: 0:00:08  Lr: 0.000047  Loss: 1.8512  Acc@1: 45.8333 (45.8333)  Acc@5: 87.5000 (87.5000)  time: 0.4158  data: 0.3106  max mem: 230\n",
            "Train: Epoch[ 4/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 1.8275  Acc@1: 50.0000 (51.1364)  Acc@5: 91.6667 (90.5303)  time: 0.1260  data: 0.0326  max mem: 230\n",
            "Train: Epoch[ 4/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 1.4101  Acc@1: 54.1667 (54.7771)  Acc@5: 91.6667 (91.5074)  time: 0.1084  data: 0.0180  max mem: 230\n",
            "Train: Epoch[ 4/10] Total time: 0:00:02 (0.1129 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.4101  Acc@1: 54.1667 (54.7771)  Acc@5: 91.6667 (91.5074)\n",
            "Train: Epoch[ 5/10]  [ 0/20]  eta: 0:00:10  Lr: 0.000047  Loss: 1.6114  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  time: 0.5276  data: 0.4211  max mem: 230\n",
            "Train: Epoch[ 5/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 1.6945  Acc@1: 62.5000 (65.5303)  Acc@5: 91.6667 (92.4242)  time: 0.1360  data: 0.0392  max mem: 230\n",
            "Train: Epoch[ 5/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 1.3199  Acc@1: 66.6667 (66.4544)  Acc@5: 95.8333 (94.4798)  time: 0.1139  data: 0.0216  max mem: 230\n",
            "Train: Epoch[ 5/10] Total time: 0:00:02 (0.1179 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.3199  Acc@1: 66.6667 (66.4544)  Acc@5: 95.8333 (94.4798)\n",
            "Train: Epoch[ 6/10]  [ 0/20]  eta: 0:00:11  Lr: 0.000047  Loss: 1.4368  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  time: 0.5696  data: 0.4714  max mem: 230\n",
            "Train: Epoch[ 6/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 1.5001  Acc@1: 66.6667 (67.4242)  Acc@5: 95.8333 (95.4545)  time: 0.1343  data: 0.0440  max mem: 230\n",
            "Train: Epoch[ 6/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 1.3742  Acc@1: 66.6667 (67.0913)  Acc@5: 95.8333 (95.1168)  time: 0.1132  data: 0.0242  max mem: 230\n",
            "Train: Epoch[ 6/10] Total time: 0:00:02 (0.1171 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.3742  Acc@1: 66.6667 (67.0913)  Acc@5: 95.8333 (95.1168)\n",
            "Train: Epoch[ 7/10]  [ 0/20]  eta: 0:00:10  Lr: 0.000047  Loss: 1.2086  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  time: 0.5418  data: 0.4450  max mem: 230\n",
            "Train: Epoch[ 7/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 1.1789  Acc@1: 79.1667 (76.5152)  Acc@5: 100.0000 (97.7273)  time: 0.1352  data: 0.0416  max mem: 230\n",
            "Train: Epoch[ 7/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 1.0688  Acc@1: 75.0000 (74.3100)  Acc@5: 95.8333 (96.8153)  time: 0.1128  data: 0.0229  max mem: 230\n",
            "Train: Epoch[ 7/10] Total time: 0:00:02 (0.1200 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.0688  Acc@1: 75.0000 (74.3100)  Acc@5: 95.8333 (96.8153)\n",
            "Train: Epoch[ 8/10]  [ 0/20]  eta: 0:00:15  Lr: 0.000047  Loss: 1.2478  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.8333)  time: 0.7901  data: 0.6197  max mem: 230\n",
            "Train: Epoch[ 8/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 1.0571  Acc@1: 79.1667 (78.0303)  Acc@5: 95.8333 (97.7273)  time: 0.1666  data: 0.0566  max mem: 230\n",
            "Train: Epoch[ 8/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 1.1477  Acc@1: 79.1667 (77.2824)  Acc@5: 95.8333 (97.4522)  time: 0.1320  data: 0.0313  max mem: 230\n",
            "Train: Epoch[ 8/10] Total time: 0:00:02 (0.1363 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.1477  Acc@1: 79.1667 (77.2824)  Acc@5: 95.8333 (97.4522)\n",
            "Train: Epoch[ 9/10]  [ 0/20]  eta: 0:00:10  Lr: 0.000047  Loss: 1.1832  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.8333)  time: 0.5438  data: 0.4512  max mem: 230\n",
            "Train: Epoch[ 9/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 1.0588  Acc@1: 75.0000 (75.7576)  Acc@5: 100.0000 (97.7273)  time: 0.1321  data: 0.0418  max mem: 230\n",
            "Train: Epoch[ 9/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 0.8955  Acc@1: 75.0000 (77.7070)  Acc@5: 100.0000 (97.8769)  time: 0.1116  data: 0.0231  max mem: 230\n",
            "Train: Epoch[ 9/10] Total time: 0:00:02 (0.1155 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.8955  Acc@1: 75.0000 (77.7070)  Acc@5: 100.0000 (97.8769)\n",
            "Train: Epoch[10/10]  [ 0/20]  eta: 0:00:07  Lr: 0.000047  Loss: 0.9191  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.3807  data: 0.2862  max mem: 230\n",
            "Train: Epoch[10/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 1.1393  Acc@1: 79.1667 (79.5455)  Acc@5: 100.0000 (98.8636)  time: 0.1245  data: 0.0316  max mem: 230\n",
            "Train: Epoch[10/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 0.8535  Acc@1: 79.1667 (77.2824)  Acc@5: 100.0000 (98.5138)  time: 0.1074  data: 0.0176  max mem: 230\n",
            "Train: Epoch[10/10] Total time: 0:00:02 (0.1116 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.8535  Acc@1: 79.1667 (77.2824)  Acc@5: 100.0000 (98.5138)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:14  Loss: 1.4132 (1.4132)  Acc@1: 87.5000 (87.5000)  Acc@5: 91.6667 (91.6667)  time: 0.3528  data: 0.2760  max mem: 230\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 1.5100 (1.5152)  Acc@1: 79.1667 (74.6212)  Acc@5: 100.0000 (97.7273)  time: 0.1180  data: 0.0307  max mem: 230\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 1.5100 (1.5240)  Acc@1: 79.1667 (75.9921)  Acc@5: 100.0000 (97.6190)  time: 0.0925  data: 0.0033  max mem: 230\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 1.4666 (1.5113)  Acc@1: 79.1667 (76.8817)  Acc@5: 95.8333 (97.4462)  time: 0.0905  data: 0.0005  max mem: 230\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 1.4585 (1.4896)  Acc@1: 83.3333 (78.7602)  Acc@5: 95.8333 (97.6626)  time: 0.0908  data: 0.0004  max mem: 230\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 1.4223 (1.4841)  Acc@1: 83.3333 (78.9000)  Acc@5: 95.8333 (97.7000)  time: 0.0891  data: 0.0003  max mem: 230\n",
            "Test: [Task 1] Total time: 0:00:04 (0.0990 s / it)\n",
            "* Acc@1 78.900 Acc@5 97.700 loss 1.484\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:20  Loss: 1.8112 (1.8112)  Acc@1: 83.3333 (83.3333)  Acc@5: 91.6667 (91.6667)  time: 0.4969  data: 0.4180  max mem: 230\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 1.8978 (1.8693)  Acc@1: 75.0000 (76.8939)  Acc@5: 91.6667 (94.3182)  time: 0.1275  data: 0.0392  max mem: 230\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 1.8782 (1.8479)  Acc@1: 75.0000 (77.9762)  Acc@5: 91.6667 (94.2460)  time: 0.0901  data: 0.0013  max mem: 230\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 1.7486 (1.8372)  Acc@1: 79.1667 (78.4946)  Acc@5: 95.8333 (94.3548)  time: 0.0896  data: 0.0012  max mem: 230\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 1.7576 (1.8332)  Acc@1: 79.1667 (78.1504)  Acc@5: 95.8333 (94.9187)  time: 0.0901  data: 0.0008  max mem: 230\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 1.7521 (1.8244)  Acc@1: 79.1667 (78.3000)  Acc@5: 95.8333 (95.0000)  time: 0.0885  data: 0.0008  max mem: 230\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1020 s / it)\n",
            "* Acc@1 78.300 Acc@5 95.000 loss 1.824\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:20  Loss: 1.6693 (1.6693)  Acc@1: 70.8333 (70.8333)  Acc@5: 100.0000 (100.0000)  time: 0.4912  data: 0.4056  max mem: 230\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:04  Loss: 1.8201 (1.8294)  Acc@1: 70.8333 (69.6970)  Acc@5: 95.8333 (95.0758)  time: 0.1267  data: 0.0394  max mem: 230\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 1.8130 (1.8112)  Acc@1: 70.8333 (71.4286)  Acc@5: 95.8333 (95.4365)  time: 0.0902  data: 0.0015  max mem: 230\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 1.7050 (1.7702)  Acc@1: 75.0000 (72.7151)  Acc@5: 95.8333 (95.4301)  time: 0.0903  data: 0.0004  max mem: 230\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 1.7671 (1.7947)  Acc@1: 75.0000 (72.8659)  Acc@5: 95.8333 (94.7154)  time: 0.0906  data: 0.0003  max mem: 230\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 1.7752 (1.8034)  Acc@1: 75.0000 (72.8000)  Acc@5: 91.6667 (94.6000)  time: 0.0887  data: 0.0003  max mem: 230\n",
            "Test: [Task 3] Total time: 0:00:04 (0.1013 s / it)\n",
            "* Acc@1 72.800 Acc@5 94.600 loss 1.803\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:17  Loss: 3.0501 (3.0501)  Acc@1: 37.5000 (37.5000)  Acc@5: 66.6667 (66.6667)  time: 0.4055  data: 0.3289  max mem: 230\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:03  Loss: 2.7935 (2.8300)  Acc@1: 37.5000 (36.7424)  Acc@5: 83.3333 (83.3333)  time: 0.1204  data: 0.0309  max mem: 230\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 2.7935 (2.8366)  Acc@1: 37.5000 (34.9206)  Acc@5: 83.3333 (82.9365)  time: 0.0905  data: 0.0007  max mem: 230\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 2.7917 (2.8126)  Acc@1: 33.3333 (34.9462)  Acc@5: 87.5000 (84.1398)  time: 0.0891  data: 0.0004  max mem: 230\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 2.7971 (2.8246)  Acc@1: 33.3333 (34.7561)  Acc@5: 83.3333 (83.9431)  time: 0.0898  data: 0.0003  max mem: 230\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 2.7624 (2.8165)  Acc@1: 33.3333 (34.8000)  Acc@5: 83.3333 (84.1000)  time: 0.0882  data: 0.0002  max mem: 230\n",
            "Test: [Task 4] Total time: 0:00:04 (0.0989 s / it)\n",
            "* Acc@1 34.800 Acc@5 84.100 loss 2.816\n",
            "[Average accuracy till task4]\tAcc@1: 66.2000\tAcc@5: 92.8500\tLoss: 1.9821\tForgetting: 3.7333\tBackward: 2.7667\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([8424, 384])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 1.1824  Acc@1: 75.0000 (74.7222)  Acc@5: 95.8333 (96.1111)\n",
            "torch.Size([8424, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 1.3417  Acc@1: 83.3333 (81.9444)  Acc@5: 100.0000 (98.6111)\n",
            "torch.Size([8424, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 0.9428  Acc@1: 87.5000 (85.1389)  Acc@5: 100.0000 (98.6111)\n",
            "torch.Size([8424, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 0.8623  Acc@1: 87.5000 (86.6667)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([8424, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 0.6940  Acc@1: 87.5000 (87.3611)  Acc@5: 100.0000 (98.6111)\n",
            "torch.Size([8424, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 0.9722  Acc@1: 91.6667 (88.4722)  Acc@5: 100.0000 (99.7222)\n",
            "torch.Size([8424, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.9766  Acc@1: 87.5000 (88.8889)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([8424, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 1.0145  Acc@1: 87.5000 (87.2222)  Acc@5: 100.0000 (98.7500)\n",
            "torch.Size([8424, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 0.9692  Acc@1: 91.6667 (88.1944)  Acc@5: 100.0000 (98.7500)\n",
            "torch.Size([8424, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.8118  Acc@1: 91.6667 (90.9722)  Acc@5: 100.0000 (98.8889)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:19  Loss: 1.0389 (1.0389)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  time: 0.4717  data: 0.4048  max mem: 230\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 1.2084 (1.1979)  Acc@1: 79.1667 (78.0303)  Acc@5: 100.0000 (98.4848)  time: 0.1252  data: 0.0377  max mem: 230\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 1.2084 (1.1962)  Acc@1: 79.1667 (79.7619)  Acc@5: 100.0000 (98.4127)  time: 0.0885  data: 0.0006  max mem: 230\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 1.1264 (1.1778)  Acc@1: 83.3333 (80.7796)  Acc@5: 95.8333 (98.1183)  time: 0.0874  data: 0.0004  max mem: 230\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 1.1030 (1.1545)  Acc@1: 83.3333 (82.0122)  Acc@5: 100.0000 (98.2724)  time: 0.0888  data: 0.0008  max mem: 230\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 1.0927 (1.1489)  Acc@1: 87.5000 (82.2000)  Acc@5: 100.0000 (98.3000)  time: 0.0870  data: 0.0008  max mem: 230\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1008 s / it)\n",
            "* Acc@1 82.200 Acc@5 98.300 loss 1.149\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:25  Loss: 1.5008 (1.5008)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  time: 0.6114  data: 0.5166  max mem: 230\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 1.5126 (1.5011)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.0758)  time: 0.1384  data: 0.0510  max mem: 230\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 1.4244 (1.4948)  Acc@1: 79.1667 (76.9841)  Acc@5: 95.8333 (94.4444)  time: 0.0893  data: 0.0030  max mem: 230\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 1.3955 (1.4771)  Acc@1: 79.1667 (77.0161)  Acc@5: 95.8333 (94.7581)  time: 0.0879  data: 0.0010  max mem: 230\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 1.3976 (1.4656)  Acc@1: 79.1667 (77.7439)  Acc@5: 95.8333 (95.4268)  time: 0.0887  data: 0.0004  max mem: 230\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 1.3958 (1.4581)  Acc@1: 79.1667 (77.8000)  Acc@5: 95.8333 (95.5000)  time: 0.0870  data: 0.0004  max mem: 230\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1028 s / it)\n",
            "* Acc@1 77.800 Acc@5 95.500 loss 1.458\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:18  Loss: 1.1688 (1.1688)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.4301  data: 0.3422  max mem: 230\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 1.3640 (1.3646)  Acc@1: 75.0000 (75.7576)  Acc@5: 95.8333 (96.2121)  time: 0.1203  data: 0.0317  max mem: 230\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 1.3640 (1.3422)  Acc@1: 75.0000 (75.5952)  Acc@5: 95.8333 (96.8254)  time: 0.0886  data: 0.0007  max mem: 230\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 1.2423 (1.3076)  Acc@1: 75.0000 (76.4785)  Acc@5: 95.8333 (97.0430)  time: 0.0882  data: 0.0006  max mem: 230\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 1.3023 (1.3254)  Acc@1: 79.1667 (76.9309)  Acc@5: 95.8333 (96.7480)  time: 0.0892  data: 0.0004  max mem: 230\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 1.3302 (1.3326)  Acc@1: 79.1667 (76.9000)  Acc@5: 95.8333 (96.8000)  time: 0.0873  data: 0.0004  max mem: 230\n",
            "Test: [Task 3] Total time: 0:00:04 (0.0984 s / it)\n",
            "* Acc@1 76.900 Acc@5 96.800 loss 1.333\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:14  Loss: 2.0107 (2.0107)  Acc@1: 62.5000 (62.5000)  Acc@5: 87.5000 (87.5000)  time: 0.3457  data: 0.2550  max mem: 230\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:03  Loss: 1.5801 (1.6169)  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.0758)  time: 0.1163  data: 0.0260  max mem: 230\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 1.5841 (1.6363)  Acc@1: 70.8333 (71.2302)  Acc@5: 95.8333 (95.6349)  time: 0.0911  data: 0.0018  max mem: 230\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 1.5847 (1.6153)  Acc@1: 70.8333 (71.5054)  Acc@5: 95.8333 (95.6989)  time: 0.0890  data: 0.0004  max mem: 230\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 1.5890 (1.6299)  Acc@1: 70.8333 (71.4431)  Acc@5: 95.8333 (95.3252)  time: 0.0896  data: 0.0003  max mem: 230\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 1.5773 (1.6228)  Acc@1: 70.8333 (71.8000)  Acc@5: 95.8333 (95.4000)  time: 0.0879  data: 0.0003  max mem: 230\n",
            "Test: [Task 4] Total time: 0:00:04 (0.0991 s / it)\n",
            "* Acc@1 71.800 Acc@5 95.400 loss 1.623\n",
            "[Average accuracy till task4]\tAcc@1: 77.1750\tAcc@5: 96.5000\tLoss: 1.3906\tForgetting: 3.7333\tBackward: -3.7333\n",
            "Train: Epoch[ 1/10]  [ 0/21]  eta: 0:00:16  Lr: 0.000047  Loss: 2.2642  Acc@1: 12.5000 (12.5000)  Acc@5: 62.5000 (62.5000)  time: 0.7824  data: 0.6533  max mem: 230\n",
            "Train: Epoch[ 1/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 2.2100  Acc@1: 12.5000 (14.3939)  Acc@5: 62.5000 (60.2273)  time: 0.1677  data: 0.0647  max mem: 230\n",
            "Train: Epoch[ 1/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 2.0044  Acc@1: 20.8333 (19.7556)  Acc@5: 70.8333 (67.8208)  time: 0.0954  data: 0.0030  max mem: 230\n",
            "Train: Epoch[ 1/10] Total time: 0:00:02 (0.1338 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 2.0044  Acc@1: 20.8333 (19.7556)  Acc@5: 70.8333 (67.8208)\n",
            "Train: Epoch[ 2/10]  [ 0/21]  eta: 0:00:10  Lr: 0.000047  Loss: 2.0497  Acc@1: 25.0000 (25.0000)  Acc@5: 87.5000 (87.5000)  time: 0.5142  data: 0.3865  max mem: 230\n",
            "Train: Epoch[ 2/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.8460  Acc@1: 29.1667 (32.5758)  Acc@5: 87.5000 (84.0909)  time: 0.1294  data: 0.0359  max mem: 230\n",
            "Train: Epoch[ 2/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 1.8115  Acc@1: 33.3333 (35.6415)  Acc@5: 87.5000 (86.5580)  time: 0.0882  data: 0.0006  max mem: 230\n",
            "Train: Epoch[ 2/10] Total time: 0:00:02 (0.1120 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.8115  Acc@1: 33.3333 (35.6415)  Acc@5: 87.5000 (86.5580)\n",
            "Train: Epoch[ 3/10]  [ 0/21]  eta: 0:00:12  Lr: 0.000047  Loss: 1.8584  Acc@1: 41.6667 (41.6667)  Acc@5: 95.8333 (95.8333)  time: 0.5875  data: 0.4843  max mem: 230\n",
            "Train: Epoch[ 3/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.6654  Acc@1: 45.8333 (46.5909)  Acc@5: 91.6667 (90.9091)  time: 0.1364  data: 0.0447  max mem: 230\n",
            "Train: Epoch[ 3/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 1.5522  Acc@1: 54.1667 (54.9898)  Acc@5: 91.6667 (92.4644)  time: 0.0883  data: 0.0005  max mem: 230\n",
            "Train: Epoch[ 3/10] Total time: 0:00:02 (0.1159 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.5522  Acc@1: 54.1667 (54.9898)  Acc@5: 91.6667 (92.4644)\n",
            "Train: Epoch[ 4/10]  [ 0/21]  eta: 0:00:07  Lr: 0.000047  Loss: 1.5780  Acc@1: 62.5000 (62.5000)  Acc@5: 87.5000 (87.5000)  time: 0.3488  data: 0.2527  max mem: 230\n",
            "Train: Epoch[ 4/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.5335  Acc@1: 54.1667 (57.9545)  Acc@5: 95.8333 (95.0758)  time: 0.1251  data: 0.0345  max mem: 230\n",
            "Train: Epoch[ 4/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 1.4344  Acc@1: 58.3333 (59.8778)  Acc@5: 95.8333 (96.1303)  time: 0.0945  data: 0.0064  max mem: 230\n",
            "Train: Epoch[ 4/10] Total time: 0:00:02 (0.1108 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.4344  Acc@1: 58.3333 (59.8778)  Acc@5: 95.8333 (96.1303)\n",
            "Train: Epoch[ 5/10]  [ 0/21]  eta: 0:00:09  Lr: 0.000047  Loss: 1.1998  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.4542  data: 0.3607  max mem: 230\n",
            "Train: Epoch[ 5/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.4072  Acc@1: 70.8333 (68.1818)  Acc@5: 95.8333 (96.9697)  time: 0.1361  data: 0.0363  max mem: 230\n",
            "Train: Epoch[ 5/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 1.4687  Acc@1: 66.6667 (67.2098)  Acc@5: 100.0000 (97.7597)  time: 0.0952  data: 0.0022  max mem: 230\n",
            "Train: Epoch[ 5/10] Total time: 0:00:02 (0.1176 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.4687  Acc@1: 66.6667 (67.2098)  Acc@5: 100.0000 (97.7597)\n",
            "Train: Epoch[ 6/10]  [ 0/21]  eta: 0:00:17  Lr: 0.000047  Loss: 1.2599  Acc@1: 70.8333 (70.8333)  Acc@5: 100.0000 (100.0000)  time: 0.8144  data: 0.6957  max mem: 230\n",
            "Train: Epoch[ 6/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.3267  Acc@1: 70.8333 (70.0758)  Acc@5: 100.0000 (96.9697)  time: 0.1673  data: 0.0636  max mem: 230\n",
            "Train: Epoch[ 6/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 1.1144  Acc@1: 70.8333 (70.2648)  Acc@5: 95.8333 (96.3340)  time: 0.0963  data: 0.0008  max mem: 230\n",
            "Train: Epoch[ 6/10] Total time: 0:00:02 (0.1368 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.1144  Acc@1: 70.8333 (70.2648)  Acc@5: 95.8333 (96.3340)\n",
            "Train: Epoch[ 7/10]  [ 0/21]  eta: 0:00:14  Lr: 0.000047  Loss: 1.3578  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.8333)  time: 0.6745  data: 0.5603  max mem: 230\n",
            "Train: Epoch[ 7/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.1146  Acc@1: 75.0000 (74.2424)  Acc@5: 100.0000 (98.1061)  time: 0.1450  data: 0.0518  max mem: 230\n",
            "Train: Epoch[ 7/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 1.1625  Acc@1: 75.0000 (74.5418)  Acc@5: 100.0000 (97.3523)  time: 0.0885  data: 0.0006  max mem: 230\n",
            "Train: Epoch[ 7/10] Total time: 0:00:02 (0.1202 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.1625  Acc@1: 75.0000 (74.5418)  Acc@5: 100.0000 (97.3523)\n",
            "Train: Epoch[ 8/10]  [ 0/21]  eta: 0:00:09  Lr: 0.000047  Loss: 1.0284  Acc@1: 70.8333 (70.8333)  Acc@5: 100.0000 (100.0000)  time: 0.4454  data: 0.3343  max mem: 230\n",
            "Train: Epoch[ 8/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 0.8913  Acc@1: 79.1667 (78.4091)  Acc@5: 100.0000 (98.4848)  time: 0.1238  data: 0.0312  max mem: 230\n",
            "Train: Epoch[ 8/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 1.4076  Acc@1: 75.0000 (74.9491)  Acc@5: 100.0000 (97.7597)  time: 0.0888  data: 0.0006  max mem: 230\n",
            "Train: Epoch[ 8/10] Total time: 0:00:02 (0.1095 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.4076  Acc@1: 75.0000 (74.9491)  Acc@5: 100.0000 (97.7597)\n",
            "Train: Epoch[ 9/10]  [ 0/21]  eta: 0:00:10  Lr: 0.000047  Loss: 0.8995  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.5207  data: 0.4174  max mem: 230\n",
            "Train: Epoch[ 9/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 0.8691  Acc@1: 83.3333 (82.9545)  Acc@5: 100.0000 (98.4848)  time: 0.1303  data: 0.0387  max mem: 230\n",
            "Train: Epoch[ 9/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 0.8082  Acc@1: 79.1667 (80.4481)  Acc@5: 100.0000 (98.7780)  time: 0.0883  data: 0.0005  max mem: 230\n",
            "Train: Epoch[ 9/10] Total time: 0:00:02 (0.1126 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.8082  Acc@1: 79.1667 (80.4481)  Acc@5: 100.0000 (98.7780)\n",
            "Train: Epoch[10/10]  [ 0/21]  eta: 0:00:09  Lr: 0.000047  Loss: 0.8519  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  time: 0.4545  data: 0.3409  max mem: 230\n",
            "Train: Epoch[10/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.1327  Acc@1: 79.1667 (79.5455)  Acc@5: 100.0000 (97.7273)  time: 0.1253  data: 0.0323  max mem: 230\n",
            "Train: Epoch[10/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 0.7492  Acc@1: 81.8182 (81.4664)  Acc@5: 100.0000 (98.5743)  time: 0.0892  data: 0.0008  max mem: 230\n",
            "Train: Epoch[10/10] Total time: 0:00:02 (0.1106 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.7492  Acc@1: 81.8182 (81.4664)  Acc@5: 100.0000 (98.5743)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:19  Loss: 1.2911 (1.2911)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.4574  data: 0.3741  max mem: 230\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 1.3190 (1.3225)  Acc@1: 79.1667 (76.8939)  Acc@5: 100.0000 (98.1061)  time: 0.1301  data: 0.0429  max mem: 230\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 1.3297 (1.3335)  Acc@1: 79.1667 (77.7778)  Acc@5: 100.0000 (97.8175)  time: 0.0932  data: 0.0056  max mem: 230\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 1.2616 (1.3206)  Acc@1: 75.0000 (77.2849)  Acc@5: 95.8333 (97.7151)  time: 0.0898  data: 0.0015  max mem: 230\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 1.2343 (1.2959)  Acc@1: 83.3333 (79.2683)  Acc@5: 100.0000 (97.9675)  time: 0.0907  data: 0.0008  max mem: 230\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 1.2216 (1.2902)  Acc@1: 83.3333 (79.4000)  Acc@5: 100.0000 (98.0000)  time: 0.0886  data: 0.0007  max mem: 230\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1027 s / it)\n",
            "* Acc@1 79.400 Acc@5 98.000 loss 1.290\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:22  Loss: 1.5672 (1.5672)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  time: 0.5473  data: 0.4759  max mem: 230\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 1.5672 (1.5447)  Acc@1: 79.1667 (78.4091)  Acc@5: 95.8333 (95.8333)  time: 0.1331  data: 0.0445  max mem: 230\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 1.4832 (1.5446)  Acc@1: 75.0000 (79.5635)  Acc@5: 95.8333 (94.8413)  time: 0.0906  data: 0.0008  max mem: 230\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 1.4630 (1.5307)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.0269)  time: 0.0897  data: 0.0003  max mem: 230\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 1.4659 (1.5215)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.6301)  time: 0.0903  data: 0.0003  max mem: 230\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 1.4630 (1.5145)  Acc@1: 79.1667 (79.2000)  Acc@5: 95.8333 (95.7000)  time: 0.0887  data: 0.0003  max mem: 230\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1027 s / it)\n",
            "* Acc@1 79.200 Acc@5 95.700 loss 1.514\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:18  Loss: 1.2324 (1.2324)  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)  time: 0.4495  data: 0.3780  max mem: 230\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 1.4457 (1.4697)  Acc@1: 75.0000 (76.5152)  Acc@5: 95.8333 (95.0758)  time: 0.1237  data: 0.0346  max mem: 230\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 1.4457 (1.4374)  Acc@1: 75.0000 (76.1905)  Acc@5: 95.8333 (95.8333)  time: 0.0903  data: 0.0003  max mem: 230\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 1.3122 (1.4061)  Acc@1: 75.0000 (76.4785)  Acc@5: 95.8333 (96.1022)  time: 0.0895  data: 0.0003  max mem: 230\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 1.3846 (1.4236)  Acc@1: 79.1667 (77.0325)  Acc@5: 95.8333 (95.8333)  time: 0.0902  data: 0.0002  max mem: 230\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 1.4125 (1.4335)  Acc@1: 79.1667 (76.8000)  Acc@5: 95.8333 (95.8000)  time: 0.0884  data: 0.0002  max mem: 230\n",
            "Test: [Task 3] Total time: 0:00:04 (0.1001 s / it)\n",
            "* Acc@1 76.800 Acc@5 95.800 loss 1.434\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:20  Loss: 2.2613 (2.2613)  Acc@1: 58.3333 (58.3333)  Acc@5: 75.0000 (75.0000)  time: 0.4853  data: 0.4107  max mem: 230\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:04  Loss: 1.7555 (1.7988)  Acc@1: 70.8333 (67.8030)  Acc@5: 91.6667 (91.6667)  time: 0.1264  data: 0.0378  max mem: 230\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 1.7729 (1.8289)  Acc@1: 62.5000 (65.2778)  Acc@5: 91.6667 (89.2857)  time: 0.0900  data: 0.0004  max mem: 230\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 1.7729 (1.8040)  Acc@1: 62.5000 (65.7258)  Acc@5: 87.5000 (89.6505)  time: 0.0896  data: 0.0004  max mem: 230\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 1.7651 (1.8159)  Acc@1: 66.6667 (66.7683)  Acc@5: 87.5000 (89.8374)  time: 0.0903  data: 0.0004  max mem: 230\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 1.7644 (1.8085)  Acc@1: 66.6667 (67.0000)  Acc@5: 91.6667 (89.9000)  time: 0.0882  data: 0.0004  max mem: 230\n",
            "Test: [Task 4] Total time: 0:00:04 (0.1024 s / it)\n",
            "* Acc@1 67.000 Acc@5 89.900 loss 1.809\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:31  Loss: 2.6292 (2.6292)  Acc@1: 20.8333 (20.8333)  Acc@5: 87.5000 (87.5000)  time: 0.7606  data: 0.6651  max mem: 230\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:04  Loss: 2.6292 (2.6242)  Acc@1: 25.0000 (27.6515)  Acc@5: 87.5000 (86.7424)  time: 0.1509  data: 0.0620  max mem: 230\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 2.6057 (2.6356)  Acc@1: 25.0000 (27.3810)  Acc@5: 83.3333 (84.5238)  time: 0.0893  data: 0.0011  max mem: 230\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 2.6783 (2.6491)  Acc@1: 20.8333 (25.4032)  Acc@5: 83.3333 (84.1398)  time: 0.0895  data: 0.0004  max mem: 230\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 2.6937 (2.6856)  Acc@1: 20.8333 (24.1870)  Acc@5: 79.1667 (82.6220)  time: 0.0902  data: 0.0003  max mem: 230\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 2.7088 (2.6927)  Acc@1: 20.8333 (23.9000)  Acc@5: 79.1667 (82.4000)  time: 0.0882  data: 0.0003  max mem: 230\n",
            "Test: [Task 5] Total time: 0:00:04 (0.1069 s / it)\n",
            "* Acc@1 23.900 Acc@5 82.400 loss 2.693\n",
            "[Average accuracy till task5]\tAcc@1: 65.2600\tAcc@5: 92.3600\tLoss: 1.7479\tForgetting: 2.6750\tBackward: 11.4750\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([10512, 384])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 0.9686  Acc@1: 75.0000 (76.5625)  Acc@5: 95.8333 (96.3542)\n",
            "torch.Size([10512, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 1.2693  Acc@1: 79.1667 (80.2083)  Acc@5: 100.0000 (96.6667)\n",
            "torch.Size([10512, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 1.2310  Acc@1: 87.5000 (84.8958)  Acc@5: 100.0000 (98.5417)\n",
            "torch.Size([10512, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 0.8625  Acc@1: 87.5000 (86.3542)  Acc@5: 100.0000 (98.5417)\n",
            "torch.Size([10512, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 1.1324  Acc@1: 83.3333 (85.7292)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([10512, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 1.0889  Acc@1: 87.5000 (86.8750)  Acc@5: 100.0000 (99.3750)\n",
            "torch.Size([10512, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.9304  Acc@1: 87.5000 (88.7500)  Acc@5: 100.0000 (98.9583)\n",
            "torch.Size([10512, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 0.9285  Acc@1: 91.6667 (90.7292)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([10512, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 0.8682  Acc@1: 91.6667 (90.0000)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([10512, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.8122  Acc@1: 91.6667 (89.3750)  Acc@5: 100.0000 (99.3750)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:21  Loss: 1.0456 (1.0456)  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  time: 0.5027  data: 0.4268  max mem: 231\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 1.0969 (1.1194)  Acc@1: 79.1667 (78.0303)  Acc@5: 100.0000 (97.3485)  time: 0.1266  data: 0.0398  max mem: 231\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 1.0992 (1.1190)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (97.0238)  time: 0.0880  data: 0.0008  max mem: 231\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 1.0437 (1.0982)  Acc@1: 83.3333 (80.5108)  Acc@5: 95.8333 (97.3118)  time: 0.0874  data: 0.0004  max mem: 231\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.9960 (1.0735)  Acc@1: 83.3333 (81.6057)  Acc@5: 100.0000 (97.5610)  time: 0.0884  data: 0.0002  max mem: 231\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.9763 (1.0674)  Acc@1: 83.3333 (81.8000)  Acc@5: 100.0000 (97.6000)  time: 0.0865  data: 0.0002  max mem: 231\n",
            "Test: [Task 1] Total time: 0:00:04 (0.0993 s / it)\n",
            "* Acc@1 81.800 Acc@5 97.600 loss 1.067\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:13  Loss: 1.4227 (1.4227)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  time: 0.3262  data: 0.2530  max mem: 231\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:03  Loss: 1.4191 (1.3849)  Acc@1: 75.0000 (73.4849)  Acc@5: 91.6667 (94.6970)  time: 0.1156  data: 0.0311  max mem: 231\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 1.3337 (1.3897)  Acc@1: 75.0000 (74.0079)  Acc@5: 95.8333 (94.2460)  time: 0.0915  data: 0.0047  max mem: 231\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 1.2876 (1.3721)  Acc@1: 75.0000 (74.0591)  Acc@5: 95.8333 (94.6237)  time: 0.0890  data: 0.0005  max mem: 231\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 1.2876 (1.3579)  Acc@1: 75.0000 (74.3902)  Acc@5: 95.8333 (95.1220)  time: 0.0894  data: 0.0003  max mem: 231\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 1.2731 (1.3506)  Acc@1: 75.0000 (74.4000)  Acc@5: 95.8333 (95.2000)  time: 0.0877  data: 0.0003  max mem: 231\n",
            "Test: [Task 2] Total time: 0:00:04 (0.0985 s / it)\n",
            "* Acc@1 74.400 Acc@5 95.200 loss 1.351\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:26  Loss: 0.9913 (0.9913)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  time: 0.6265  data: 0.5461  max mem: 231\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:04  Loss: 1.2007 (1.2023)  Acc@1: 75.0000 (77.6515)  Acc@5: 95.8333 (95.8333)  time: 0.1394  data: 0.0504  max mem: 231\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 1.2007 (1.1780)  Acc@1: 75.0000 (77.9762)  Acc@5: 95.8333 (96.4286)  time: 0.0893  data: 0.0006  max mem: 231\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 1.0999 (1.1462)  Acc@1: 79.1667 (78.6290)  Acc@5: 95.8333 (96.2366)  time: 0.0886  data: 0.0004  max mem: 231\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 1.1524 (1.1610)  Acc@1: 75.0000 (78.7602)  Acc@5: 95.8333 (96.0366)  time: 0.0896  data: 0.0004  max mem: 231\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 1.1575 (1.1700)  Acc@1: 75.0000 (78.5000)  Acc@5: 95.8333 (96.0000)  time: 0.0879  data: 0.0004  max mem: 231\n",
            "Test: [Task 3] Total time: 0:00:04 (0.1037 s / it)\n",
            "* Acc@1 78.500 Acc@5 96.000 loss 1.170\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:20  Loss: 1.6555 (1.6555)  Acc@1: 62.5000 (62.5000)  Acc@5: 87.5000 (87.5000)  time: 0.4845  data: 0.4118  max mem: 231\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:04  Loss: 1.2736 (1.2940)  Acc@1: 75.0000 (74.6212)  Acc@5: 95.8333 (96.2121)  time: 0.1271  data: 0.0379  max mem: 231\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 1.2834 (1.3184)  Acc@1: 75.0000 (75.3968)  Acc@5: 95.8333 (96.2302)  time: 0.0898  data: 0.0004  max mem: 231\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 1.3020 (1.2925)  Acc@1: 75.0000 (76.2097)  Acc@5: 95.8333 (96.3710)  time: 0.0889  data: 0.0003  max mem: 231\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 1.2595 (1.3072)  Acc@1: 75.0000 (75.8130)  Acc@5: 95.8333 (96.0366)  time: 0.0895  data: 0.0002  max mem: 231\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 1.2144 (1.3007)  Acc@1: 75.0000 (76.1000)  Acc@5: 95.8333 (96.1000)  time: 0.0877  data: 0.0002  max mem: 231\n",
            "Test: [Task 4] Total time: 0:00:04 (0.1005 s / it)\n",
            "* Acc@1 76.100 Acc@5 96.100 loss 1.301\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:19  Loss: 1.4635 (1.4635)  Acc@1: 62.5000 (62.5000)  Acc@5: 100.0000 (100.0000)  time: 0.4754  data: 0.3893  max mem: 231\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:03  Loss: 1.4808 (1.4668)  Acc@1: 66.6667 (70.4545)  Acc@5: 95.8333 (96.5909)  time: 0.1248  data: 0.0365  max mem: 231\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 1.4930 (1.4971)  Acc@1: 66.6667 (69.0476)  Acc@5: 95.8333 (95.6349)  time: 0.0890  data: 0.0008  max mem: 231\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 1.5299 (1.5223)  Acc@1: 66.6667 (67.6075)  Acc@5: 95.8333 (95.0269)  time: 0.0890  data: 0.0004  max mem: 231\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 1.6562 (1.5734)  Acc@1: 62.5000 (64.9390)  Acc@5: 91.6667 (94.3089)  time: 0.0904  data: 0.0005  max mem: 231\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 1.6622 (1.5855)  Acc@1: 62.5000 (64.5000)  Acc@5: 91.6667 (94.2000)  time: 0.0887  data: 0.0005  max mem: 231\n",
            "Test: [Task 5] Total time: 0:00:04 (0.1020 s / it)\n",
            "* Acc@1 64.500 Acc@5 94.200 loss 1.585\n",
            "[Average accuracy till task5]\tAcc@1: 75.0600\tAcc@5: 95.8200\tLoss: 1.2948\tForgetting: 3.7000\tBackward: -2.2750\n",
            "Train: Epoch[ 1/10]  [ 0/20]  eta: 0:00:16  Lr: 0.000047  Loss: 2.3531  Acc@1: 12.5000 (12.5000)  Acc@5: 41.6667 (41.6667)  time: 0.8280  data: 0.6673  max mem: 231\n",
            "Train: Epoch[ 1/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 2.2028  Acc@1: 12.5000 (11.7424)  Acc@5: 58.3333 (52.2727)  time: 0.1580  data: 0.0616  max mem: 231\n",
            "Train: Epoch[ 1/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 2.0987  Acc@1: 12.5000 (12.5000)  Acc@5: 58.3333 (59.7917)  time: 0.1273  data: 0.0340  max mem: 231\n",
            "Train: Epoch[ 1/10] Total time: 0:00:02 (0.1315 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 2.0987  Acc@1: 12.5000 (12.5000)  Acc@5: 58.3333 (59.7917)\n",
            "Train: Epoch[ 2/10]  [ 0/20]  eta: 0:00:09  Lr: 0.000047  Loss: 2.1152  Acc@1: 33.3333 (33.3333)  Acc@5: 75.0000 (75.0000)  time: 0.4943  data: 0.3698  max mem: 231\n",
            "Train: Epoch[ 2/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 2.0999  Acc@1: 29.1667 (27.6515)  Acc@5: 79.1667 (79.9242)  time: 0.1349  data: 0.0371  max mem: 231\n",
            "Train: Epoch[ 2/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 2.0110  Acc@1: 33.3333 (30.8333)  Acc@5: 83.3333 (82.0833)  time: 0.1145  data: 0.0205  max mem: 231\n",
            "Train: Epoch[ 2/10] Total time: 0:00:02 (0.1203 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 2.0110  Acc@1: 33.3333 (30.8333)  Acc@5: 83.3333 (82.0833)\n",
            "Train: Epoch[ 3/10]  [ 0/20]  eta: 0:00:07  Lr: 0.000047  Loss: 1.9408  Acc@1: 37.5000 (37.5000)  Acc@5: 87.5000 (87.5000)  time: 0.3966  data: 0.2954  max mem: 231\n",
            "Train: Epoch[ 3/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 1.7074  Acc@1: 37.5000 (37.8788)  Acc@5: 87.5000 (88.6364)  time: 0.1308  data: 0.0382  max mem: 231\n",
            "Train: Epoch[ 3/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 1.6240  Acc@1: 41.6667 (40.6250)  Acc@5: 91.6667 (90.6250)  time: 0.1124  data: 0.0212  max mem: 231\n",
            "Train: Epoch[ 3/10] Total time: 0:00:02 (0.1163 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.6240  Acc@1: 41.6667 (40.6250)  Acc@5: 91.6667 (90.6250)\n",
            "Train: Epoch[ 4/10]  [ 0/20]  eta: 0:00:11  Lr: 0.000047  Loss: 1.5591  Acc@1: 62.5000 (62.5000)  Acc@5: 100.0000 (100.0000)  time: 0.5617  data: 0.4591  max mem: 231\n",
            "Train: Epoch[ 4/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 1.5237  Acc@1: 50.0000 (50.3788)  Acc@5: 95.8333 (96.2121)  time: 0.1345  data: 0.0434  max mem: 231\n",
            "Train: Epoch[ 4/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 1.5532  Acc@1: 50.0000 (52.5000)  Acc@5: 95.8333 (94.3750)  time: 0.1142  data: 0.0239  max mem: 231\n",
            "Train: Epoch[ 4/10] Total time: 0:00:02 (0.1184 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.5532  Acc@1: 50.0000 (52.5000)  Acc@5: 95.8333 (94.3750)\n",
            "Train: Epoch[ 5/10]  [ 0/20]  eta: 0:00:08  Lr: 0.000047  Loss: 1.3902  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  time: 0.4166  data: 0.3140  max mem: 231\n",
            "Train: Epoch[ 5/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 1.5742  Acc@1: 54.1667 (56.4394)  Acc@5: 95.8333 (96.9697)  time: 0.1410  data: 0.0371  max mem: 231\n",
            "Train: Epoch[ 5/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 1.4335  Acc@1: 58.3333 (59.7917)  Acc@5: 95.8333 (97.5000)  time: 0.1211  data: 0.0205  max mem: 231\n",
            "Train: Epoch[ 5/10] Total time: 0:00:02 (0.1282 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.4335  Acc@1: 58.3333 (59.7917)  Acc@5: 95.8333 (97.5000)\n",
            "Train: Epoch[ 6/10]  [ 0/20]  eta: 0:00:16  Lr: 0.000047  Loss: 1.3451  Acc@1: 70.8333 (70.8333)  Acc@5: 100.0000 (100.0000)  time: 0.8011  data: 0.6579  max mem: 231\n",
            "Train: Epoch[ 6/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 1.4441  Acc@1: 66.6667 (62.5000)  Acc@5: 95.8333 (96.5909)  time: 0.1738  data: 0.0699  max mem: 231\n",
            "Train: Epoch[ 6/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 1.3537  Acc@1: 66.6667 (64.1667)  Acc@5: 100.0000 (97.7083)  time: 0.1380  data: 0.0388  max mem: 231\n",
            "Train: Epoch[ 6/10] Total time: 0:00:02 (0.1426 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.3537  Acc@1: 66.6667 (64.1667)  Acc@5: 100.0000 (97.7083)\n",
            "Train: Epoch[ 7/10]  [ 0/20]  eta: 0:00:10  Lr: 0.000047  Loss: 1.2088  Acc@1: 54.1667 (54.1667)  Acc@5: 100.0000 (100.0000)  time: 0.5015  data: 0.3988  max mem: 231\n",
            "Train: Epoch[ 7/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 1.2094  Acc@1: 70.8333 (68.5606)  Acc@5: 100.0000 (98.8636)  time: 0.1294  data: 0.0375  max mem: 231\n",
            "Train: Epoch[ 7/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 1.0961  Acc@1: 70.8333 (67.9167)  Acc@5: 100.0000 (98.7500)  time: 0.1118  data: 0.0208  max mem: 231\n",
            "Train: Epoch[ 7/10] Total time: 0:00:02 (0.1159 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.0961  Acc@1: 70.8333 (67.9167)  Acc@5: 100.0000 (98.7500)\n",
            "Train: Epoch[ 8/10]  [ 0/20]  eta: 0:00:08  Lr: 0.000047  Loss: 1.1839  Acc@1: 66.6667 (66.6667)  Acc@5: 95.8333 (95.8333)  time: 0.4331  data: 0.3079  max mem: 231\n",
            "Train: Epoch[ 8/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 1.0853  Acc@1: 70.8333 (70.8333)  Acc@5: 100.0000 (98.1061)  time: 0.1280  data: 0.0334  max mem: 231\n",
            "Train: Epoch[ 8/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 1.1694  Acc@1: 70.8333 (71.2500)  Acc@5: 95.8333 (97.5000)  time: 0.1108  data: 0.0185  max mem: 231\n",
            "Train: Epoch[ 8/10] Total time: 0:00:02 (0.1148 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.1694  Acc@1: 70.8333 (71.2500)  Acc@5: 95.8333 (97.5000)\n",
            "Train: Epoch[ 9/10]  [ 0/20]  eta: 0:00:07  Lr: 0.000047  Loss: 1.0929  Acc@1: 70.8333 (70.8333)  Acc@5: 100.0000 (100.0000)  time: 0.3783  data: 0.2746  max mem: 231\n",
            "Train: Epoch[ 9/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 1.0788  Acc@1: 75.0000 (73.8636)  Acc@5: 100.0000 (97.3485)  time: 0.1282  data: 0.0349  max mem: 231\n",
            "Train: Epoch[ 9/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 0.9987  Acc@1: 75.0000 (73.1250)  Acc@5: 100.0000 (97.5000)  time: 0.1110  data: 0.0194  max mem: 231\n",
            "Train: Epoch[ 9/10] Total time: 0:00:02 (0.1158 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.9987  Acc@1: 75.0000 (73.1250)  Acc@5: 100.0000 (97.5000)\n",
            "Train: Epoch[10/10]  [ 0/20]  eta: 0:00:08  Lr: 0.000047  Loss: 1.0537  Acc@1: 70.8333 (70.8333)  Acc@5: 100.0000 (100.0000)  time: 0.4144  data: 0.3075  max mem: 231\n",
            "Train: Epoch[10/10]  [10/20]  eta: 0:00:01  Lr: 0.000047  Loss: 0.9317  Acc@1: 75.0000 (76.8939)  Acc@5: 100.0000 (98.4848)  time: 0.1248  data: 0.0295  max mem: 231\n",
            "Train: Epoch[10/10]  [19/20]  eta: 0:00:00  Lr: 0.000047  Loss: 0.9907  Acc@1: 75.0000 (74.7917)  Acc@5: 100.0000 (98.7500)  time: 0.1093  data: 0.0163  max mem: 231\n",
            "Train: Epoch[10/10] Total time: 0:00:02 (0.1134 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.9907  Acc@1: 75.0000 (74.7917)  Acc@5: 100.0000 (98.7500)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:21  Loss: 1.1776 (1.1776)  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  time: 0.5129  data: 0.3874  max mem: 231\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 1.2188 (1.2201)  Acc@1: 79.1667 (76.8939)  Acc@5: 95.8333 (96.2121)  time: 0.1300  data: 0.0362  max mem: 231\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 1.2188 (1.2229)  Acc@1: 79.1667 (78.1746)  Acc@5: 95.8333 (96.0317)  time: 0.0909  data: 0.0010  max mem: 231\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 1.1447 (1.2004)  Acc@1: 79.1667 (78.2258)  Acc@5: 95.8333 (96.3710)  time: 0.0903  data: 0.0021  max mem: 231\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 1.1238 (1.1761)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (96.9512)  time: 0.0907  data: 0.0019  max mem: 231\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 1.0920 (1.1701)  Acc@1: 79.1667 (79.4000)  Acc@5: 95.8333 (97.0000)  time: 0.0890  data: 0.0016  max mem: 231\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1020 s / it)\n",
            "* Acc@1 79.400 Acc@5 97.000 loss 1.170\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:19  Loss: 1.4690 (1.4690)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  time: 0.4645  data: 0.3813  max mem: 231\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 1.4690 (1.4666)  Acc@1: 70.8333 (72.7273)  Acc@5: 91.6667 (93.9394)  time: 0.1262  data: 0.0353  max mem: 231\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 1.4064 (1.4728)  Acc@1: 75.0000 (74.0079)  Acc@5: 91.6667 (93.4524)  time: 0.0912  data: 0.0005  max mem: 231\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 1.3538 (1.4568)  Acc@1: 75.0000 (73.3871)  Acc@5: 95.8333 (93.9516)  time: 0.0902  data: 0.0003  max mem: 231\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 1.3563 (1.4429)  Acc@1: 70.8333 (73.4756)  Acc@5: 95.8333 (94.7154)  time: 0.0905  data: 0.0002  max mem: 231\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 1.3518 (1.4352)  Acc@1: 70.8333 (73.5000)  Acc@5: 95.8333 (94.8000)  time: 0.0891  data: 0.0002  max mem: 231\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1011 s / it)\n",
            "* Acc@1 73.500 Acc@5 94.800 loss 1.435\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:20  Loss: 1.1057 (1.1057)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.4954  data: 0.3840  max mem: 231\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:04  Loss: 1.3247 (1.3291)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (94.3182)  time: 0.1280  data: 0.0360  max mem: 231\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 1.3111 (1.3066)  Acc@1: 75.0000 (74.4048)  Acc@5: 95.8333 (95.0397)  time: 0.0906  data: 0.0007  max mem: 231\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 1.2138 (1.2751)  Acc@1: 70.8333 (74.5968)  Acc@5: 95.8333 (95.0269)  time: 0.0899  data: 0.0003  max mem: 231\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 1.2617 (1.2929)  Acc@1: 75.0000 (74.2886)  Acc@5: 95.8333 (94.9187)  time: 0.0907  data: 0.0002  max mem: 231\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 1.2977 (1.3035)  Acc@1: 75.0000 (74.2000)  Acc@5: 95.8333 (94.8000)  time: 0.0890  data: 0.0002  max mem: 231\n",
            "Test: [Task 3] Total time: 0:00:04 (0.1015 s / it)\n",
            "* Acc@1 74.200 Acc@5 94.800 loss 1.303\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:14  Loss: 1.8892 (1.8892)  Acc@1: 50.0000 (50.0000)  Acc@5: 83.3333 (83.3333)  time: 0.3568  data: 0.2730  max mem: 231\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:03  Loss: 1.3904 (1.4332)  Acc@1: 75.0000 (70.4545)  Acc@5: 95.8333 (93.9394)  time: 0.1217  data: 0.0325  max mem: 231\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 1.4309 (1.4562)  Acc@1: 70.8333 (70.4365)  Acc@5: 95.8333 (92.8571)  time: 0.0937  data: 0.0049  max mem: 231\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 1.4375 (1.4319)  Acc@1: 70.8333 (70.6989)  Acc@5: 91.6667 (93.4140)  time: 0.0901  data: 0.0012  max mem: 231\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 1.4129 (1.4484)  Acc@1: 70.8333 (70.9350)  Acc@5: 91.6667 (92.9878)  time: 0.0912  data: 0.0008  max mem: 231\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 1.3589 (1.4423)  Acc@1: 70.8333 (71.3000)  Acc@5: 93.7500 (93.0000)  time: 0.0892  data: 0.0008  max mem: 231\n",
            "Test: [Task 4] Total time: 0:00:04 (0.1011 s / it)\n",
            "* Acc@1 71.300 Acc@5 93.000 loss 1.442\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:29  Loss: 1.5374 (1.5374)  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  time: 0.6995  data: 0.6246  max mem: 231\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:04  Loss: 1.5870 (1.5627)  Acc@1: 70.8333 (71.2121)  Acc@5: 95.8333 (96.2121)  time: 0.1468  data: 0.0571  max mem: 231\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 1.5857 (1.5814)  Acc@1: 70.8333 (69.0476)  Acc@5: 95.8333 (94.6429)  time: 0.0906  data: 0.0004  max mem: 231\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 1.5857 (1.6065)  Acc@1: 66.6667 (67.7419)  Acc@5: 91.6667 (94.3548)  time: 0.0897  data: 0.0005  max mem: 231\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 1.7705 (1.6608)  Acc@1: 58.3333 (65.3455)  Acc@5: 91.6667 (93.5976)  time: 0.0900  data: 0.0004  max mem: 231\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 1.7831 (1.6733)  Acc@1: 58.3333 (64.8000)  Acc@5: 91.6667 (93.4000)  time: 0.0883  data: 0.0004  max mem: 231\n",
            "Test: [Task 5] Total time: 0:00:04 (0.1074 s / it)\n",
            "* Acc@1 64.800 Acc@5 93.400 loss 1.673\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:30  Loss: 2.9788 (2.9788)  Acc@1: 16.6667 (16.6667)  Acc@5: 66.6667 (66.6667)  time: 0.7253  data: 0.6429  max mem: 231\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:04  Loss: 3.1866 (3.1530)  Acc@1: 4.1667 (6.0606)  Acc@5: 66.6667 (64.3939)  time: 0.1478  data: 0.0608  max mem: 231\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 3.1875 (3.1642)  Acc@1: 4.1667 (5.5556)  Acc@5: 62.5000 (62.6984)  time: 0.0898  data: 0.0021  max mem: 231\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 3.2092 (3.1940)  Acc@1: 4.1667 (5.2419)  Acc@5: 62.5000 (62.5000)  time: 0.0901  data: 0.0011  max mem: 231\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 3.2446 (3.2012)  Acc@1: 4.1667 (5.0813)  Acc@5: 62.5000 (61.3821)  time: 0.0906  data: 0.0004  max mem: 231\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 3.2298 (3.2012)  Acc@1: 4.1667 (5.0000)  Acc@5: 62.5000 (61.4000)  time: 0.0892  data: 0.0004  max mem: 231\n",
            "Test: [Task 6] Total time: 0:00:04 (0.1069 s / it)\n",
            "* Acc@1 5.000 Acc@5 61.400 loss 3.201\n",
            "[Average accuracy till task6]\tAcc@1: 61.3667\tAcc@5: 89.0667\tLoss: 1.7043\tForgetting: 3.8000\tBackward: 16.5600\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([12624, 384])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 1.3549  Acc@1: 79.1667 (75.6667)  Acc@5: 95.8333 (95.8333)\n",
            "torch.Size([12624, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 1.0872  Acc@1: 83.3333 (81.0833)  Acc@5: 100.0000 (98.1667)\n",
            "torch.Size([12624, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 0.9919  Acc@1: 87.5000 (84.1667)  Acc@5: 100.0000 (98.6667)\n",
            "torch.Size([12624, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 0.7325  Acc@1: 87.5000 (86.9167)  Acc@5: 100.0000 (99.4167)\n",
            "torch.Size([12624, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 1.1519  Acc@1: 87.5000 (88.0833)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([12624, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 0.6797  Acc@1: 87.5000 (88.9167)  Acc@5: 100.0000 (99.7500)\n",
            "torch.Size([12624, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.9651  Acc@1: 91.6667 (88.5000)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([12624, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 1.0284  Acc@1: 91.6667 (89.7500)  Acc@5: 100.0000 (99.5000)\n",
            "torch.Size([12624, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 0.8257  Acc@1: 87.5000 (89.8333)  Acc@5: 100.0000 (99.3333)\n",
            "torch.Size([12624, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.8240  Acc@1: 87.5000 (89.1667)  Acc@5: 100.0000 (99.5000)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:23  Loss: 1.1210 (1.1210)  Acc@1: 75.0000 (75.0000)  Acc@5: 87.5000 (87.5000)  time: 0.5521  data: 0.4725  max mem: 231\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 1.1210 (1.1096)  Acc@1: 75.0000 (76.8939)  Acc@5: 95.8333 (94.6970)  time: 0.1343  data: 0.0472  max mem: 231\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 1.1143 (1.1010)  Acc@1: 75.0000 (77.1825)  Acc@5: 95.8333 (95.2381)  time: 0.0900  data: 0.0025  max mem: 231\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 1.0189 (1.0783)  Acc@1: 75.0000 (77.5538)  Acc@5: 95.8333 (95.8333)  time: 0.0881  data: 0.0004  max mem: 231\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.9940 (1.0545)  Acc@1: 79.1667 (78.5569)  Acc@5: 95.8333 (96.4431)  time: 0.0890  data: 0.0002  max mem: 231\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.9844 (1.0482)  Acc@1: 79.1667 (78.8000)  Acc@5: 95.8333 (96.5000)  time: 0.0871  data: 0.0002  max mem: 231\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1018 s / it)\n",
            "* Acc@1 78.800 Acc@5 96.500 loss 1.048\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:18  Loss: 1.3432 (1.3432)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  time: 0.4468  data: 0.3580  max mem: 231\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:03  Loss: 1.3432 (1.3284)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (92.4242)  time: 0.1214  data: 0.0340  max mem: 231\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 1.3034 (1.3456)  Acc@1: 70.8333 (71.6270)  Acc@5: 91.6667 (92.4603)  time: 0.0884  data: 0.0014  max mem: 231\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 1.2391 (1.3299)  Acc@1: 70.8333 (71.7742)  Acc@5: 95.8333 (93.0108)  time: 0.0883  data: 0.0008  max mem: 231\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 1.2161 (1.3108)  Acc@1: 70.8333 (72.5610)  Acc@5: 95.8333 (93.8008)  time: 0.0892  data: 0.0004  max mem: 231\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 1.2076 (1.3047)  Acc@1: 75.0000 (72.6000)  Acc@5: 95.8333 (93.9000)  time: 0.0875  data: 0.0003  max mem: 231\n",
            "Test: [Task 2] Total time: 0:00:04 (0.0985 s / it)\n",
            "* Acc@1 72.600 Acc@5 93.900 loss 1.305\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:15  Loss: 0.8278 (0.8278)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.3576  data: 0.2828  max mem: 231\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 1.0751 (1.0875)  Acc@1: 75.0000 (75.3788)  Acc@5: 95.8333 (95.8333)  time: 0.1193  data: 0.0341  max mem: 231\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 1.0751 (1.0687)  Acc@1: 75.0000 (76.3889)  Acc@5: 95.8333 (95.8333)  time: 0.0921  data: 0.0048  max mem: 231\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 1.0534 (1.0441)  Acc@1: 79.1667 (77.0161)  Acc@5: 95.8333 (95.8333)  time: 0.0897  data: 0.0005  max mem: 231\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 1.0534 (1.0572)  Acc@1: 75.0000 (77.2358)  Acc@5: 95.8333 (95.6301)  time: 0.0904  data: 0.0005  max mem: 231\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 1.0534 (1.0656)  Acc@1: 75.0000 (77.1000)  Acc@5: 95.8333 (95.6000)  time: 0.0881  data: 0.0005  max mem: 231\n",
            "Test: [Task 3] Total time: 0:00:04 (0.0995 s / it)\n",
            "* Acc@1 77.100 Acc@5 95.600 loss 1.066\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:33  Loss: 1.6033 (1.6033)  Acc@1: 62.5000 (62.5000)  Acc@5: 87.5000 (87.5000)  time: 0.7877  data: 0.7010  max mem: 231\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:04  Loss: 1.1910 (1.2162)  Acc@1: 75.0000 (74.2424)  Acc@5: 95.8333 (95.0758)  time: 0.1539  data: 0.0646  max mem: 231\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 1.2221 (1.2386)  Acc@1: 75.0000 (75.1984)  Acc@5: 95.8333 (95.4365)  time: 0.0896  data: 0.0010  max mem: 231\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 1.2148 (1.2077)  Acc@1: 79.1667 (75.9409)  Acc@5: 95.8333 (95.6989)  time: 0.0889  data: 0.0006  max mem: 231\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 1.1630 (1.2227)  Acc@1: 75.0000 (75.5081)  Acc@5: 95.8333 (95.1220)  time: 0.0898  data: 0.0002  max mem: 231\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 1.1101 (1.2167)  Acc@1: 75.0000 (75.8000)  Acc@5: 95.8333 (95.1000)  time: 0.0882  data: 0.0002  max mem: 231\n",
            "Test: [Task 4] Total time: 0:00:04 (0.1083 s / it)\n",
            "* Acc@1 75.800 Acc@5 95.100 loss 1.217\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:20  Loss: 1.2185 (1.2185)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  time: 0.4980  data: 0.4270  max mem: 231\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:04  Loss: 1.2556 (1.2439)  Acc@1: 75.0000 (73.4849)  Acc@5: 95.8333 (96.9697)  time: 0.1289  data: 0.0399  max mem: 231\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 1.2694 (1.2765)  Acc@1: 70.8333 (72.6191)  Acc@5: 95.8333 (95.8333)  time: 0.0906  data: 0.0010  max mem: 231\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 1.3101 (1.2934)  Acc@1: 70.8333 (72.3118)  Acc@5: 95.8333 (95.2957)  time: 0.0895  data: 0.0005  max mem: 231\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 1.3936 (1.3426)  Acc@1: 66.6667 (69.9187)  Acc@5: 95.8333 (94.7154)  time: 0.0902  data: 0.0002  max mem: 231\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 1.3945 (1.3546)  Acc@1: 66.6667 (69.6000)  Acc@5: 91.6667 (94.6000)  time: 0.0888  data: 0.0002  max mem: 231\n",
            "Test: [Task 5] Total time: 0:00:04 (0.1015 s / it)\n",
            "* Acc@1 69.600 Acc@5 94.600 loss 1.355\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:12  Loss: 1.3071 (1.3071)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  time: 0.2935  data: 0.2237  max mem: 231\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:03  Loss: 1.5479 (1.5623)  Acc@1: 58.3333 (62.1212)  Acc@5: 95.8333 (94.3182)  time: 0.1225  data: 0.0362  max mem: 231\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 1.5480 (1.5848)  Acc@1: 58.3333 (59.3254)  Acc@5: 95.8333 (94.8413)  time: 0.0972  data: 0.0089  max mem: 231\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 1.6864 (1.6225)  Acc@1: 54.1667 (57.9301)  Acc@5: 95.8333 (94.4892)  time: 0.0901  data: 0.0003  max mem: 231\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.7053 (1.6364)  Acc@1: 54.1667 (57.9268)  Acc@5: 91.6667 (94.0041)  time: 0.0911  data: 0.0003  max mem: 231\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.7053 (1.6386)  Acc@1: 54.1667 (57.9000)  Acc@5: 93.7500 (94.0000)  time: 0.0888  data: 0.0003  max mem: 231\n",
            "Test: [Task 6] Total time: 0:00:04 (0.1012 s / it)\n",
            "* Acc@1 57.900 Acc@5 94.000 loss 1.639\n",
            "[Average accuracy till task6]\tAcc@1: 71.9667\tAcc@5: 94.9500\tLoss: 1.2714\tForgetting: 4.2600\tBackward: -2.1000\n",
            "Train: Epoch[ 1/10]  [ 0/23]  eta: 0:00:13  Lr: 0.000047  Loss: 2.4145  Acc@1: 16.6667 (16.6667)  Acc@5: 37.5000 (37.5000)  time: 0.5783  data: 0.4422  max mem: 231\n",
            "Train: Epoch[ 1/10]  [10/23]  eta: 0:00:02  Lr: 0.000047  Loss: 2.2214  Acc@1: 12.5000 (13.6364)  Acc@5: 54.1667 (53.7879)  time: 0.1832  data: 0.0777  max mem: 231\n",
            "Train: Epoch[ 1/10]  [20/23]  eta: 0:00:00  Lr: 0.000047  Loss: 1.9590  Acc@1: 12.5000 (17.0635)  Acc@5: 58.3333 (62.1032)  time: 0.1176  data: 0.0207  max mem: 231\n",
            "Train: Epoch[ 1/10]  [22/23]  eta: 0:00:00  Lr: 0.000047  Loss: 2.1494  Acc@1: 16.6667 (16.9091)  Acc@5: 62.5000 (62.3636)  time: 0.0993  data: 0.0029  max mem: 231\n",
            "Train: Epoch[ 1/10] Total time: 0:00:03 (0.1388 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 2.1494  Acc@1: 16.6667 (16.9091)  Acc@5: 62.5000 (62.3636)\n",
            "Train: Epoch[ 2/10]  [ 0/23]  eta: 0:00:10  Lr: 0.000047  Loss: 2.0759  Acc@1: 29.1667 (29.1667)  Acc@5: 66.6667 (66.6667)  time: 0.4483  data: 0.3393  max mem: 231\n",
            "Train: Epoch[ 2/10]  [10/23]  eta: 0:00:01  Lr: 0.000047  Loss: 1.8623  Acc@1: 33.3333 (33.3333)  Acc@5: 83.3333 (78.7879)  time: 0.1345  data: 0.0415  max mem: 231\n",
            "Train: Epoch[ 2/10]  [20/23]  eta: 0:00:00  Lr: 0.000047  Loss: 1.7236  Acc@1: 37.5000 (38.2937)  Acc@5: 87.5000 (81.5476)  time: 0.0967  data: 0.0059  max mem: 231\n",
            "Train: Epoch[ 2/10]  [22/23]  eta: 0:00:00  Lr: 0.000047  Loss: 1.8735  Acc@1: 37.5000 (38.3636)  Acc@5: 87.5000 (82.1818)  time: 0.0905  data: 0.0007  max mem: 231\n",
            "Train: Epoch[ 2/10] Total time: 0:00:02 (0.1147 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.8735  Acc@1: 37.5000 (38.3636)  Acc@5: 87.5000 (82.1818)\n",
            "Train: Epoch[ 3/10]  [ 0/23]  eta: 0:00:13  Lr: 0.000047  Loss: 1.6864  Acc@1: 62.5000 (62.5000)  Acc@5: 91.6667 (91.6667)  time: 0.5816  data: 0.4820  max mem: 231\n",
            "Train: Epoch[ 3/10]  [10/23]  eta: 0:00:01  Lr: 0.000047  Loss: 1.8266  Acc@1: 50.0000 (48.4848)  Acc@5: 87.5000 (90.1515)  time: 0.1351  data: 0.0444  max mem: 231\n",
            "Train: Epoch[ 3/10]  [20/23]  eta: 0:00:00  Lr: 0.000047  Loss: 1.6221  Acc@1: 50.0000 (50.9921)  Acc@5: 91.6667 (92.2619)  time: 0.0905  data: 0.0004  max mem: 231\n",
            "Train: Epoch[ 3/10]  [22/23]  eta: 0:00:00  Lr: 0.000047  Loss: 1.6474  Acc@1: 54.1667 (52.0000)  Acc@5: 95.4546 (92.3636)  time: 0.0903  data: 0.0004  max mem: 231\n",
            "Train: Epoch[ 3/10] Total time: 0:00:02 (0.1154 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.6474  Acc@1: 54.1667 (52.0000)  Acc@5: 95.4546 (92.3636)\n",
            "Train: Epoch[ 4/10]  [ 0/23]  eta: 0:00:09  Lr: 0.000047  Loss: 1.6018  Acc@1: 50.0000 (50.0000)  Acc@5: 91.6667 (91.6667)  time: 0.3928  data: 0.2932  max mem: 231\n",
            "Train: Epoch[ 4/10]  [10/23]  eta: 0:00:01  Lr: 0.000047  Loss: 1.5017  Acc@1: 62.5000 (59.0909)  Acc@5: 95.8333 (94.6970)  time: 0.1245  data: 0.0310  max mem: 231\n",
            "Train: Epoch[ 4/10]  [20/23]  eta: 0:00:00  Lr: 0.000047  Loss: 1.0960  Acc@1: 62.5000 (63.2937)  Acc@5: 95.8333 (95.6349)  time: 0.0944  data: 0.0025  max mem: 231\n",
            "Train: Epoch[ 4/10]  [22/23]  eta: 0:00:00  Lr: 0.000047  Loss: 1.5135  Acc@1: 62.5000 (63.2727)  Acc@5: 95.8333 (95.6364)  time: 0.0932  data: 0.0012  max mem: 231\n",
            "Train: Epoch[ 4/10] Total time: 0:00:02 (0.1110 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.5135  Acc@1: 62.5000 (63.2727)  Acc@5: 95.8333 (95.6364)\n",
            "Train: Epoch[ 5/10]  [ 0/23]  eta: 0:00:14  Lr: 0.000047  Loss: 1.2934  Acc@1: 66.6667 (66.6667)  Acc@5: 91.6667 (91.6667)  time: 0.6158  data: 0.5223  max mem: 231\n",
            "Train: Epoch[ 5/10]  [10/23]  eta: 0:00:01  Lr: 0.000047  Loss: 1.2354  Acc@1: 62.5000 (61.7424)  Acc@5: 95.8333 (95.8333)  time: 0.1411  data: 0.0495  max mem: 231\n",
            "Train: Epoch[ 5/10]  [20/23]  eta: 0:00:00  Lr: 0.000047  Loss: 1.6262  Acc@1: 62.5000 (62.8968)  Acc@5: 95.8333 (96.2302)  time: 0.0972  data: 0.0020  max mem: 231\n",
            "Train: Epoch[ 5/10]  [22/23]  eta: 0:00:00  Lr: 0.000047  Loss: 1.4325  Acc@1: 58.3333 (63.8182)  Acc@5: 95.8333 (96.1818)  time: 0.0965  data: 0.0019  max mem: 231\n",
            "Train: Epoch[ 5/10] Total time: 0:00:02 (0.1239 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.4325  Acc@1: 58.3333 (63.8182)  Acc@5: 95.8333 (96.1818)\n",
            "Train: Epoch[ 6/10]  [ 0/23]  eta: 0:00:17  Lr: 0.000047  Loss: 1.0855  Acc@1: 70.8333 (70.8333)  Acc@5: 100.0000 (100.0000)  time: 0.7693  data: 0.6406  max mem: 231\n",
            "Train: Epoch[ 6/10]  [10/23]  eta: 0:00:02  Lr: 0.000047  Loss: 1.3342  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (96.5909)  time: 0.1657  data: 0.0600  max mem: 231\n",
            "Train: Epoch[ 6/10]  [20/23]  eta: 0:00:00  Lr: 0.000047  Loss: 0.8832  Acc@1: 66.6667 (69.2460)  Acc@5: 95.8333 (96.8254)  time: 0.0993  data: 0.0012  max mem: 231\n",
            "Train: Epoch[ 6/10]  [22/23]  eta: 0:00:00  Lr: 0.000047  Loss: 1.0866  Acc@1: 70.8333 (69.6364)  Acc@5: 95.8333 (97.0909)  time: 0.0991  data: 0.0011  max mem: 231\n",
            "Train: Epoch[ 6/10] Total time: 0:00:03 (0.1314 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.0866  Acc@1: 70.8333 (69.6364)  Acc@5: 95.8333 (97.0909)\n",
            "Train: Epoch[ 7/10]  [ 0/23]  eta: 0:00:09  Lr: 0.000047  Loss: 1.2468  Acc@1: 66.6667 (66.6667)  Acc@5: 100.0000 (100.0000)  time: 0.4277  data: 0.3144  max mem: 231\n",
            "Train: Epoch[ 7/10]  [10/23]  eta: 0:00:01  Lr: 0.000047  Loss: 1.0955  Acc@1: 70.8333 (73.8636)  Acc@5: 100.0000 (98.4848)  time: 0.1278  data: 0.0353  max mem: 231\n",
            "Train: Epoch[ 7/10]  [20/23]  eta: 0:00:00  Lr: 0.000047  Loss: 1.0871  Acc@1: 70.8333 (73.8095)  Acc@5: 100.0000 (98.2143)  time: 0.0942  data: 0.0040  max mem: 231\n",
            "Train: Epoch[ 7/10]  [22/23]  eta: 0:00:00  Lr: 0.000047  Loss: 0.9176  Acc@1: 70.8333 (74.5455)  Acc@5: 100.0000 (98.0000)  time: 0.0905  data: 0.0007  max mem: 231\n",
            "Train: Epoch[ 7/10] Total time: 0:00:02 (0.1119 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.9176  Acc@1: 70.8333 (74.5455)  Acc@5: 100.0000 (98.0000)\n",
            "Train: Epoch[ 8/10]  [ 0/23]  eta: 0:00:10  Lr: 0.000047  Loss: 1.0819  Acc@1: 66.6667 (66.6667)  Acc@5: 100.0000 (100.0000)  time: 0.4427  data: 0.3198  max mem: 231\n",
            "Train: Epoch[ 8/10]  [10/23]  eta: 0:00:01  Lr: 0.000047  Loss: 0.6770  Acc@1: 79.1667 (77.2727)  Acc@5: 100.0000 (99.2424)  time: 0.1288  data: 0.0346  max mem: 231\n",
            "Train: Epoch[ 8/10]  [20/23]  eta: 0:00:00  Lr: 0.000047  Loss: 0.9327  Acc@1: 79.1667 (74.8016)  Acc@5: 100.0000 (98.6111)  time: 0.0935  data: 0.0031  max mem: 231\n",
            "Train: Epoch[ 8/10]  [22/23]  eta: 0:00:00  Lr: 0.000047  Loss: 0.9033  Acc@1: 75.0000 (74.1818)  Acc@5: 100.0000 (98.7273)  time: 0.0911  data: 0.0007  max mem: 231\n",
            "Train: Epoch[ 8/10] Total time: 0:00:02 (0.1121 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.9033  Acc@1: 75.0000 (74.1818)  Acc@5: 100.0000 (98.7273)\n",
            "Train: Epoch[ 9/10]  [ 0/23]  eta: 0:00:09  Lr: 0.000047  Loss: 1.0553  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.8333)  time: 0.4219  data: 0.3277  max mem: 231\n",
            "Train: Epoch[ 9/10]  [10/23]  eta: 0:00:01  Lr: 0.000047  Loss: 1.0775  Acc@1: 75.0000 (75.3788)  Acc@5: 95.8333 (97.3485)  time: 0.1240  data: 0.0315  max mem: 231\n",
            "Train: Epoch[ 9/10]  [20/23]  eta: 0:00:00  Lr: 0.000047  Loss: 0.9407  Acc@1: 75.0000 (74.6032)  Acc@5: 95.8333 (97.4206)  time: 0.0921  data: 0.0013  max mem: 231\n",
            "Train: Epoch[ 9/10]  [22/23]  eta: 0:00:00  Lr: 0.000047  Loss: 0.9156  Acc@1: 75.0000 (75.2727)  Acc@5: 95.8333 (97.4545)  time: 0.0910  data: 0.0013  max mem: 231\n",
            "Train: Epoch[ 9/10] Total time: 0:00:02 (0.1095 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.9156  Acc@1: 75.0000 (75.2727)  Acc@5: 95.8333 (97.4545)\n",
            "Train: Epoch[10/10]  [ 0/23]  eta: 0:00:13  Lr: 0.000047  Loss: 1.0037  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.5754  data: 0.4705  max mem: 231\n",
            "Train: Epoch[10/10]  [10/23]  eta: 0:00:01  Lr: 0.000047  Loss: 0.6735  Acc@1: 79.1667 (80.3030)  Acc@5: 100.0000 (97.7273)  time: 0.1368  data: 0.0436  max mem: 231\n",
            "Train: Epoch[10/10]  [20/23]  eta: 0:00:00  Lr: 0.000047  Loss: 0.7383  Acc@1: 79.1667 (80.7540)  Acc@5: 100.0000 (98.2143)  time: 0.0919  data: 0.0008  max mem: 231\n",
            "Train: Epoch[10/10]  [22/23]  eta: 0:00:00  Lr: 0.000047  Loss: 1.0589  Acc@1: 79.1667 (79.8182)  Acc@5: 100.0000 (97.8182)  time: 0.0915  data: 0.0008  max mem: 231\n",
            "Train: Epoch[10/10] Total time: 0:00:02 (0.1180 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.0589  Acc@1: 79.1667 (79.8182)  Acc@5: 100.0000 (97.8182)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:30  Loss: 1.2602 (1.2602)  Acc@1: 75.0000 (75.0000)  Acc@5: 87.5000 (87.5000)  time: 0.7227  data: 0.6278  max mem: 231\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 1.2354 (1.2195)  Acc@1: 75.0000 (72.7273)  Acc@5: 95.8333 (95.0758)  time: 0.1494  data: 0.0622  max mem: 231\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 1.2092 (1.2153)  Acc@1: 75.0000 (74.2064)  Acc@5: 95.8333 (95.4365)  time: 0.0909  data: 0.0035  max mem: 231\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 1.1191 (1.1983)  Acc@1: 75.0000 (74.4624)  Acc@5: 95.8333 (95.8333)  time: 0.0893  data: 0.0011  max mem: 231\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 1.0828 (1.1717)  Acc@1: 79.1667 (76.1179)  Acc@5: 95.8333 (96.2398)  time: 0.0892  data: 0.0007  max mem: 231\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 1.0627 (1.1650)  Acc@1: 79.1667 (76.4000)  Acc@5: 95.8333 (96.3000)  time: 0.0876  data: 0.0006  max mem: 231\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1075 s / it)\n",
            "* Acc@1 76.400 Acc@5 96.300 loss 1.165\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:28  Loss: 1.3980 (1.3980)  Acc@1: 66.6667 (66.6667)  Acc@5: 95.8333 (95.8333)  time: 0.6799  data: 0.5502  max mem: 231\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 1.3980 (1.3898)  Acc@1: 70.8333 (71.2121)  Acc@5: 95.8333 (94.6970)  time: 0.1443  data: 0.0520  max mem: 231\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 1.3661 (1.4048)  Acc@1: 75.0000 (72.6191)  Acc@5: 95.8333 (93.8492)  time: 0.0901  data: 0.0013  max mem: 231\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 1.2813 (1.3933)  Acc@1: 70.8333 (71.9086)  Acc@5: 95.8333 (93.8172)  time: 0.0897  data: 0.0007  max mem: 231\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 1.3059 (1.3797)  Acc@1: 70.8333 (72.1545)  Acc@5: 95.8333 (94.1057)  time: 0.0900  data: 0.0006  max mem: 231\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 1.2648 (1.3723)  Acc@1: 70.8333 (72.2000)  Acc@5: 95.8333 (94.2000)  time: 0.0884  data: 0.0005  max mem: 231\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1056 s / it)\n",
            "* Acc@1 72.200 Acc@5 94.200 loss 1.372\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:16  Loss: 0.8996 (0.8996)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.4029  data: 0.3157  max mem: 231\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 1.1945 (1.1674)  Acc@1: 79.1667 (76.1364)  Acc@5: 95.8333 (95.0758)  time: 0.1181  data: 0.0294  max mem: 231\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 1.1945 (1.1553)  Acc@1: 79.1667 (76.5873)  Acc@5: 95.8333 (94.4444)  time: 0.0891  data: 0.0005  max mem: 231\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 1.0942 (1.1274)  Acc@1: 79.1667 (77.0161)  Acc@5: 95.8333 (94.8925)  time: 0.0889  data: 0.0004  max mem: 231\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 1.0942 (1.1402)  Acc@1: 75.0000 (76.9309)  Acc@5: 95.8333 (94.6138)  time: 0.0897  data: 0.0004  max mem: 231\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 1.0942 (1.1474)  Acc@1: 75.0000 (76.8000)  Acc@5: 95.8333 (94.7000)  time: 0.0878  data: 0.0004  max mem: 231\n",
            "Test: [Task 3] Total time: 0:00:04 (0.0993 s / it)\n",
            "* Acc@1 76.800 Acc@5 94.700 loss 1.147\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:29  Loss: 1.8044 (1.8044)  Acc@1: 50.0000 (50.0000)  Acc@5: 83.3333 (83.3333)  time: 0.6977  data: 0.5850  max mem: 231\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:04  Loss: 1.2424 (1.2872)  Acc@1: 75.0000 (71.9697)  Acc@5: 95.8333 (93.5606)  time: 0.1479  data: 0.0587  max mem: 231\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 1.2991 (1.3102)  Acc@1: 75.0000 (72.0238)  Acc@5: 95.8333 (92.6587)  time: 0.0917  data: 0.0036  max mem: 231\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 1.3250 (1.2854)  Acc@1: 70.8333 (73.1183)  Acc@5: 95.8333 (93.1452)  time: 0.0900  data: 0.0009  max mem: 231\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 1.2378 (1.3008)  Acc@1: 75.0000 (73.3740)  Acc@5: 95.8333 (93.1911)  time: 0.0897  data: 0.0005  max mem: 231\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 1.2256 (1.2956)  Acc@1: 75.0000 (73.7000)  Acc@5: 95.8333 (93.2000)  time: 0.0882  data: 0.0003  max mem: 231\n",
            "Test: [Task 4] Total time: 0:00:04 (0.1065 s / it)\n",
            "* Acc@1 73.700 Acc@5 93.200 loss 1.296\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:15  Loss: 1.3668 (1.3668)  Acc@1: 62.5000 (62.5000)  Acc@5: 95.8333 (95.8333)  time: 0.3573  data: 0.2822  max mem: 231\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:03  Loss: 1.3582 (1.3226)  Acc@1: 70.8333 (72.7273)  Acc@5: 95.8333 (96.9697)  time: 0.1151  data: 0.0271  max mem: 231\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 1.3582 (1.3595)  Acc@1: 70.8333 (70.4365)  Acc@5: 95.8333 (95.4365)  time: 0.0899  data: 0.0009  max mem: 231\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 1.3963 (1.3778)  Acc@1: 66.6667 (70.2957)  Acc@5: 91.6667 (94.7581)  time: 0.0889  data: 0.0003  max mem: 231\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 1.4405 (1.4226)  Acc@1: 66.6667 (68.1911)  Acc@5: 91.6667 (94.2073)  time: 0.0896  data: 0.0003  max mem: 231\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 1.4702 (1.4340)  Acc@1: 66.6667 (68.1000)  Acc@5: 91.6667 (94.1000)  time: 0.0879  data: 0.0002  max mem: 231\n",
            "Test: [Task 5] Total time: 0:00:04 (0.0975 s / it)\n",
            "* Acc@1 68.100 Acc@5 94.100 loss 1.434\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:20  Loss: 1.4657 (1.4657)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  time: 0.4928  data: 0.4177  max mem: 231\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:04  Loss: 1.8290 (1.7966)  Acc@1: 50.0000 (52.2727)  Acc@5: 91.6667 (91.2879)  time: 0.1272  data: 0.0391  max mem: 231\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 1.8290 (1.8226)  Acc@1: 45.8333 (51.1905)  Acc@5: 91.6667 (91.0714)  time: 0.0897  data: 0.0010  max mem: 231\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 1.9470 (1.8659)  Acc@1: 45.8333 (50.0000)  Acc@5: 91.6667 (90.4570)  time: 0.0892  data: 0.0005  max mem: 231\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.9542 (1.8753)  Acc@1: 50.0000 (50.2033)  Acc@5: 87.5000 (90.3455)  time: 0.0899  data: 0.0003  max mem: 231\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.9470 (1.8765)  Acc@1: 50.0000 (50.2000)  Acc@5: 87.5000 (90.3000)  time: 0.0881  data: 0.0003  max mem: 231\n",
            "Test: [Task 6] Total time: 0:00:04 (0.1012 s / it)\n",
            "* Acc@1 50.200 Acc@5 90.300 loss 1.877\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:30  Loss: 3.1037 (3.1037)  Acc@1: 12.5000 (12.5000)  Acc@5: 66.6667 (66.6667)  time: 0.7373  data: 0.6682  max mem: 231\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:04  Loss: 3.0780 (3.0816)  Acc@1: 12.5000 (12.5000)  Acc@5: 66.6667 (61.7424)  time: 0.1511  data: 0.0649  max mem: 231\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:02  Loss: 3.0151 (3.0570)  Acc@1: 12.5000 (14.6825)  Acc@5: 62.5000 (61.3095)  time: 0.0912  data: 0.0031  max mem: 231\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:01  Loss: 3.0151 (3.0678)  Acc@1: 16.6667 (14.9194)  Acc@5: 62.5000 (60.0806)  time: 0.0896  data: 0.0015  max mem: 231\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 3.0030 (3.0639)  Acc@1: 12.5000 (15.1423)  Acc@5: 62.5000 (61.3821)  time: 0.0895  data: 0.0008  max mem: 231\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 3.0030 (3.0570)  Acc@1: 16.6667 (15.4000)  Acc@5: 62.5000 (61.4000)  time: 0.0883  data: 0.0007  max mem: 231\n",
            "Test: [Task 7] Total time: 0:00:04 (0.1071 s / it)\n",
            "* Acc@1 15.400 Acc@5 61.400 loss 3.057\n",
            "[Average accuracy till task7]\tAcc@1: 61.8286\tAcc@5: 89.1714\tLoss: 1.6211\tForgetting: 3.4500\tBackward: 22.0000\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([14736, 384])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 1.1855  Acc@1: 83.3333 (80.0000)  Acc@5: 95.8333 (95.9722)\n",
            "torch.Size([14736, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 1.2063  Acc@1: 83.3333 (83.8889)  Acc@5: 100.0000 (97.7778)\n",
            "torch.Size([14736, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 0.8950  Acc@1: 87.5000 (85.0000)  Acc@5: 100.0000 (98.5417)\n",
            "torch.Size([14736, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 0.7967  Acc@1: 87.5000 (88.9583)  Acc@5: 100.0000 (99.3056)\n",
            "torch.Size([14736, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 0.6150  Acc@1: 91.6667 (89.5833)  Acc@5: 100.0000 (99.2361)\n",
            "torch.Size([14736, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 0.8911  Acc@1: 91.6667 (90.7639)  Acc@5: 100.0000 (99.5139)\n",
            "torch.Size([14736, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.7627  Acc@1: 91.6667 (89.7917)  Acc@5: 100.0000 (99.3750)\n",
            "torch.Size([14736, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 0.6739  Acc@1: 91.6667 (91.7361)  Acc@5: 100.0000 (99.5139)\n",
            "torch.Size([14736, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 0.8436  Acc@1: 91.6667 (90.8333)  Acc@5: 100.0000 (99.4444)\n",
            "torch.Size([14736, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.7441  Acc@1: 87.5000 (90.7639)  Acc@5: 100.0000 (99.0972)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:14  Loss: 1.0764 (1.0764)  Acc@1: 75.0000 (75.0000)  Acc@5: 87.5000 (87.5000)  time: 0.3336  data: 0.2579  max mem: 231\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 1.0764 (1.0480)  Acc@1: 75.0000 (75.7576)  Acc@5: 95.8333 (94.6970)  time: 0.1157  data: 0.0295  max mem: 232\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 1.0627 (1.0358)  Acc@1: 79.1667 (77.7778)  Acc@5: 95.8333 (95.0397)  time: 0.0909  data: 0.0035  max mem: 232\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.9333 (1.0101)  Acc@1: 79.1667 (78.0914)  Acc@5: 95.8333 (95.4301)  time: 0.0883  data: 0.0003  max mem: 232\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.9038 (0.9856)  Acc@1: 79.1667 (78.9634)  Acc@5: 95.8333 (96.1382)  time: 0.0894  data: 0.0002  max mem: 232\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.8990 (0.9794)  Acc@1: 83.3333 (79.2000)  Acc@5: 95.8333 (96.2000)  time: 0.0873  data: 0.0002  max mem: 232\n",
            "Test: [Task 1] Total time: 0:00:04 (0.0980 s / it)\n",
            "* Acc@1 79.200 Acc@5 96.200 loss 0.979\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:28  Loss: 1.2891 (1.2891)  Acc@1: 62.5000 (62.5000)  Acc@5: 95.8333 (95.8333)  time: 0.6797  data: 0.5844  max mem: 232\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 1.2891 (1.2974)  Acc@1: 66.6667 (68.1818)  Acc@5: 91.6667 (92.4242)  time: 0.1465  data: 0.0598  max mem: 232\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 1.2613 (1.3147)  Acc@1: 66.6667 (69.0476)  Acc@5: 91.6667 (91.8651)  time: 0.0930  data: 0.0066  max mem: 232\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 1.1778 (1.3014)  Acc@1: 66.6667 (68.8172)  Acc@5: 95.8333 (92.6075)  time: 0.0912  data: 0.0034  max mem: 232\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 1.1857 (1.2816)  Acc@1: 70.8333 (70.0203)  Acc@5: 95.8333 (93.2927)  time: 0.0900  data: 0.0006  max mem: 232\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 1.1751 (1.2752)  Acc@1: 70.8333 (70.0000)  Acc@5: 95.8333 (93.4000)  time: 0.0887  data: 0.0006  max mem: 232\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1071 s / it)\n",
            "* Acc@1 70.000 Acc@5 93.400 loss 1.275\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:20  Loss: 0.8012 (0.8012)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.4972  data: 0.4163  max mem: 232\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:04  Loss: 1.0324 (1.0554)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.4545)  time: 0.1273  data: 0.0386  max mem: 232\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 1.0600 (1.0343)  Acc@1: 75.0000 (75.9921)  Acc@5: 95.8333 (95.2381)  time: 0.0895  data: 0.0006  max mem: 232\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.9941 (1.0103)  Acc@1: 75.0000 (76.0753)  Acc@5: 95.8333 (95.1613)  time: 0.0888  data: 0.0003  max mem: 232\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 1.0046 (1.0260)  Acc@1: 75.0000 (76.3211)  Acc@5: 95.8333 (94.9187)  time: 0.0895  data: 0.0002  max mem: 232\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 1.0046 (1.0338)  Acc@1: 75.0000 (76.2000)  Acc@5: 95.8333 (94.9000)  time: 0.0878  data: 0.0002  max mem: 232\n",
            "Test: [Task 3] Total time: 0:00:04 (0.1005 s / it)\n",
            "* Acc@1 76.200 Acc@5 94.900 loss 1.034\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:15  Loss: 1.5029 (1.5029)  Acc@1: 62.5000 (62.5000)  Acc@5: 83.3333 (83.3333)  time: 0.3626  data: 0.2849  max mem: 232\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:03  Loss: 1.1374 (1.1471)  Acc@1: 75.0000 (72.7273)  Acc@5: 91.6667 (92.8030)  time: 0.1175  data: 0.0294  max mem: 232\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 1.1630 (1.1679)  Acc@1: 75.0000 (73.6111)  Acc@5: 91.6667 (93.6508)  time: 0.0912  data: 0.0022  max mem: 232\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 1.1474 (1.1294)  Acc@1: 75.0000 (74.0591)  Acc@5: 95.8333 (94.2204)  time: 0.0898  data: 0.0005  max mem: 232\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 1.0881 (1.1482)  Acc@1: 70.8333 (73.7805)  Acc@5: 95.8333 (93.8008)  time: 0.0909  data: 0.0008  max mem: 232\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 1.0371 (1.1424)  Acc@1: 70.8333 (74.1000)  Acc@5: 95.8333 (93.8000)  time: 0.0891  data: 0.0008  max mem: 232\n",
            "Test: [Task 4] Total time: 0:00:04 (0.1000 s / it)\n",
            "* Acc@1 74.100 Acc@5 93.800 loss 1.142\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:28  Loss: 1.1637 (1.1637)  Acc@1: 66.6667 (66.6667)  Acc@5: 95.8333 (95.8333)  time: 0.6706  data: 0.5729  max mem: 232\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:04  Loss: 1.1637 (1.1678)  Acc@1: 70.8333 (72.7273)  Acc@5: 95.8333 (95.8333)  time: 0.1432  data: 0.0526  max mem: 232\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 1.1969 (1.2067)  Acc@1: 70.8333 (71.4286)  Acc@5: 91.6667 (94.8413)  time: 0.0904  data: 0.0005  max mem: 232\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 1.2223 (1.2248)  Acc@1: 66.6667 (71.6398)  Acc@5: 91.6667 (94.3548)  time: 0.0907  data: 0.0008  max mem: 232\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 1.2904 (1.2710)  Acc@1: 66.6667 (70.2236)  Acc@5: 91.6667 (93.9024)  time: 0.0911  data: 0.0007  max mem: 232\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 1.3120 (1.2820)  Acc@1: 66.6667 (69.9000)  Acc@5: 91.6667 (93.8000)  time: 0.0891  data: 0.0006  max mem: 232\n",
            "Test: [Task 5] Total time: 0:00:04 (0.1063 s / it)\n",
            "* Acc@1 69.900 Acc@5 93.800 loss 1.282\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:20  Loss: 1.0532 (1.0532)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  time: 0.4836  data: 0.3963  max mem: 232\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:04  Loss: 1.2856 (1.3210)  Acc@1: 70.8333 (68.9394)  Acc@5: 95.8333 (94.6970)  time: 0.1267  data: 0.0365  max mem: 232\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 1.3212 (1.3335)  Acc@1: 66.6667 (67.4603)  Acc@5: 95.8333 (95.0397)  time: 0.0905  data: 0.0004  max mem: 232\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 1.4290 (1.3710)  Acc@1: 66.6667 (66.6667)  Acc@5: 95.8333 (94.7581)  time: 0.0901  data: 0.0003  max mem: 232\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.4464 (1.3866)  Acc@1: 66.6667 (66.5650)  Acc@5: 95.8333 (94.8171)  time: 0.0907  data: 0.0003  max mem: 232\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.4464 (1.3889)  Acc@1: 66.6667 (66.6000)  Acc@5: 95.8333 (94.8000)  time: 0.0888  data: 0.0003  max mem: 232\n",
            "Test: [Task 6] Total time: 0:00:04 (0.1014 s / it)\n",
            "* Acc@1 66.600 Acc@5 94.800 loss 1.389\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:16  Loss: 1.6830 (1.6830)  Acc@1: 54.1667 (54.1667)  Acc@5: 87.5000 (87.5000)  time: 0.3918  data: 0.3082  max mem: 232\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:03  Loss: 1.6127 (1.6441)  Acc@1: 58.3333 (59.4697)  Acc@5: 87.5000 (89.7727)  time: 0.1192  data: 0.0299  max mem: 232\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:02  Loss: 1.5397 (1.6365)  Acc@1: 62.5000 (59.1270)  Acc@5: 91.6667 (89.8810)  time: 0.0913  data: 0.0012  max mem: 232\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:01  Loss: 1.6048 (1.6574)  Acc@1: 62.5000 (59.5430)  Acc@5: 91.6667 (89.5161)  time: 0.0905  data: 0.0003  max mem: 232\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 1.5872 (1.6537)  Acc@1: 62.5000 (60.6707)  Acc@5: 87.5000 (89.6341)  time: 0.0907  data: 0.0003  max mem: 232\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 1.5872 (1.6505)  Acc@1: 62.5000 (60.9000)  Acc@5: 87.5000 (89.6000)  time: 0.0890  data: 0.0002  max mem: 232\n",
            "Test: [Task 7] Total time: 0:00:04 (0.0997 s / it)\n",
            "* Acc@1 60.900 Acc@5 89.600 loss 1.650\n",
            "[Average accuracy till task7]\tAcc@1: 70.9857\tAcc@5: 93.7857\tLoss: 1.2503\tForgetting: 4.3500\tBackward: -1.0500\n",
            "Train: Epoch[ 1/10]  [ 0/22]  eta: 0:00:21  Lr: 0.000047  Loss: 2.4181  Acc@1: 12.5000 (12.5000)  Acc@5: 45.8333 (45.8333)  time: 0.9626  data: 0.8447  max mem: 232\n",
            "Train: Epoch[ 1/10]  [10/22]  eta: 0:00:02  Lr: 0.000047  Loss: 2.2072  Acc@1: 12.5000 (14.3939)  Acc@5: 66.6667 (65.5303)  time: 0.1805  data: 0.0784  max mem: 232\n",
            "Train: Epoch[ 1/10]  [20/22]  eta: 0:00:00  Lr: 0.000047  Loss: 2.0770  Acc@1: 20.8333 (20.0397)  Acc@5: 70.8333 (71.0317)  time: 0.0986  data: 0.0014  max mem: 232\n",
            "Train: Epoch[ 1/10]  [21/22]  eta: 0:00:00  Lr: 0.000047  Loss: 1.8362  Acc@1: 20.8333 (20.5479)  Acc@5: 75.0000 (71.4286)  time: 0.0952  data: 0.0011  max mem: 232\n",
            "Train: Epoch[ 1/10] Total time: 0:00:03 (0.1399 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.8362  Acc@1: 20.8333 (20.5479)  Acc@5: 75.0000 (71.4286)\n",
            "Train: Epoch[ 2/10]  [ 0/22]  eta: 0:00:13  Lr: 0.000047  Loss: 2.0916  Acc@1: 33.3333 (33.3333)  Acc@5: 70.8333 (70.8333)  time: 0.6004  data: 0.4612  max mem: 232\n",
            "Train: Epoch[ 2/10]  [10/22]  eta: 0:00:01  Lr: 0.000047  Loss: 1.8959  Acc@1: 37.5000 (39.0152)  Acc@5: 83.3333 (82.9545)  time: 0.1463  data: 0.0497  max mem: 232\n",
            "Train: Epoch[ 2/10]  [20/22]  eta: 0:00:00  Lr: 0.000047  Loss: 1.7590  Acc@1: 41.6667 (43.0556)  Acc@5: 87.5000 (85.9127)  time: 0.0959  data: 0.0044  max mem: 232\n",
            "Train: Epoch[ 2/10]  [21/22]  eta: 0:00:00  Lr: 0.000047  Loss: 1.9272  Acc@1: 42.8571 (43.0528)  Acc@5: 87.5000 (86.1057)  time: 0.0898  data: 0.0006  max mem: 232\n",
            "Train: Epoch[ 2/10] Total time: 0:00:02 (0.1198 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.9272  Acc@1: 42.8571 (43.0528)  Acc@5: 87.5000 (86.1057)\n",
            "Train: Epoch[ 3/10]  [ 0/22]  eta: 0:00:09  Lr: 0.000047  Loss: 1.8752  Acc@1: 45.8333 (45.8333)  Acc@5: 83.3333 (83.3333)  time: 0.4168  data: 0.3130  max mem: 232\n",
            "Train: Epoch[ 3/10]  [10/22]  eta: 0:00:01  Lr: 0.000047  Loss: 1.6455  Acc@1: 54.1667 (55.6818)  Acc@5: 87.5000 (87.8788)  time: 0.1269  data: 0.0332  max mem: 232\n",
            "Train: Epoch[ 3/10]  [20/22]  eta: 0:00:00  Lr: 0.000047  Loss: 1.4557  Acc@1: 54.1667 (57.1429)  Acc@5: 91.6667 (91.4683)  time: 0.0941  data: 0.0029  max mem: 232\n",
            "Train: Epoch[ 3/10]  [21/22]  eta: 0:00:00  Lr: 0.000047  Loss: 1.2487  Acc@1: 54.1667 (57.7299)  Acc@5: 91.6667 (91.5851)  time: 0.0895  data: 0.0009  max mem: 232\n",
            "Train: Epoch[ 3/10] Total time: 0:00:02 (0.1095 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.2487  Acc@1: 54.1667 (57.7299)  Acc@5: 91.6667 (91.5851)\n",
            "Train: Epoch[ 4/10]  [ 0/22]  eta: 0:00:08  Lr: 0.000047  Loss: 1.6112  Acc@1: 58.3333 (58.3333)  Acc@5: 95.8333 (95.8333)  time: 0.3864  data: 0.2985  max mem: 232\n",
            "Train: Epoch[ 4/10]  [10/22]  eta: 0:00:01  Lr: 0.000047  Loss: 1.4583  Acc@1: 62.5000 (62.8788)  Acc@5: 95.8333 (96.5909)  time: 0.1242  data: 0.0305  max mem: 232\n",
            "Train: Epoch[ 4/10]  [20/22]  eta: 0:00:00  Lr: 0.000047  Loss: 1.5030  Acc@1: 62.5000 (61.7064)  Acc@5: 95.8333 (96.0317)  time: 0.0946  data: 0.0021  max mem: 232\n",
            "Train: Epoch[ 4/10]  [21/22]  eta: 0:00:00  Lr: 0.000047  Loss: 1.6572  Acc@1: 62.5000 (61.6438)  Acc@5: 95.8333 (96.0861)  time: 0.0906  data: 0.0007  max mem: 232\n",
            "Train: Epoch[ 4/10] Total time: 0:00:02 (0.1088 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.6572  Acc@1: 62.5000 (61.6438)  Acc@5: 95.8333 (96.0861)\n",
            "Train: Epoch[ 5/10]  [ 0/22]  eta: 0:00:16  Lr: 0.000047  Loss: 1.4657  Acc@1: 62.5000 (62.5000)  Acc@5: 100.0000 (100.0000)  time: 0.7526  data: 0.6480  max mem: 232\n",
            "Train: Epoch[ 5/10]  [10/22]  eta: 0:00:01  Lr: 0.000047  Loss: 1.3005  Acc@1: 70.8333 (70.4545)  Acc@5: 95.8333 (94.3182)  time: 0.1533  data: 0.0598  max mem: 232\n",
            "Train: Epoch[ 5/10]  [20/22]  eta: 0:00:00  Lr: 0.000047  Loss: 1.2042  Acc@1: 70.8333 (70.6349)  Acc@5: 95.8333 (96.6270)  time: 0.0919  data: 0.0007  max mem: 232\n",
            "Train: Epoch[ 5/10]  [21/22]  eta: 0:00:00  Lr: 0.000047  Loss: 0.9508  Acc@1: 70.8333 (71.0372)  Acc@5: 100.0000 (96.6732)  time: 0.0889  data: 0.0007  max mem: 232\n",
            "Train: Epoch[ 5/10] Total time: 0:00:02 (0.1228 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.9508  Acc@1: 70.8333 (71.0372)  Acc@5: 100.0000 (96.6732)\n",
            "Train: Epoch[ 6/10]  [ 0/22]  eta: 0:00:09  Lr: 0.000047  Loss: 1.1646  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  time: 0.4193  data: 0.3072  max mem: 232\n",
            "Train: Epoch[ 6/10]  [10/22]  eta: 0:00:01  Lr: 0.000047  Loss: 1.3170  Acc@1: 75.0000 (73.8636)  Acc@5: 95.8333 (95.8333)  time: 0.1409  data: 0.0395  max mem: 232\n",
            "Train: Epoch[ 6/10]  [20/22]  eta: 0:00:00  Lr: 0.000047  Loss: 1.0333  Acc@1: 75.0000 (73.4127)  Acc@5: 100.0000 (96.8254)  time: 0.1092  data: 0.0119  max mem: 232\n",
            "Train: Epoch[ 6/10]  [21/22]  eta: 0:00:00  Lr: 0.000047  Loss: 1.1611  Acc@1: 75.0000 (73.3855)  Acc@5: 100.0000 (96.8689)  time: 0.1037  data: 0.0097  max mem: 232\n",
            "Train: Epoch[ 6/10] Total time: 0:00:02 (0.1255 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.1611  Acc@1: 75.0000 (73.3855)  Acc@5: 100.0000 (96.8689)\n",
            "Train: Epoch[ 7/10]  [ 0/22]  eta: 0:00:17  Lr: 0.000047  Loss: 1.1080  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  time: 0.7875  data: 0.6676  max mem: 232\n",
            "Train: Epoch[ 7/10]  [10/22]  eta: 0:00:01  Lr: 0.000047  Loss: 1.1476  Acc@1: 75.0000 (74.2424)  Acc@5: 95.8333 (96.5909)  time: 0.1627  data: 0.0622  max mem: 232\n",
            "Train: Epoch[ 7/10]  [20/22]  eta: 0:00:00  Lr: 0.000047  Loss: 1.1587  Acc@1: 75.0000 (74.8016)  Acc@5: 95.8333 (97.0238)  time: 0.0958  data: 0.0011  max mem: 232\n",
            "Train: Epoch[ 7/10]  [21/22]  eta: 0:00:00  Lr: 0.000047  Loss: 0.8263  Acc@1: 75.0000 (74.9511)  Acc@5: 95.8333 (97.0646)  time: 0.0929  data: 0.0011  max mem: 232\n",
            "Train: Epoch[ 7/10] Total time: 0:00:02 (0.1282 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.8263  Acc@1: 75.0000 (74.9511)  Acc@5: 95.8333 (97.0646)\n",
            "Train: Epoch[ 8/10]  [ 0/22]  eta: 0:00:10  Lr: 0.000047  Loss: 1.1671  Acc@1: 75.0000 (75.0000)  Acc@5: 83.3333 (83.3333)  time: 0.4921  data: 0.3832  max mem: 232\n",
            "Train: Epoch[ 8/10]  [10/22]  eta: 0:00:01  Lr: 0.000047  Loss: 0.9831  Acc@1: 79.1667 (79.5455)  Acc@5: 100.0000 (96.5909)  time: 0.1292  data: 0.0357  max mem: 232\n",
            "Train: Epoch[ 8/10]  [20/22]  eta: 0:00:00  Lr: 0.000047  Loss: 1.1186  Acc@1: 79.1667 (78.7698)  Acc@5: 100.0000 (97.4206)  time: 0.0917  data: 0.0007  max mem: 232\n",
            "Train: Epoch[ 8/10]  [21/22]  eta: 0:00:00  Lr: 0.000047  Loss: 1.0638  Acc@1: 75.0000 (78.6693)  Acc@5: 100.0000 (97.4560)  time: 0.0884  data: 0.0007  max mem: 232\n",
            "Train: Epoch[ 8/10] Total time: 0:00:02 (0.1111 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.0638  Acc@1: 75.0000 (78.6693)  Acc@5: 100.0000 (97.4560)\n",
            "Train: Epoch[ 9/10]  [ 0/22]  eta: 0:00:08  Lr: 0.000047  Loss: 0.7422  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.4035  data: 0.3045  max mem: 232\n",
            "Train: Epoch[ 9/10]  [10/22]  eta: 0:00:01  Lr: 0.000047  Loss: 0.9266  Acc@1: 79.1667 (80.6818)  Acc@5: 100.0000 (96.5909)  time: 0.1312  data: 0.0392  max mem: 232\n",
            "Train: Epoch[ 9/10]  [20/22]  eta: 0:00:00  Lr: 0.000047  Loss: 0.7535  Acc@1: 79.1667 (80.7540)  Acc@5: 95.8333 (97.2222)  time: 0.0966  data: 0.0065  max mem: 232\n",
            "Train: Epoch[ 9/10]  [21/22]  eta: 0:00:00  Lr: 0.000047  Loss: 0.6466  Acc@1: 83.3333 (80.8219)  Acc@5: 95.8333 (97.2603)  time: 0.0881  data: 0.0007  max mem: 232\n",
            "Train: Epoch[ 9/10] Total time: 0:00:02 (0.1114 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.6466  Acc@1: 83.3333 (80.8219)  Acc@5: 95.8333 (97.2603)\n",
            "Train: Epoch[10/10]  [ 0/22]  eta: 0:00:11  Lr: 0.000047  Loss: 0.7472  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.5351  data: 0.4376  max mem: 232\n",
            "Train: Epoch[10/10]  [10/22]  eta: 0:00:01  Lr: 0.000047  Loss: 0.7676  Acc@1: 79.1667 (80.6818)  Acc@5: 100.0000 (98.4848)  time: 0.1327  data: 0.0406  max mem: 232\n",
            "Train: Epoch[10/10]  [20/22]  eta: 0:00:00  Lr: 0.000047  Loss: 0.9630  Acc@1: 83.3333 (80.5556)  Acc@5: 100.0000 (98.2143)  time: 0.0912  data: 0.0005  max mem: 232\n",
            "Train: Epoch[10/10]  [21/22]  eta: 0:00:00  Lr: 0.000047  Loss: 0.7389  Acc@1: 83.3333 (80.6262)  Acc@5: 100.0000 (98.2387)  time: 0.0882  data: 0.0005  max mem: 232\n",
            "Train: Epoch[10/10] Total time: 0:00:02 (0.1122 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.7389  Acc@1: 83.3333 (80.6262)  Acc@5: 100.0000 (98.2387)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:14  Loss: 1.1823 (1.1823)  Acc@1: 75.0000 (75.0000)  Acc@5: 87.5000 (87.5000)  time: 0.3488  data: 0.2695  max mem: 232\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 1.1534 (1.1399)  Acc@1: 75.0000 (74.2424)  Acc@5: 95.8333 (95.8333)  time: 0.1161  data: 0.0320  max mem: 232\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 1.1198 (1.1255)  Acc@1: 79.1667 (76.5873)  Acc@5: 95.8333 (95.6349)  time: 0.0917  data: 0.0046  max mem: 232\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 1.0103 (1.1020)  Acc@1: 79.1667 (76.4785)  Acc@5: 95.8333 (95.5645)  time: 0.0908  data: 0.0016  max mem: 232\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 1.0022 (1.0779)  Acc@1: 79.1667 (78.1504)  Acc@5: 95.8333 (96.0366)  time: 0.0908  data: 0.0011  max mem: 232\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.9683 (1.0721)  Acc@1: 79.1667 (78.4000)  Acc@5: 95.8333 (96.1000)  time: 0.0892  data: 0.0010  max mem: 232\n",
            "Test: [Task 1] Total time: 0:00:04 (0.0999 s / it)\n",
            "* Acc@1 78.400 Acc@5 96.100 loss 1.072\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:29  Loss: 1.3281 (1.3281)  Acc@1: 66.6667 (66.6667)  Acc@5: 95.8333 (95.8333)  time: 0.7089  data: 0.6179  max mem: 232\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 1.3678 (1.3562)  Acc@1: 66.6667 (66.6667)  Acc@5: 91.6667 (92.8030)  time: 0.1457  data: 0.0569  max mem: 232\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 1.3678 (1.3728)  Acc@1: 70.8333 (68.4524)  Acc@5: 87.5000 (91.8651)  time: 0.0888  data: 0.0006  max mem: 232\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 1.2646 (1.3700)  Acc@1: 66.6667 (68.4140)  Acc@5: 95.8333 (92.3387)  time: 0.0884  data: 0.0003  max mem: 232\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 1.2729 (1.3567)  Acc@1: 66.6667 (68.9024)  Acc@5: 95.8333 (92.7846)  time: 0.0893  data: 0.0002  max mem: 232\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 1.2299 (1.3506)  Acc@1: 68.7500 (68.9000)  Acc@5: 95.8333 (92.9000)  time: 0.0875  data: 0.0002  max mem: 232\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1052 s / it)\n",
            "* Acc@1 68.900 Acc@5 92.900 loss 1.351\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:20  Loss: 0.8200 (0.8200)  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  time: 0.4779  data: 0.3942  max mem: 232\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 1.0366 (1.0832)  Acc@1: 79.1667 (76.1364)  Acc@5: 95.8333 (95.0758)  time: 0.1248  data: 0.0363  max mem: 232\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 1.0640 (1.0758)  Acc@1: 79.1667 (76.9841)  Acc@5: 95.8333 (93.8492)  time: 0.0889  data: 0.0004  max mem: 232\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.9773 (1.0476)  Acc@1: 79.1667 (77.4194)  Acc@5: 95.8333 (94.4892)  time: 0.0888  data: 0.0003  max mem: 232\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 1.0286 (1.0635)  Acc@1: 75.0000 (77.3374)  Acc@5: 95.8333 (94.3089)  time: 0.0898  data: 0.0002  max mem: 232\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 1.0286 (1.0702)  Acc@1: 75.0000 (77.3000)  Acc@5: 95.8333 (94.3000)  time: 0.0878  data: 0.0002  max mem: 232\n",
            "Test: [Task 3] Total time: 0:00:04 (0.0999 s / it)\n",
            "* Acc@1 77.300 Acc@5 94.300 loss 1.070\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:19  Loss: 1.5813 (1.5813)  Acc@1: 50.0000 (50.0000)  Acc@5: 83.3333 (83.3333)  time: 0.4625  data: 0.3867  max mem: 232\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:03  Loss: 1.1585 (1.1880)  Acc@1: 75.0000 (71.2121)  Acc@5: 95.8333 (94.3182)  time: 0.1239  data: 0.0360  max mem: 232\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 1.1776 (1.2179)  Acc@1: 75.0000 (71.8254)  Acc@5: 95.8333 (93.6508)  time: 0.0892  data: 0.0007  max mem: 232\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 1.2437 (1.1867)  Acc@1: 75.0000 (72.7151)  Acc@5: 95.8333 (94.0860)  time: 0.0890  data: 0.0006  max mem: 232\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 1.1341 (1.2033)  Acc@1: 70.8333 (72.8659)  Acc@5: 95.8333 (93.6992)  time: 0.0896  data: 0.0008  max mem: 232\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 1.1299 (1.1963)  Acc@1: 70.8333 (73.2000)  Acc@5: 95.8333 (93.7000)  time: 0.0877  data: 0.0008  max mem: 232\n",
            "Test: [Task 4] Total time: 0:00:04 (0.1006 s / it)\n",
            "* Acc@1 73.200 Acc@5 93.700 loss 1.196\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:30  Loss: 1.2700 (1.2700)  Acc@1: 58.3333 (58.3333)  Acc@5: 91.6667 (91.6667)  time: 0.7378  data: 0.6611  max mem: 232\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:04  Loss: 1.2062 (1.2221)  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (96.2121)  time: 0.1509  data: 0.0654  max mem: 232\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 1.2509 (1.2624)  Acc@1: 70.8333 (69.4444)  Acc@5: 95.8333 (94.6429)  time: 0.0907  data: 0.0036  max mem: 232\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 1.2829 (1.2796)  Acc@1: 70.8333 (69.8925)  Acc@5: 91.6667 (94.0860)  time: 0.0893  data: 0.0009  max mem: 232\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 1.3263 (1.3295)  Acc@1: 66.6667 (67.8862)  Acc@5: 91.6667 (93.4959)  time: 0.0896  data: 0.0003  max mem: 232\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 1.3359 (1.3415)  Acc@1: 62.5000 (67.6000)  Acc@5: 91.6667 (93.3000)  time: 0.0879  data: 0.0003  max mem: 232\n",
            "Test: [Task 5] Total time: 0:00:04 (0.1069 s / it)\n",
            "* Acc@1 67.600 Acc@5 93.300 loss 1.341\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:12  Loss: 1.1364 (1.1364)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  time: 0.2941  data: 0.2215  max mem: 232\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:03  Loss: 1.4341 (1.4392)  Acc@1: 66.6667 (64.7727)  Acc@5: 91.6667 (93.5606)  time: 0.1219  data: 0.0370  max mem: 232\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 1.4643 (1.4609)  Acc@1: 66.6667 (64.0873)  Acc@5: 91.6667 (93.6508)  time: 0.0966  data: 0.0094  max mem: 232\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 1.5846 (1.5074)  Acc@1: 58.3333 (62.9032)  Acc@5: 91.6667 (92.7419)  time: 0.0894  data: 0.0003  max mem: 232\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.5985 (1.5160)  Acc@1: 58.3333 (63.4146)  Acc@5: 91.6667 (92.6829)  time: 0.0903  data: 0.0002  max mem: 232\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.5846 (1.5174)  Acc@1: 58.3333 (63.4000)  Acc@5: 91.6667 (92.6000)  time: 0.0878  data: 0.0002  max mem: 232\n",
            "Test: [Task 6] Total time: 0:00:04 (0.0993 s / it)\n",
            "* Acc@1 63.400 Acc@5 92.600 loss 1.517\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:20  Loss: 1.9593 (1.9593)  Acc@1: 50.0000 (50.0000)  Acc@5: 83.3333 (83.3333)  time: 0.4926  data: 0.4149  max mem: 232\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:04  Loss: 1.8146 (1.8788)  Acc@1: 50.0000 (53.0303)  Acc@5: 87.5000 (86.3636)  time: 0.1273  data: 0.0384  max mem: 232\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:02  Loss: 1.7816 (1.8533)  Acc@1: 54.1667 (54.5635)  Acc@5: 87.5000 (86.1111)  time: 0.0892  data: 0.0005  max mem: 232\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:01  Loss: 1.8212 (1.8711)  Acc@1: 54.1667 (54.0323)  Acc@5: 83.3333 (84.9462)  time: 0.0884  data: 0.0004  max mem: 232\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 1.8212 (1.8706)  Acc@1: 58.3333 (55.1829)  Acc@5: 87.5000 (85.6707)  time: 0.0894  data: 0.0004  max mem: 232\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 1.8212 (1.8672)  Acc@1: 58.3333 (55.5000)  Acc@5: 83.3333 (85.6000)  time: 0.0878  data: 0.0004  max mem: 232\n",
            "Test: [Task 7] Total time: 0:00:04 (0.1018 s / it)\n",
            "* Acc@1 55.500 Acc@5 85.600 loss 1.867\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:30  Loss: 3.2393 (3.2393)  Acc@1: 0.0000 (0.0000)  Acc@5: 54.1667 (54.1667)  time: 0.7267  data: 0.6466  max mem: 232\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:04  Loss: 3.3120 (3.3098)  Acc@1: 8.3333 (6.8182)  Acc@5: 54.1667 (56.0606)  time: 0.1477  data: 0.0608  max mem: 232\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:02  Loss: 3.2884 (3.2777)  Acc@1: 8.3333 (7.7381)  Acc@5: 50.0000 (54.7619)  time: 0.0897  data: 0.0015  max mem: 232\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:01  Loss: 3.2311 (3.2826)  Acc@1: 8.3333 (8.0645)  Acc@5: 50.0000 (52.9570)  time: 0.0896  data: 0.0009  max mem: 232\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 3.2311 (3.2902)  Acc@1: 8.3333 (8.4350)  Acc@5: 50.0000 (52.4390)  time: 0.0899  data: 0.0007  max mem: 232\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 3.2311 (3.2885)  Acc@1: 8.3333 (8.6000)  Acc@5: 50.0000 (52.7000)  time: 0.0884  data: 0.0007  max mem: 232\n",
            "Test: [Task 8] Total time: 0:00:04 (0.1065 s / it)\n",
            "* Acc@1 8.600 Acc@5 52.700 loss 3.289\n",
            "[Average accuracy till task8]\tAcc@1: 61.6125\tAcc@5: 87.6500\tLoss: 1.5880\tForgetting: 3.2857\tBackward: 26.2143\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([16872, 384])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 0.8848  Acc@1: 79.1667 (79.1071)  Acc@5: 95.8333 (94.8810)\n",
            "torch.Size([16872, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 0.7595  Acc@1: 87.5000 (84.1667)  Acc@5: 100.0000 (97.9762)\n",
            "torch.Size([16872, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 1.0061  Acc@1: 87.5000 (85.8929)  Acc@5: 100.0000 (98.9286)\n",
            "torch.Size([16872, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 0.6499  Acc@1: 87.5000 (86.7857)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([16872, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 0.4500  Acc@1: 91.6667 (88.6310)  Acc@5: 100.0000 (99.2857)\n",
            "torch.Size([16872, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 0.8221  Acc@1: 91.6667 (90.3571)  Acc@5: 100.0000 (99.4643)\n",
            "torch.Size([16872, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.9663  Acc@1: 91.6667 (90.3571)  Acc@5: 100.0000 (99.5238)\n",
            "torch.Size([16872, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 0.8375  Acc@1: 87.5000 (90.4167)  Acc@5: 100.0000 (99.5238)\n",
            "torch.Size([16872, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 0.6791  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (99.5238)\n",
            "torch.Size([16872, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.8372  Acc@1: 87.5000 (90.8929)  Acc@5: 100.0000 (99.8214)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:17  Loss: 0.9882 (0.9882)  Acc@1: 75.0000 (75.0000)  Acc@5: 83.3333 (83.3333)  time: 0.4192  data: 0.3312  max mem: 232\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 1.0257 (1.0199)  Acc@1: 75.0000 (73.4849)  Acc@5: 95.8333 (94.6970)  time: 0.1354  data: 0.0510  max mem: 232\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 1.0024 (0.9992)  Acc@1: 75.0000 (76.1905)  Acc@5: 95.8333 (94.8413)  time: 0.0983  data: 0.0120  max mem: 232\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.8942 (0.9707)  Acc@1: 79.1667 (76.8817)  Acc@5: 95.8333 (95.2957)  time: 0.0891  data: 0.0021  max mem: 232\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.8790 (0.9470)  Acc@1: 79.1667 (78.4553)  Acc@5: 95.8333 (96.0366)  time: 0.0889  data: 0.0018  max mem: 232\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.8610 (0.9404)  Acc@1: 83.3333 (78.7000)  Acc@5: 95.8333 (96.1000)  time: 0.0878  data: 0.0017  max mem: 232\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1030 s / it)\n",
            "* Acc@1 78.700 Acc@5 96.100 loss 0.940\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:14  Loss: 1.2347 (1.2347)  Acc@1: 62.5000 (62.5000)  Acc@5: 95.8333 (95.8333)  time: 0.3398  data: 0.2640  max mem: 232\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:03  Loss: 1.2347 (1.2327)  Acc@1: 66.6667 (66.2879)  Acc@5: 91.6667 (93.1818)  time: 0.1194  data: 0.0351  max mem: 232\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 1.2174 (1.2474)  Acc@1: 70.8333 (68.4524)  Acc@5: 91.6667 (92.2619)  time: 0.0931  data: 0.0064  max mem: 232\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 1.1058 (1.2381)  Acc@1: 70.8333 (68.8172)  Acc@5: 95.8333 (92.4731)  time: 0.0898  data: 0.0004  max mem: 232\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 1.1305 (1.2228)  Acc@1: 70.8333 (69.4106)  Acc@5: 95.8333 (92.8862)  time: 0.0908  data: 0.0003  max mem: 232\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 1.0869 (1.2173)  Acc@1: 70.8333 (69.4000)  Acc@5: 95.8333 (93.0000)  time: 0.0887  data: 0.0003  max mem: 232\n",
            "Test: [Task 2] Total time: 0:00:04 (0.0991 s / it)\n",
            "* Acc@1 69.400 Acc@5 93.000 loss 1.217\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:17  Loss: 0.7433 (0.7433)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.4267  data: 0.3577  max mem: 232\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 0.9835 (1.0316)  Acc@1: 79.1667 (75.3788)  Acc@5: 95.8333 (95.4545)  time: 0.1242  data: 0.0371  max mem: 232\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 1.0462 (1.0206)  Acc@1: 75.0000 (75.9921)  Acc@5: 95.8333 (94.2460)  time: 0.0917  data: 0.0027  max mem: 232\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.9808 (0.9973)  Acc@1: 75.0000 (75.8065)  Acc@5: 95.8333 (94.4892)  time: 0.0901  data: 0.0004  max mem: 232\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 1.0001 (1.0134)  Acc@1: 75.0000 (76.0163)  Acc@5: 95.8333 (94.3089)  time: 0.0912  data: 0.0003  max mem: 232\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 1.0001 (1.0197)  Acc@1: 75.0000 (75.9000)  Acc@5: 95.8333 (94.3000)  time: 0.0893  data: 0.0003  max mem: 232\n",
            "Test: [Task 3] Total time: 0:00:04 (0.1006 s / it)\n",
            "* Acc@1 75.900 Acc@5 94.300 loss 1.020\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:22  Loss: 1.4204 (1.4204)  Acc@1: 66.6667 (66.6667)  Acc@5: 83.3333 (83.3333)  time: 0.5277  data: 0.4531  max mem: 232\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:04  Loss: 1.1183 (1.1254)  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (93.1818)  time: 0.1315  data: 0.0419  max mem: 232\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 1.1285 (1.1497)  Acc@1: 70.8333 (71.6270)  Acc@5: 95.8333 (93.6508)  time: 0.0913  data: 0.0011  max mem: 232\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 1.1148 (1.1100)  Acc@1: 70.8333 (72.1774)  Acc@5: 95.8333 (94.0860)  time: 0.0907  data: 0.0011  max mem: 232\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 1.1043 (1.1242)  Acc@1: 70.8333 (71.5447)  Acc@5: 95.8333 (93.5976)  time: 0.0910  data: 0.0005  max mem: 232\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 1.0452 (1.1176)  Acc@1: 70.8333 (71.9000)  Acc@5: 95.8333 (93.6000)  time: 0.0891  data: 0.0005  max mem: 232\n",
            "Test: [Task 4] Total time: 0:00:04 (0.1041 s / it)\n",
            "* Acc@1 71.900 Acc@5 93.600 loss 1.118\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:26  Loss: 1.0673 (1.0673)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  time: 0.6329  data: 0.5302  max mem: 232\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:04  Loss: 1.0536 (1.0729)  Acc@1: 75.0000 (74.6212)  Acc@5: 95.8333 (95.8333)  time: 0.1411  data: 0.0494  max mem: 232\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 1.1028 (1.1156)  Acc@1: 70.8333 (74.4048)  Acc@5: 91.6667 (94.4444)  time: 0.0918  data: 0.0009  max mem: 232\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 1.1557 (1.1336)  Acc@1: 70.8333 (74.0591)  Acc@5: 95.8333 (94.3548)  time: 0.0919  data: 0.0003  max mem: 232\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 1.1919 (1.1839)  Acc@1: 66.6667 (72.2561)  Acc@5: 91.6667 (93.6992)  time: 0.0922  data: 0.0002  max mem: 232\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 1.2000 (1.1969)  Acc@1: 66.6667 (71.9000)  Acc@5: 91.6667 (93.6000)  time: 0.0905  data: 0.0002  max mem: 232\n",
            "Test: [Task 5] Total time: 0:00:04 (0.1062 s / it)\n",
            "* Acc@1 71.900 Acc@5 93.600 loss 1.197\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:15  Loss: 1.0234 (1.0234)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  time: 0.3660  data: 0.2966  max mem: 232\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:03  Loss: 1.2580 (1.2901)  Acc@1: 62.5000 (63.6364)  Acc@5: 95.8333 (93.9394)  time: 0.1216  data: 0.0341  max mem: 232\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 1.2757 (1.2994)  Acc@1: 62.5000 (64.2857)  Acc@5: 95.8333 (94.4444)  time: 0.0945  data: 0.0041  max mem: 232\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 1.4089 (1.3365)  Acc@1: 58.3333 (63.4409)  Acc@5: 95.8333 (93.8172)  time: 0.0918  data: 0.0003  max mem: 232\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.4155 (1.3506)  Acc@1: 66.6667 (63.7195)  Acc@5: 91.6667 (93.8008)  time: 0.0919  data: 0.0003  max mem: 232\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.4163 (1.3536)  Acc@1: 62.5000 (63.7000)  Acc@5: 91.6667 (93.7000)  time: 0.0902  data: 0.0002  max mem: 232\n",
            "Test: [Task 6] Total time: 0:00:04 (0.1010 s / it)\n",
            "* Acc@1 63.700 Acc@5 93.700 loss 1.354\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:20  Loss: 1.5234 (1.5234)  Acc@1: 58.3333 (58.3333)  Acc@5: 87.5000 (87.5000)  time: 0.4789  data: 0.4040  max mem: 232\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:04  Loss: 1.4447 (1.4437)  Acc@1: 62.5000 (63.6364)  Acc@5: 95.8333 (93.1818)  time: 0.1281  data: 0.0377  max mem: 232\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:02  Loss: 1.3655 (1.4438)  Acc@1: 62.5000 (62.6984)  Acc@5: 95.8333 (92.8571)  time: 0.0921  data: 0.0014  max mem: 232\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:01  Loss: 1.3694 (1.4648)  Acc@1: 62.5000 (62.9032)  Acc@5: 91.6667 (91.8011)  time: 0.0915  data: 0.0019  max mem: 232\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 1.4134 (1.4634)  Acc@1: 66.6667 (63.6179)  Acc@5: 91.6667 (91.2602)  time: 0.0920  data: 0.0011  max mem: 232\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 1.4134 (1.4613)  Acc@1: 66.6667 (63.8000)  Acc@5: 91.6667 (91.2000)  time: 0.0902  data: 0.0010  max mem: 232\n",
            "Test: [Task 7] Total time: 0:00:04 (0.1036 s / it)\n",
            "* Acc@1 63.800 Acc@5 91.200 loss 1.461\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:29  Loss: 1.6067 (1.6067)  Acc@1: 37.5000 (37.5000)  Acc@5: 95.8333 (95.8333)  time: 0.6984  data: 0.6204  max mem: 232\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:04  Loss: 1.6442 (1.6953)  Acc@1: 54.1667 (54.5455)  Acc@5: 91.6667 (89.3939)  time: 0.1470  data: 0.0596  max mem: 232\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:02  Loss: 1.6076 (1.6486)  Acc@1: 54.1667 (56.3492)  Acc@5: 91.6667 (89.6825)  time: 0.0914  data: 0.0020  max mem: 232\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:01  Loss: 1.5865 (1.6309)  Acc@1: 58.3333 (56.1828)  Acc@5: 91.6667 (91.5323)  time: 0.0910  data: 0.0004  max mem: 232\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 1.6321 (1.6503)  Acc@1: 54.1667 (55.6911)  Acc@5: 91.6667 (90.2439)  time: 0.0911  data: 0.0003  max mem: 232\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 1.6321 (1.6450)  Acc@1: 54.1667 (55.9000)  Acc@5: 91.6667 (90.3000)  time: 0.0896  data: 0.0003  max mem: 232\n",
            "Test: [Task 8] Total time: 0:00:04 (0.1070 s / it)\n",
            "* Acc@1 55.900 Acc@5 90.300 loss 1.645\n",
            "[Average accuracy till task8]\tAcc@1: 68.9000\tAcc@5: 93.2250\tLoss: 1.2440\tForgetting: 4.6571\tBackward: -1.1286\n",
            "Train: Epoch[ 1/10]  [ 0/21]  eta: 0:00:10  Lr: 0.000047  Loss: 2.2897  Acc@1: 20.8333 (20.8333)  Acc@5: 54.1667 (54.1667)  time: 0.5077  data: 0.4048  max mem: 232\n",
            "Train: Epoch[ 1/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 2.1783  Acc@1: 16.6667 (16.6667)  Acc@5: 58.3333 (57.5758)  time: 0.1317  data: 0.0376  max mem: 232\n",
            "Train: Epoch[ 1/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 2.1361  Acc@1: 16.6667 (17.6353)  Acc@5: 63.1579 (62.3247)  time: 0.0919  data: 0.0006  max mem: 232\n",
            "Train: Epoch[ 1/10] Total time: 0:00:02 (0.1153 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 2.1361  Acc@1: 16.6667 (17.6353)  Acc@5: 63.1579 (62.3247)\n",
            "Train: Epoch[ 2/10]  [ 0/21]  eta: 0:00:11  Lr: 0.000047  Loss: 2.0544  Acc@1: 41.6667 (41.6667)  Acc@5: 75.0000 (75.0000)  time: 0.5492  data: 0.4250  max mem: 232\n",
            "Train: Epoch[ 2/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.9159  Acc@1: 33.3333 (33.3333)  Acc@5: 75.0000 (79.1667)  time: 0.1332  data: 0.0399  max mem: 232\n",
            "Train: Epoch[ 2/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 1.8553  Acc@1: 37.5000 (39.2786)  Acc@5: 83.3333 (82.5651)  time: 0.0902  data: 0.0007  max mem: 232\n",
            "Train: Epoch[ 2/10] Total time: 0:00:02 (0.1161 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.8553  Acc@1: 37.5000 (39.2786)  Acc@5: 83.3333 (82.5651)\n",
            "Train: Epoch[ 3/10]  [ 0/21]  eta: 0:00:12  Lr: 0.000047  Loss: 1.9887  Acc@1: 45.8333 (45.8333)  Acc@5: 83.3333 (83.3333)  time: 0.5997  data: 0.4863  max mem: 232\n",
            "Train: Epoch[ 3/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.7574  Acc@1: 50.0000 (51.1364)  Acc@5: 95.8333 (93.9394)  time: 0.1371  data: 0.0450  max mem: 232\n",
            "Train: Epoch[ 3/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 1.6106  Acc@1: 50.0000 (52.3046)  Acc@5: 95.8333 (94.5892)  time: 0.0900  data: 0.0005  max mem: 232\n",
            "Train: Epoch[ 3/10] Total time: 0:00:02 (0.1197 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.6106  Acc@1: 50.0000 (52.3046)  Acc@5: 95.8333 (94.5892)\n",
            "Train: Epoch[ 4/10]  [ 0/21]  eta: 0:00:17  Lr: 0.000047  Loss: 1.6162  Acc@1: 66.6667 (66.6667)  Acc@5: 91.6667 (91.6667)  time: 0.8441  data: 0.6868  max mem: 232\n",
            "Train: Epoch[ 4/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.4566  Acc@1: 66.6667 (62.8788)  Acc@5: 95.8333 (95.4545)  time: 0.1699  data: 0.0630  max mem: 232\n",
            "Train: Epoch[ 4/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 1.4335  Acc@1: 66.6667 (63.9279)  Acc@5: 95.8333 (96.1924)  time: 0.1006  data: 0.0036  max mem: 232\n",
            "Train: Epoch[ 4/10] Total time: 0:00:02 (0.1410 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.4335  Acc@1: 66.6667 (63.9279)  Acc@5: 95.8333 (96.1924)\n",
            "Train: Epoch[ 5/10]  [ 0/21]  eta: 0:00:10  Lr: 0.000047  Loss: 1.4525  Acc@1: 54.1667 (54.1667)  Acc@5: 95.8333 (95.8333)  time: 0.5059  data: 0.3872  max mem: 232\n",
            "Train: Epoch[ 5/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.2531  Acc@1: 62.5000 (64.7727)  Acc@5: 95.8333 (95.8333)  time: 0.1472  data: 0.0501  max mem: 232\n",
            "Train: Epoch[ 5/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 1.2833  Acc@1: 70.8333 (66.5331)  Acc@5: 95.8333 (96.5932)  time: 0.1001  data: 0.0084  max mem: 232\n",
            "Train: Epoch[ 5/10] Total time: 0:00:02 (0.1234 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.2833  Acc@1: 70.8333 (66.5331)  Acc@5: 95.8333 (96.5932)\n",
            "Train: Epoch[ 6/10]  [ 0/21]  eta: 0:00:09  Lr: 0.000047  Loss: 1.1710  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.4556  data: 0.3396  max mem: 232\n",
            "Train: Epoch[ 6/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.2794  Acc@1: 79.1667 (77.6515)  Acc@5: 95.8333 (95.8333)  time: 0.1274  data: 0.0322  max mem: 232\n",
            "Train: Epoch[ 6/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 1.1776  Acc@1: 78.9474 (74.9499)  Acc@5: 95.8333 (96.5932)  time: 0.0913  data: 0.0008  max mem: 232\n",
            "Train: Epoch[ 6/10] Total time: 0:00:02 (0.1126 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.1776  Acc@1: 78.9474 (74.9499)  Acc@5: 95.8333 (96.5932)\n",
            "Train: Epoch[ 7/10]  [ 0/21]  eta: 0:00:10  Lr: 0.000047  Loss: 1.2225  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  time: 0.5130  data: 0.4114  max mem: 232\n",
            "Train: Epoch[ 7/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.0506  Acc@1: 79.1667 (75.7576)  Acc@5: 95.8333 (96.9697)  time: 0.1303  data: 0.0394  max mem: 232\n",
            "Train: Epoch[ 7/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 0.8494  Acc@1: 79.1667 (77.1543)  Acc@5: 95.8333 (96.7936)  time: 0.0896  data: 0.0013  max mem: 232\n",
            "Train: Epoch[ 7/10] Total time: 0:00:02 (0.1146 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.8494  Acc@1: 79.1667 (77.1543)  Acc@5: 95.8333 (96.7936)\n",
            "Train: Epoch[ 8/10]  [ 0/21]  eta: 0:00:11  Lr: 0.000047  Loss: 1.1211  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  time: 0.5634  data: 0.4662  max mem: 232\n",
            "Train: Epoch[ 8/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.3106  Acc@1: 79.1667 (80.3030)  Acc@5: 95.8333 (96.5909)  time: 0.1343  data: 0.0432  max mem: 232\n",
            "Train: Epoch[ 8/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 0.8460  Acc@1: 83.3333 (80.9619)  Acc@5: 100.0000 (97.3948)  time: 0.0890  data: 0.0006  max mem: 232\n",
            "Train: Epoch[ 8/10] Total time: 0:00:02 (0.1177 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.8460  Acc@1: 83.3333 (80.9619)  Acc@5: 100.0000 (97.3948)\n",
            "Train: Epoch[ 9/10]  [ 0/21]  eta: 0:00:19  Lr: 0.000047  Loss: 0.8970  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.9318  data: 0.7461  max mem: 232\n",
            "Train: Epoch[ 9/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.0756  Acc@1: 83.3333 (82.5758)  Acc@5: 100.0000 (99.2424)  time: 0.1805  data: 0.0692  max mem: 232\n",
            "Train: Epoch[ 9/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 0.6587  Acc@1: 83.3333 (81.7635)  Acc@5: 100.0000 (98.3968)  time: 0.0994  data: 0.0018  max mem: 232\n",
            "Train: Epoch[ 9/10] Total time: 0:00:03 (0.1450 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.6587  Acc@1: 83.3333 (81.7635)  Acc@5: 100.0000 (98.3968)\n",
            "Train: Epoch[10/10]  [ 0/21]  eta: 0:00:15  Lr: 0.000047  Loss: 0.6978  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  time: 0.7412  data: 0.6408  max mem: 232\n",
            "Train: Epoch[10/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 0.9587  Acc@1: 79.1667 (81.8182)  Acc@5: 100.0000 (98.1061)  time: 0.1522  data: 0.0594  max mem: 232\n",
            "Train: Epoch[10/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 0.9490  Acc@1: 79.1667 (80.7615)  Acc@5: 100.0000 (97.9960)  time: 0.0905  data: 0.0009  max mem: 232\n",
            "Train: Epoch[10/10] Total time: 0:00:02 (0.1254 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.9490  Acc@1: 79.1667 (80.7615)  Acc@5: 100.0000 (97.9960)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:13  Loss: 1.0473 (1.0473)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  time: 0.3298  data: 0.2522  max mem: 232\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 1.1117 (1.0943)  Acc@1: 75.0000 (73.8636)  Acc@5: 95.8333 (92.8030)  time: 0.1149  data: 0.0315  max mem: 232\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 1.0009 (1.0638)  Acc@1: 75.0000 (76.5873)  Acc@5: 95.8333 (93.4524)  time: 0.0914  data: 0.0050  max mem: 232\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.9936 (1.0359)  Acc@1: 75.0000 (77.0161)  Acc@5: 95.8333 (94.3548)  time: 0.0896  data: 0.0004  max mem: 232\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.9334 (1.0122)  Acc@1: 79.1667 (78.1504)  Acc@5: 95.8333 (95.0203)  time: 0.0897  data: 0.0003  max mem: 232\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.8869 (1.0060)  Acc@1: 83.3333 (78.4000)  Acc@5: 95.8333 (95.1000)  time: 0.0882  data: 0.0003  max mem: 232\n",
            "Test: [Task 1] Total time: 0:00:04 (0.0974 s / it)\n",
            "* Acc@1 78.400 Acc@5 95.100 loss 1.006\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:21  Loss: 1.3801 (1.3801)  Acc@1: 58.3333 (58.3333)  Acc@5: 91.6667 (91.6667)  time: 0.5174  data: 0.4232  max mem: 232\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 1.4492 (1.3892)  Acc@1: 66.6667 (62.8788)  Acc@5: 91.6667 (91.2879)  time: 0.1295  data: 0.0391  max mem: 232\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 1.4492 (1.4030)  Acc@1: 66.6667 (64.8810)  Acc@5: 91.6667 (90.8730)  time: 0.0893  data: 0.0007  max mem: 232\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 1.3257 (1.3923)  Acc@1: 66.6667 (65.1882)  Acc@5: 91.6667 (91.2634)  time: 0.0882  data: 0.0006  max mem: 232\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 1.2586 (1.3766)  Acc@1: 66.6667 (65.3455)  Acc@5: 91.6667 (91.9715)  time: 0.0890  data: 0.0004  max mem: 232\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 1.2287 (1.3713)  Acc@1: 66.6667 (65.4000)  Acc@5: 91.6667 (92.0000)  time: 0.0872  data: 0.0004  max mem: 232\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1014 s / it)\n",
            "* Acc@1 65.400 Acc@5 92.000 loss 1.371\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:32  Loss: 0.8254 (0.8254)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.7703  data: 0.6878  max mem: 232\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:04  Loss: 1.0983 (1.1131)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (93.1818)  time: 0.1523  data: 0.0636  max mem: 232\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 1.1078 (1.0917)  Acc@1: 75.0000 (75.5952)  Acc@5: 95.8333 (93.6508)  time: 0.0900  data: 0.0023  max mem: 232\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 1.0431 (1.0673)  Acc@1: 75.0000 (75.9409)  Acc@5: 95.8333 (93.6828)  time: 0.0891  data: 0.0022  max mem: 232\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 1.0681 (1.0810)  Acc@1: 75.0000 (76.4228)  Acc@5: 91.6667 (93.5976)  time: 0.0890  data: 0.0006  max mem: 232\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 1.0681 (1.0885)  Acc@1: 75.0000 (76.1000)  Acc@5: 91.6667 (93.5000)  time: 0.0877  data: 0.0006  max mem: 232\n",
            "Test: [Task 3] Total time: 0:00:04 (0.1072 s / it)\n",
            "* Acc@1 76.100 Acc@5 93.500 loss 1.088\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:14  Loss: 1.3945 (1.3945)  Acc@1: 62.5000 (62.5000)  Acc@5: 83.3333 (83.3333)  time: 0.3546  data: 0.2846  max mem: 232\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:03  Loss: 1.0987 (1.1285)  Acc@1: 75.0000 (72.7273)  Acc@5: 95.8333 (93.5606)  time: 0.1208  data: 0.0365  max mem: 232\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 1.1437 (1.1563)  Acc@1: 75.0000 (73.0159)  Acc@5: 95.8333 (93.8492)  time: 0.0933  data: 0.0061  max mem: 232\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 1.1362 (1.1184)  Acc@1: 75.0000 (74.3280)  Acc@5: 95.8333 (94.3548)  time: 0.0898  data: 0.0004  max mem: 232\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 1.0731 (1.1319)  Acc@1: 75.0000 (74.2886)  Acc@5: 95.8333 (93.9024)  time: 0.0903  data: 0.0003  max mem: 232\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 1.0312 (1.1253)  Acc@1: 75.0000 (74.5000)  Acc@5: 95.8333 (94.0000)  time: 0.0883  data: 0.0003  max mem: 232\n",
            "Test: [Task 4] Total time: 0:00:04 (0.0993 s / it)\n",
            "* Acc@1 74.500 Acc@5 94.000 loss 1.125\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:20  Loss: 1.1358 (1.1358)  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.8333)  time: 0.4865  data: 0.4145  max mem: 232\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:04  Loss: 1.1560 (1.1453)  Acc@1: 70.8333 (73.4849)  Acc@5: 95.8333 (95.0758)  time: 0.1265  data: 0.0385  max mem: 232\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 1.1976 (1.1776)  Acc@1: 70.8333 (72.4206)  Acc@5: 91.6667 (94.0476)  time: 0.0892  data: 0.0006  max mem: 232\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 1.1668 (1.1940)  Acc@1: 70.8333 (72.0430)  Acc@5: 91.6667 (94.2204)  time: 0.0882  data: 0.0003  max mem: 232\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 1.2096 (1.2493)  Acc@1: 66.6667 (70.1220)  Acc@5: 95.8333 (93.5976)  time: 0.0893  data: 0.0003  max mem: 232\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 1.3051 (1.2627)  Acc@1: 66.6667 (69.8000)  Acc@5: 91.6667 (93.3000)  time: 0.0875  data: 0.0003  max mem: 232\n",
            "Test: [Task 5] Total time: 0:00:04 (0.0999 s / it)\n",
            "* Acc@1 69.800 Acc@5 93.300 loss 1.263\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:17  Loss: 1.0664 (1.0664)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  time: 0.4104  data: 0.3315  max mem: 232\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:04  Loss: 1.2318 (1.2722)  Acc@1: 66.6667 (68.1818)  Acc@5: 95.8333 (95.0758)  time: 0.1400  data: 0.0567  max mem: 232\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 1.2419 (1.2800)  Acc@1: 66.6667 (68.4524)  Acc@5: 95.8333 (96.0317)  time: 0.1017  data: 0.0150  max mem: 232\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 1.3869 (1.3171)  Acc@1: 66.6667 (66.5323)  Acc@5: 95.8333 (95.6989)  time: 0.0903  data: 0.0009  max mem: 232\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.3908 (1.3382)  Acc@1: 66.6667 (66.9715)  Acc@5: 95.8333 (95.0203)  time: 0.0895  data: 0.0006  max mem: 232\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.4027 (1.3401)  Acc@1: 62.5000 (66.9000)  Acc@5: 95.8333 (95.0000)  time: 0.0884  data: 0.0006  max mem: 232\n",
            "Test: [Task 6] Total time: 0:00:04 (0.1048 s / it)\n",
            "* Acc@1 66.900 Acc@5 95.000 loss 1.340\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:13  Loss: 1.5479 (1.5479)  Acc@1: 62.5000 (62.5000)  Acc@5: 87.5000 (87.5000)  time: 0.3252  data: 0.2479  max mem: 232\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:03  Loss: 1.5033 (1.5010)  Acc@1: 62.5000 (63.2576)  Acc@5: 95.8333 (93.1818)  time: 0.1219  data: 0.0352  max mem: 232\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:02  Loss: 1.4378 (1.4959)  Acc@1: 62.5000 (63.2937)  Acc@5: 91.6667 (93.2540)  time: 0.0948  data: 0.0071  max mem: 232\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:01  Loss: 1.4378 (1.5153)  Acc@1: 62.5000 (63.0376)  Acc@5: 91.6667 (91.9355)  time: 0.0893  data: 0.0003  max mem: 232\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 1.4775 (1.5099)  Acc@1: 66.6667 (63.8211)  Acc@5: 91.6667 (91.8699)  time: 0.0906  data: 0.0003  max mem: 232\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 1.4775 (1.5077)  Acc@1: 66.6667 (63.9000)  Acc@5: 91.6667 (91.8000)  time: 0.0880  data: 0.0003  max mem: 232\n",
            "Test: [Task 7] Total time: 0:00:04 (0.0995 s / it)\n",
            "* Acc@1 63.900 Acc@5 91.800 loss 1.508\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:20  Loss: 1.7911 (1.7911)  Acc@1: 37.5000 (37.5000)  Acc@5: 95.8333 (95.8333)  time: 0.4917  data: 0.4090  max mem: 232\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:04  Loss: 1.8361 (1.8855)  Acc@1: 45.8333 (46.5909)  Acc@5: 91.6667 (87.8788)  time: 0.1272  data: 0.0376  max mem: 232\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:02  Loss: 1.8147 (1.8398)  Acc@1: 45.8333 (48.8095)  Acc@5: 87.5000 (88.0952)  time: 0.0898  data: 0.0005  max mem: 232\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:01  Loss: 1.7563 (1.8181)  Acc@1: 50.0000 (49.0591)  Acc@5: 91.6667 (89.1129)  time: 0.0894  data: 0.0004  max mem: 232\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 1.7683 (1.8304)  Acc@1: 50.0000 (48.9837)  Acc@5: 87.5000 (87.7033)  time: 0.0902  data: 0.0002  max mem: 232\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 1.7683 (1.8246)  Acc@1: 50.0000 (49.3000)  Acc@5: 87.5000 (87.8000)  time: 0.0886  data: 0.0002  max mem: 232\n",
            "Test: [Task 8] Total time: 0:00:04 (0.1009 s / it)\n",
            "* Acc@1 49.300 Acc@5 87.800 loss 1.825\n",
            "Test: [Task 9]  [ 0/42]  eta: 0:00:15  Loss: 3.6040 (3.6040)  Acc@1: 0.0000 (0.0000)  Acc@5: 25.0000 (25.0000)  time: 0.3690  data: 0.2950  max mem: 232\n",
            "Test: [Task 9]  [10/42]  eta: 0:00:03  Loss: 3.5837 (3.5622)  Acc@1: 0.0000 (0.3788)  Acc@5: 37.5000 (37.8788)  time: 0.1193  data: 0.0335  max mem: 232\n",
            "Test: [Task 9]  [20/42]  eta: 0:00:02  Loss: 3.5687 (3.5529)  Acc@1: 0.0000 (0.9921)  Acc@5: 37.5000 (38.0952)  time: 0.0922  data: 0.0043  max mem: 232\n",
            "Test: [Task 9]  [30/42]  eta: 0:00:01  Loss: 3.5784 (3.5774)  Acc@1: 0.0000 (1.2097)  Acc@5: 37.5000 (36.8280)  time: 0.0904  data: 0.0011  max mem: 232\n",
            "Test: [Task 9]  [40/42]  eta: 0:00:00  Loss: 3.4872 (3.5517)  Acc@1: 0.0000 (1.1179)  Acc@5: 41.6667 (37.9065)  time: 0.0906  data: 0.0006  max mem: 232\n",
            "Test: [Task 9]  [41/42]  eta: 0:00:00  Loss: 3.4584 (3.5426)  Acc@1: 0.0000 (1.1000)  Acc@5: 41.6667 (38.3000)  time: 0.0888  data: 0.0006  max mem: 232\n",
            "Test: [Task 9] Total time: 0:00:04 (0.1001 s / it)\n",
            "* Acc@1 1.100 Acc@5 38.300 loss 3.543\n",
            "[Average accuracy till task9]\tAcc@1: 60.6000\tAcc@5: 86.7556\tLoss: 1.5632\tForgetting: 3.3375\tBackward: 29.3625\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([18936, 384])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 0.9279  Acc@1: 79.1667 (80.8333)  Acc@5: 95.8333 (95.3646)\n",
            "torch.Size([18936, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 0.8315  Acc@1: 83.3333 (84.2708)  Acc@5: 100.0000 (98.0729)\n",
            "torch.Size([18936, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 0.6492  Acc@1: 83.3333 (85.0000)  Acc@5: 100.0000 (98.7500)\n",
            "torch.Size([18936, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 0.7191  Acc@1: 87.5000 (88.1771)  Acc@5: 100.0000 (98.6979)\n",
            "torch.Size([18936, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 0.7061  Acc@1: 91.6667 (87.8125)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([18936, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 0.5955  Acc@1: 91.6667 (89.0625)  Acc@5: 100.0000 (99.2188)\n",
            "torch.Size([18936, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.7003  Acc@1: 91.6667 (91.0938)  Acc@5: 100.0000 (99.3229)\n",
            "torch.Size([18936, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 0.8349  Acc@1: 91.6667 (91.9271)  Acc@5: 100.0000 (99.4271)\n",
            "torch.Size([18936, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 0.7572  Acc@1: 91.6667 (91.3021)  Acc@5: 100.0000 (99.2188)\n",
            "torch.Size([18936, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.7535  Acc@1: 87.5000 (91.1458)  Acc@5: 100.0000 (99.3750)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:25  Loss: 1.0428 (1.0428)  Acc@1: 70.8333 (70.8333)  Acc@5: 83.3333 (83.3333)  time: 0.6024  data: 0.5246  max mem: 233\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 1.0678 (1.0747)  Acc@1: 70.8333 (71.9697)  Acc@5: 91.6667 (92.4242)  time: 0.1373  data: 0.0490  max mem: 233\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 1.0185 (1.0444)  Acc@1: 75.0000 (74.0079)  Acc@5: 95.8333 (93.0556)  time: 0.0893  data: 0.0016  max mem: 233\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.9249 (1.0136)  Acc@1: 79.1667 (75.4032)  Acc@5: 95.8333 (94.0860)  time: 0.0885  data: 0.0018  max mem: 233\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.8858 (0.9850)  Acc@1: 79.1667 (76.5244)  Acc@5: 95.8333 (94.8171)  time: 0.0895  data: 0.0013  max mem: 233\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.8819 (0.9774)  Acc@1: 79.1667 (76.8000)  Acc@5: 95.8333 (94.9000)  time: 0.0880  data: 0.0013  max mem: 233\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1032 s / it)\n",
            "* Acc@1 76.800 Acc@5 94.900 loss 0.977\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:14  Loss: 1.2378 (1.2378)  Acc@1: 62.5000 (62.5000)  Acc@5: 95.8333 (95.8333)  time: 0.3434  data: 0.2721  max mem: 233\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:03  Loss: 1.2378 (1.2277)  Acc@1: 66.6667 (66.2879)  Acc@5: 95.8333 (92.8030)  time: 0.1155  data: 0.0283  max mem: 233\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 1.2037 (1.2506)  Acc@1: 66.6667 (67.6587)  Acc@5: 91.6667 (91.8651)  time: 0.0910  data: 0.0021  max mem: 233\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 1.1785 (1.2358)  Acc@1: 66.6667 (69.3548)  Acc@5: 91.6667 (91.9355)  time: 0.0896  data: 0.0003  max mem: 233\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 1.1133 (1.2206)  Acc@1: 70.8333 (69.8171)  Acc@5: 95.8333 (92.6829)  time: 0.0900  data: 0.0002  max mem: 233\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 1.0685 (1.2169)  Acc@1: 70.8333 (69.9000)  Acc@5: 95.8333 (92.8000)  time: 0.0884  data: 0.0002  max mem: 233\n",
            "Test: [Task 2] Total time: 0:00:04 (0.0978 s / it)\n",
            "* Acc@1 69.900 Acc@5 92.800 loss 1.217\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:14  Loss: 0.7738 (0.7738)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.3498  data: 0.2735  max mem: 233\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 0.9999 (1.0546)  Acc@1: 75.0000 (75.3788)  Acc@5: 95.8333 (94.3182)  time: 0.1227  data: 0.0365  max mem: 233\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 1.0671 (1.0422)  Acc@1: 75.0000 (75.9921)  Acc@5: 95.8333 (93.8492)  time: 0.0950  data: 0.0065  max mem: 233\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.9916 (1.0161)  Acc@1: 75.0000 (75.8065)  Acc@5: 95.8333 (93.9516)  time: 0.0907  data: 0.0003  max mem: 233\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.9916 (1.0279)  Acc@1: 75.0000 (76.0163)  Acc@5: 91.6667 (93.5976)  time: 0.0916  data: 0.0003  max mem: 233\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.9916 (1.0327)  Acc@1: 75.0000 (75.9000)  Acc@5: 91.6667 (93.6000)  time: 0.0894  data: 0.0003  max mem: 233\n",
            "Test: [Task 3] Total time: 0:00:04 (0.1009 s / it)\n",
            "* Acc@1 75.900 Acc@5 93.600 loss 1.033\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:19  Loss: 1.4117 (1.4117)  Acc@1: 66.6667 (66.6667)  Acc@5: 83.3333 (83.3333)  time: 0.4683  data: 0.3849  max mem: 233\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:04  Loss: 1.0940 (1.1338)  Acc@1: 70.8333 (70.4545)  Acc@5: 95.8333 (92.8030)  time: 0.1250  data: 0.0358  max mem: 233\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 1.1361 (1.1636)  Acc@1: 70.8333 (70.6349)  Acc@5: 91.6667 (92.8571)  time: 0.0904  data: 0.0010  max mem: 233\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 1.1139 (1.1143)  Acc@1: 70.8333 (72.5806)  Acc@5: 95.8333 (93.6828)  time: 0.0905  data: 0.0014  max mem: 233\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 1.0705 (1.1282)  Acc@1: 70.8333 (72.0528)  Acc@5: 95.8333 (93.1911)  time: 0.0908  data: 0.0008  max mem: 233\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 1.0360 (1.1212)  Acc@1: 70.8333 (72.4000)  Acc@5: 95.8333 (93.3000)  time: 0.0892  data: 0.0008  max mem: 233\n",
            "Test: [Task 4] Total time: 0:00:04 (0.1028 s / it)\n",
            "* Acc@1 72.400 Acc@5 93.300 loss 1.121\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:31  Loss: 1.0264 (1.0264)  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.8333)  time: 0.7400  data: 0.6548  max mem: 233\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:04  Loss: 1.0521 (1.0487)  Acc@1: 70.8333 (73.8636)  Acc@5: 95.8333 (95.8333)  time: 0.1502  data: 0.0598  max mem: 233\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 1.0858 (1.0950)  Acc@1: 70.8333 (73.2143)  Acc@5: 91.6667 (94.6429)  time: 0.0907  data: 0.0004  max mem: 233\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 1.1056 (1.1104)  Acc@1: 70.8333 (72.9839)  Acc@5: 95.8333 (94.4893)  time: 0.0905  data: 0.0004  max mem: 233\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 1.1610 (1.1594)  Acc@1: 70.8333 (71.2398)  Acc@5: 95.8333 (93.8008)  time: 0.0911  data: 0.0003  max mem: 233\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 1.2224 (1.1728)  Acc@1: 70.8333 (70.9000)  Acc@5: 91.6667 (93.7000)  time: 0.0894  data: 0.0003  max mem: 233\n",
            "Test: [Task 5] Total time: 0:00:04 (0.1077 s / it)\n",
            "* Acc@1 70.900 Acc@5 93.700 loss 1.173\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:21  Loss: 0.9738 (0.9738)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  time: 0.5043  data: 0.4154  max mem: 233\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:04  Loss: 1.1555 (1.2043)  Acc@1: 66.6667 (67.0455)  Acc@5: 95.8333 (94.3182)  time: 0.1286  data: 0.0382  max mem: 233\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 1.1910 (1.2144)  Acc@1: 66.6667 (66.8651)  Acc@5: 95.8333 (94.4444)  time: 0.0907  data: 0.0004  max mem: 233\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 1.2876 (1.2462)  Acc@1: 62.5000 (66.3979)  Acc@5: 95.8333 (94.4892)  time: 0.0907  data: 0.0005  max mem: 233\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.3256 (1.2659)  Acc@1: 66.6667 (66.5650)  Acc@5: 91.6667 (93.8008)  time: 0.0912  data: 0.0007  max mem: 233\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.3365 (1.2692)  Acc@1: 62.5000 (66.5000)  Acc@5: 91.6667 (93.7000)  time: 0.0892  data: 0.0007  max mem: 233\n",
            "Test: [Task 6] Total time: 0:00:04 (0.1021 s / it)\n",
            "* Acc@1 66.500 Acc@5 93.700 loss 1.269\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:15  Loss: 1.4729 (1.4729)  Acc@1: 54.1667 (54.1667)  Acc@5: 87.5000 (87.5000)  time: 0.3717  data: 0.2865  max mem: 233\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:03  Loss: 1.3735 (1.3745)  Acc@1: 62.5000 (60.9849)  Acc@5: 91.6667 (93.1818)  time: 0.1185  data: 0.0300  max mem: 233\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:02  Loss: 1.3303 (1.3734)  Acc@1: 62.5000 (60.7143)  Acc@5: 95.8333 (93.4524)  time: 0.0922  data: 0.0023  max mem: 233\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:01  Loss: 1.3334 (1.3970)  Acc@1: 62.5000 (61.6935)  Acc@5: 91.6667 (92.2043)  time: 0.0913  data: 0.0006  max mem: 233\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 1.3464 (1.3929)  Acc@1: 62.5000 (61.7886)  Acc@5: 91.6667 (91.8699)  time: 0.0913  data: 0.0006  max mem: 233\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 1.3326 (1.3900)  Acc@1: 62.5000 (62.1000)  Acc@5: 91.6667 (91.8000)  time: 0.0895  data: 0.0006  max mem: 233\n",
            "Test: [Task 7] Total time: 0:00:04 (0.1010 s / it)\n",
            "* Acc@1 62.100 Acc@5 91.800 loss 1.390\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:30  Loss: 1.2627 (1.2627)  Acc@1: 58.3333 (58.3333)  Acc@5: 95.8333 (95.8333)  time: 0.7210  data: 0.6301  max mem: 233\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:04  Loss: 1.3754 (1.4104)  Acc@1: 62.5000 (64.0152)  Acc@5: 91.6667 (90.1515)  time: 0.1490  data: 0.0597  max mem: 233\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:02  Loss: 1.3307 (1.3695)  Acc@1: 62.5000 (65.2778)  Acc@5: 91.6667 (90.8730)  time: 0.0910  data: 0.0015  max mem: 233\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:01  Loss: 1.2915 (1.3465)  Acc@1: 66.6667 (66.6667)  Acc@5: 95.8333 (92.4731)  time: 0.0907  data: 0.0004  max mem: 233\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 1.3347 (1.3692)  Acc@1: 66.6667 (65.9553)  Acc@5: 95.8333 (91.8699)  time: 0.0911  data: 0.0003  max mem: 233\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 1.3347 (1.3651)  Acc@1: 66.6667 (66.0000)  Acc@5: 93.7500 (91.9000)  time: 0.0895  data: 0.0002  max mem: 233\n",
            "Test: [Task 8] Total time: 0:00:04 (0.1074 s / it)\n",
            "* Acc@1 66.000 Acc@5 91.900 loss 1.365\n",
            "Test: [Task 9]  [ 0/42]  eta: 0:00:13  Loss: 1.5371 (1.5371)  Acc@1: 50.0000 (50.0000)  Acc@5: 100.0000 (100.0000)  time: 0.3179  data: 0.2438  max mem: 233\n",
            "Test: [Task 9]  [10/42]  eta: 0:00:03  Loss: 1.5717 (1.6122)  Acc@1: 50.0000 (53.0303)  Acc@5: 95.8333 (94.3182)  time: 0.1174  data: 0.0296  max mem: 233\n",
            "Test: [Task 9]  [20/42]  eta: 0:00:02  Loss: 1.6031 (1.5880)  Acc@1: 54.1667 (56.1508)  Acc@5: 91.6667 (93.4524)  time: 0.0935  data: 0.0043  max mem: 233\n",
            "Test: [Task 9]  [30/42]  eta: 0:00:01  Loss: 1.6072 (1.6353)  Acc@1: 54.1667 (54.7043)  Acc@5: 91.6667 (91.9355)  time: 0.0903  data: 0.0003  max mem: 233\n",
            "Test: [Task 9]  [40/42]  eta: 0:00:00  Loss: 1.5157 (1.5958)  Acc@1: 54.1667 (56.3008)  Acc@5: 91.6667 (92.5813)  time: 0.0914  data: 0.0003  max mem: 233\n",
            "Test: [Task 9]  [41/42]  eta: 0:00:00  Loss: 1.5091 (1.5843)  Acc@1: 54.1667 (56.5000)  Acc@5: 91.6667 (92.7000)  time: 0.0895  data: 0.0003  max mem: 233\n",
            "Test: [Task 9] Total time: 0:00:04 (0.0992 s / it)\n",
            "* Acc@1 56.500 Acc@5 92.700 loss 1.584\n",
            "[Average accuracy till task9]\tAcc@1: 68.5556\tAcc@5: 93.1556\tLoss: 1.2366\tForgetting: 4.1750\tBackward: 0.1750\n",
            "Train: Epoch[ 1/10]  [ 0/21]  eta: 0:00:11  Lr: 0.000047  Loss: 2.4436  Acc@1: 0.0000 (0.0000)  Acc@5: 54.1667 (54.1667)  time: 0.5594  data: 0.4665  max mem: 233\n",
            "Train: Epoch[ 1/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 2.3209  Acc@1: 12.5000 (13.2576)  Acc@5: 58.3333 (59.4697)  time: 0.1339  data: 0.0437  max mem: 233\n",
            "Train: Epoch[ 1/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 2.1080  Acc@1: 20.8333 (19.3416)  Acc@5: 62.5000 (62.7572)  time: 0.0882  data: 0.0008  max mem: 233\n",
            "Train: Epoch[ 1/10] Total time: 0:00:02 (0.1144 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 2.1080  Acc@1: 20.8333 (19.3416)  Acc@5: 62.5000 (62.7572)\n",
            "Train: Epoch[ 2/10]  [ 0/21]  eta: 0:00:17  Lr: 0.000047  Loss: 2.0921  Acc@1: 29.1667 (29.1667)  Acc@5: 62.5000 (62.5000)  time: 0.8144  data: 0.6968  max mem: 233\n",
            "Train: Epoch[ 2/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.9355  Acc@1: 41.6667 (37.8788)  Acc@5: 83.3333 (78.4091)  time: 0.1712  data: 0.0641  max mem: 233\n",
            "Train: Epoch[ 2/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 1.9089  Acc@1: 41.6667 (43.8272)  Acc@5: 83.3333 (82.7161)  time: 0.0963  data: 0.0009  max mem: 233\n",
            "Train: Epoch[ 2/10] Total time: 0:00:02 (0.1363 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.9089  Acc@1: 41.6667 (43.8272)  Acc@5: 83.3333 (82.7161)\n",
            "Train: Epoch[ 3/10]  [ 0/21]  eta: 0:00:14  Lr: 0.000047  Loss: 1.7219  Acc@1: 45.8333 (45.8333)  Acc@5: 91.6667 (91.6667)  time: 0.7109  data: 0.5554  max mem: 233\n",
            "Train: Epoch[ 3/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.6606  Acc@1: 58.3333 (57.9545)  Acc@5: 91.6667 (92.0455)  time: 0.1598  data: 0.0517  max mem: 233\n",
            "Train: Epoch[ 3/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 1.5154  Acc@1: 58.3333 (59.2593)  Acc@5: 95.8333 (93.6214)  time: 0.0949  data: 0.0008  max mem: 233\n",
            "Train: Epoch[ 3/10] Total time: 0:00:02 (0.1288 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.5154  Acc@1: 58.3333 (59.2593)  Acc@5: 95.8333 (93.6214)\n",
            "Train: Epoch[ 4/10]  [ 0/21]  eta: 0:00:08  Lr: 0.000047  Loss: 1.4076  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.4136  data: 0.2978  max mem: 233\n",
            "Train: Epoch[ 4/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.3802  Acc@1: 62.5000 (68.5606)  Acc@5: 95.8333 (96.9697)  time: 0.1353  data: 0.0436  max mem: 233\n",
            "Train: Epoch[ 4/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 1.1378  Acc@1: 70.8333 (71.8107)  Acc@5: 95.8333 (96.9136)  time: 0.0951  data: 0.0093  max mem: 233\n",
            "Train: Epoch[ 4/10] Total time: 0:00:02 (0.1149 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.1378  Acc@1: 70.8333 (71.8107)  Acc@5: 95.8333 (96.9136)\n",
            "Train: Epoch[ 5/10]  [ 0/21]  eta: 0:00:13  Lr: 0.000047  Loss: 1.1999  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.6499  data: 0.4832  max mem: 233\n",
            "Train: Epoch[ 5/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.2798  Acc@1: 75.0000 (75.7576)  Acc@5: 100.0000 (97.7273)  time: 0.1485  data: 0.0458  max mem: 233\n",
            "Train: Epoch[ 5/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 1.2217  Acc@1: 75.0000 (75.7202)  Acc@5: 100.0000 (97.5309)  time: 0.0910  data: 0.0011  max mem: 233\n",
            "Train: Epoch[ 5/10] Total time: 0:00:02 (0.1214 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.2217  Acc@1: 75.0000 (75.7202)  Acc@5: 100.0000 (97.5309)\n",
            "Train: Epoch[ 6/10]  [ 0/21]  eta: 0:00:10  Lr: 0.000047  Loss: 1.0083  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.4897  data: 0.3560  max mem: 233\n",
            "Train: Epoch[ 6/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.1665  Acc@1: 83.3333 (81.0606)  Acc@5: 100.0000 (98.8636)  time: 0.1297  data: 0.0342  max mem: 233\n",
            "Train: Epoch[ 6/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 0.9253  Acc@1: 83.3333 (83.1276)  Acc@5: 100.0000 (98.3539)  time: 0.0886  data: 0.0011  max mem: 233\n",
            "Train: Epoch[ 6/10] Total time: 0:00:02 (0.1116 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.9253  Acc@1: 83.3333 (83.1276)  Acc@5: 100.0000 (98.3539)\n",
            "Train: Epoch[ 7/10]  [ 0/21]  eta: 0:00:09  Lr: 0.000047  Loss: 1.0803  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  time: 0.4671  data: 0.3692  max mem: 233\n",
            "Train: Epoch[ 7/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 0.8354  Acc@1: 83.3333 (85.2273)  Acc@5: 100.0000 (98.4848)  time: 0.1446  data: 0.0375  max mem: 233\n",
            "Train: Epoch[ 7/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 0.9252  Acc@1: 83.3333 (84.3621)  Acc@5: 100.0000 (98.5597)  time: 0.0987  data: 0.0025  max mem: 233\n",
            "Train: Epoch[ 7/10] Total time: 0:00:02 (0.1223 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.9252  Acc@1: 83.3333 (84.3621)  Acc@5: 100.0000 (98.5597)\n",
            "Train: Epoch[ 8/10]  [ 0/21]  eta: 0:00:14  Lr: 0.000047  Loss: 0.7414  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.6903  data: 0.5808  max mem: 233\n",
            "Train: Epoch[ 8/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 1.0613  Acc@1: 83.3333 (80.6818)  Acc@5: 100.0000 (98.4848)  time: 0.1668  data: 0.0587  max mem: 233\n",
            "Train: Epoch[ 8/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 0.7850  Acc@1: 83.3333 (82.9218)  Acc@5: 100.0000 (98.5597)  time: 0.1055  data: 0.0086  max mem: 233\n",
            "Train: Epoch[ 8/10] Total time: 0:00:02 (0.1377 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.7850  Acc@1: 83.3333 (82.9218)  Acc@5: 100.0000 (98.5597)\n",
            "Train: Epoch[ 9/10]  [ 0/21]  eta: 0:00:11  Lr: 0.000047  Loss: 0.8246  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.5331  data: 0.4333  max mem: 233\n",
            "Train: Epoch[ 9/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 0.9115  Acc@1: 83.3333 (82.9545)  Acc@5: 100.0000 (98.4848)  time: 0.1319  data: 0.0408  max mem: 233\n",
            "Train: Epoch[ 9/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 0.7499  Acc@1: 87.5000 (85.1852)  Acc@5: 100.0000 (98.9712)  time: 0.0877  data: 0.0009  max mem: 233\n",
            "Train: Epoch[ 9/10] Total time: 0:00:02 (0.1130 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.7499  Acc@1: 87.5000 (85.1852)  Acc@5: 100.0000 (98.9712)\n",
            "Train: Epoch[10/10]  [ 0/21]  eta: 0:00:13  Lr: 0.000047  Loss: 0.6229  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.6289  data: 0.4678  max mem: 233\n",
            "Train: Epoch[10/10]  [10/21]  eta: 0:00:01  Lr: 0.000047  Loss: 0.6348  Acc@1: 83.3333 (84.0909)  Acc@5: 100.0000 (98.8636)  time: 0.1474  data: 0.0432  max mem: 233\n",
            "Train: Epoch[10/10]  [20/21]  eta: 0:00:00  Lr: 0.000047  Loss: 0.9250  Acc@1: 83.3333 (84.1564)  Acc@5: 100.0000 (99.1770)  time: 0.0947  data: 0.0007  max mem: 233\n",
            "Train: Epoch[10/10] Total time: 0:00:02 (0.1281 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.9250  Acc@1: 83.3333 (84.1564)  Acc@5: 100.0000 (99.1770)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:34  Loss: 1.0937 (1.0937)  Acc@1: 75.0000 (75.0000)  Acc@5: 87.5000 (87.5000)  time: 0.8167  data: 0.7186  max mem: 233\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 1.0999 (1.1178)  Acc@1: 70.8333 (70.4545)  Acc@5: 95.8333 (92.4242)  time: 0.1550  data: 0.0660  max mem: 233\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 1.0507 (1.0879)  Acc@1: 75.0000 (74.0079)  Acc@5: 95.8333 (93.0556)  time: 0.0886  data: 0.0007  max mem: 233\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.9692 (1.0561)  Acc@1: 79.1667 (75.0000)  Acc@5: 91.6667 (93.6828)  time: 0.0887  data: 0.0006  max mem: 233\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.9124 (1.0279)  Acc@1: 79.1667 (76.4228)  Acc@5: 95.8333 (94.7154)  time: 0.0891  data: 0.0004  max mem: 233\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.8933 (1.0207)  Acc@1: 83.3333 (76.7000)  Acc@5: 95.8333 (94.8000)  time: 0.0874  data: 0.0004  max mem: 233\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1078 s / it)\n",
            "* Acc@1 76.700 Acc@5 94.800 loss 1.021\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:32  Loss: 1.2645 (1.2645)  Acc@1: 62.5000 (62.5000)  Acc@5: 95.8333 (95.8333)  time: 0.7736  data: 0.6577  max mem: 233\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 1.2645 (1.2724)  Acc@1: 70.8333 (66.2879)  Acc@5: 91.6667 (92.8030)  time: 0.1523  data: 0.0608  max mem: 233\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 1.2505 (1.2905)  Acc@1: 70.8333 (68.4524)  Acc@5: 91.6667 (91.8651)  time: 0.0893  data: 0.0012  max mem: 233\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 1.1620 (1.2840)  Acc@1: 66.6667 (68.6828)  Acc@5: 91.6667 (92.0699)  time: 0.0881  data: 0.0009  max mem: 233\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 1.1511 (1.2735)  Acc@1: 66.6667 (68.6992)  Acc@5: 91.6667 (92.2764)  time: 0.0887  data: 0.0008  max mem: 233\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 1.1511 (1.2712)  Acc@1: 66.6667 (68.7000)  Acc@5: 95.8333 (92.4000)  time: 0.0867  data: 0.0008  max mem: 233\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1069 s / it)\n",
            "* Acc@1 68.700 Acc@5 92.400 loss 1.271\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:18  Loss: 0.8170 (0.8170)  Acc@1: 70.8333 (70.8333)  Acc@5: 100.0000 (100.0000)  time: 0.4376  data: 0.3569  max mem: 233\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 1.0530 (1.1043)  Acc@1: 75.0000 (73.8636)  Acc@5: 95.8333 (93.1818)  time: 0.1225  data: 0.0331  max mem: 233\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 1.0791 (1.1017)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (93.4524)  time: 0.0894  data: 0.0006  max mem: 233\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.9968 (1.0748)  Acc@1: 75.0000 (74.4624)  Acc@5: 91.6667 (93.2796)  time: 0.0882  data: 0.0005  max mem: 233\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 1.0098 (1.0861)  Acc@1: 75.0000 (74.6951)  Acc@5: 91.6667 (93.2927)  time: 0.0891  data: 0.0003  max mem: 233\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 1.0098 (1.0932)  Acc@1: 75.0000 (74.6000)  Acc@5: 91.6667 (93.3000)  time: 0.0876  data: 0.0003  max mem: 233\n",
            "Test: [Task 3] Total time: 0:00:04 (0.0989 s / it)\n",
            "* Acc@1 74.600 Acc@5 93.300 loss 1.093\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:19  Loss: 1.5460 (1.5460)  Acc@1: 62.5000 (62.5000)  Acc@5: 83.3333 (83.3333)  time: 0.4711  data: 0.3941  max mem: 233\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:03  Loss: 1.1695 (1.1573)  Acc@1: 70.8333 (70.4545)  Acc@5: 91.6667 (92.4242)  time: 0.1249  data: 0.0362  max mem: 233\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 1.2037 (1.1753)  Acc@1: 70.8333 (70.4365)  Acc@5: 91.6667 (92.6587)  time: 0.0891  data: 0.0003  max mem: 233\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 1.0833 (1.1341)  Acc@1: 70.8333 (72.0430)  Acc@5: 95.8333 (93.4140)  time: 0.0879  data: 0.0004  max mem: 233\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 1.0833 (1.1489)  Acc@1: 70.8333 (71.5447)  Acc@5: 95.8333 (92.6829)  time: 0.0889  data: 0.0004  max mem: 233\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 1.0573 (1.1413)  Acc@1: 70.8333 (71.9000)  Acc@5: 95.8333 (92.8000)  time: 0.0875  data: 0.0003  max mem: 233\n",
            "Test: [Task 4] Total time: 0:00:04 (0.0995 s / it)\n",
            "* Acc@1 71.900 Acc@5 92.800 loss 1.141\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:21  Loss: 1.0115 (1.0115)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  time: 0.5163  data: 0.4393  max mem: 233\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:04  Loss: 1.0161 (1.0643)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (96.2121)  time: 0.1282  data: 0.0409  max mem: 233\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 1.1205 (1.1061)  Acc@1: 75.0000 (74.6032)  Acc@5: 95.8333 (94.6429)  time: 0.0889  data: 0.0011  max mem: 233\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 1.1177 (1.1216)  Acc@1: 75.0000 (74.4624)  Acc@5: 91.6667 (94.4892)  time: 0.0889  data: 0.0010  max mem: 233\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 1.1177 (1.1725)  Acc@1: 70.8333 (72.3577)  Acc@5: 91.6667 (93.6992)  time: 0.0895  data: 0.0006  max mem: 233\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 1.2509 (1.1858)  Acc@1: 70.8333 (71.9000)  Acc@5: 91.6667 (93.6000)  time: 0.0880  data: 0.0005  max mem: 233\n",
            "Test: [Task 5] Total time: 0:00:04 (0.1022 s / it)\n",
            "* Acc@1 71.900 Acc@5 93.600 loss 1.186\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:24  Loss: 1.0261 (1.0261)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  time: 0.5828  data: 0.4841  max mem: 233\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:04  Loss: 1.2170 (1.2553)  Acc@1: 66.6667 (67.4242)  Acc@5: 95.8333 (94.3182)  time: 0.1363  data: 0.0479  max mem: 233\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 1.2278 (1.2714)  Acc@1: 66.6667 (68.0556)  Acc@5: 95.8333 (93.8492)  time: 0.0907  data: 0.0024  max mem: 233\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 1.3544 (1.3088)  Acc@1: 66.6667 (66.3979)  Acc@5: 91.6667 (93.4140)  time: 0.0900  data: 0.0005  max mem: 233\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.4090 (1.3260)  Acc@1: 66.6667 (66.0569)  Acc@5: 91.6667 (93.0894)  time: 0.0900  data: 0.0003  max mem: 233\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.4279 (1.3284)  Acc@1: 62.5000 (66.0000)  Acc@5: 91.6667 (93.0000)  time: 0.0885  data: 0.0003  max mem: 233\n",
            "Test: [Task 6] Total time: 0:00:04 (0.1034 s / it)\n",
            "* Acc@1 66.000 Acc@5 93.000 loss 1.328\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:14  Loss: 1.5285 (1.5285)  Acc@1: 62.5000 (62.5000)  Acc@5: 87.5000 (87.5000)  time: 0.3428  data: 0.2718  max mem: 233\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:03  Loss: 1.4601 (1.4327)  Acc@1: 62.5000 (62.5000)  Acc@5: 91.6667 (92.4242)  time: 0.1228  data: 0.0383  max mem: 233\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:02  Loss: 1.3359 (1.4268)  Acc@1: 62.5000 (62.6984)  Acc@5: 91.6667 (92.0635)  time: 0.0950  data: 0.0077  max mem: 233\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:01  Loss: 1.3815 (1.4476)  Acc@1: 66.6667 (63.3065)  Acc@5: 91.6667 (91.2634)  time: 0.0896  data: 0.0004  max mem: 233\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 1.3815 (1.4438)  Acc@1: 66.6667 (64.0244)  Acc@5: 91.6667 (91.1585)  time: 0.0904  data: 0.0003  max mem: 233\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 1.3620 (1.4416)  Acc@1: 66.6667 (64.1000)  Acc@5: 91.6667 (91.1000)  time: 0.0880  data: 0.0003  max mem: 233\n",
            "Test: [Task 7] Total time: 0:00:04 (0.0999 s / it)\n",
            "* Acc@1 64.100 Acc@5 91.100 loss 1.442\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:20  Loss: 1.4007 (1.4007)  Acc@1: 58.3333 (58.3333)  Acc@5: 95.8333 (95.8333)  time: 0.4907  data: 0.4025  max mem: 233\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:04  Loss: 1.5375 (1.5434)  Acc@1: 58.3333 (60.6061)  Acc@5: 87.5000 (88.2576)  time: 0.1275  data: 0.0372  max mem: 233\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:02  Loss: 1.4897 (1.4952)  Acc@1: 58.3333 (62.3016)  Acc@5: 87.5000 (89.0873)  time: 0.0897  data: 0.0006  max mem: 233\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:01  Loss: 1.4355 (1.4751)  Acc@1: 58.3333 (63.1720)  Acc@5: 91.6667 (90.4570)  time: 0.0887  data: 0.0005  max mem: 233\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 1.4464 (1.5036)  Acc@1: 58.3333 (61.6870)  Acc@5: 87.5000 (89.5325)  time: 0.0897  data: 0.0004  max mem: 233\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 1.4464 (1.5010)  Acc@1: 58.3333 (61.8000)  Acc@5: 87.5000 (89.6000)  time: 0.0877  data: 0.0004  max mem: 233\n",
            "Test: [Task 8] Total time: 0:00:04 (0.1016 s / it)\n",
            "* Acc@1 61.800 Acc@5 89.600 loss 1.501\n",
            "Test: [Task 9]  [ 0/42]  eta: 0:00:37  Loss: 1.7071 (1.7071)  Acc@1: 33.3333 (33.3333)  Acc@5: 95.8333 (95.8333)  time: 0.8810  data: 0.7778  max mem: 233\n",
            "Test: [Task 9]  [10/42]  eta: 0:00:05  Loss: 1.8666 (1.8569)  Acc@1: 45.8333 (43.1818)  Acc@5: 91.6667 (92.0455)  time: 0.1635  data: 0.0720  max mem: 233\n",
            "Test: [Task 9]  [20/42]  eta: 0:00:02  Loss: 1.8666 (1.8422)  Acc@1: 45.8333 (46.4286)  Acc@5: 91.6667 (91.0714)  time: 0.0903  data: 0.0014  max mem: 233\n",
            "Test: [Task 9]  [30/42]  eta: 0:00:01  Loss: 1.8866 (1.8937)  Acc@1: 45.8333 (44.8925)  Acc@5: 87.5000 (89.2473)  time: 0.0889  data: 0.0009  max mem: 233\n",
            "Test: [Task 9]  [40/42]  eta: 0:00:00  Loss: 1.7662 (1.8492)  Acc@1: 45.8333 (45.9350)  Acc@5: 87.5000 (90.2439)  time: 0.0895  data: 0.0003  max mem: 233\n",
            "Test: [Task 9]  [41/42]  eta: 0:00:00  Loss: 1.7376 (1.8372)  Acc@1: 45.8333 (46.0000)  Acc@5: 91.6667 (90.4000)  time: 0.0880  data: 0.0003  max mem: 233\n",
            "Test: [Task 9] Total time: 0:00:04 (0.1101 s / it)\n",
            "* Acc@1 46.000 Acc@5 90.400 loss 1.837\n",
            "Test: [Task 10]  [ 0/42]  eta: 0:00:14  Loss: 3.6679 (3.6679)  Acc@1: 0.0000 (0.0000)  Acc@5: 20.8333 (20.8333)  time: 0.3541  data: 0.2762  max mem: 233\n",
            "Test: [Task 10]  [10/42]  eta: 0:00:03  Loss: 3.5719 (3.5996)  Acc@1: 0.0000 (1.1364)  Acc@5: 33.3333 (32.1970)  time: 0.1230  data: 0.0371  max mem: 233\n",
            "Test: [Task 10]  [20/42]  eta: 0:00:02  Loss: 3.5719 (3.5784)  Acc@1: 0.0000 (0.9921)  Acc@5: 37.5000 (37.1032)  time: 0.0946  data: 0.0067  max mem: 233\n",
            "Test: [Task 10]  [30/42]  eta: 0:00:01  Loss: 3.6369 (3.5854)  Acc@1: 0.0000 (0.6720)  Acc@5: 37.5000 (37.3656)  time: 0.0897  data: 0.0003  max mem: 233\n",
            "Test: [Task 10]  [40/42]  eta: 0:00:00  Loss: 3.6057 (3.5971)  Acc@1: 0.0000 (0.6098)  Acc@5: 37.5000 (38.1098)  time: 0.0905  data: 0.0003  max mem: 233\n",
            "Test: [Task 10]  [41/42]  eta: 0:00:00  Loss: 3.5937 (3.5911)  Acc@1: 0.0000 (0.6000)  Acc@5: 37.5000 (38.2000)  time: 0.0881  data: 0.0003  max mem: 233\n",
            "Test: [Task 10] Total time: 0:00:04 (0.0999 s / it)\n",
            "* Acc@1 0.600 Acc@5 38.200 loss 3.591\n",
            "[Average accuracy till task10]\tAcc@1: 60.2300\tAcc@5: 86.9200\tLoss: 1.5411\tForgetting: 3.3444\tBackward: 32.3556\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([20832, 384])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 1.0052  Acc@1: 83.3333 (83.2870)  Acc@5: 95.8333 (95.7407)\n",
            "torch.Size([20832, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 0.8424  Acc@1: 87.5000 (85.5556)  Acc@5: 95.8333 (96.9907)\n",
            "torch.Size([20832, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 0.7975  Acc@1: 83.3333 (85.2315)  Acc@5: 100.0000 (98.6574)\n",
            "torch.Size([20832, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 0.8957  Acc@1: 87.5000 (89.3056)  Acc@5: 100.0000 (99.0278)\n",
            "torch.Size([20832, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 0.7830  Acc@1: 87.5000 (88.0093)  Acc@5: 100.0000 (99.2593)\n",
            "torch.Size([20832, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 0.8080  Acc@1: 91.6667 (88.9815)  Acc@5: 100.0000 (99.2130)\n",
            "torch.Size([20832, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.6625  Acc@1: 91.6667 (88.8889)  Acc@5: 100.0000 (99.1204)\n",
            "torch.Size([20832, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 0.5868  Acc@1: 91.6667 (91.1111)  Acc@5: 100.0000 (99.3056)\n",
            "torch.Size([20832, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 0.6781  Acc@1: 91.6667 (90.8333)  Acc@5: 100.0000 (99.7222)\n",
            "torch.Size([20832, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.7337  Acc@1: 87.5000 (90.5556)  Acc@5: 100.0000 (99.5833)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:21  Loss: 1.0724 (1.0724)  Acc@1: 70.8333 (70.8333)  Acc@5: 83.3333 (83.3333)  time: 0.5060  data: 0.4361  max mem: 233\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 1.0724 (1.0988)  Acc@1: 70.8333 (71.5909)  Acc@5: 91.6667 (92.0455)  time: 0.1277  data: 0.0401  max mem: 233\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 1.0672 (1.0601)  Acc@1: 75.0000 (74.2064)  Acc@5: 95.8333 (92.8571)  time: 0.0886  data: 0.0005  max mem: 233\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.9467 (1.0270)  Acc@1: 75.0000 (74.5968)  Acc@5: 95.8333 (93.8172)  time: 0.0877  data: 0.0005  max mem: 233\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.8766 (0.9971)  Acc@1: 79.1667 (75.8130)  Acc@5: 95.8333 (94.6138)  time: 0.0888  data: 0.0002  max mem: 233\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.8745 (0.9886)  Acc@1: 79.1667 (76.1000)  Acc@5: 95.8333 (94.7000)  time: 0.0868  data: 0.0002  max mem: 233\n",
            "Test: [Task 1] Total time: 0:00:04 (0.0998 s / it)\n",
            "* Acc@1 76.100 Acc@5 94.700 loss 0.989\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:28  Loss: 1.2045 (1.2045)  Acc@1: 62.5000 (62.5000)  Acc@5: 91.6667 (91.6667)  time: 0.6867  data: 0.5873  max mem: 233\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 1.2123 (1.2020)  Acc@1: 66.6667 (67.4242)  Acc@5: 91.6667 (92.0455)  time: 0.1457  data: 0.0559  max mem: 233\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 1.2123 (1.2354)  Acc@1: 66.6667 (66.8651)  Acc@5: 91.6667 (91.2698)  time: 0.0901  data: 0.0023  max mem: 233\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 1.1403 (1.2246)  Acc@1: 66.6667 (68.5484)  Acc@5: 91.6667 (91.3979)  time: 0.0892  data: 0.0012  max mem: 233\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 1.0838 (1.2104)  Acc@1: 66.6667 (68.1911)  Acc@5: 95.8333 (91.9715)  time: 0.0905  data: 0.0005  max mem: 233\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 1.0838 (1.2076)  Acc@1: 66.6667 (68.3000)  Acc@5: 95.8333 (92.0000)  time: 0.0890  data: 0.0005  max mem: 233\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1060 s / it)\n",
            "* Acc@1 68.300 Acc@5 92.000 loss 1.208\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:15  Loss: 0.7331 (0.7331)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.3806  data: 0.3028  max mem: 233\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 0.9956 (1.0260)  Acc@1: 79.1667 (76.8939)  Acc@5: 95.8333 (93.1818)  time: 0.1229  data: 0.0360  max mem: 233\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 1.0220 (1.0155)  Acc@1: 79.1667 (76.9841)  Acc@5: 95.8333 (93.4524)  time: 0.0934  data: 0.0048  max mem: 233\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.9600 (0.9943)  Acc@1: 75.0000 (76.3441)  Acc@5: 95.8333 (93.6828)  time: 0.0901  data: 0.0004  max mem: 233\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.9600 (1.0015)  Acc@1: 79.1667 (76.6260)  Acc@5: 91.6667 (93.4959)  time: 0.0910  data: 0.0003  max mem: 233\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.9600 (1.0067)  Acc@1: 79.1667 (76.5000)  Acc@5: 93.7500 (93.5000)  time: 0.0891  data: 0.0002  max mem: 233\n",
            "Test: [Task 3] Total time: 0:00:04 (0.1003 s / it)\n",
            "* Acc@1 76.500 Acc@5 93.500 loss 1.007\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:18  Loss: 1.4891 (1.4891)  Acc@1: 66.6667 (66.6667)  Acc@5: 79.1667 (79.1667)  time: 0.4387  data: 0.3308  max mem: 233\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:03  Loss: 1.1638 (1.1726)  Acc@1: 66.6667 (68.5606)  Acc@5: 91.6667 (91.2879)  time: 0.1224  data: 0.0316  max mem: 233\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 1.1758 (1.1944)  Acc@1: 70.8333 (69.4444)  Acc@5: 91.6667 (91.0714)  time: 0.0914  data: 0.0011  max mem: 233\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 1.1012 (1.1425)  Acc@1: 70.8333 (70.9677)  Acc@5: 91.6667 (91.6667)  time: 0.0919  data: 0.0004  max mem: 233\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 1.0917 (1.1600)  Acc@1: 70.8333 (70.3252)  Acc@5: 91.6667 (91.3618)  time: 0.0923  data: 0.0003  max mem: 233\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 1.0517 (1.1530)  Acc@1: 70.8333 (70.7000)  Acc@5: 91.6667 (91.5000)  time: 0.0909  data: 0.0003  max mem: 233\n",
            "Test: [Task 4] Total time: 0:00:04 (0.1017 s / it)\n",
            "* Acc@1 70.700 Acc@5 91.500 loss 1.153\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:18  Loss: 1.0124 (1.0124)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  time: 0.4471  data: 0.3633  max mem: 233\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:04  Loss: 1.0124 (1.0507)  Acc@1: 70.8333 (72.3485)  Acc@5: 95.8333 (95.4545)  time: 0.1250  data: 0.0337  max mem: 233\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 1.1206 (1.1016)  Acc@1: 70.8333 (72.4206)  Acc@5: 91.6667 (93.8492)  time: 0.0921  data: 0.0010  max mem: 233\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 1.1221 (1.1144)  Acc@1: 70.8333 (71.7742)  Acc@5: 91.6667 (93.8172)  time: 0.0916  data: 0.0016  max mem: 233\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 1.1677 (1.1655)  Acc@1: 66.6667 (70.0203)  Acc@5: 91.6667 (93.0894)  time: 0.0921  data: 0.0015  max mem: 233\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 1.2272 (1.1800)  Acc@1: 66.6667 (69.5000)  Acc@5: 91.6667 (93.0000)  time: 0.0909  data: 0.0014  max mem: 233\n",
            "Test: [Task 5] Total time: 0:00:04 (0.1031 s / it)\n",
            "* Acc@1 69.500 Acc@5 93.000 loss 1.180\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:23  Loss: 0.9812 (0.9812)  Acc@1: 83.3333 (83.3333)  Acc@5: 91.6667 (91.6667)  time: 0.5599  data: 0.4732  max mem: 233\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:04  Loss: 1.1385 (1.1978)  Acc@1: 70.8333 (69.6970)  Acc@5: 95.8333 (95.0758)  time: 0.1361  data: 0.0435  max mem: 233\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 1.1445 (1.2123)  Acc@1: 70.8333 (68.0556)  Acc@5: 95.8333 (94.6429)  time: 0.0931  data: 0.0004  max mem: 233\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 1.2963 (1.2496)  Acc@1: 62.5000 (66.2634)  Acc@5: 91.6667 (93.8172)  time: 0.0929  data: 0.0004  max mem: 233\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.3290 (1.2681)  Acc@1: 62.5000 (65.8537)  Acc@5: 91.6667 (93.5976)  time: 0.0933  data: 0.0003  max mem: 233\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.3430 (1.2712)  Acc@1: 62.5000 (65.9000)  Acc@5: 91.6667 (93.5000)  time: 0.0917  data: 0.0003  max mem: 233\n",
            "Test: [Task 6] Total time: 0:00:04 (0.1057 s / it)\n",
            "* Acc@1 65.900 Acc@5 93.500 loss 1.271\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:19  Loss: 1.4698 (1.4698)  Acc@1: 58.3333 (58.3333)  Acc@5: 87.5000 (87.5000)  time: 0.4630  data: 0.3677  max mem: 233\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:04  Loss: 1.2834 (1.3002)  Acc@1: 62.5000 (64.0152)  Acc@5: 91.6667 (92.4242)  time: 0.1271  data: 0.0343  max mem: 233\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:02  Loss: 1.2531 (1.3021)  Acc@1: 62.5000 (63.8889)  Acc@5: 95.8333 (92.6587)  time: 0.0933  data: 0.0007  max mem: 233\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:01  Loss: 1.2742 (1.3284)  Acc@1: 62.5000 (64.3817)  Acc@5: 91.6667 (91.5323)  time: 0.0935  data: 0.0005  max mem: 233\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 1.2769 (1.3271)  Acc@1: 62.5000 (64.3293)  Acc@5: 91.6667 (91.3618)  time: 0.0934  data: 0.0004  max mem: 233\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 1.2769 (1.3239)  Acc@1: 66.6667 (64.5000)  Acc@5: 91.6667 (91.3000)  time: 0.0919  data: 0.0003  max mem: 233\n",
            "Test: [Task 7] Total time: 0:00:04 (0.1035 s / it)\n",
            "* Acc@1 64.500 Acc@5 91.300 loss 1.324\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:15  Loss: 1.2442 (1.2442)  Acc@1: 62.5000 (62.5000)  Acc@5: 95.8333 (95.8333)  time: 0.3584  data: 0.2790  max mem: 233\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:03  Loss: 1.3959 (1.3698)  Acc@1: 62.5000 (67.0455)  Acc@5: 91.6667 (89.3939)  time: 0.1190  data: 0.0301  max mem: 233\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:02  Loss: 1.2666 (1.3230)  Acc@1: 66.6667 (68.0556)  Acc@5: 87.5000 (89.2857)  time: 0.0936  data: 0.0030  max mem: 233\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:01  Loss: 1.2298 (1.2960)  Acc@1: 66.6667 (68.5484)  Acc@5: 91.6667 (90.7258)  time: 0.0923  data: 0.0015  max mem: 233\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 1.2989 (1.3181)  Acc@1: 62.5000 (67.3781)  Acc@5: 91.6667 (90.4472)  time: 0.0926  data: 0.0012  max mem: 233\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 1.2989 (1.3155)  Acc@1: 62.5000 (67.4000)  Acc@5: 91.6667 (90.5000)  time: 0.0906  data: 0.0011  max mem: 233\n",
            "Test: [Task 8] Total time: 0:00:04 (0.1016 s / it)\n",
            "* Acc@1 67.400 Acc@5 90.500 loss 1.315\n",
            "Test: [Task 9]  [ 0/42]  eta: 0:00:30  Loss: 1.1944 (1.1944)  Acc@1: 66.6667 (66.6667)  Acc@5: 100.0000 (100.0000)  time: 0.7242  data: 0.5770  max mem: 233\n",
            "Test: [Task 9]  [10/42]  eta: 0:00:04  Loss: 1.3323 (1.3202)  Acc@1: 62.5000 (63.2576)  Acc@5: 95.8333 (95.8333)  time: 0.1495  data: 0.0533  max mem: 233\n",
            "Test: [Task 9]  [20/42]  eta: 0:00:02  Loss: 1.3316 (1.2813)  Acc@1: 62.5000 (66.8651)  Acc@5: 95.8333 (95.0397)  time: 0.0919  data: 0.0006  max mem: 233\n",
            "Test: [Task 9]  [30/42]  eta: 0:00:01  Loss: 1.3316 (1.3325)  Acc@1: 62.5000 (65.8602)  Acc@5: 91.6667 (93.8172)  time: 0.0917  data: 0.0004  max mem: 233\n",
            "Test: [Task 9]  [40/42]  eta: 0:00:00  Loss: 1.2103 (1.2841)  Acc@1: 70.8333 (67.6829)  Acc@5: 91.6667 (93.6992)  time: 0.0916  data: 0.0004  max mem: 233\n",
            "Test: [Task 9]  [41/42]  eta: 0:00:00  Loss: 1.1468 (1.2719)  Acc@1: 70.8333 (67.9000)  Acc@5: 91.6667 (93.8000)  time: 0.0899  data: 0.0004  max mem: 233\n",
            "Test: [Task 9] Total time: 0:00:04 (0.1084 s / it)\n",
            "* Acc@1 67.900 Acc@5 93.800 loss 1.272\n",
            "Test: [Task 10]  [ 0/42]  eta: 0:00:17  Loss: 1.8132 (1.8132)  Acc@1: 45.8333 (45.8333)  Acc@5: 91.6667 (91.6667)  time: 0.4114  data: 0.3326  max mem: 233\n",
            "Test: [Task 10]  [10/42]  eta: 0:00:03  Loss: 1.7827 (1.7780)  Acc@1: 45.8333 (45.4545)  Acc@5: 91.6667 (91.2879)  time: 0.1209  data: 0.0310  max mem: 233\n",
            "Test: [Task 10]  [20/42]  eta: 0:00:02  Loss: 1.7827 (1.7534)  Acc@1: 45.8333 (46.2302)  Acc@5: 91.6667 (91.4683)  time: 0.0913  data: 0.0005  max mem: 233\n",
            "Test: [Task 10]  [30/42]  eta: 0:00:01  Loss: 1.7878 (1.7526)  Acc@1: 45.8333 (47.0430)  Acc@5: 87.5000 (90.8602)  time: 0.0909  data: 0.0003  max mem: 233\n",
            "Test: [Task 10]  [40/42]  eta: 0:00:00  Loss: 1.7611 (1.7651)  Acc@1: 45.8333 (45.5285)  Acc@5: 87.5000 (90.4472)  time: 0.0911  data: 0.0002  max mem: 233\n",
            "Test: [Task 10]  [41/42]  eta: 0:00:00  Loss: 1.7396 (1.7585)  Acc@1: 45.8333 (45.7000)  Acc@5: 87.5000 (90.4000)  time: 0.0895  data: 0.0002  max mem: 233\n",
            "Test: [Task 10] Total time: 0:00:04 (0.1001 s / it)\n",
            "* Acc@1 45.700 Acc@5 90.400 loss 1.759\n",
            "[Average accuracy till task10]\tAcc@1: 67.2500\tAcc@5: 92.4200\tLoss: 1.2477\tForgetting: 4.1222\tBackward: 1.2444\n",
            "Total training time: 0:14:52\n",
            "[rank0]:[W1008 14:21:56.711413361 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
          ]
        }
      ],
      "source": [
        "!torchrun --nproc_per_node=1 main.py cifar100_hideprompt_5e --original_model vit_small_patch16_224.dino --model vit_small_patch16_224.dino --batch-size 24 --data-path ./datasets/ --output_dir ./output/cifar100_full_dino_5epoch_100pct --epochs 10 --crct_epochs 10 --sched constant --seed 20 --train_inference_task_only --lr 0.0005 --pct 1.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "902dd0fc",
      "metadata": {
        "id": "902dd0fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed5be923-964e-4c9d-e4b2-18bdbb25e21f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(subparser_name='cifar100_hideprompt_5e', pct=0.1, batch_size=24, epochs=10, original_model='vit_small_patch16_224.dino', model='vit_small_patch16_224.dino', input_size=224, pretrained=True, drop=0.0, drop_path=0.0, opt='adam', opt_eps=1e-08, opt_betas=(0.9, 0.999), clip_grad=1.0, momentum=0.9, weight_decay=0.0, reinit_optimizer=True, sched='step', lr=0.03, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=0, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, unscale_lr=True, color_jitter=None, aa=None, smoothing=0.1, train_interpolation='bicubic', reprob=0.0, remode='pixel', recount=1, data_path='./datasets/', dataset='Split-CIFAR100', shuffle=False, output_dir='./output/cifar100_full_dino_5epoch_final_10pct', device='cuda', seed=20, eval=False, num_workers=4, pin_mem=True, world_size=1, dist_url='env://', num_tasks=10, train_mask=True, task_inc=False, use_g_prompt=False, g_prompt_length=5, g_prompt_layer_idx=[], use_prefix_tune_for_g_prompt=False, use_e_prompt=True, e_prompt_layer_idx=[0, 1, 2, 3, 4], use_prefix_tune_for_e_prompt=True, larger_prompt_lr=True, prompt_pool=True, size=10, length=5, top_k=1, initializer='uniform', prompt_key=False, prompt_key_init='uniform', use_prompt_mask=True, mask_first_epoch=False, shared_prompt_pool=True, shared_prompt_key=False, batchwise_prompt=False, embedding_key='cls', predefined_key='', pull_constraint=True, pull_constraint_coeff=1.0, same_key_value=False, global_pool='token', head_type='token', freeze=['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed'], crct_epochs=10, train_inference_task_only=False, original_model_mlp_structure=[2], ca_lr=0.005, milestones=[10], trained_original_model='./output/cifar100_full_dino_5epoch_10pct', prompt_momentum=0.1, reg=0.1, not_train_ca=False, ca_epochs=30, ca_storage_efficient_method='multi-centroid', n_centroids=10, print_freq=10, config='cifar100_hideprompt_5e')\n",
            "| distributed init (rank 0): env://\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "[rank0]:[W1008 14:29:26.699095975 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/helpers.py:7: FutureWarning: Importing from timm.models.helpers is deprecated, please import via timm.models\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:886: UserWarning: Overwriting vit_tiny_patch16_224 in registry with vits.hide_prompt_vision_transformer.vit_tiny_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:895: UserWarning: Overwriting vit_tiny_patch16_384 in registry with vits.hide_prompt_vision_transformer.vit_tiny_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:904: UserWarning: Overwriting vit_small_patch32_224 in registry with vits.hide_prompt_vision_transformer.vit_small_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:913: UserWarning: Overwriting vit_small_patch32_384 in registry with vits.hide_prompt_vision_transformer.vit_small_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:922: UserWarning: Overwriting vit_small_patch16_224 in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:932: UserWarning: Overwriting vit_small_patch16_384 in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:942: UserWarning: Overwriting vit_base_patch32_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:952: UserWarning: Overwriting vit_base_patch32_384 in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:962: UserWarning: Overwriting vit_base_patch16_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:972: UserWarning: Overwriting vit_base_patch16_384 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:982: UserWarning: Overwriting vit_base_patch8_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:992: UserWarning: Overwriting vit_large_patch32_224 in registry with vits.hide_prompt_vision_transformer.vit_large_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1001: UserWarning: Overwriting vit_large_patch32_384 in registry with vits.hide_prompt_vision_transformer.vit_large_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1011: UserWarning: Overwriting vit_large_patch16_224 in registry with vits.hide_prompt_vision_transformer.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1021: UserWarning: Overwriting vit_large_patch16_384 in registry with vits.hide_prompt_vision_transformer.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1031: UserWarning: Overwriting vit_large_patch14_224 in registry with vits.hide_prompt_vision_transformer.vit_large_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1040: UserWarning: Overwriting vit_huge_patch14_224 in registry with vits.hide_prompt_vision_transformer.vit_huge_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1049: UserWarning: Overwriting vit_giant_patch14_224 in registry with vits.hide_prompt_vision_transformer.vit_giant_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1058: UserWarning: Overwriting vit_gigantic_patch14_224 in registry with vits.hide_prompt_vision_transformer.vit_gigantic_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1067: UserWarning: Overwriting vit_tiny_patch16_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_tiny_patch16_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1078: UserWarning: Overwriting vit_small_patch32_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_small_patch32_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1089: UserWarning: Overwriting vit_small_patch16_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1100: UserWarning: Overwriting vit_base_patch32_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1111: UserWarning: Overwriting vit_base_patch16_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1122: UserWarning: Overwriting vit_base_patch8_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_base_patch8_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1133: UserWarning: Overwriting vit_large_patch32_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_large_patch32_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1144: UserWarning: Overwriting vit_large_patch16_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_large_patch16_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1155: UserWarning: Overwriting vit_huge_patch14_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_huge_patch14_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1166: UserWarning: Overwriting vit_base_patch16_224_sam in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_sam. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1175: UserWarning: Overwriting vit_base_patch32_224_sam in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_224_sam. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1184: UserWarning: Overwriting vit_small_patch16_224_dino in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_224_dino. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1193: UserWarning: Overwriting vit_small_patch8_224_dino in registry with vits.hide_prompt_vision_transformer.vit_small_patch8_224_dino. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1211: UserWarning: Overwriting vit_base_patch8_224_dino in registry with vits.hide_prompt_vision_transformer.vit_base_patch8_224_dino. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1220: UserWarning: Overwriting vit_base_patch16_224_miil_in21k in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_miil_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1230: UserWarning: Overwriting vit_base_patch16_224_miil in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_miil. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1242: UserWarning: Overwriting vit_base_patch32_plus_256 in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_plus_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1251: UserWarning: Overwriting vit_base_patch16_plus_240 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_plus_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1260: UserWarning: Overwriting vit_base_patch16_rpn_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_rpn_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1271: UserWarning: Overwriting vit_small_patch16_36x1_224 in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_36x1_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1282: UserWarning: Overwriting vit_small_patch16_18x2_224 in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1294: UserWarning: Overwriting vit_base_patch16_18x2_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1331: UserWarning: Overwriting vit_base_patch16_224_dino in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_dino. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "Original train size:  50000\n",
            "Sampled train size:  5000\n",
            "Original train size:  50000\n",
            "Sampled train size:  5000\n",
            "100\n",
            "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, 13, 14, 15, 16, 17, 18, 19], [20, 21, 22, 23, 24, 25, 26, 27, 28, 29], [30, 31, 32, 33, 34, 35, 36, 37, 38, 39], [40, 41, 42, 43, 44, 45, 46, 47, 48, 49], [50, 51, 52, 53, 54, 55, 56, 57, 58, 59], [60, 61, 62, 63, 64, 65, 66, 67, 68, 69], [70, 71, 72, 73, 74, 75, 76, 77, 78, 79], [80, 81, 82, 83, 84, 85, 86, 87, 88, 89], [90, 91, 92, 93, 94, 95, 96, 97, 98, 99]]\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Creating original model: vit_small_patch16_224.dino\n",
            "[Sequential(\n",
            "  (0): Linear(in_features=384, out_features=768, bias=True)\n",
            "  (1): GELU(approximate='none')\n",
            "  (2): Dropout(p=0.0, inplace=False)\n",
            "), Sequential(\n",
            "  (0): Linear(in_features=768, out_features=384, bias=True)\n",
            "  (1): Dropout(p=0.0, inplace=False)\n",
            ")]\n",
            "Creating model: vit_small_patch16_224.dino\n",
            "Namespace(subparser_name='cifar100_hideprompt_5e', pct=0.1, batch_size=24, epochs=10, original_model='vit_small_patch16_224.dino', model='vit_small_patch16_224.dino', input_size=224, pretrained=True, drop=0.0, drop_path=0.0, opt='adam', opt_eps=1e-08, opt_betas=(0.9, 0.999), clip_grad=1.0, momentum=0.9, weight_decay=0.0, reinit_optimizer=True, sched='step', lr=0.03, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=0, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, unscale_lr=True, color_jitter=None, aa=None, smoothing=0.1, train_interpolation='bicubic', reprob=0.0, remode='pixel', recount=1, data_path='./datasets/', dataset='Split-CIFAR100', shuffle=False, output_dir='./output/cifar100_full_dino_5epoch_final_10pct', device='cuda', seed=20, eval=False, num_workers=4, pin_mem=True, world_size=1, dist_url='env://', num_tasks=10, train_mask=True, task_inc=False, use_g_prompt=False, g_prompt_length=5, g_prompt_layer_idx=[], use_prefix_tune_for_g_prompt=False, use_e_prompt=True, e_prompt_layer_idx=[0, 1, 2, 3, 4], use_prefix_tune_for_e_prompt=True, larger_prompt_lr=True, prompt_pool=True, size=10, length=5, top_k=1, initializer='uniform', prompt_key=False, prompt_key_init='uniform', use_prompt_mask=True, mask_first_epoch=False, shared_prompt_pool=True, shared_prompt_key=False, batchwise_prompt=False, embedding_key='cls', predefined_key='', pull_constraint=True, pull_constraint_coeff=1.0, same_key_value=False, global_pool='token', head_type='token', freeze=['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed'], crct_epochs=10, train_inference_task_only=False, original_model_mlp_structure=[2], ca_lr=0.005, milestones=[10], trained_original_model='./output/cifar100_full_dino_5epoch_10pct', prompt_momentum=0.1, reg=0.1, not_train_ca=False, ca_epochs=30, ca_storage_efficient_method='multi-centroid', n_centroids=10, print_freq=10, config='cifar100_hideprompt_5e', rank=0, gpu=0, distributed=True, dist_backend='nccl', nb_classes=100)\n",
            "number of params: 230500\n",
            "Start training for 10 epochs\n",
            "Loading checkpoint from: ./output/cifar100_full_dino_5epoch_10pct/checkpoint/task1_checkpoint.pth\n",
            "[rank0]:[W1008 14:29:37.398338277 reducer.cpp:1457] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/engines/hide_promtp_wtp_and_tap_engine.py:579: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  loss = torch.nn.functional.cross_entropy(sim, torch.range(0, sim.shape[0] - 1).long().to(device))\n",
            "Train: Epoch[ 1/10]  [ 0/23]  eta: 0:01:01  Lr: 0.002812  Loss: 3.3526  Acc@1: 0.0000 (0.0000)  Acc@5: 58.3333 (58.3333)  time: 2.6729  data: 1.4663  max mem: 1368\n",
            "Train: Epoch[ 1/10]  [10/23]  eta: 0:00:05  Lr: 0.002812  Loss: 2.5234  Acc@1: 12.5000 (12.5000)  Acc@5: 62.5000 (58.3333)  time: 0.4558  data: 0.1341  max mem: 1370\n",
            "Train: Epoch[ 1/10]  [20/23]  eta: 0:00:01  Lr: 0.002812  Loss: 2.5889  Acc@1: 16.6667 (16.0714)  Acc@5: 62.5000 (60.5159)  time: 0.2348  data: 0.0008  max mem: 1370\n",
            "Train: Epoch[ 1/10]  [22/23]  eta: 0:00:00  Lr: 0.002812  Loss: 2.2597  Acc@1: 16.6667 (16.0448)  Acc@5: 62.5000 (60.2612)  time: 0.2321  data: 0.0007  max mem: 1370\n",
            "Train: Epoch[ 1/10] Total time: 0:00:07 (0.3405 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.002812  Loss: 2.2597  Acc@1: 16.6667 (16.0448)  Acc@5: 62.5000 (60.2612)\n",
            "Train: Epoch[ 2/10]  [ 0/23]  eta: 0:00:10  Lr: 0.002812  Loss: 2.2210  Acc@1: 25.0000 (25.0000)  Acc@5: 87.5000 (87.5000)  time: 0.4748  data: 0.2349  max mem: 1370\n",
            "Train: Epoch[ 2/10]  [10/23]  eta: 0:00:03  Lr: 0.002812  Loss: 1.8084  Acc@1: 25.0000 (25.3788)  Acc@5: 79.1667 (80.3030)  time: 0.2592  data: 0.0216  max mem: 1370\n",
            "Train: Epoch[ 2/10]  [20/23]  eta: 0:00:00  Lr: 0.002812  Loss: 1.5670  Acc@1: 37.5000 (35.7143)  Acc@5: 83.3333 (82.5397)  time: 0.2373  data: 0.0002  max mem: 1370\n",
            "Train: Epoch[ 2/10]  [22/23]  eta: 0:00:00  Lr: 0.002812  Loss: 2.2539  Acc@1: 37.5000 (35.4478)  Acc@5: 83.3333 (82.0896)  time: 0.2298  data: 0.0002  max mem: 1370\n",
            "Train: Epoch[ 2/10] Total time: 0:00:05 (0.2443 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 2.2539  Acc@1: 37.5000 (35.4478)  Acc@5: 83.3333 (82.0896)\n",
            "Train: Epoch[ 3/10]  [ 0/23]  eta: 0:00:15  Lr: 0.002812  Loss: 1.7544  Acc@1: 45.8333 (45.8333)  Acc@5: 83.3333 (83.3333)  time: 0.6555  data: 0.4219  max mem: 1370\n",
            "Train: Epoch[ 3/10]  [10/23]  eta: 0:00:03  Lr: 0.002812  Loss: 1.3077  Acc@1: 50.0000 (49.2424)  Acc@5: 87.5000 (90.1515)  time: 0.2764  data: 0.0387  max mem: 1370\n",
            "Train: Epoch[ 3/10]  [20/23]  eta: 0:00:00  Lr: 0.002812  Loss: 1.3304  Acc@1: 54.1667 (54.3651)  Acc@5: 91.6667 (90.8730)  time: 0.2398  data: 0.0004  max mem: 1370\n",
            "Train: Epoch[ 3/10]  [22/23]  eta: 0:00:00  Lr: 0.002812  Loss: 1.5279  Acc@1: 54.1667 (54.2910)  Acc@5: 91.6667 (91.0448)  time: 0.2328  data: 0.0004  max mem: 1370\n",
            "Train: Epoch[ 3/10] Total time: 0:00:05 (0.2544 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.5279  Acc@1: 54.1667 (54.2910)  Acc@5: 91.6667 (91.0448)\n",
            "Train: Epoch[ 4/10]  [ 0/23]  eta: 0:00:15  Lr: 0.002812  Loss: 1.1222  Acc@1: 62.5000 (62.5000)  Acc@5: 91.6667 (91.6667)  time: 0.6691  data: 0.4331  max mem: 1370\n",
            "Train: Epoch[ 4/10]  [10/23]  eta: 0:00:03  Lr: 0.002812  Loss: 1.1628  Acc@1: 70.8333 (65.5303)  Acc@5: 95.8333 (96.9697)  time: 0.2802  data: 0.0397  max mem: 1370\n",
            "Train: Epoch[ 4/10]  [20/23]  eta: 0:00:00  Lr: 0.002812  Loss: 0.8374  Acc@1: 66.6667 (64.8810)  Acc@5: 95.8333 (95.8333)  time: 0.2428  data: 0.0003  max mem: 1370\n",
            "Train: Epoch[ 4/10]  [22/23]  eta: 0:00:00  Lr: 0.002812  Loss: 1.1000  Acc@1: 66.6667 (65.1119)  Acc@5: 95.8333 (95.7090)  time: 0.2357  data: 0.0002  max mem: 1370\n",
            "Train: Epoch[ 4/10] Total time: 0:00:05 (0.2584 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.1000  Acc@1: 66.6667 (65.1119)  Acc@5: 95.8333 (95.7090)\n",
            "Train: Epoch[ 5/10]  [ 0/23]  eta: 0:00:14  Lr: 0.002812  Loss: 0.8152  Acc@1: 70.8333 (70.8333)  Acc@5: 100.0000 (100.0000)  time: 0.6464  data: 0.4040  max mem: 1370\n",
            "Train: Epoch[ 5/10]  [10/23]  eta: 0:00:03  Lr: 0.002812  Loss: 1.0390  Acc@1: 75.0000 (72.3485)  Acc@5: 95.8333 (95.8333)  time: 0.2823  data: 0.0371  max mem: 1370\n",
            "Train: Epoch[ 5/10]  [20/23]  eta: 0:00:00  Lr: 0.002812  Loss: 0.9919  Acc@1: 70.8333 (72.2222)  Acc@5: 95.8333 (95.4365)  time: 0.2472  data: 0.0004  max mem: 1370\n",
            "Train: Epoch[ 5/10]  [22/23]  eta: 0:00:00  Lr: 0.002812  Loss: 0.5659  Acc@1: 70.8333 (73.1343)  Acc@5: 95.8333 (95.5224)  time: 0.2401  data: 0.0003  max mem: 1370\n",
            "Train: Epoch[ 5/10] Total time: 0:00:06 (0.2628 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.5659  Acc@1: 70.8333 (73.1343)  Acc@5: 95.8333 (95.5224)\n",
            "Train: Epoch[ 6/10]  [ 0/23]  eta: 0:00:20  Lr: 0.002812  Loss: 0.8625  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.8333)  time: 0.8706  data: 0.5851  max mem: 1370\n",
            "Train: Epoch[ 6/10]  [10/23]  eta: 0:00:03  Lr: 0.002812  Loss: 0.8331  Acc@1: 75.0000 (76.5152)  Acc@5: 100.0000 (98.4848)  time: 0.3052  data: 0.0536  max mem: 1370\n",
            "Train: Epoch[ 6/10]  [20/23]  eta: 0:00:00  Lr: 0.002812  Loss: 0.7379  Acc@1: 75.0000 (77.9762)  Acc@5: 100.0000 (97.6190)  time: 0.2511  data: 0.0003  max mem: 1370\n",
            "Train: Epoch[ 6/10]  [22/23]  eta: 0:00:00  Lr: 0.002812  Loss: 0.4226  Acc@1: 75.0000 (77.7985)  Acc@5: 100.0000 (97.3881)  time: 0.2438  data: 0.0002  max mem: 1370\n",
            "Train: Epoch[ 6/10] Total time: 0:00:06 (0.2744 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.4226  Acc@1: 75.0000 (77.7985)  Acc@5: 100.0000 (97.3881)\n",
            "Train: Epoch[ 7/10]  [ 0/23]  eta: 0:00:16  Lr: 0.002812  Loss: 0.5736  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.7386  data: 0.4871  max mem: 1370\n",
            "Train: Epoch[ 7/10]  [10/23]  eta: 0:00:03  Lr: 0.002812  Loss: 0.6378  Acc@1: 75.0000 (78.4091)  Acc@5: 100.0000 (98.1061)  time: 0.2923  data: 0.0447  max mem: 1370\n",
            "Train: Epoch[ 7/10]  [20/23]  eta: 0:00:00  Lr: 0.002812  Loss: 0.5757  Acc@1: 75.0000 (77.9762)  Acc@5: 95.8333 (97.0238)  time: 0.2501  data: 0.0005  max mem: 1370\n",
            "Train: Epoch[ 7/10]  [22/23]  eta: 0:00:00  Lr: 0.002812  Loss: 0.6968  Acc@1: 79.1667 (77.9851)  Acc@5: 95.8333 (97.2015)  time: 0.2427  data: 0.0005  max mem: 1370\n",
            "Train: Epoch[ 7/10] Total time: 0:00:06 (0.2702 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.6968  Acc@1: 79.1667 (77.9851)  Acc@5: 95.8333 (97.2015)\n",
            "Train: Epoch[ 8/10]  [ 0/23]  eta: 0:00:23  Lr: 0.002812  Loss: 0.5767  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 1.0083  data: 0.7493  max mem: 1370\n",
            "Train: Epoch[ 8/10]  [10/23]  eta: 0:00:04  Lr: 0.002812  Loss: 0.4877  Acc@1: 83.3333 (81.4394)  Acc@5: 95.8333 (96.9697)  time: 0.3230  data: 0.0687  max mem: 1370\n",
            "Train: Epoch[ 8/10]  [20/23]  eta: 0:00:00  Lr: 0.002812  Loss: 0.4433  Acc@1: 83.3333 (83.5317)  Acc@5: 95.8333 (97.4206)  time: 0.2551  data: 0.0004  max mem: 1370\n",
            "Train: Epoch[ 8/10]  [22/23]  eta: 0:00:00  Lr: 0.002812  Loss: 0.5435  Acc@1: 83.3333 (83.5821)  Acc@5: 95.8333 (97.3881)  time: 0.2472  data: 0.0004  max mem: 1370\n",
            "Train: Epoch[ 8/10] Total time: 0:00:06 (0.2843 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.5435  Acc@1: 83.3333 (83.5821)  Acc@5: 95.8333 (97.3881)\n",
            "Train: Epoch[ 9/10]  [ 0/23]  eta: 0:00:12  Lr: 0.002812  Loss: 0.4117  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.5373  data: 0.2869  max mem: 1370\n",
            "Train: Epoch[ 9/10]  [10/23]  eta: 0:00:03  Lr: 0.002812  Loss: 0.6104  Acc@1: 83.3333 (84.4697)  Acc@5: 100.0000 (98.1061)  time: 0.2836  data: 0.0264  max mem: 1370\n",
            "Train: Epoch[ 9/10]  [20/23]  eta: 0:00:00  Lr: 0.002812  Loss: 0.4689  Acc@1: 83.3333 (83.5317)  Acc@5: 100.0000 (98.4127)  time: 0.2593  data: 0.0003  max mem: 1370\n",
            "Train: Epoch[ 9/10]  [22/23]  eta: 0:00:00  Lr: 0.002812  Loss: 0.2168  Acc@1: 83.3333 (83.9552)  Acc@5: 100.0000 (98.5075)  time: 0.2517  data: 0.0003  max mem: 1370\n",
            "Train: Epoch[ 9/10] Total time: 0:00:06 (0.2675 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.2168  Acc@1: 83.3333 (83.9552)  Acc@5: 100.0000 (98.5075)\n",
            "Train: Epoch[10/10]  [ 0/23]  eta: 0:00:16  Lr: 0.002812  Loss: 0.5395  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.7244  data: 0.4156  max mem: 1370\n",
            "Train: Epoch[10/10]  [10/23]  eta: 0:00:03  Lr: 0.002812  Loss: 0.4933  Acc@1: 83.3333 (84.4697)  Acc@5: 100.0000 (98.4848)  time: 0.3024  data: 0.0389  max mem: 1370\n",
            "Train: Epoch[10/10]  [20/23]  eta: 0:00:00  Lr: 0.002812  Loss: 0.1869  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (99.0079)  time: 0.2586  data: 0.0008  max mem: 1370\n",
            "Train: Epoch[10/10]  [22/23]  eta: 0:00:00  Lr: 0.002812  Loss: 0.6031  Acc@1: 87.5000 (87.1269)  Acc@5: 100.0000 (98.8806)  time: 0.2491  data: 0.0006  max mem: 1370\n",
            "Train: Epoch[10/10] Total time: 0:00:06 (0.2754 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.6031  Acc@1: 87.5000 (87.1269)  Acc@5: 100.0000 (98.8806)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/23]  eta: 0:00:14  Loss: 1.7767 (1.7767)  Acc@1: 54.1667 (54.1667)  Acc@5: 83.3333 (83.3333)  Acc@task: 100.0000 (100.0000)  time: 0.6266  data: 0.4690  max mem: 1370\n",
            "Test: [Task 1]  [10/23]  eta: 0:00:02  Loss: 1.8770 (1.8325)  Acc@1: 54.1667 (57.1970)  Acc@5: 79.1667 (79.5455)  Acc@task: 100.0000 (100.0000)  time: 0.2093  data: 0.0431  max mem: 1370\n",
            "Test: [Task 1]  [20/23]  eta: 0:00:00  Loss: 1.7829 (1.7978)  Acc@1: 58.3333 (59.5238)  Acc@5: 79.1667 (79.3651)  Acc@task: 100.0000 (100.0000)  time: 0.1673  data: 0.0007  max mem: 1370\n",
            "Test: [Task 1]  [22/23]  eta: 0:00:00  Loss: 1.7774 (1.7868)  Acc@1: 62.5000 (59.8881)  Acc@5: 79.1667 (79.2910)  Acc@task: 100.0000 (100.0000)  time: 0.1621  data: 0.0006  max mem: 1370\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1882 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@task 100.000 Acc@1 59.888 Acc@5 79.291 loss 1.787\n",
            "[Average accuracy till task1]\tAcc@task: 100.0000\tAcc@1: 59.8881\tAcc@5: 79.2910\tLoss: 1.7868\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:33  Loss: 1.1381 (1.1381)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  Acc@task: 100.0000 (100.0000)  time: 0.7927  data: 0.5763  max mem: 1370\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:07  Loss: 1.1381 (1.0913)  Acc@1: 79.1667 (78.7879)  Acc@5: 91.6667 (90.1515)  Acc@task: 100.0000 (100.0000)  time: 0.2231  data: 0.0529  max mem: 1370\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 1.1495 (1.1032)  Acc@1: 75.0000 (76.1905)  Acc@5: 91.6667 (90.0794)  Acc@task: 100.0000 (100.0000)  time: 0.1665  data: 0.0004  max mem: 1370\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 1.0479 (1.0730)  Acc@1: 75.0000 (76.2097)  Acc@5: 91.6667 (90.3226)  Acc@task: 100.0000 (100.0000)  time: 0.1678  data: 0.0003  max mem: 1370\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.9549 (1.0462)  Acc@1: 75.0000 (76.8293)  Acc@5: 91.6667 (91.1585)  Acc@task: 100.0000 (100.0000)  time: 0.1690  data: 0.0002  max mem: 1370\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.9490 (1.0353)  Acc@1: 75.0000 (77.1000)  Acc@5: 91.6667 (91.3000)  Acc@task: 100.0000 (100.0000)  time: 0.1663  data: 0.0002  max mem: 1370\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1837 s / it)\n",
            "* Acc@task 100.000 Acc@1 77.100 Acc@5 91.300 loss 1.035\n",
            "[Average accuracy till task1]\tAcc@task: 100.0000\tAcc@1: 77.1000\tAcc@5: 91.3000\tLoss: 1.0353\n",
            "Test: [Task 1]  [ 0/23]  eta: 0:00:12  Loss: 1.5724 (1.5724)  Acc@1: 58.3333 (58.3333)  Acc@5: 87.5000 (87.5000)  Acc@task: 100.0000 (100.0000)  time: 0.5540  data: 0.3902  max mem: 1370\n",
            "Test: [Task 1]  [10/23]  eta: 0:00:02  Loss: 1.7432 (1.7488)  Acc@1: 58.3333 (59.8485)  Acc@5: 79.1667 (78.7879)  Acc@task: 100.0000 (100.0000)  time: 0.2046  data: 0.0359  max mem: 1370\n",
            "Test: [Task 1]  [20/23]  eta: 0:00:00  Loss: 1.7370 (1.6669)  Acc@1: 58.3333 (60.9127)  Acc@5: 79.1667 (81.5476)  Acc@task: 100.0000 (100.0000)  time: 0.1698  data: 0.0005  max mem: 1370\n",
            "Test: [Task 1]  [22/23]  eta: 0:00:00  Loss: 1.7370 (1.6794)  Acc@1: 58.3333 (60.2612)  Acc@5: 79.1667 (80.9702)  Acc@task: 100.0000 (100.0000)  time: 0.1652  data: 0.0004  max mem: 1370\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1886 s / it)\n",
            "* Acc@task 100.000 Acc@1 60.261 Acc@5 80.970 loss 1.679\n",
            "[Average accuracy till task1]\tAcc@task: 100.0000\tAcc@1: 60.2612\tAcc@5: 80.9702\tLoss: 1.6794\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:36  Loss: 1.1381 (1.1381)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  Acc@task: 100.0000 (100.0000)  time: 0.8608  data: 0.6592  max mem: 1370\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:07  Loss: 1.1381 (1.0913)  Acc@1: 79.1667 (78.7879)  Acc@5: 91.6667 (90.1515)  Acc@task: 100.0000 (100.0000)  time: 0.2344  data: 0.0611  max mem: 1370\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 1.1495 (1.1032)  Acc@1: 75.0000 (76.1905)  Acc@5: 91.6667 (90.0794)  Acc@task: 100.0000 (100.0000)  time: 0.1716  data: 0.0008  max mem: 1370\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 1.0479 (1.0730)  Acc@1: 75.0000 (76.2097)  Acc@5: 91.6667 (90.3226)  Acc@task: 100.0000 (100.0000)  time: 0.1723  data: 0.0003  max mem: 1370\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.9549 (1.0462)  Acc@1: 75.0000 (76.8293)  Acc@5: 91.6667 (91.1585)  Acc@task: 100.0000 (100.0000)  time: 0.1732  data: 0.0003  max mem: 1370\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.9490 (1.0353)  Acc@1: 75.0000 (77.1000)  Acc@5: 91.6667 (91.3000)  Acc@task: 100.0000 (100.0000)  time: 0.1702  data: 0.0003  max mem: 1370\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1894 s / it)\n",
            "* Acc@task 100.000 Acc@1 77.100 Acc@5 91.300 loss 1.035\n",
            "[Average accuracy till task1]\tAcc@task: 100.0000\tAcc@1: 77.1000\tAcc@5: 91.3000\tLoss: 1.0353\n",
            "Loading checkpoint from: ./output/cifar100_full_dino_5epoch_10pct/checkpoint/task2_checkpoint.pth\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/engines/hide_promtp_wtp_and_tap_engine.py:574: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  loss = torch.nn.functional.cross_entropy(sim, torch.range(0, sim.shape[0] - 1).long().to(device))\n",
            "Train: Epoch[ 1/10]  [ 0/20]  eta: 0:00:14  Lr: 0.002812  Loss: 4.6146  Acc@1: 12.5000 (12.5000)  Acc@5: 29.1667 (29.1667)  time: 0.7258  data: 0.3437  max mem: 1373\n",
            "Train: Epoch[ 1/10]  [10/20]  eta: 0:00:03  Lr: 0.002812  Loss: 2.8360  Acc@1: 12.5000 (12.1212)  Acc@5: 41.6667 (45.4545)  time: 0.3113  data: 0.0316  max mem: 1373\n",
            "Train: Epoch[ 1/10]  [19/20]  eta: 0:00:00  Lr: 0.002812  Loss: 2.5047  Acc@1: 12.5000 (14.1962)  Acc@5: 50.0000 (51.7745)  time: 0.2939  data: 0.0175  max mem: 1373\n",
            "Train: Epoch[ 1/10] Total time: 0:00:06 (0.3062 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 2.5047  Acc@1: 12.5000 (14.1962)  Acc@5: 50.0000 (51.7745)\n",
            "Train: Epoch[ 2/10]  [ 0/20]  eta: 0:00:20  Lr: 0.002812  Loss: 2.7036  Acc@1: 8.3333 (8.3333)  Acc@5: 66.6667 (66.6667)  time: 1.0040  data: 0.5314  max mem: 1375\n",
            "Train: Epoch[ 2/10]  [10/20]  eta: 0:00:03  Lr: 0.002812  Loss: 2.0107  Acc@1: 29.1667 (26.8939)  Acc@5: 70.8333 (73.8636)  time: 0.3498  data: 0.0589  max mem: 1375\n",
            "Train: Epoch[ 2/10]  [19/20]  eta: 0:00:00  Lr: 0.002812  Loss: 2.0856  Acc@1: 33.3333 (32.7766)  Acc@5: 79.1667 (77.8706)  time: 0.3172  data: 0.0325  max mem: 1375\n",
            "Train: Epoch[ 2/10] Total time: 0:00:06 (0.3215 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 2.0856  Acc@1: 33.3333 (32.7766)  Acc@5: 79.1667 (77.8706)\n",
            "Train: Epoch[ 3/10]  [ 0/20]  eta: 0:00:11  Lr: 0.002812  Loss: 1.5415  Acc@1: 50.0000 (50.0000)  Acc@5: 87.5000 (87.5000)  time: 0.5797  data: 0.3070  max mem: 1375\n",
            "Train: Epoch[ 3/10]  [10/20]  eta: 0:00:03  Lr: 0.002812  Loss: 1.9186  Acc@1: 50.0000 (49.6212)  Acc@5: 91.6667 (87.8788)  time: 0.3090  data: 0.0282  max mem: 1375\n",
            "Train: Epoch[ 3/10]  [19/20]  eta: 0:00:00  Lr: 0.002812  Loss: 1.8672  Acc@1: 50.0000 (52.6096)  Acc@5: 91.6667 (88.9353)  time: 0.2967  data: 0.0156  max mem: 1375\n",
            "Train: Epoch[ 3/10] Total time: 0:00:06 (0.3023 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.8672  Acc@1: 50.0000 (52.6096)  Acc@5: 91.6667 (88.9353)\n",
            "Train: Epoch[ 4/10]  [ 0/20]  eta: 0:00:22  Lr: 0.002812  Loss: 1.0433  Acc@1: 62.5000 (62.5000)  Acc@5: 100.0000 (100.0000)  time: 1.1134  data: 0.7974  max mem: 1375\n",
            "Train: Epoch[ 4/10]  [10/20]  eta: 0:00:03  Lr: 0.002812  Loss: 1.1050  Acc@1: 62.5000 (59.0909)  Acc@5: 95.8333 (91.2879)  time: 0.3636  data: 0.0730  max mem: 1375\n",
            "Train: Epoch[ 4/10]  [19/20]  eta: 0:00:00  Lr: 0.002812  Loss: 1.2897  Acc@1: 62.5000 (62.0042)  Acc@5: 95.8333 (92.6931)  time: 0.3297  data: 0.0403  max mem: 1375\n",
            "Train: Epoch[ 4/10] Total time: 0:00:06 (0.3336 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.2897  Acc@1: 62.5000 (62.0042)  Acc@5: 95.8333 (92.6931)\n",
            "Train: Epoch[ 5/10]  [ 0/20]  eta: 0:00:12  Lr: 0.002812  Loss: 0.9670  Acc@1: 66.6667 (66.6667)  Acc@5: 100.0000 (100.0000)  time: 0.6167  data: 0.3189  max mem: 1375\n",
            "Train: Epoch[ 5/10]  [10/20]  eta: 0:00:03  Lr: 0.002812  Loss: 0.6750  Acc@1: 75.0000 (73.4849)  Acc@5: 95.8333 (97.3485)  time: 0.3218  data: 0.0297  max mem: 1375\n",
            "Train: Epoch[ 5/10]  [19/20]  eta: 0:00:00  Lr: 0.002812  Loss: 1.0858  Acc@1: 75.0000 (72.6514)  Acc@5: 95.8333 (96.2422)  time: 0.3086  data: 0.0164  max mem: 1375\n",
            "Train: Epoch[ 5/10] Total time: 0:00:06 (0.3169 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.0858  Acc@1: 75.0000 (72.6514)  Acc@5: 95.8333 (96.2422)\n",
            "Train: Epoch[ 6/10]  [ 0/20]  eta: 0:00:20  Lr: 0.002812  Loss: 0.7957  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  time: 1.0474  data: 0.7425  max mem: 1375\n",
            "Train: Epoch[ 6/10]  [10/20]  eta: 0:00:03  Lr: 0.002812  Loss: 0.8834  Acc@1: 70.8333 (72.3485)  Acc@5: 95.8333 (96.2121)  time: 0.3595  data: 0.0691  max mem: 1375\n",
            "Train: Epoch[ 6/10]  [19/20]  eta: 0:00:00  Lr: 0.002812  Loss: 0.6625  Acc@1: 75.0000 (75.9917)  Acc@5: 95.8333 (96.2422)  time: 0.3271  data: 0.0383  max mem: 1375\n",
            "Train: Epoch[ 6/10] Total time: 0:00:06 (0.3312 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.6625  Acc@1: 75.0000 (75.9917)  Acc@5: 95.8333 (96.2422)\n",
            "Train: Epoch[ 7/10]  [ 0/20]  eta: 0:00:13  Lr: 0.002812  Loss: 0.7858  Acc@1: 66.6667 (66.6667)  Acc@5: 100.0000 (100.0000)  time: 0.6645  data: 0.3795  max mem: 1375\n",
            "Train: Epoch[ 7/10]  [10/20]  eta: 0:00:03  Lr: 0.002812  Loss: 0.8233  Acc@1: 75.0000 (77.2727)  Acc@5: 95.8333 (97.3485)  time: 0.3203  data: 0.0348  max mem: 1375\n",
            "Train: Epoch[ 7/10]  [19/20]  eta: 0:00:00  Lr: 0.002812  Loss: 0.4724  Acc@1: 75.0000 (78.2881)  Acc@5: 95.8333 (97.4948)  time: 0.3030  data: 0.0192  max mem: 1375\n",
            "Train: Epoch[ 7/10] Total time: 0:00:06 (0.3069 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.4724  Acc@1: 75.0000 (78.2881)  Acc@5: 95.8333 (97.4948)\n",
            "Train: Epoch[ 8/10]  [ 0/20]  eta: 0:00:14  Lr: 0.002812  Loss: 0.7827  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  time: 0.7132  data: 0.4092  max mem: 1375\n",
            "Train: Epoch[ 8/10]  [10/20]  eta: 0:00:03  Lr: 0.002812  Loss: 0.4691  Acc@1: 83.3333 (82.1970)  Acc@5: 100.0000 (98.8636)  time: 0.3216  data: 0.0378  max mem: 1375\n",
            "Train: Epoch[ 8/10]  [19/20]  eta: 0:00:00  Lr: 0.002812  Loss: 0.9815  Acc@1: 83.3333 (81.0021)  Acc@5: 100.0000 (97.4948)  time: 0.3030  data: 0.0209  max mem: 1375\n",
            "Train: Epoch[ 8/10] Total time: 0:00:06 (0.3069 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.9815  Acc@1: 83.3333 (81.0021)  Acc@5: 100.0000 (97.4948)\n",
            "Train: Epoch[ 9/10]  [ 0/20]  eta: 0:00:14  Lr: 0.002812  Loss: 0.7097  Acc@1: 87.5000 (87.5000)  Acc@5: 91.6667 (91.6667)  time: 0.7055  data: 0.4235  max mem: 1375\n",
            "Train: Epoch[ 9/10]  [10/20]  eta: 0:00:03  Lr: 0.002812  Loss: 0.8618  Acc@1: 87.5000 (81.4394)  Acc@5: 100.0000 (97.7273)  time: 0.3192  data: 0.0387  max mem: 1375\n",
            "Train: Epoch[ 9/10]  [19/20]  eta: 0:00:00  Lr: 0.002812  Loss: 0.6141  Acc@1: 83.3333 (81.8372)  Acc@5: 100.0000 (98.3299)  time: 0.3001  data: 0.0214  max mem: 1375\n",
            "Train: Epoch[ 9/10] Total time: 0:00:06 (0.3044 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.6141  Acc@1: 83.3333 (81.8372)  Acc@5: 100.0000 (98.3299)\n",
            "Train: Epoch[10/10]  [ 0/20]  eta: 0:00:14  Lr: 0.002812  Loss: 0.6118  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  time: 0.7292  data: 0.4671  max mem: 1375\n",
            "Train: Epoch[10/10]  [10/20]  eta: 0:00:03  Lr: 0.002812  Loss: 0.2601  Acc@1: 83.3333 (83.7121)  Acc@5: 100.0000 (98.8636)  time: 0.3178  data: 0.0429  max mem: 1375\n",
            "Train: Epoch[10/10]  [19/20]  eta: 0:00:00  Lr: 0.002812  Loss: 0.5162  Acc@1: 83.3333 (84.3424)  Acc@5: 100.0000 (98.9562)  time: 0.2990  data: 0.0237  max mem: 1375\n",
            "Train: Epoch[10/10] Total time: 0:00:06 (0.3029 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.5162  Acc@1: 83.3333 (84.3424)  Acc@5: 100.0000 (98.9562)\n",
            "torch.Size([5, 2, 5, 6, 64])\n",
            "torch.Size([5, 2, 1, 5, 6, 64])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/23]  eta: 0:00:10  Loss: 1.4782 (1.4782)  Acc@1: 70.8333 (70.8333)  Acc@5: 87.5000 (87.5000)  Acc@task: 83.3333 (83.3333)  time: 0.4677  data: 0.3063  max mem: 1375\n",
            "Test: [Task 1]  [10/23]  eta: 0:00:02  Loss: 1.6257 (1.6340)  Acc@1: 62.5000 (64.3939)  Acc@5: 83.3333 (82.5758)  Acc@task: 83.3333 (83.7121)  time: 0.2046  data: 0.0324  max mem: 1375\n",
            "Test: [Task 1]  [20/23]  eta: 0:00:00  Loss: 1.6257 (1.6517)  Acc@1: 62.5000 (62.3016)  Acc@5: 79.1667 (83.1349)  Acc@task: 87.5000 (86.3095)  time: 0.1771  data: 0.0027  max mem: 1375\n",
            "Test: [Task 1]  [22/23]  eta: 0:00:00  Loss: 1.6843 (1.7416)  Acc@1: 58.3333 (61.3806)  Acc@5: 79.1667 (81.7164)  Acc@task: 87.5000 (86.5672)  time: 0.1704  data: 0.0004  max mem: 1375\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1896 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@task 86.567 Acc@1 61.381 Acc@5 81.716 loss 1.742\n",
            "Test: [Task 2]  [ 0/20]  eta: 0:00:16  Loss: 1.4727 (1.4727)  Acc@1: 62.5000 (62.5000)  Acc@5: 91.6667 (91.6667)  Acc@task: 91.6667 (91.6667)  time: 0.8018  data: 0.6111  max mem: 1375\n",
            "Test: [Task 2]  [10/20]  eta: 0:00:02  Loss: 2.0201 (2.0643)  Acc@1: 54.1667 (53.7879)  Acc@5: 70.8333 (72.7273)  Acc@task: 79.1667 (75.7576)  time: 0.2343  data: 0.0571  max mem: 1375\n",
            "Test: [Task 2]  [19/20]  eta: 0:00:00  Loss: 2.0371 (2.1565)  Acc@1: 50.0000 (49.4781)  Acc@5: 70.8333 (72.2338)  Acc@task: 75.0000 (75.3653)  time: 0.2077  data: 0.0315  max mem: 1375\n",
            "Test: [Task 2] Total time: 0:00:04 (0.2122 s / it)\n",
            "* Acc@task 75.365 Acc@1 49.478 Acc@5 72.234 loss 2.157\n",
            "[Average accuracy till task2]\tAcc@task: 80.9663\tAcc@1: 55.4293\tAcc@5: 76.9751\tLoss: 1.9491\tForgetting: 0.0000\tBackward: 1.1194\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:24  Loss: 1.2165 (1.2165)  Acc@1: 70.8333 (70.8333)  Acc@5: 87.5000 (87.5000)  Acc@task: 95.8333 (95.8333)  time: 0.5767  data: 0.3966  max mem: 1375\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 1.2165 (1.1832)  Acc@1: 75.0000 (71.5909)  Acc@5: 87.5000 (89.3939)  Acc@task: 91.6667 (88.2576)  time: 0.2133  data: 0.0363  max mem: 1375\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 1.1834 (1.1840)  Acc@1: 70.8333 (71.2302)  Acc@5: 91.6667 (89.4841)  Acc@task: 87.5000 (88.6905)  time: 0.1761  data: 0.0003  max mem: 1375\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 1.1791 (1.1505)  Acc@1: 70.8333 (71.5054)  Acc@5: 91.6667 (89.7849)  Acc@task: 91.6667 (89.6505)  time: 0.1767  data: 0.0003  max mem: 1375\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.9969 (1.1185)  Acc@1: 70.8333 (72.5610)  Acc@5: 91.6667 (90.6504)  Acc@task: 91.6667 (90.1423)  time: 0.1789  data: 0.0003  max mem: 1375\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.9561 (1.1064)  Acc@1: 75.0000 (72.9000)  Acc@5: 91.6667 (90.8000)  Acc@task: 91.6667 (90.2000)  time: 0.1752  data: 0.0003  max mem: 1375\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1882 s / it)\n",
            "* Acc@task 90.200 Acc@1 72.900 Acc@5 90.800 loss 1.106\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:22  Loss: 1.7741 (1.7741)  Acc@1: 54.1667 (54.1667)  Acc@5: 79.1667 (79.1667)  Acc@task: 87.5000 (87.5000)  time: 0.5470  data: 0.3751  max mem: 1375\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 1.6532 (1.6565)  Acc@1: 62.5000 (60.9849)  Acc@5: 83.3333 (83.7121)  Acc@task: 83.3333 (84.4697)  time: 0.2119  data: 0.0346  max mem: 1375\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 1.4908 (1.5844)  Acc@1: 62.5000 (61.7064)  Acc@5: 83.3333 (84.7222)  Acc@task: 87.5000 (86.9048)  time: 0.1778  data: 0.0011  max mem: 1375\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 1.4868 (1.5618)  Acc@1: 62.5000 (63.3065)  Acc@5: 87.5000 (86.0215)  Acc@task: 87.5000 (87.7688)  time: 0.1783  data: 0.0011  max mem: 1375\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 1.5113 (1.5312)  Acc@1: 66.6667 (64.1260)  Acc@5: 87.5000 (86.4837)  Acc@task: 87.5000 (87.7033)  time: 0.1794  data: 0.0003  max mem: 1375\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 1.5016 (1.5221)  Acc@1: 66.6667 (64.1000)  Acc@5: 87.5000 (86.7000)  Acc@task: 87.5000 (87.7000)  time: 0.1761  data: 0.0003  max mem: 1375\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1879 s / it)\n",
            "* Acc@task 87.700 Acc@1 64.100 Acc@5 86.700 loss 1.522\n",
            "[Average accuracy till task2]\tAcc@task: 88.9500\tAcc@1: 68.5000\tAcc@5: 88.7500\tLoss: 1.3142\tForgetting: 4.2000\tBackward: -4.2000\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:19  Loss: 1.2165 (1.2165)  Acc@1: 70.8333 (70.8333)  Acc@5: 87.5000 (87.5000)  Acc@task: 95.8333 (95.8333)  time: 0.4546  data: 0.2874  max mem: 1375\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 1.2165 (1.1832)  Acc@1: 75.0000 (71.5909)  Acc@5: 87.5000 (89.3939)  Acc@task: 91.6667 (88.2576)  time: 0.2037  data: 0.0274  max mem: 1375\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 1.1834 (1.1840)  Acc@1: 70.8333 (71.2302)  Acc@5: 91.6667 (89.4841)  Acc@task: 87.5000 (88.6905)  time: 0.1788  data: 0.0009  max mem: 1375\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 1.1791 (1.1505)  Acc@1: 70.8333 (71.5054)  Acc@5: 91.6667 (89.7849)  Acc@task: 91.6667 (89.6505)  time: 0.1798  data: 0.0003  max mem: 1375\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.9969 (1.1185)  Acc@1: 70.8333 (72.5610)  Acc@5: 91.6667 (90.6504)  Acc@task: 91.6667 (90.1423)  time: 0.1815  data: 0.0002  max mem: 1375\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.9561 (1.1064)  Acc@1: 75.0000 (72.9000)  Acc@5: 91.6667 (90.8000)  Acc@task: 91.6667 (90.2000)  time: 0.1781  data: 0.0002  max mem: 1375\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1885 s / it)\n",
            "* Acc@task 90.200 Acc@1 72.900 Acc@5 90.800 loss 1.106\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:31  Loss: 1.7741 (1.7741)  Acc@1: 54.1667 (54.1667)  Acc@5: 79.1667 (79.1667)  Acc@task: 87.5000 (87.5000)  time: 0.7473  data: 0.5588  max mem: 1375\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:07  Loss: 1.6532 (1.6565)  Acc@1: 62.5000 (60.9849)  Acc@5: 83.3333 (83.7121)  Acc@task: 83.3333 (84.4697)  time: 0.2322  data: 0.0517  max mem: 1375\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 1.4908 (1.5844)  Acc@1: 62.5000 (61.7064)  Acc@5: 83.3333 (84.7222)  Acc@task: 87.5000 (86.9048)  time: 0.1812  data: 0.0006  max mem: 1375\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 1.4868 (1.5618)  Acc@1: 62.5000 (63.3065)  Acc@5: 87.5000 (86.0215)  Acc@task: 87.5000 (87.7688)  time: 0.1820  data: 0.0003  max mem: 1375\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 1.5113 (1.5312)  Acc@1: 66.6667 (64.1260)  Acc@5: 87.5000 (86.4837)  Acc@task: 87.5000 (87.7033)  time: 0.1826  data: 0.0002  max mem: 1375\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 1.5016 (1.5221)  Acc@1: 66.6667 (64.1000)  Acc@5: 87.5000 (86.7000)  Acc@task: 87.5000 (87.7000)  time: 0.1793  data: 0.0002  max mem: 1375\n",
            "Test: [Task 2] Total time: 0:00:08 (0.1962 s / it)\n",
            "* Acc@task 87.700 Acc@1 64.100 Acc@5 86.700 loss 1.522\n",
            "[Average accuracy till task2]\tAcc@task: 88.9500\tAcc@1: 68.5000\tAcc@5: 88.7500\tLoss: 1.3142\tForgetting: 0.0000\tBackward: 72.9000\n",
            "torch.Size([21000, 384])\n",
            "Averaged stats: Lr: 0.005000  Loss: 0.2773  Acc@1: 82.5000 (84.5833)  Acc@5: 99.1667 (98.9167)\n",
            "torch.Size([21000, 384])\n",
            "Averaged stats: Lr: 0.004878  Loss: 0.1004  Acc@1: 95.8333 (95.9167)  Acc@5: 100.0000 (99.8333)\n",
            "torch.Size([21000, 384])\n",
            "Averaged stats: Lr: 0.004523  Loss: 0.0255  Acc@1: 98.3333 (98.4167)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([21000, 384])\n",
            "Averaged stats: Lr: 0.003969  Loss: 0.0267  Acc@1: 99.1667 (99.1667)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([21000, 384])\n",
            "Averaged stats: Lr: 0.003273  Loss: 0.0535  Acc@1: 100.0000 (99.7500)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([21000, 384])\n",
            "Averaged stats: Lr: 0.002500  Loss: 0.0086  Acc@1: 100.0000 (99.7500)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([21000, 384])\n",
            "Averaged stats: Lr: 0.001727  Loss: 0.0092  Acc@1: 100.0000 (99.9167)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([21000, 384])\n",
            "Averaged stats: Lr: 0.001031  Loss: 0.0127  Acc@1: 100.0000 (99.7500)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([21000, 384])\n",
            "Averaged stats: Lr: 0.000477  Loss: 0.0147  Acc@1: 100.0000 (99.9167)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([21000, 384])\n",
            "Averaged stats: Lr: 0.000122  Loss: 0.0083  Acc@1: 100.0000 (99.6667)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [Task 1]  [ 0/23]  eta: 0:00:20  Loss: 0.8211 (0.8211)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  Acc@task: 87.5000 (87.5000)  time: 0.8954  data: 0.7091  max mem: 1375\n",
            "Test: [Task 1]  [10/23]  eta: 0:00:03  Loss: 0.9428 (0.9974)  Acc@1: 79.1667 (76.8939)  Acc@5: 91.6667 (90.1515)  Acc@task: 83.3333 (83.7121)  time: 0.2472  data: 0.0670  max mem: 1375\n",
            "Test: [Task 1]  [20/23]  eta: 0:00:00  Loss: 1.0320 (1.0517)  Acc@1: 70.8333 (74.8016)  Acc@5: 87.5000 (88.4921)  Acc@task: 83.3333 (83.9286)  time: 0.1815  data: 0.0016  max mem: 1375\n",
            "Test: [Task 1]  [22/23]  eta: 0:00:00  Loss: 1.0320 (1.0836)  Acc@1: 70.8333 (75.0000)  Acc@5: 87.5000 (88.6194)  Acc@task: 83.3333 (84.1418)  time: 0.1760  data: 0.0012  max mem: 1375\n",
            "Test: [Task 1] Total time: 0:00:04 (0.2114 s / it)\n",
            "* Acc@task 84.142 Acc@1 75.000 Acc@5 88.619 loss 1.084\n",
            "Test: [Task 2]  [ 0/20]  eta: 0:00:10  Loss: 2.1719 (2.1719)  Acc@1: 45.8333 (45.8333)  Acc@5: 75.0000 (75.0000)  Acc@task: 66.6667 (66.6667)  time: 0.5248  data: 0.3401  max mem: 1375\n",
            "Test: [Task 2]  [10/20]  eta: 0:00:02  Loss: 1.7130 (1.5524)  Acc@1: 66.6667 (65.9091)  Acc@5: 79.1667 (82.5758)  Acc@task: 75.0000 (75.3788)  time: 0.2136  data: 0.0314  max mem: 1375\n",
            "Test: [Task 2]  [19/20]  eta: 0:00:00  Loss: 1.7130 (1.5740)  Acc@1: 62.5000 (64.3006)  Acc@5: 79.1667 (81.6284)  Acc@task: 75.0000 (75.1566)  time: 0.1983  data: 0.0173  max mem: 1375\n",
            "Test: [Task 2] Total time: 0:00:04 (0.2025 s / it)\n",
            "* Acc@task 75.157 Acc@1 64.301 Acc@5 81.628 loss 1.574\n",
            "[Average accuracy till task2]\tAcc@task: 79.6492\tAcc@1: 69.6503\tAcc@5: 85.1239\tLoss: 1.3288\tForgetting: 0.0000\tBackward: 75.0000\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:20  Loss: 0.1846 (0.1846)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  Acc@task: 95.8333 (95.8333)  time: 0.4969  data: 0.3239  max mem: 1375\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.4545 (0.5509)  Acc@1: 87.5000 (85.2273)  Acc@5: 100.0000 (96.2121)  Acc@task: 91.6667 (88.2576)  time: 0.2091  data: 0.0297  max mem: 1375\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.4762 (0.5353)  Acc@1: 83.3333 (85.3175)  Acc@5: 95.8333 (96.8254)  Acc@task: 87.5000 (88.6905)  time: 0.1806  data: 0.0003  max mem: 1375\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.3651 (0.4768)  Acc@1: 87.5000 (87.3656)  Acc@5: 100.0000 (97.3118)  Acc@task: 91.6667 (89.6505)  time: 0.1814  data: 0.0004  max mem: 1375\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.2991 (0.4557)  Acc@1: 91.6667 (87.8049)  Acc@5: 100.0000 (97.7642)  Acc@task: 91.6667 (90.1423)  time: 0.1827  data: 0.0003  max mem: 1375\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.2976 (0.4493)  Acc@1: 91.6667 (87.9000)  Acc@5: 100.0000 (97.8000)  Acc@task: 91.6667 (90.2000)  time: 0.1791  data: 0.0003  max mem: 1375\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1897 s / it)\n",
            "* Acc@task 90.200 Acc@1 87.900 Acc@5 97.800 loss 0.449\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:19  Loss: 0.3917 (0.3917)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  Acc@task: 87.5000 (87.5000)  time: 0.4644  data: 0.2937  max mem: 1375\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.4689 (0.5409)  Acc@1: 87.5000 (87.1212)  Acc@5: 100.0000 (97.3485)  Acc@task: 83.3333 (84.4697)  time: 0.2078  data: 0.0273  max mem: 1375\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.4886 (0.5664)  Acc@1: 87.5000 (86.9048)  Acc@5: 95.8333 (96.4286)  Acc@task: 87.5000 (86.9048)  time: 0.1812  data: 0.0005  max mem: 1375\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.4389 (0.5337)  Acc@1: 87.5000 (86.9624)  Acc@5: 95.8333 (96.5054)  Acc@task: 87.5000 (87.7688)  time: 0.1813  data: 0.0003  max mem: 1375\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.4168 (0.5134)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (96.9512)  Acc@task: 87.5000 (87.7033)  time: 0.1826  data: 0.0002  max mem: 1375\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.4073 (0.5096)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (97.0000)  Acc@task: 87.5000 (87.7000)  time: 0.1790  data: 0.0002  max mem: 1375\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1891 s / it)\n",
            "* Acc@task 87.700 Acc@1 87.500 Acc@5 97.000 loss 0.510\n",
            "[Average accuracy till task2]\tAcc@task: 88.9500\tAcc@1: 87.7000\tAcc@5: 97.4000\tLoss: 0.4794\tForgetting: 0.0000\tBackward: 87.9000\n",
            "Loading checkpoint from: ./output/cifar100_full_dino_5epoch_10pct/checkpoint/task3_checkpoint.pth\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/engines/hide_promtp_wtp_and_tap_engine.py:574: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  loss = torch.nn.functional.cross_entropy(sim, torch.range(0, sim.shape[0] - 1).long().to(device))\n",
            "Train: Epoch[ 1/10]  [ 0/21]  eta: 0:00:15  Lr: 0.002812  Loss: 3.5862  Acc@1: 16.6667 (16.6667)  Acc@5: 45.8333 (45.8333)  time: 0.7533  data: 0.4879  max mem: 1375\n",
            "Train: Epoch[ 1/10]  [10/21]  eta: 0:00:03  Lr: 0.002812  Loss: 2.5821  Acc@1: 12.5000 (12.8788)  Acc@5: 45.8333 (52.2727)  time: 0.3227  data: 0.0453  max mem: 1375\n",
            "Train: Epoch[ 1/10]  [20/21]  eta: 0:00:00  Lr: 0.002812  Loss: 2.5762  Acc@1: 12.5000 (15.2918)  Acc@5: 58.3333 (56.1368)  time: 0.2825  data: 0.0008  max mem: 1375\n",
            "Train: Epoch[ 1/10] Total time: 0:00:06 (0.3088 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 2.5762  Acc@1: 12.5000 (15.2918)  Acc@5: 58.3333 (56.1368)\n",
            "Train: Epoch[ 2/10]  [ 0/21]  eta: 0:00:14  Lr: 0.002812  Loss: 2.2687  Acc@1: 37.5000 (37.5000)  Acc@5: 70.8333 (70.8333)  time: 0.7114  data: 0.3416  max mem: 1375\n",
            "Train: Epoch[ 2/10]  [10/21]  eta: 0:00:03  Lr: 0.002812  Loss: 2.0711  Acc@1: 29.1667 (29.5455)  Acc@5: 70.8333 (73.1061)  time: 0.3210  data: 0.0318  max mem: 1375\n",
            "Train: Epoch[ 2/10]  [20/21]  eta: 0:00:00  Lr: 0.002812  Loss: 1.6063  Acc@1: 37.5000 (38.2294)  Acc@5: 79.1667 (80.6841)  time: 0.2774  data: 0.0006  max mem: 1375\n",
            "Train: Epoch[ 2/10] Total time: 0:00:06 (0.3020 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.6063  Acc@1: 37.5000 (38.2294)  Acc@5: 79.1667 (80.6841)\n",
            "Train: Epoch[ 3/10]  [ 0/21]  eta: 0:00:12  Lr: 0.002812  Loss: 1.7055  Acc@1: 45.8333 (45.8333)  Acc@5: 87.5000 (87.5000)  time: 0.5769  data: 0.3100  max mem: 1375\n",
            "Train: Epoch[ 3/10]  [10/21]  eta: 0:00:03  Lr: 0.002812  Loss: 1.4115  Acc@1: 58.3333 (54.9242)  Acc@5: 87.5000 (89.7727)  time: 0.3056  data: 0.0288  max mem: 1375\n",
            "Train: Epoch[ 3/10]  [20/21]  eta: 0:00:00  Lr: 0.002812  Loss: 1.3827  Acc@1: 58.3333 (60.9658)  Acc@5: 91.6667 (92.1529)  time: 0.2741  data: 0.0005  max mem: 1375\n",
            "Train: Epoch[ 3/10] Total time: 0:00:06 (0.2926 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.3827  Acc@1: 58.3333 (60.9658)  Acc@5: 91.6667 (92.1529)\n",
            "Train: Epoch[ 4/10]  [ 0/21]  eta: 0:00:12  Lr: 0.002812  Loss: 1.0775  Acc@1: 66.6667 (66.6667)  Acc@5: 91.6667 (91.6667)  time: 0.5883  data: 0.3099  max mem: 1375\n",
            "Train: Epoch[ 4/10]  [10/21]  eta: 0:00:03  Lr: 0.002812  Loss: 1.3357  Acc@1: 70.8333 (69.6970)  Acc@5: 95.8333 (94.6970)  time: 0.3078  data: 0.0286  max mem: 1375\n",
            "Train: Epoch[ 4/10]  [20/21]  eta: 0:00:00  Lr: 0.002812  Loss: 0.8208  Acc@1: 70.8333 (68.8129)  Acc@5: 95.8333 (95.7746)  time: 0.2750  data: 0.0003  max mem: 1375\n",
            "Train: Epoch[ 4/10] Total time: 0:00:06 (0.2937 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.8208  Acc@1: 70.8333 (68.8129)  Acc@5: 95.8333 (95.7746)\n",
            "Train: Epoch[ 5/10]  [ 0/21]  eta: 0:00:14  Lr: 0.002812  Loss: 1.1339  Acc@1: 62.5000 (62.5000)  Acc@5: 91.6667 (91.6667)  time: 0.6942  data: 0.3920  max mem: 1375\n",
            "Train: Epoch[ 5/10]  [10/21]  eta: 0:00:03  Lr: 0.002812  Loss: 1.1476  Acc@1: 75.0000 (72.7273)  Acc@5: 95.8333 (96.2121)  time: 0.3171  data: 0.0361  max mem: 1375\n",
            "Train: Epoch[ 5/10]  [20/21]  eta: 0:00:00  Lr: 0.002812  Loss: 0.9526  Acc@1: 75.0000 (74.2455)  Acc@5: 100.0000 (97.1831)  time: 0.2750  data: 0.0004  max mem: 1375\n",
            "Train: Epoch[ 5/10] Total time: 0:00:06 (0.2988 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.9526  Acc@1: 75.0000 (74.2455)  Acc@5: 100.0000 (97.1831)\n",
            "Train: Epoch[ 6/10]  [ 0/21]  eta: 0:00:13  Lr: 0.002812  Loss: 1.3889  Acc@1: 66.6667 (66.6667)  Acc@5: 87.5000 (87.5000)  time: 0.6202  data: 0.3288  max mem: 1375\n",
            "Train: Epoch[ 6/10]  [10/21]  eta: 0:00:03  Lr: 0.002812  Loss: 0.8776  Acc@1: 79.1667 (79.5455)  Acc@5: 100.0000 (97.3485)  time: 0.3082  data: 0.0301  max mem: 1375\n",
            "Train: Epoch[ 6/10]  [20/21]  eta: 0:00:00  Lr: 0.002812  Loss: 1.0844  Acc@1: 79.1667 (76.8612)  Acc@5: 95.8333 (96.9819)  time: 0.2741  data: 0.0003  max mem: 1375\n",
            "Train: Epoch[ 6/10] Total time: 0:00:06 (0.2948 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.0844  Acc@1: 79.1667 (76.8612)  Acc@5: 95.8333 (96.9819)\n",
            "Train: Epoch[ 7/10]  [ 0/21]  eta: 0:00:13  Lr: 0.002812  Loss: 0.8362  Acc@1: 62.5000 (62.5000)  Acc@5: 100.0000 (100.0000)  time: 0.6617  data: 0.3512  max mem: 1375\n",
            "Train: Epoch[ 7/10]  [10/21]  eta: 0:00:03  Lr: 0.002812  Loss: 0.9322  Acc@1: 83.3333 (80.3030)  Acc@5: 95.8333 (96.5909)  time: 0.3109  data: 0.0326  max mem: 1375\n",
            "Train: Epoch[ 7/10]  [20/21]  eta: 0:00:00  Lr: 0.002812  Loss: 0.3256  Acc@1: 83.3333 (83.0986)  Acc@5: 95.8333 (97.1831)  time: 0.2730  data: 0.0005  max mem: 1375\n",
            "Train: Epoch[ 7/10] Total time: 0:00:06 (0.2972 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.3256  Acc@1: 83.3333 (83.0986)  Acc@5: 95.8333 (97.1831)\n",
            "Train: Epoch[ 8/10]  [ 0/21]  eta: 0:00:13  Lr: 0.002812  Loss: 0.5169  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.6545  data: 0.3836  max mem: 1375\n",
            "Train: Epoch[ 8/10]  [10/21]  eta: 0:00:03  Lr: 0.002812  Loss: 0.8788  Acc@1: 83.3333 (81.8182)  Acc@5: 100.0000 (99.2424)  time: 0.3123  data: 0.0353  max mem: 1375\n",
            "Train: Epoch[ 8/10]  [20/21]  eta: 0:00:00  Lr: 0.002812  Loss: 0.3342  Acc@1: 83.3333 (84.1046)  Acc@5: 100.0000 (98.9940)  time: 0.2747  data: 0.0003  max mem: 1375\n",
            "Train: Epoch[ 8/10] Total time: 0:00:06 (0.2970 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.3342  Acc@1: 83.3333 (84.1046)  Acc@5: 100.0000 (98.9940)\n",
            "Train: Epoch[ 9/10]  [ 0/21]  eta: 0:00:14  Lr: 0.002812  Loss: 0.4915  Acc@1: 91.6667 (91.6667)  Acc@5: 95.8333 (95.8333)  time: 0.6957  data: 0.4227  max mem: 1375\n",
            "Train: Epoch[ 9/10]  [10/21]  eta: 0:00:03  Lr: 0.002812  Loss: 0.7716  Acc@1: 87.5000 (85.6061)  Acc@5: 100.0000 (98.4848)  time: 0.3163  data: 0.0387  max mem: 1375\n",
            "Train: Epoch[ 9/10]  [20/21]  eta: 0:00:00  Lr: 0.002812  Loss: 0.7155  Acc@1: 83.3333 (84.5070)  Acc@5: 100.0000 (98.3903)  time: 0.2742  data: 0.0003  max mem: 1375\n",
            "Train: Epoch[ 9/10] Total time: 0:00:06 (0.3005 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.7155  Acc@1: 83.3333 (84.5070)  Acc@5: 100.0000 (98.3903)\n",
            "Train: Epoch[10/10]  [ 0/21]  eta: 0:00:18  Lr: 0.002812  Loss: 0.3673  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)  time: 0.8924  data: 0.5947  max mem: 1375\n",
            "Train: Epoch[10/10]  [10/21]  eta: 0:00:03  Lr: 0.002812  Loss: 0.6580  Acc@1: 83.3333 (85.2273)  Acc@5: 100.0000 (99.2424)  time: 0.3346  data: 0.0547  max mem: 1375\n",
            "Train: Epoch[10/10]  [20/21]  eta: 0:00:00  Lr: 0.002812  Loss: 0.6923  Acc@1: 83.3333 (84.5070)  Acc@5: 100.0000 (98.7928)  time: 0.2754  data: 0.0004  max mem: 1375\n",
            "Train: Epoch[10/10] Total time: 0:00:06 (0.3088 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.6923  Acc@1: 83.3333 (84.5070)  Acc@5: 100.0000 (98.7928)\n",
            "torch.Size([5, 2, 5, 6, 64])\n",
            "torch.Size([5, 2, 1, 5, 6, 64])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/23]  eta: 0:00:22  Loss: 1.4098 (1.4098)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  Acc@task: 66.6667 (66.6667)  time: 0.9937  data: 0.7957  max mem: 1375\n",
            "Test: [Task 1]  [10/23]  eta: 0:00:03  Loss: 1.0866 (1.0214)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (90.1515)  Acc@task: 79.1667 (76.1364)  time: 0.2508  data: 0.0735  max mem: 1375\n",
            "Test: [Task 1]  [20/23]  eta: 0:00:00  Loss: 0.9407 (0.9938)  Acc@1: 79.1667 (76.7857)  Acc@5: 91.6667 (90.4762)  Acc@task: 79.1667 (77.5794)  time: 0.1771  data: 0.0008  max mem: 1375\n",
            "Test: [Task 1]  [22/23]  eta: 0:00:00  Loss: 0.8116 (0.9435)  Acc@1: 79.1667 (77.0522)  Acc@5: 91.6667 (90.6716)  Acc@task: 79.1667 (77.7985)  time: 0.1722  data: 0.0008  max mem: 1375\n",
            "Test: [Task 1] Total time: 0:00:04 (0.2117 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@task 77.799 Acc@1 77.052 Acc@5 90.672 loss 0.943\n",
            "Test: [Task 2]  [ 0/20]  eta: 0:00:12  Loss: 1.8482 (1.8482)  Acc@1: 58.3333 (58.3333)  Acc@5: 75.0000 (75.0000)  Acc@task: 58.3333 (58.3333)  time: 0.6272  data: 0.4662  max mem: 1375\n",
            "Test: [Task 2]  [10/20]  eta: 0:00:02  Loss: 1.3921 (1.4573)  Acc@1: 58.3333 (63.6364)  Acc@5: 87.5000 (84.4697)  Acc@task: 70.8333 (68.1818)  time: 0.2193  data: 0.0427  max mem: 1375\n",
            "Test: [Task 2]  [19/20]  eta: 0:00:00  Loss: 1.3921 (1.4051)  Acc@1: 62.5000 (66.8058)  Acc@5: 83.3333 (85.1775)  Acc@task: 70.8333 (68.6848)  time: 0.2000  data: 0.0236  max mem: 1375\n",
            "Test: [Task 2] Total time: 0:00:04 (0.2052 s / it)\n",
            "* Acc@task 68.685 Acc@1 66.806 Acc@5 85.177 loss 1.405\n",
            "Test: [Task 3]  [ 0/21]  eta: 0:00:11  Loss: 2.6309 (2.6309)  Acc@1: 29.1667 (29.1667)  Acc@5: 79.1667 (79.1667)  Acc@task: 62.5000 (62.5000)  time: 0.5643  data: 0.3954  max mem: 1375\n",
            "Test: [Task 3]  [10/21]  eta: 0:00:02  Loss: 3.3016 (3.3334)  Acc@1: 25.0000 (22.3485)  Acc@5: 70.8333 (67.4242)  Acc@task: 70.8333 (70.8333)  time: 0.2134  data: 0.0362  max mem: 1375\n",
            "Test: [Task 3]  [20/21]  eta: 0:00:00  Loss: 3.2030 (3.1970)  Acc@1: 23.5294 (22.7364)  Acc@5: 70.8333 (70.4225)  Acc@task: 70.8333 (72.2334)  time: 0.1752  data: 0.0002  max mem: 1375\n",
            "Test: [Task 3] Total time: 0:00:04 (0.2010 s / it)\n",
            "* Acc@task 72.233 Acc@1 22.736 Acc@5 70.423 loss 3.197\n",
            "[Average accuracy till task3]\tAcc@task: 72.9056\tAcc@1: 55.5315\tAcc@5: 82.0905\tLoss: 1.8485\tForgetting: 0.0000\tBackward: 39.7787\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:33  Loss: 0.1876 (0.1876)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  Acc@task: 95.8333 (95.8333)  time: 0.7971  data: 0.6172  max mem: 1375\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:07  Loss: 0.4901 (0.5696)  Acc@1: 87.5000 (84.4697)  Acc@5: 95.8333 (96.2121)  Acc@task: 79.1667 (80.3030)  time: 0.2358  data: 0.0578  max mem: 1375\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.5278 (0.5451)  Acc@1: 83.3333 (84.9206)  Acc@5: 95.8333 (96.8254)  Acc@task: 83.3333 (81.7460)  time: 0.1797  data: 0.0011  max mem: 1375\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.3638 (0.4866)  Acc@1: 87.5000 (87.0968)  Acc@5: 100.0000 (97.3118)  Acc@task: 83.3333 (81.9892)  time: 0.1797  data: 0.0004  max mem: 1375\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.3092 (0.4639)  Acc@1: 91.6667 (87.6016)  Acc@5: 100.0000 (97.6626)  Acc@task: 87.5000 (84.0447)  time: 0.1800  data: 0.0004  max mem: 1375\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.2996 (0.4573)  Acc@1: 91.6667 (87.7000)  Acc@5: 100.0000 (97.7000)  Acc@task: 91.6667 (84.2000)  time: 0.1766  data: 0.0003  max mem: 1375\n",
            "Test: [Task 1] Total time: 0:00:08 (0.1954 s / it)\n",
            "* Acc@task 84.200 Acc@1 87.700 Acc@5 97.700 loss 0.457\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:22  Loss: 0.3577 (0.3577)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  Acc@task: 87.5000 (87.5000)  time: 0.5466  data: 0.3781  max mem: 1375\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.4334 (0.5297)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (97.3485)  Acc@task: 83.3333 (82.1970)  time: 0.2128  data: 0.0346  max mem: 1375\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.4824 (0.5581)  Acc@1: 87.5000 (87.6984)  Acc@5: 95.8333 (96.2302)  Acc@task: 83.3333 (84.1270)  time: 0.1796  data: 0.0003  max mem: 1375\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.4237 (0.5272)  Acc@1: 87.5000 (87.6344)  Acc@5: 95.8333 (96.2366)  Acc@task: 83.3333 (85.2151)  time: 0.1801  data: 0.0006  max mem: 1375\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.4070 (0.5067)  Acc@1: 87.5000 (88.1098)  Acc@5: 95.8333 (96.6463)  Acc@task: 83.3333 (84.5528)  time: 0.1810  data: 0.0008  max mem: 1375\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.4029 (0.5030)  Acc@1: 87.5000 (88.1000)  Acc@5: 100.0000 (96.7000)  Acc@task: 83.3333 (84.6000)  time: 0.1777  data: 0.0007  max mem: 1375\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1897 s / it)\n",
            "* Acc@task 84.600 Acc@1 88.100 Acc@5 96.700 loss 0.503\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:17  Loss: 3.2972 (3.2972)  Acc@1: 16.6667 (16.6667)  Acc@5: 83.3333 (83.3333)  Acc@task: 91.6667 (91.6667)  time: 0.4059  data: 0.2364  max mem: 1375\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 3.0850 (3.1523)  Acc@1: 20.8333 (26.5152)  Acc@5: 83.3333 (82.5758)  Acc@task: 79.1667 (81.8182)  time: 0.2050  data: 0.0267  max mem: 1375\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 2.9599 (2.8979)  Acc@1: 29.1667 (31.7460)  Acc@5: 83.3333 (82.7381)  Acc@task: 79.1667 (81.3492)  time: 0.1829  data: 0.0031  max mem: 1375\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 2.8386 (2.9056)  Acc@1: 29.1667 (31.8548)  Acc@5: 83.3333 (84.0054)  Acc@task: 83.3333 (81.1828)  time: 0.1816  data: 0.0004  max mem: 1375\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 2.8593 (2.8845)  Acc@1: 29.1667 (31.7073)  Acc@5: 87.5000 (84.0447)  Acc@task: 79.1667 (81.0976)  time: 0.1820  data: 0.0003  max mem: 1375\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 2.8593 (2.8870)  Acc@1: 33.3333 (31.9000)  Acc@5: 87.5000 (84.1000)  Acc@task: 79.1667 (81.0000)  time: 0.1786  data: 0.0003  max mem: 1375\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1883 s / it)\n",
            "* Acc@task 81.000 Acc@1 31.900 Acc@5 84.100 loss 2.887\n",
            "[Average accuracy till task3]\tAcc@task: 83.2667\tAcc@1: 69.2333\tAcc@5: 92.8333\tLoss: 1.2824\tForgetting: 0.1000\tBackward: 44.1500\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:19  Loss: 0.1876 (0.1876)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  Acc@task: 95.8333 (95.8333)  time: 0.4532  data: 0.2806  max mem: 1375\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.4901 (0.5696)  Acc@1: 87.5000 (84.4697)  Acc@5: 95.8333 (96.2121)  Acc@task: 79.1667 (80.3030)  time: 0.2061  data: 0.0286  max mem: 1375\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.5278 (0.5451)  Acc@1: 83.3333 (84.9206)  Acc@5: 95.8333 (96.8254)  Acc@task: 83.3333 (81.7460)  time: 0.1808  data: 0.0025  max mem: 1375\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.3638 (0.4866)  Acc@1: 87.5000 (87.0968)  Acc@5: 100.0000 (97.3118)  Acc@task: 83.3333 (81.9892)  time: 0.1812  data: 0.0012  max mem: 1375\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.3092 (0.4639)  Acc@1: 91.6667 (87.6016)  Acc@5: 100.0000 (97.6626)  Acc@task: 87.5000 (84.0447)  time: 0.1819  data: 0.0005  max mem: 1375\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.2996 (0.4573)  Acc@1: 91.6667 (87.7000)  Acc@5: 100.0000 (97.7000)  Acc@task: 91.6667 (84.2000)  time: 0.1784  data: 0.0004  max mem: 1375\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1886 s / it)\n",
            "* Acc@task 84.200 Acc@1 87.700 Acc@5 97.700 loss 0.457\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:18  Loss: 0.3577 (0.3577)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  Acc@task: 87.5000 (87.5000)  time: 0.4313  data: 0.2609  max mem: 1375\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.4334 (0.5297)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (97.3485)  Acc@task: 83.3333 (82.1970)  time: 0.2028  data: 0.0243  max mem: 1375\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.4824 (0.5581)  Acc@1: 87.5000 (87.6984)  Acc@5: 95.8333 (96.2302)  Acc@task: 83.3333 (84.1270)  time: 0.1799  data: 0.0005  max mem: 1375\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.4237 (0.5272)  Acc@1: 87.5000 (87.6344)  Acc@5: 95.8333 (96.2366)  Acc@task: 83.3333 (85.2151)  time: 0.1807  data: 0.0003  max mem: 1375\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.4070 (0.5067)  Acc@1: 87.5000 (88.1098)  Acc@5: 95.8333 (96.6463)  Acc@task: 83.3333 (84.5528)  time: 0.1810  data: 0.0002  max mem: 1375\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.4029 (0.5030)  Acc@1: 87.5000 (88.1000)  Acc@5: 100.0000 (96.7000)  Acc@task: 83.3333 (84.6000)  time: 0.1774  data: 0.0002  max mem: 1375\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1880 s / it)\n",
            "* Acc@task 84.600 Acc@1 88.100 Acc@5 96.700 loss 0.503\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:29  Loss: 3.2972 (3.2972)  Acc@1: 16.6667 (16.6667)  Acc@5: 83.3333 (83.3333)  Acc@task: 91.6667 (91.6667)  time: 0.7126  data: 0.5097  max mem: 1375\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:07  Loss: 3.0850 (3.1523)  Acc@1: 20.8333 (26.5152)  Acc@5: 83.3333 (82.5758)  Acc@task: 79.1667 (81.8182)  time: 0.2277  data: 0.0471  max mem: 1375\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 2.9599 (2.8979)  Acc@1: 29.1667 (31.7460)  Acc@5: 83.3333 (82.7381)  Acc@task: 79.1667 (81.3492)  time: 0.1797  data: 0.0006  max mem: 1375\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 2.8386 (2.9056)  Acc@1: 29.1667 (31.8548)  Acc@5: 83.3333 (84.0054)  Acc@task: 83.3333 (81.1828)  time: 0.1800  data: 0.0003  max mem: 1375\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 2.8593 (2.8845)  Acc@1: 29.1667 (31.7073)  Acc@5: 87.5000 (84.0447)  Acc@task: 79.1667 (81.0976)  time: 0.1809  data: 0.0002  max mem: 1375\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 2.8593 (2.8870)  Acc@1: 33.3333 (31.9000)  Acc@5: 87.5000 (84.1000)  Acc@task: 79.1667 (81.0000)  time: 0.1777  data: 0.0002  max mem: 1375\n",
            "Test: [Task 3] Total time: 0:00:08 (0.1938 s / it)\n",
            "* Acc@task 81.000 Acc@1 31.900 Acc@5 84.100 loss 2.887\n",
            "[Average accuracy till task3]\tAcc@task: 83.2667\tAcc@1: 69.2333\tAcc@5: 92.8333\tLoss: 1.2824\tForgetting: 0.0000\tBackward: 55.8500\n",
            "torch.Size([31440, 384])\n",
            "Averaged stats: Lr: 0.005000  Loss: 0.0649  Acc@1: 93.3333 (92.0000)  Acc@5: 100.0000 (99.6250)\n",
            "torch.Size([31440, 384])\n",
            "Averaged stats: Lr: 0.004878  Loss: 0.0497  Acc@1: 99.1667 (99.0417)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([31440, 384])\n",
            "Averaged stats: Lr: 0.004523  Loss: 0.0301  Acc@1: 100.0000 (99.7917)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([31440, 384])\n",
            "Averaged stats: Lr: 0.003969  Loss: 0.0197  Acc@1: 100.0000 (99.9167)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([31440, 384])\n",
            "Averaged stats: Lr: 0.003273  Loss: 0.0058  Acc@1: 100.0000 (99.7917)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([31440, 384])\n",
            "Averaged stats: Lr: 0.002500  Loss: 0.0255  Acc@1: 100.0000 (99.8750)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([31440, 384])\n",
            "Averaged stats: Lr: 0.001727  Loss: 0.0098  Acc@1: 100.0000 (99.9167)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([31440, 384])\n",
            "Averaged stats: Lr: 0.001031  Loss: 0.0050  Acc@1: 100.0000 (99.9167)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([31440, 384])\n",
            "Averaged stats: Lr: 0.000477  Loss: 0.0090  Acc@1: 100.0000 (99.8750)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([31440, 384])\n",
            "Averaged stats: Lr: 0.000122  Loss: 0.0053  Acc@1: 100.0000 (99.9167)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [Task 1]  [ 0/23]  eta: 0:00:19  Loss: 0.6493 (0.6493)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (83.3333)  time: 0.8597  data: 0.6785  max mem: 1375\n",
            "Test: [Task 1]  [10/23]  eta: 0:00:03  Loss: 0.9300 (0.8728)  Acc@1: 79.1667 (78.4091)  Acc@5: 95.8333 (93.1818)  Acc@task: 83.3333 (81.4394)  time: 0.2409  data: 0.0620  max mem: 1375\n",
            "Test: [Task 1]  [20/23]  eta: 0:00:00  Loss: 0.9300 (0.9597)  Acc@1: 75.0000 (75.1984)  Acc@5: 95.8333 (93.4524)  Acc@task: 79.1667 (78.3730)  time: 0.1791  data: 0.0004  max mem: 1375\n",
            "Test: [Task 1]  [22/23]  eta: 0:00:00  Loss: 0.8049 (0.9146)  Acc@1: 75.0000 (75.7463)  Acc@5: 95.8333 (93.6567)  Acc@task: 79.1667 (78.5448)  time: 0.1743  data: 0.0003  max mem: 1375\n",
            "Test: [Task 1] Total time: 0:00:04 (0.2078 s / it)\n",
            "* Acc@task 78.545 Acc@1 75.746 Acc@5 93.657 loss 0.915\n",
            "Test: [Task 2]  [ 0/20]  eta: 0:00:12  Loss: 1.8147 (1.8147)  Acc@1: 54.1667 (54.1667)  Acc@5: 83.3333 (83.3333)  Acc@task: 75.0000 (75.0000)  time: 0.6400  data: 0.4673  max mem: 1375\n",
            "Test: [Task 2]  [10/20]  eta: 0:00:02  Loss: 1.8147 (1.7655)  Acc@1: 62.5000 (60.6061)  Acc@5: 83.3333 (82.9545)  Acc@task: 66.6667 (66.6667)  time: 0.2206  data: 0.0427  max mem: 1375\n",
            "Test: [Task 2]  [19/20]  eta: 0:00:00  Loss: 1.4666 (1.6500)  Acc@1: 62.5000 (62.8393)  Acc@5: 87.5000 (84.5511)  Acc@task: 70.8333 (68.0585)  time: 0.2019  data: 0.0236  max mem: 1375\n",
            "Test: [Task 2] Total time: 0:00:04 (0.2064 s / it)\n",
            "* Acc@task 68.058 Acc@1 62.839 Acc@5 84.551 loss 1.650\n",
            "Test: [Task 3]  [ 0/21]  eta: 0:00:13  Loss: 1.2280 (1.2280)  Acc@1: 66.6667 (66.6667)  Acc@5: 91.6667 (91.6667)  Acc@task: 83.3333 (83.3333)  time: 0.6385  data: 0.4763  max mem: 1375\n",
            "Test: [Task 3]  [10/21]  eta: 0:00:02  Loss: 1.5906 (1.7024)  Acc@1: 66.6667 (63.6364)  Acc@5: 87.5000 (84.0909)  Acc@task: 75.0000 (73.1061)  time: 0.2217  data: 0.0437  max mem: 1375\n",
            "Test: [Task 3]  [20/21]  eta: 0:00:00  Loss: 1.7758 (1.7593)  Acc@1: 62.5000 (63.7827)  Acc@5: 83.3333 (82.8974)  Acc@task: 66.6667 (70.6237)  time: 0.1767  data: 0.0005  max mem: 1375\n",
            "Test: [Task 3] Total time: 0:00:04 (0.2054 s / it)\n",
            "* Acc@task 70.624 Acc@1 63.783 Acc@5 82.897 loss 1.759\n",
            "[Average accuracy till task3]\tAcc@task: 72.4090\tAcc@1: 67.4561\tAcc@5: 87.0351\tLoss: 1.4413\tForgetting: 0.0000\tBackward: 69.2928\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:32  Loss: 0.1858 (0.1858)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  Acc@task: 95.8333 (95.8333)  time: 0.7730  data: 0.5545  max mem: 1375\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:07  Loss: 0.5859 (0.6686)  Acc@1: 83.3333 (82.1970)  Acc@5: 100.0000 (97.3485)  Acc@task: 79.1667 (80.3030)  time: 0.2357  data: 0.0508  max mem: 1375\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.5859 (0.6135)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (97.4206)  Acc@task: 83.3333 (81.7460)  time: 0.1802  data: 0.0004  max mem: 1375\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.4469 (0.5706)  Acc@1: 87.5000 (84.8118)  Acc@5: 95.8333 (97.7151)  Acc@task: 83.3333 (81.9892)  time: 0.1802  data: 0.0004  max mem: 1375\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.4565 (0.5509)  Acc@1: 87.5000 (85.2642)  Acc@5: 100.0000 (98.0691)  Acc@task: 87.5000 (84.0447)  time: 0.1812  data: 0.0002  max mem: 1375\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.3898 (0.5426)  Acc@1: 87.5000 (85.4000)  Acc@5: 100.0000 (98.1000)  Acc@task: 91.6667 (84.2000)  time: 0.1778  data: 0.0002  max mem: 1375\n",
            "Test: [Task 1] Total time: 0:00:08 (0.1956 s / it)\n",
            "* Acc@task 84.200 Acc@1 85.400 Acc@5 98.100 loss 0.543\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:18  Loss: 0.4092 (0.4092)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  Acc@task: 87.5000 (87.5000)  time: 0.4431  data: 0.2709  max mem: 1375\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.4542 (0.5922)  Acc@1: 87.5000 (86.7424)  Acc@5: 100.0000 (96.9697)  Acc@task: 83.3333 (82.1970)  time: 0.2083  data: 0.0316  max mem: 1375\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.5558 (0.6229)  Acc@1: 83.3333 (85.9127)  Acc@5: 95.8333 (96.0317)  Acc@task: 83.3333 (84.1270)  time: 0.1833  data: 0.0042  max mem: 1375\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.5472 (0.5875)  Acc@1: 87.5000 (86.4247)  Acc@5: 95.8333 (96.2366)  Acc@task: 83.3333 (85.2151)  time: 0.1821  data: 0.0012  max mem: 1375\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.5031 (0.5621)  Acc@1: 87.5000 (86.4837)  Acc@5: 95.8333 (96.5447)  Acc@task: 83.3333 (84.5528)  time: 0.1826  data: 0.0009  max mem: 1375\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.4296 (0.5552)  Acc@1: 87.5000 (86.6000)  Acc@5: 95.8333 (96.6000)  Acc@task: 83.3333 (84.6000)  time: 0.1791  data: 0.0009  max mem: 1375\n",
            "Test: [Task 2] Total time: 0:00:08 (0.1912 s / it)\n",
            "* Acc@task 84.600 Acc@1 86.600 Acc@5 96.600 loss 0.555\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:34  Loss: 0.1857 (0.1857)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  Acc@task: 91.6667 (91.6667)  time: 0.8192  data: 0.6247  max mem: 1375\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:07  Loss: 0.4921 (0.6146)  Acc@1: 83.3333 (82.1970)  Acc@5: 100.0000 (97.7273)  Acc@task: 79.1667 (81.8182)  time: 0.2375  data: 0.0572  max mem: 1375\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.4822 (0.5555)  Acc@1: 83.3333 (84.7222)  Acc@5: 95.8333 (97.4206)  Acc@task: 79.1667 (81.3492)  time: 0.1803  data: 0.0004  max mem: 1375\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.4947 (0.5519)  Acc@1: 87.5000 (84.9462)  Acc@5: 95.8333 (97.4462)  Acc@task: 83.3333 (81.1828)  time: 0.1816  data: 0.0003  max mem: 1375\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.5132 (0.5629)  Acc@1: 83.3333 (84.7561)  Acc@5: 95.8333 (97.1545)  Acc@task: 79.1667 (81.0976)  time: 0.1820  data: 0.0002  max mem: 1375\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.5132 (0.5619)  Acc@1: 83.3333 (84.6000)  Acc@5: 95.8333 (97.1000)  Acc@task: 79.1667 (81.0000)  time: 0.1785  data: 0.0002  max mem: 1375\n",
            "Test: [Task 3] Total time: 0:00:08 (0.1972 s / it)\n",
            "* Acc@task 81.000 Acc@1 84.600 Acc@5 97.100 loss 0.562\n",
            "[Average accuracy till task3]\tAcc@task: 83.2667\tAcc@1: 85.5333\tAcc@5: 97.2667\tLoss: 0.5532\tForgetting: 0.0000\tBackward: 86.0000\n",
            "Loading checkpoint from: ./output/cifar100_full_dino_5epoch_10pct/checkpoint/task4_checkpoint.pth\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/engines/hide_promtp_wtp_and_tap_engine.py:574: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  loss = torch.nn.functional.cross_entropy(sim, torch.range(0, sim.shape[0] - 1).long().to(device))\n",
            "Train: Epoch[ 1/10]  [ 0/20]  eta: 0:00:21  Lr: 0.002812  Loss: 3.1188  Acc@1: 8.3333 (8.3333)  Acc@5: 58.3333 (58.3333)  time: 1.0755  data: 0.6913  max mem: 1375\n",
            "Train: Epoch[ 1/10]  [10/20]  eta: 0:00:03  Lr: 0.002812  Loss: 2.4901  Acc@1: 12.5000 (13.6364)  Acc@5: 58.3333 (60.9849)  time: 0.3529  data: 0.0642  max mem: 1378\n",
            "Train: Epoch[ 1/10]  [19/20]  eta: 0:00:00  Lr: 0.002812  Loss: 1.9887  Acc@1: 16.6667 (19.7452)  Acc@5: 66.6667 (66.8790)  time: 0.3154  data: 0.0354  max mem: 1378\n",
            "Train: Epoch[ 1/10] Total time: 0:00:06 (0.3200 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.9887  Acc@1: 16.6667 (19.7452)  Acc@5: 66.6667 (66.8790)\n",
            "Train: Epoch[ 2/10]  [ 0/20]  eta: 0:00:14  Lr: 0.002812  Loss: 2.3106  Acc@1: 33.3333 (33.3333)  Acc@5: 66.6667 (66.6667)  time: 0.7022  data: 0.4317  max mem: 1378\n",
            "Train: Epoch[ 2/10]  [10/20]  eta: 0:00:03  Lr: 0.002812  Loss: 2.2120  Acc@1: 41.6667 (40.5303)  Acc@5: 83.3333 (82.1970)  time: 0.3194  data: 0.0397  max mem: 1378\n",
            "Train: Epoch[ 2/10]  [19/20]  eta: 0:00:00  Lr: 0.002812  Loss: 1.2425  Acc@1: 41.6667 (45.8599)  Acc@5: 83.3333 (85.9873)  time: 0.2962  data: 0.0219  max mem: 1378\n",
            "Train: Epoch[ 2/10] Total time: 0:00:06 (0.3004 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.2425  Acc@1: 41.6667 (45.8599)  Acc@5: 83.3333 (85.9873)\n",
            "Train: Epoch[ 3/10]  [ 0/20]  eta: 0:00:11  Lr: 0.002812  Loss: 1.1901  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  time: 0.5637  data: 0.2795  max mem: 1378\n",
            "Train: Epoch[ 3/10]  [10/20]  eta: 0:00:03  Lr: 0.002812  Loss: 1.1245  Acc@1: 58.3333 (61.3636)  Acc@5: 91.6667 (91.6667)  time: 0.3068  data: 0.0271  max mem: 1378\n",
            "Train: Epoch[ 3/10]  [19/20]  eta: 0:00:00  Lr: 0.002812  Loss: 1.1121  Acc@1: 58.3333 (61.1465)  Acc@5: 91.6667 (92.9936)  time: 0.2901  data: 0.0151  max mem: 1378\n",
            "Train: Epoch[ 3/10] Total time: 0:00:05 (0.2944 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.1121  Acc@1: 58.3333 (61.1465)  Acc@5: 91.6667 (92.9936)\n",
            "Train: Epoch[ 4/10]  [ 0/20]  eta: 0:00:12  Lr: 0.002812  Loss: 0.9031  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  time: 0.6343  data: 0.3440  max mem: 1378\n",
            "Train: Epoch[ 4/10]  [10/20]  eta: 0:00:03  Lr: 0.002812  Loss: 0.9182  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (96.5909)  time: 0.3123  data: 0.0317  max mem: 1378\n",
            "Train: Epoch[ 4/10]  [19/20]  eta: 0:00:00  Lr: 0.002812  Loss: 0.9254  Acc@1: 70.8333 (71.7622)  Acc@5: 95.8333 (96.1783)  time: 0.2924  data: 0.0175  max mem: 1378\n",
            "Train: Epoch[ 4/10] Total time: 0:00:05 (0.2968 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.9254  Acc@1: 70.8333 (71.7622)  Acc@5: 95.8333 (96.1783)\n",
            "Train: Epoch[ 5/10]  [ 0/20]  eta: 0:00:15  Lr: 0.002812  Loss: 0.5564  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.7516  data: 0.4720  max mem: 1378\n",
            "Train: Epoch[ 5/10]  [10/20]  eta: 0:00:03  Lr: 0.002812  Loss: 0.4789  Acc@1: 79.1667 (79.5455)  Acc@5: 100.0000 (98.1061)  time: 0.3236  data: 0.0435  max mem: 1378\n",
            "Train: Epoch[ 5/10]  [19/20]  eta: 0:00:00  Lr: 0.002812  Loss: 1.0988  Acc@1: 79.1667 (78.3440)  Acc@5: 95.8333 (97.4522)  time: 0.2989  data: 0.0240  max mem: 1378\n",
            "Train: Epoch[ 5/10] Total time: 0:00:06 (0.3031 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.0988  Acc@1: 79.1667 (78.3440)  Acc@5: 95.8333 (97.4522)\n",
            "Train: Epoch[ 6/10]  [ 0/20]  eta: 0:00:14  Lr: 0.002812  Loss: 0.5109  Acc@1: 87.5000 (87.5000)  Acc@5: 95.8333 (95.8333)  time: 0.7006  data: 0.4319  max mem: 1378\n",
            "Train: Epoch[ 6/10]  [10/20]  eta: 0:00:03  Lr: 0.002812  Loss: 0.4243  Acc@1: 79.1667 (79.9242)  Acc@5: 100.0000 (96.9697)  time: 0.3189  data: 0.0395  max mem: 1378\n",
            "Train: Epoch[ 6/10]  [19/20]  eta: 0:00:00  Lr: 0.002812  Loss: 0.9688  Acc@1: 79.1667 (80.4671)  Acc@5: 95.8333 (96.6030)  time: 0.2959  data: 0.0219  max mem: 1378\n",
            "Train: Epoch[ 6/10] Total time: 0:00:06 (0.3001 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.9688  Acc@1: 79.1667 (80.4671)  Acc@5: 95.8333 (96.6030)\n",
            "Train: Epoch[ 7/10]  [ 0/20]  eta: 0:00:10  Lr: 0.002812  Loss: 0.3598  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.5487  data: 0.2736  max mem: 1378\n",
            "Train: Epoch[ 7/10]  [10/20]  eta: 0:00:03  Lr: 0.002812  Loss: 0.4184  Acc@1: 83.3333 (80.6818)  Acc@5: 95.8333 (97.7273)  time: 0.3050  data: 0.0256  max mem: 1378\n",
            "Train: Epoch[ 7/10]  [19/20]  eta: 0:00:00  Lr: 0.002812  Loss: 0.8689  Acc@1: 83.3333 (80.8917)  Acc@5: 100.0000 (97.8769)  time: 0.2890  data: 0.0143  max mem: 1378\n",
            "Train: Epoch[ 7/10] Total time: 0:00:05 (0.2953 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.8689  Acc@1: 83.3333 (80.8917)  Acc@5: 100.0000 (97.8769)\n",
            "Train: Epoch[ 8/10]  [ 0/20]  eta: 0:00:16  Lr: 0.002812  Loss: 0.7437  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  time: 0.8384  data: 0.5514  max mem: 1378\n",
            "Train: Epoch[ 8/10]  [10/20]  eta: 0:00:03  Lr: 0.002812  Loss: 0.4677  Acc@1: 87.5000 (83.3333)  Acc@5: 100.0000 (97.7273)  time: 0.3309  data: 0.0504  max mem: 1378\n",
            "Train: Epoch[ 8/10]  [19/20]  eta: 0:00:00  Lr: 0.002812  Loss: 0.5193  Acc@1: 86.6667 (83.2272)  Acc@5: 100.0000 (97.2399)  time: 0.3026  data: 0.0279  max mem: 1378\n",
            "Train: Epoch[ 8/10] Total time: 0:00:06 (0.3070 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.5193  Acc@1: 86.6667 (83.2272)  Acc@5: 100.0000 (97.2399)\n",
            "Train: Epoch[ 9/10]  [ 0/20]  eta: 0:00:14  Lr: 0.002812  Loss: 0.5484  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.7052  data: 0.4217  max mem: 1378\n",
            "Train: Epoch[ 9/10]  [10/20]  eta: 0:00:03  Lr: 0.002812  Loss: 0.6860  Acc@1: 83.3333 (82.5758)  Acc@5: 100.0000 (97.3485)  time: 0.3182  data: 0.0386  max mem: 1378\n",
            "Train: Epoch[ 9/10]  [19/20]  eta: 0:00:00  Lr: 0.002812  Loss: 0.8843  Acc@1: 87.5000 (84.7134)  Acc@5: 100.0000 (98.0892)  time: 0.2962  data: 0.0213  max mem: 1378\n",
            "Train: Epoch[ 9/10] Total time: 0:00:06 (0.3043 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.8843  Acc@1: 87.5000 (84.7134)  Acc@5: 100.0000 (98.0892)\n",
            "Train: Epoch[10/10]  [ 0/20]  eta: 0:00:20  Lr: 0.002812  Loss: 0.4305  Acc@1: 91.6667 (91.6667)  Acc@5: 95.8333 (95.8333)  time: 1.0317  data: 0.6677  max mem: 1378\n",
            "Train: Epoch[10/10]  [10/20]  eta: 0:00:03  Lr: 0.002812  Loss: 0.2305  Acc@1: 87.5000 (85.2273)  Acc@5: 95.8333 (97.7273)  time: 0.3474  data: 0.0611  max mem: 1378\n",
            "Train: Epoch[10/10]  [19/20]  eta: 0:00:00  Lr: 0.002812  Loss: 0.3602  Acc@1: 87.5000 (85.5626)  Acc@5: 100.0000 (98.3015)  time: 0.3116  data: 0.0337  max mem: 1378\n",
            "Train: Epoch[10/10] Total time: 0:00:06 (0.3162 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.3602  Acc@1: 87.5000 (85.5626)  Acc@5: 100.0000 (98.3015)\n",
            "torch.Size([5, 2, 5, 6, 64])\n",
            "torch.Size([5, 2, 1, 5, 6, 64])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/23]  eta: 0:00:18  Loss: 0.7964 (0.7964)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  Acc@task: 79.1667 (79.1667)  time: 0.7936  data: 0.6003  max mem: 1378\n",
            "Test: [Task 1]  [10/23]  eta: 0:00:03  Loss: 0.7964 (0.9698)  Acc@1: 75.0000 (73.4849)  Acc@5: 91.6667 (92.8030)  Acc@task: 75.0000 (75.7576)  time: 0.2346  data: 0.0559  max mem: 1378\n",
            "Test: [Task 1]  [20/23]  eta: 0:00:00  Loss: 0.7483 (0.9338)  Acc@1: 75.0000 (76.3889)  Acc@5: 91.6667 (92.6587)  Acc@task: 75.0000 (76.7857)  time: 0.1788  data: 0.0012  max mem: 1378\n",
            "Test: [Task 1]  [22/23]  eta: 0:00:00  Loss: 0.7424 (0.9127)  Acc@1: 79.1667 (76.4925)  Acc@5: 95.8333 (92.9105)  Acc@task: 79.1667 (76.8657)  time: 0.1736  data: 0.0011  max mem: 1378\n",
            "Test: [Task 1] Total time: 0:00:04 (0.2048 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@task 76.866 Acc@1 76.493 Acc@5 92.910 loss 0.913\n",
            "Test: [Task 2]  [ 0/20]  eta: 0:00:10  Loss: 1.1069 (1.1069)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  Acc@task: 83.3333 (83.3333)  time: 0.5242  data: 0.3457  max mem: 1378\n",
            "Test: [Task 2]  [10/20]  eta: 0:00:02  Loss: 1.5239 (1.5871)  Acc@1: 66.6667 (63.6364)  Acc@5: 83.3333 (82.5758)  Acc@task: 62.5000 (63.2576)  time: 0.2099  data: 0.0322  max mem: 1378\n",
            "Test: [Task 2]  [19/20]  eta: 0:00:00  Loss: 1.5239 (1.5741)  Acc@1: 62.5000 (62.4217)  Acc@5: 83.3333 (83.7161)  Acc@task: 62.5000 (64.5094)  time: 0.1953  data: 0.0178  max mem: 1378\n",
            "Test: [Task 2] Total time: 0:00:04 (0.2026 s / it)\n",
            "* Acc@task 64.509 Acc@1 62.422 Acc@5 83.716 loss 1.574\n",
            "Test: [Task 3]  [ 0/21]  eta: 0:00:18  Loss: 1.6343 (1.6343)  Acc@1: 66.6667 (66.6667)  Acc@5: 87.5000 (87.5000)  Acc@task: 62.5000 (62.5000)  time: 0.9042  data: 0.7225  max mem: 1378\n",
            "Test: [Task 3]  [10/21]  eta: 0:00:02  Loss: 1.3666 (1.5715)  Acc@1: 62.5000 (63.2576)  Acc@5: 87.5000 (84.4697)  Acc@task: 70.8333 (72.7273)  time: 0.2442  data: 0.0666  max mem: 1378\n",
            "Test: [Task 3]  [20/21]  eta: 0:00:00  Loss: 1.8315 (1.8677)  Acc@1: 62.5000 (60.3622)  Acc@5: 79.1667 (80.4829)  Acc@task: 70.8333 (71.2274)  time: 0.1762  data: 0.0008  max mem: 1378\n",
            "Test: [Task 3] Total time: 0:00:04 (0.2156 s / it)\n",
            "* Acc@task 71.227 Acc@1 60.362 Acc@5 80.483 loss 1.868\n",
            "Test: [Task 4]  [ 0/20]  eta: 0:00:11  Loss: 3.5138 (3.5138)  Acc@1: 20.8333 (20.8333)  Acc@5: 70.8333 (70.8333)  Acc@task: 70.8333 (70.8333)  time: 0.5963  data: 0.4217  max mem: 1378\n",
            "Test: [Task 4]  [10/20]  eta: 0:00:02  Loss: 3.4779 (3.4349)  Acc@1: 20.8333 (22.3485)  Acc@5: 66.6667 (65.9091)  Acc@task: 66.6667 (63.2576)  time: 0.2162  data: 0.0399  max mem: 1378\n",
            "Test: [Task 4]  [19/20]  eta: 0:00:00  Loss: 3.4779 (3.4177)  Acc@1: 20.8333 (21.8684)  Acc@5: 66.6667 (66.2420)  Acc@task: 66.6667 (64.7558)  time: 0.1966  data: 0.0220  max mem: 1378\n",
            "Test: [Task 4] Total time: 0:00:04 (0.2012 s / it)\n",
            "* Acc@task 64.756 Acc@1 21.868 Acc@5 66.242 loss 3.418\n",
            "[Average accuracy till task4]\tAcc@task: 69.3396\tAcc@1: 55.2862\tAcc@5: 80.8379\tLoss: 1.9431\tForgetting: 1.2794\tBackward: 45.1646\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:23  Loss: 0.1839 (0.1839)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  Acc@task: 87.5000 (87.5000)  time: 0.5611  data: 0.3878  max mem: 1378\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.5593 (0.6616)  Acc@1: 83.3333 (81.8182)  Acc@5: 100.0000 (97.7273)  Acc@task: 83.3333 (82.1970)  time: 0.2140  data: 0.0355  max mem: 1378\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.5593 (0.6138)  Acc@1: 83.3333 (82.9365)  Acc@5: 100.0000 (97.4206)  Acc@task: 87.5000 (83.3333)  time: 0.1795  data: 0.0004  max mem: 1378\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.4722 (0.5745)  Acc@1: 83.3333 (84.4086)  Acc@5: 100.0000 (97.8495)  Acc@task: 87.5000 (83.4677)  time: 0.1805  data: 0.0005  max mem: 1378\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.4768 (0.5547)  Acc@1: 87.5000 (85.0610)  Acc@5: 100.0000 (98.1707)  Acc@task: 87.5000 (85.1626)  time: 0.1809  data: 0.0004  max mem: 1378\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.3899 (0.5463)  Acc@1: 87.5000 (85.2000)  Acc@5: 100.0000 (98.2000)  Acc@task: 91.6667 (85.4000)  time: 0.1776  data: 0.0004  max mem: 1378\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1905 s / it)\n",
            "* Acc@task 85.400 Acc@1 85.200 Acc@5 98.200 loss 0.546\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:23  Loss: 0.4293 (0.4293)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  Acc@task: 79.1667 (79.1667)  time: 0.5495  data: 0.3749  max mem: 1378\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.4620 (0.5957)  Acc@1: 87.5000 (86.3636)  Acc@5: 100.0000 (96.9697)  Acc@task: 79.1667 (78.7879)  time: 0.2146  data: 0.0345  max mem: 1378\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.5598 (0.6302)  Acc@1: 83.3333 (85.1191)  Acc@5: 95.8333 (96.0317)  Acc@task: 79.1667 (80.5556)  time: 0.1803  data: 0.0011  max mem: 1378\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.5478 (0.5943)  Acc@1: 83.3333 (85.7527)  Acc@5: 95.8333 (96.2366)  Acc@task: 83.3333 (81.0484)  time: 0.1805  data: 0.0013  max mem: 1378\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.5026 (0.5687)  Acc@1: 87.5000 (85.9756)  Acc@5: 95.8333 (96.6463)  Acc@task: 83.3333 (81.3008)  time: 0.1819  data: 0.0005  max mem: 1378\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.4297 (0.5617)  Acc@1: 87.5000 (86.1000)  Acc@5: 95.8333 (96.7000)  Acc@task: 83.3333 (81.3000)  time: 0.1783  data: 0.0005  max mem: 1378\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1904 s / it)\n",
            "* Acc@task 81.300 Acc@1 86.100 Acc@5 96.700 loss 0.562\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:21  Loss: 0.1858 (0.1858)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  Acc@task: 91.6667 (91.6667)  time: 0.5096  data: 0.3040  max mem: 1378\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 0.4712 (0.6053)  Acc@1: 87.5000 (83.3333)  Acc@5: 100.0000 (98.1061)  Acc@task: 79.1667 (79.9242)  time: 0.2092  data: 0.0281  max mem: 1378\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.4485 (0.5491)  Acc@1: 87.5000 (85.3175)  Acc@5: 100.0000 (97.6190)  Acc@task: 75.0000 (78.9683)  time: 0.1801  data: 0.0006  max mem: 1378\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.4740 (0.5449)  Acc@1: 87.5000 (85.2151)  Acc@5: 95.8333 (97.5806)  Acc@task: 75.0000 (79.3011)  time: 0.1814  data: 0.0005  max mem: 1378\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.4979 (0.5550)  Acc@1: 83.3333 (85.2642)  Acc@5: 95.8333 (97.2561)  Acc@task: 83.3333 (79.8781)  time: 0.1821  data: 0.0003  max mem: 1378\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.4979 (0.5544)  Acc@1: 83.3333 (85.1000)  Acc@5: 95.8333 (97.2000)  Acc@task: 83.3333 (79.8000)  time: 0.1788  data: 0.0003  max mem: 1378\n",
            "Test: [Task 3] Total time: 0:00:08 (0.1910 s / it)\n",
            "* Acc@task 79.800 Acc@1 85.100 Acc@5 97.200 loss 0.554\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:35  Loss: 3.2917 (3.2917)  Acc@1: 25.0000 (25.0000)  Acc@5: 62.5000 (62.5000)  Acc@task: 70.8333 (70.8333)  time: 0.8525  data: 0.6417  max mem: 1378\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:07  Loss: 3.0800 (3.0770)  Acc@1: 25.0000 (26.8939)  Acc@5: 79.1667 (78.4091)  Acc@task: 75.0000 (75.0000)  time: 0.2416  data: 0.0603  max mem: 1378\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 3.0652 (3.1209)  Acc@1: 25.0000 (27.1825)  Acc@5: 79.1667 (78.9683)  Acc@task: 75.0000 (75.5952)  time: 0.1806  data: 0.0012  max mem: 1378\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 2.9338 (2.9942)  Acc@1: 29.1667 (29.5699)  Acc@5: 83.3333 (81.3172)  Acc@task: 75.0000 (75.4032)  time: 0.1808  data: 0.0003  max mem: 1378\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 2.9338 (3.0190)  Acc@1: 29.1667 (28.9634)  Acc@5: 83.3333 (80.4878)  Acc@task: 70.8333 (75.2033)  time: 0.1816  data: 0.0002  max mem: 1378\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 2.9338 (3.0019)  Acc@1: 29.1667 (29.1000)  Acc@5: 83.3333 (80.7000)  Acc@task: 75.0000 (75.5000)  time: 0.1783  data: 0.0002  max mem: 1378\n",
            "Test: [Task 4] Total time: 0:00:08 (0.1977 s / it)\n",
            "* Acc@task 75.500 Acc@1 29.100 Acc@5 80.700 loss 3.002\n",
            "[Average accuracy till task4]\tAcc@task: 80.5000\tAcc@1: 71.3750\tAcc@5: 93.2000\tLoss: 1.1661\tForgetting: 0.2333\tBackward: 57.2667\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:18  Loss: 0.1839 (0.1839)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  Acc@task: 87.5000 (87.5000)  time: 0.4402  data: 0.2696  max mem: 1378\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.5593 (0.6616)  Acc@1: 83.3333 (81.8182)  Acc@5: 100.0000 (97.7273)  Acc@task: 83.3333 (82.1970)  time: 0.2047  data: 0.0248  max mem: 1378\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.5593 (0.6138)  Acc@1: 83.3333 (82.9365)  Acc@5: 100.0000 (97.4206)  Acc@task: 87.5000 (83.3333)  time: 0.1803  data: 0.0003  max mem: 1378\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.4722 (0.5745)  Acc@1: 83.3333 (84.4086)  Acc@5: 100.0000 (97.8495)  Acc@task: 87.5000 (83.4677)  time: 0.1801  data: 0.0004  max mem: 1378\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.4768 (0.5547)  Acc@1: 87.5000 (85.0610)  Acc@5: 100.0000 (98.1707)  Acc@task: 87.5000 (85.1626)  time: 0.1809  data: 0.0004  max mem: 1378\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.3899 (0.5463)  Acc@1: 87.5000 (85.2000)  Acc@5: 100.0000 (98.2000)  Acc@task: 91.6667 (85.4000)  time: 0.1771  data: 0.0004  max mem: 1378\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1874 s / it)\n",
            "* Acc@task 85.400 Acc@1 85.200 Acc@5 98.200 loss 0.546\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:23  Loss: 0.4293 (0.4293)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  Acc@task: 79.1667 (79.1667)  time: 0.5644  data: 0.3744  max mem: 1378\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.4620 (0.5957)  Acc@1: 87.5000 (86.3636)  Acc@5: 100.0000 (96.9697)  Acc@task: 79.1667 (78.7879)  time: 0.2159  data: 0.0344  max mem: 1378\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.5598 (0.6302)  Acc@1: 83.3333 (85.1191)  Acc@5: 95.8333 (96.0317)  Acc@task: 79.1667 (80.5556)  time: 0.1799  data: 0.0003  max mem: 1378\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.5478 (0.5943)  Acc@1: 83.3333 (85.7527)  Acc@5: 95.8333 (96.2366)  Acc@task: 83.3333 (81.0484)  time: 0.1794  data: 0.0004  max mem: 1378\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.5026 (0.5687)  Acc@1: 87.5000 (85.9756)  Acc@5: 95.8333 (96.6463)  Acc@task: 83.3333 (81.3008)  time: 0.1803  data: 0.0003  max mem: 1378\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.4297 (0.5617)  Acc@1: 87.5000 (86.1000)  Acc@5: 95.8333 (96.7000)  Acc@task: 83.3333 (81.3000)  time: 0.1769  data: 0.0003  max mem: 1378\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1899 s / it)\n",
            "* Acc@task 81.300 Acc@1 86.100 Acc@5 96.700 loss 0.562\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:24  Loss: 0.1858 (0.1858)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  Acc@task: 91.6667 (91.6667)  time: 0.5912  data: 0.4137  max mem: 1378\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 0.4712 (0.6053)  Acc@1: 87.5000 (83.3333)  Acc@5: 100.0000 (98.1061)  Acc@task: 79.1667 (79.9242)  time: 0.2174  data: 0.0380  max mem: 1378\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.4485 (0.5491)  Acc@1: 87.5000 (85.3175)  Acc@5: 100.0000 (97.6190)  Acc@task: 75.0000 (78.9683)  time: 0.1791  data: 0.0004  max mem: 1378\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.4740 (0.5449)  Acc@1: 87.5000 (85.2151)  Acc@5: 95.8333 (97.5806)  Acc@task: 75.0000 (79.3011)  time: 0.1794  data: 0.0005  max mem: 1378\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.4979 (0.5550)  Acc@1: 83.3333 (85.2642)  Acc@5: 95.8333 (97.2561)  Acc@task: 83.3333 (79.8781)  time: 0.1811  data: 0.0006  max mem: 1378\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.4979 (0.5544)  Acc@1: 83.3333 (85.1000)  Acc@5: 95.8333 (97.2000)  Acc@task: 83.3333 (79.8000)  time: 0.1774  data: 0.0005  max mem: 1378\n",
            "Test: [Task 3] Total time: 0:00:08 (0.1905 s / it)\n",
            "* Acc@task 79.800 Acc@1 85.100 Acc@5 97.200 loss 0.554\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:19  Loss: 3.2917 (3.2917)  Acc@1: 25.0000 (25.0000)  Acc@5: 62.5000 (62.5000)  Acc@task: 70.8333 (70.8333)  time: 0.4545  data: 0.2897  max mem: 1378\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:06  Loss: 3.0800 (3.0770)  Acc@1: 25.0000 (26.8939)  Acc@5: 79.1667 (78.4091)  Acc@task: 75.0000 (75.0000)  time: 0.2049  data: 0.0271  max mem: 1378\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 3.0652 (3.1209)  Acc@1: 25.0000 (27.1825)  Acc@5: 79.1667 (78.9683)  Acc@task: 75.0000 (75.5952)  time: 0.1793  data: 0.0006  max mem: 1378\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 2.9338 (2.9942)  Acc@1: 29.1667 (29.5699)  Acc@5: 83.3333 (81.3172)  Acc@task: 75.0000 (75.4032)  time: 0.1790  data: 0.0003  max mem: 1378\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 2.9338 (3.0190)  Acc@1: 29.1667 (28.9634)  Acc@5: 83.3333 (80.4878)  Acc@task: 70.8333 (75.2033)  time: 0.1800  data: 0.0002  max mem: 1378\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 2.9338 (3.0019)  Acc@1: 29.1667 (29.1000)  Acc@5: 83.3333 (80.7000)  Acc@task: 75.0000 (75.5000)  time: 0.1764  data: 0.0002  max mem: 1378\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1878 s / it)\n",
            "* Acc@task 75.500 Acc@1 29.100 Acc@5 80.700 loss 3.002\n",
            "[Average accuracy till task4]\tAcc@task: 80.5000\tAcc@1: 71.3750\tAcc@5: 93.2000\tLoss: 1.1661\tForgetting: 1.5000\tBackward: 53.4667\n",
            "torch.Size([42120, 384])\n",
            "Averaged stats: Lr: 0.005000  Loss: 0.0785  Acc@1: 97.5000 (95.6667)  Acc@5: 100.0000 (99.6389)\n",
            "torch.Size([42120, 384])\n",
            "Averaged stats: Lr: 0.004878  Loss: 0.0185  Acc@1: 99.1667 (99.3056)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([42120, 384])\n",
            "Averaged stats: Lr: 0.004523  Loss: 0.0104  Acc@1: 100.0000 (99.6667)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([42120, 384])\n",
            "Averaged stats: Lr: 0.003969  Loss: 0.0229  Acc@1: 100.0000 (99.8056)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([42120, 384])\n",
            "Averaged stats: Lr: 0.003273  Loss: 0.0221  Acc@1: 100.0000 (99.6111)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([42120, 384])\n",
            "Averaged stats: Lr: 0.002500  Loss: 0.0171  Acc@1: 100.0000 (99.7222)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([42120, 384])\n",
            "Averaged stats: Lr: 0.001727  Loss: 0.0156  Acc@1: 100.0000 (99.8889)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([42120, 384])\n",
            "Averaged stats: Lr: 0.001031  Loss: 0.0122  Acc@1: 100.0000 (99.8611)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([42120, 384])\n",
            "Averaged stats: Lr: 0.000477  Loss: 0.0130  Acc@1: 100.0000 (99.7778)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([42120, 384])\n",
            "Averaged stats: Lr: 0.000122  Loss: 0.0070  Acc@1: 100.0000 (99.8333)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [Task 1]  [ 0/23]  eta: 0:00:12  Loss: 0.9923 (0.9923)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  Acc@task: 75.0000 (75.0000)  time: 0.5295  data: 0.3609  max mem: 1378\n",
            "Test: [Task 1]  [10/23]  eta: 0:00:02  Loss: 0.9990 (1.0959)  Acc@1: 70.8333 (71.5909)  Acc@5: 91.6667 (91.2879)  Acc@task: 75.0000 (76.8939)  time: 0.2082  data: 0.0331  max mem: 1378\n",
            "Test: [Task 1]  [20/23]  eta: 0:00:00  Loss: 0.9156 (1.0105)  Acc@1: 75.0000 (73.8095)  Acc@5: 91.6667 (92.4603)  Acc@task: 75.0000 (75.0000)  time: 0.1765  data: 0.0003  max mem: 1378\n",
            "Test: [Task 1]  [22/23]  eta: 0:00:00  Loss: 0.9689 (1.0343)  Acc@1: 75.0000 (73.6940)  Acc@5: 91.6667 (92.5373)  Acc@task: 70.8333 (75.0000)  time: 0.1720  data: 0.0003  max mem: 1378\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1934 s / it)\n",
            "* Acc@task 75.000 Acc@1 73.694 Acc@5 92.537 loss 1.034\n",
            "Test: [Task 2]  [ 0/20]  eta: 0:00:17  Loss: 1.0482 (1.0482)  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.8333)  Acc@task: 66.6667 (66.6667)  time: 0.8720  data: 0.6903  max mem: 1378\n",
            "Test: [Task 2]  [10/20]  eta: 0:00:02  Loss: 1.7997 (1.6571)  Acc@1: 58.3333 (60.6061)  Acc@5: 83.3333 (85.2273)  Acc@task: 62.5000 (62.1212)  time: 0.2409  data: 0.0639  max mem: 1378\n",
            "Test: [Task 2]  [19/20]  eta: 0:00:00  Loss: 1.7997 (1.6645)  Acc@1: 58.3333 (62.2129)  Acc@5: 83.3333 (84.7599)  Acc@task: 58.3333 (62.0042)  time: 0.2129  data: 0.0352  max mem: 1378\n",
            "Test: [Task 2] Total time: 0:00:04 (0.2174 s / it)\n",
            "* Acc@task 62.004 Acc@1 62.213 Acc@5 84.760 loss 1.664\n",
            "Test: [Task 3]  [ 0/21]  eta: 0:00:13  Loss: 2.3462 (2.3462)  Acc@1: 54.1667 (54.1667)  Acc@5: 83.3333 (83.3333)  Acc@task: 62.5000 (62.5000)  time: 0.6378  data: 0.4582  max mem: 1378\n",
            "Test: [Task 3]  [10/21]  eta: 0:00:02  Loss: 2.1467 (2.1204)  Acc@1: 62.5000 (60.9849)  Acc@5: 79.1667 (79.5455)  Acc@task: 66.6667 (66.2879)  time: 0.2195  data: 0.0421  max mem: 1378\n",
            "Test: [Task 3]  [20/21]  eta: 0:00:00  Loss: 1.9364 (1.9388)  Acc@1: 62.5000 (61.5694)  Acc@5: 83.3333 (82.4950)  Acc@task: 70.8333 (69.4165)  time: 0.1754  data: 0.0003  max mem: 1378\n",
            "Test: [Task 3] Total time: 0:00:04 (0.2015 s / it)\n",
            "* Acc@task 69.417 Acc@1 61.569 Acc@5 82.495 loss 1.939\n",
            "Test: [Task 4]  [ 0/20]  eta: 0:00:09  Loss: 2.6759 (2.6759)  Acc@1: 50.0000 (50.0000)  Acc@5: 79.1667 (79.1667)  Acc@task: 45.8333 (45.8333)  time: 0.4661  data: 0.2992  max mem: 1378\n",
            "Test: [Task 4]  [10/20]  eta: 0:00:02  Loss: 1.8573 (1.8430)  Acc@1: 58.3333 (60.2273)  Acc@5: 83.3333 (84.4697)  Acc@task: 66.6667 (63.2576)  time: 0.2078  data: 0.0322  max mem: 1378\n",
            "Test: [Task 4]  [19/20]  eta: 0:00:00  Loss: 1.5745 (1.7248)  Acc@1: 58.3333 (60.9342)  Acc@5: 86.6667 (85.3503)  Acc@task: 66.6667 (65.3928)  time: 0.1927  data: 0.0178  max mem: 1378\n",
            "Test: [Task 4] Total time: 0:00:03 (0.1973 s / it)\n",
            "* Acc@task 65.393 Acc@1 60.934 Acc@5 85.350 loss 1.725\n",
            "[Average accuracy till task4]\tAcc@task: 67.9534\tAcc@1: 64.6026\tAcc@5: 86.2856\tLoss: 1.5906\tForgetting: 0.0000\tBackward: 65.8255\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:26  Loss: 0.3021 (0.3021)  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)  Acc@task: 87.5000 (87.5000)  time: 0.6381  data: 0.4648  max mem: 1378\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:07  Loss: 0.6044 (0.7726)  Acc@1: 75.0000 (78.0303)  Acc@5: 100.0000 (97.3485)  Acc@task: 83.3333 (82.1970)  time: 0.2218  data: 0.0435  max mem: 1378\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.6044 (0.7115)  Acc@1: 79.1667 (80.5556)  Acc@5: 95.8333 (97.0238)  Acc@task: 87.5000 (83.3333)  time: 0.1798  data: 0.0014  max mem: 1378\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.5267 (0.6566)  Acc@1: 83.3333 (82.1237)  Acc@5: 100.0000 (97.5806)  Acc@task: 87.5000 (83.4677)  time: 0.1798  data: 0.0008  max mem: 1378\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.4950 (0.6236)  Acc@1: 83.3333 (83.0285)  Acc@5: 100.0000 (98.0691)  Acc@task: 87.5000 (85.1626)  time: 0.1808  data: 0.0002  max mem: 1378\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.4146 (0.6132)  Acc@1: 91.6667 (83.2000)  Acc@5: 100.0000 (98.1000)  Acc@task: 91.6667 (85.4000)  time: 0.1774  data: 0.0002  max mem: 1378\n",
            "Test: [Task 1] Total time: 0:00:08 (0.1921 s / it)\n",
            "* Acc@task 85.400 Acc@1 83.200 Acc@5 98.100 loss 0.613\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:19  Loss: 0.7412 (0.7412)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  Acc@task: 79.1667 (79.1667)  time: 0.4560  data: 0.2934  max mem: 1378\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.7294 (0.7544)  Acc@1: 83.3333 (82.1970)  Acc@5: 95.8333 (95.8333)  Acc@task: 79.1667 (78.7879)  time: 0.2100  data: 0.0330  max mem: 1378\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.5980 (0.7959)  Acc@1: 83.3333 (81.1508)  Acc@5: 95.8333 (95.6349)  Acc@task: 79.1667 (80.5556)  time: 0.1832  data: 0.0036  max mem: 1378\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.5980 (0.7524)  Acc@1: 79.1667 (81.5860)  Acc@5: 95.8333 (95.9677)  Acc@task: 83.3333 (81.0484)  time: 0.1812  data: 0.0003  max mem: 1378\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.5804 (0.7111)  Acc@1: 83.3333 (82.3171)  Acc@5: 95.8333 (96.4431)  Acc@task: 83.3333 (81.3008)  time: 0.1820  data: 0.0002  max mem: 1378\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.5407 (0.7020)  Acc@1: 83.3333 (82.4000)  Acc@5: 95.8333 (96.5000)  Acc@task: 83.3333 (81.3000)  time: 0.1785  data: 0.0002  max mem: 1378\n",
            "Test: [Task 2] Total time: 0:00:08 (0.1916 s / it)\n",
            "* Acc@task 81.300 Acc@1 82.400 Acc@5 96.500 loss 0.702\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:30  Loss: 0.1469 (0.1469)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  Acc@task: 91.6667 (91.6667)  time: 0.7364  data: 0.5494  max mem: 1378\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:07  Loss: 0.5604 (0.6661)  Acc@1: 83.3333 (82.5758)  Acc@5: 95.8333 (97.7273)  Acc@task: 79.1667 (79.9242)  time: 0.2308  data: 0.0503  max mem: 1378\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.5413 (0.6269)  Acc@1: 83.3333 (83.7302)  Acc@5: 95.8333 (97.4206)  Acc@task: 75.0000 (78.9683)  time: 0.1801  data: 0.0004  max mem: 1378\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.5288 (0.6145)  Acc@1: 83.3333 (84.0054)  Acc@5: 95.8333 (97.4462)  Acc@task: 75.0000 (79.3011)  time: 0.1807  data: 0.0003  max mem: 1378\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.6004 (0.6229)  Acc@1: 83.3333 (84.2480)  Acc@5: 95.8333 (97.0528)  Acc@task: 83.3333 (79.8781)  time: 0.1815  data: 0.0002  max mem: 1378\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.5962 (0.6198)  Acc@1: 83.3333 (84.1000)  Acc@5: 95.8333 (97.0000)  Acc@task: 83.3333 (79.8000)  time: 0.1780  data: 0.0002  max mem: 1378\n",
            "Test: [Task 3] Total time: 0:00:08 (0.1949 s / it)\n",
            "* Acc@task 79.800 Acc@1 84.100 Acc@5 97.000 loss 0.620\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:18  Loss: 0.6295 (0.6295)  Acc@1: 83.3333 (83.3333)  Acc@5: 91.6667 (91.6667)  Acc@task: 70.8333 (70.8333)  time: 0.4416  data: 0.2612  max mem: 1378\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:06  Loss: 0.6057 (0.6064)  Acc@1: 83.3333 (83.7121)  Acc@5: 95.8333 (96.5909)  Acc@task: 75.0000 (75.0000)  time: 0.2055  data: 0.0265  max mem: 1378\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.5846 (0.6384)  Acc@1: 83.3333 (83.1349)  Acc@5: 95.8333 (97.0238)  Acc@task: 75.0000 (75.5952)  time: 0.1809  data: 0.0019  max mem: 1378\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.4821 (0.6110)  Acc@1: 83.3333 (83.8710)  Acc@5: 100.0000 (97.1774)  Acc@task: 75.0000 (75.4032)  time: 0.1805  data: 0.0012  max mem: 1378\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.5256 (0.6414)  Acc@1: 83.3333 (82.9268)  Acc@5: 95.8333 (96.3415)  Acc@task: 70.8333 (75.2033)  time: 0.1814  data: 0.0010  max mem: 1378\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.4335 (0.6270)  Acc@1: 83.3333 (83.2000)  Acc@5: 95.8333 (96.4000)  Acc@task: 75.0000 (75.5000)  time: 0.1779  data: 0.0010  max mem: 1378\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1882 s / it)\n",
            "* Acc@task 75.500 Acc@1 83.200 Acc@5 96.400 loss 0.627\n",
            "[Average accuracy till task4]\tAcc@task: 80.5000\tAcc@1: 83.2250\tAcc@5: 97.0000\tLoss: 0.6405\tForgetting: 0.0000\tBackward: 83.2333\n",
            "Loading checkpoint from: ./output/cifar100_full_dino_5epoch_10pct/checkpoint/task5_checkpoint.pth\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/engines/hide_promtp_wtp_and_tap_engine.py:574: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  loss = torch.nn.functional.cross_entropy(sim, torch.range(0, sim.shape[0] - 1).long().to(device))\n",
            "Train: Epoch[ 1/10]  [ 0/21]  eta: 0:00:16  Lr: 0.002812  Loss: 3.0590  Acc@1: 12.5000 (12.5000)  Acc@5: 58.3333 (58.3333)  time: 0.7656  data: 0.4551  max mem: 1378\n",
            "Train: Epoch[ 1/10]  [10/21]  eta: 0:00:03  Lr: 0.002812  Loss: 2.1568  Acc@1: 20.8333 (19.6970)  Acc@5: 58.3333 (60.9849)  time: 0.3249  data: 0.0418  max mem: 1378\n",
            "Train: Epoch[ 1/10]  [20/21]  eta: 0:00:00  Lr: 0.002812  Loss: 3.1619  Acc@1: 20.8333 (21.9959)  Acc@5: 62.5000 (64.5621)  time: 0.2730  data: 0.0003  max mem: 1378\n",
            "Train: Epoch[ 1/10] Total time: 0:00:06 (0.3007 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 3.1619  Acc@1: 20.8333 (21.9959)  Acc@5: 62.5000 (64.5621)\n",
            "Train: Epoch[ 2/10]  [ 0/21]  eta: 0:00:14  Lr: 0.002812  Loss: 2.2014  Acc@1: 20.8333 (20.8333)  Acc@5: 83.3333 (83.3333)  time: 0.7129  data: 0.4415  max mem: 1378\n",
            "Train: Epoch[ 2/10]  [10/21]  eta: 0:00:03  Lr: 0.002812  Loss: 1.9300  Acc@1: 41.6667 (39.7727)  Acc@5: 83.3333 (83.7121)  time: 0.3195  data: 0.0410  max mem: 1378\n",
            "Train: Epoch[ 2/10]  [20/21]  eta: 0:00:00  Lr: 0.002812  Loss: 0.6675  Acc@1: 45.8333 (43.7882)  Acc@5: 87.5000 (87.9837)  time: 0.2740  data: 0.0007  max mem: 1378\n",
            "Train: Epoch[ 2/10] Total time: 0:00:06 (0.2993 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.6675  Acc@1: 45.8333 (43.7882)  Acc@5: 87.5000 (87.9837)\n",
            "Train: Epoch[ 3/10]  [ 0/21]  eta: 0:00:14  Lr: 0.002812  Loss: 0.9846  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  time: 0.6843  data: 0.4191  max mem: 1378\n",
            "Train: Epoch[ 3/10]  [10/21]  eta: 0:00:03  Lr: 0.002812  Loss: 0.8506  Acc@1: 66.6667 (64.7727)  Acc@5: 95.8333 (93.1818)  time: 0.3172  data: 0.0385  max mem: 1378\n",
            "Train: Epoch[ 3/10]  [20/21]  eta: 0:00:00  Lr: 0.002812  Loss: 1.2606  Acc@1: 62.5000 (63.9511)  Acc@5: 95.8333 (93.8900)  time: 0.2724  data: 0.0003  max mem: 1378\n",
            "Train: Epoch[ 3/10] Total time: 0:00:06 (0.2962 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.2606  Acc@1: 62.5000 (63.9511)  Acc@5: 95.8333 (93.8900)\n",
            "Train: Epoch[ 4/10]  [ 0/21]  eta: 0:00:12  Lr: 0.002812  Loss: 0.6576  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.6143  data: 0.3379  max mem: 1378\n",
            "Train: Epoch[ 4/10]  [10/21]  eta: 0:00:03  Lr: 0.002812  Loss: 1.0170  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (96.5909)  time: 0.3107  data: 0.0316  max mem: 1378\n",
            "Train: Epoch[ 4/10]  [20/21]  eta: 0:00:00  Lr: 0.002812  Loss: 1.2269  Acc@1: 75.0000 (73.5234)  Acc@5: 95.8333 (97.3523)  time: 0.2728  data: 0.0009  max mem: 1378\n",
            "Train: Epoch[ 4/10] Total time: 0:00:06 (0.2932 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.2269  Acc@1: 75.0000 (73.5234)  Acc@5: 95.8333 (97.3523)\n",
            "Train: Epoch[ 5/10]  [ 0/21]  eta: 0:00:12  Lr: 0.002812  Loss: 0.9562  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  time: 0.6103  data: 0.3336  max mem: 1378\n",
            "Train: Epoch[ 5/10]  [10/21]  eta: 0:00:03  Lr: 0.002812  Loss: 0.5629  Acc@1: 75.0000 (74.2424)  Acc@5: 100.0000 (97.3485)  time: 0.3100  data: 0.0306  max mem: 1378\n",
            "Train: Epoch[ 5/10]  [20/21]  eta: 0:00:00  Lr: 0.002812  Loss: 0.6237  Acc@1: 75.0000 (74.9491)  Acc@5: 100.0000 (96.9450)  time: 0.2725  data: 0.0002  max mem: 1378\n",
            "Train: Epoch[ 5/10] Total time: 0:00:06 (0.2929 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.6237  Acc@1: 75.0000 (74.9491)  Acc@5: 100.0000 (96.9450)\n",
            "Train: Epoch[ 6/10]  [ 0/21]  eta: 0:00:15  Lr: 0.002812  Loss: 0.6160  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  time: 0.7400  data: 0.4733  max mem: 1378\n",
            "Train: Epoch[ 6/10]  [10/21]  eta: 0:00:03  Lr: 0.002812  Loss: 1.2263  Acc@1: 79.1667 (77.2727)  Acc@5: 100.0000 (98.1061)  time: 0.3222  data: 0.0435  max mem: 1378\n",
            "Train: Epoch[ 6/10]  [20/21]  eta: 0:00:00  Lr: 0.002812  Loss: 0.1534  Acc@1: 79.1667 (78.6151)  Acc@5: 100.0000 (97.7597)  time: 0.2726  data: 0.0005  max mem: 1378\n",
            "Train: Epoch[ 6/10] Total time: 0:00:06 (0.2991 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.1534  Acc@1: 79.1667 (78.6151)  Acc@5: 100.0000 (97.7597)\n",
            "Train: Epoch[ 7/10]  [ 0/21]  eta: 0:00:13  Lr: 0.002812  Loss: 0.5805  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.6553  data: 0.3756  max mem: 1378\n",
            "Train: Epoch[ 7/10]  [10/21]  eta: 0:00:03  Lr: 0.002812  Loss: 0.6697  Acc@1: 79.1667 (81.0606)  Acc@5: 95.8333 (97.3485)  time: 0.3130  data: 0.0346  max mem: 1378\n",
            "Train: Epoch[ 7/10]  [20/21]  eta: 0:00:00  Lr: 0.002812  Loss: 0.3807  Acc@1: 79.1667 (81.4664)  Acc@5: 95.8333 (97.5560)  time: 0.2715  data: 0.0003  max mem: 1378\n",
            "Train: Epoch[ 7/10] Total time: 0:00:06 (0.2941 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.3807  Acc@1: 79.1667 (81.4664)  Acc@5: 95.8333 (97.5560)\n",
            "Train: Epoch[ 8/10]  [ 0/21]  eta: 0:00:15  Lr: 0.002812  Loss: 0.4584  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)  time: 0.7461  data: 0.4710  max mem: 1378\n",
            "Train: Epoch[ 8/10]  [10/21]  eta: 0:00:03  Lr: 0.002812  Loss: 0.6458  Acc@1: 87.5000 (85.2273)  Acc@5: 100.0000 (98.1061)  time: 0.3207  data: 0.0431  max mem: 1378\n",
            "Train: Epoch[ 8/10]  [20/21]  eta: 0:00:00  Lr: 0.002812  Loss: 1.0543  Acc@1: 83.3333 (84.1141)  Acc@5: 95.8333 (97.7597)  time: 0.2718  data: 0.0003  max mem: 1378\n",
            "Train: Epoch[ 8/10] Total time: 0:00:06 (0.3002 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.0543  Acc@1: 83.3333 (84.1141)  Acc@5: 95.8333 (97.7597)\n",
            "Train: Epoch[ 9/10]  [ 0/21]  eta: 0:00:16  Lr: 0.002812  Loss: 0.4965  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  time: 0.8032  data: 0.5399  max mem: 1378\n",
            "Train: Epoch[ 9/10]  [10/21]  eta: 0:00:03  Lr: 0.002812  Loss: 0.2898  Acc@1: 87.5000 (86.3636)  Acc@5: 100.0000 (98.1061)  time: 0.3262  data: 0.0493  max mem: 1378\n",
            "Train: Epoch[ 9/10]  [20/21]  eta: 0:00:00  Lr: 0.002812  Loss: 0.6369  Acc@1: 87.5000 (85.9470)  Acc@5: 100.0000 (98.5743)  time: 0.2710  data: 0.0002  max mem: 1378\n",
            "Train: Epoch[ 9/10] Total time: 0:00:06 (0.3007 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.6369  Acc@1: 87.5000 (85.9470)  Acc@5: 100.0000 (98.5743)\n",
            "Train: Epoch[10/10]  [ 0/21]  eta: 0:00:13  Lr: 0.002812  Loss: 0.5320  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.6474  data: 0.3541  max mem: 1378\n",
            "Train: Epoch[10/10]  [10/21]  eta: 0:00:03  Lr: 0.002812  Loss: 0.5888  Acc@1: 83.3333 (85.6061)  Acc@5: 100.0000 (99.6212)  time: 0.3134  data: 0.0325  max mem: 1378\n",
            "Train: Epoch[10/10]  [20/21]  eta: 0:00:00  Lr: 0.002812  Loss: 1.1449  Acc@1: 83.3333 (84.5214)  Acc@5: 100.0000 (99.7963)  time: 0.2724  data: 0.0003  max mem: 1378\n",
            "Train: Epoch[10/10] Total time: 0:00:06 (0.2983 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.1449  Acc@1: 83.3333 (84.5214)  Acc@5: 100.0000 (99.7963)\n",
            "torch.Size([5, 2, 5, 6, 64])\n",
            "torch.Size([5, 2, 1, 5, 6, 64])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/23]  eta: 0:00:11  Loss: 0.8483 (0.8483)  Acc@1: 83.3333 (83.3333)  Acc@5: 91.6667 (91.6667)  Acc@task: 79.1667 (79.1667)  time: 0.4968  data: 0.3233  max mem: 1378\n",
            "Test: [Task 1]  [10/23]  eta: 0:00:02  Loss: 0.9533 (0.9782)  Acc@1: 75.0000 (77.2727)  Acc@5: 91.6667 (92.8030)  Acc@task: 75.0000 (73.1061)  time: 0.2083  data: 0.0330  max mem: 1378\n",
            "Test: [Task 1]  [20/23]  eta: 0:00:00  Loss: 0.9185 (0.9756)  Acc@1: 75.0000 (74.4048)  Acc@5: 95.8333 (93.2540)  Acc@task: 70.8333 (71.0317)  time: 0.1792  data: 0.0021  max mem: 1378\n",
            "Test: [Task 1]  [22/23]  eta: 0:00:00  Loss: 0.9356 (0.9932)  Acc@1: 75.0000 (74.4403)  Acc@5: 95.8333 (93.4702)  Acc@task: 75.0000 (71.4552)  time: 0.1727  data: 0.0003  max mem: 1378\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1922 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@task 71.455 Acc@1 74.440 Acc@5 93.470 loss 0.993\n",
            "Test: [Task 2]  [ 0/20]  eta: 0:00:12  Loss: 0.8300 (0.8300)  Acc@1: 83.3333 (83.3333)  Acc@5: 91.6667 (91.6667)  Acc@task: 66.6667 (66.6667)  time: 0.6077  data: 0.4415  max mem: 1378\n",
            "Test: [Task 2]  [10/20]  eta: 0:00:02  Loss: 1.6252 (1.5052)  Acc@1: 66.6667 (68.5606)  Acc@5: 83.3333 (85.9849)  Acc@task: 62.5000 (63.2576)  time: 0.2179  data: 0.0407  max mem: 1378\n",
            "Test: [Task 2]  [19/20]  eta: 0:00:00  Loss: 1.5479 (1.4965)  Acc@1: 66.6667 (66.8058)  Acc@5: 83.3333 (84.5512)  Acc@task: 58.3333 (61.5866)  time: 0.1998  data: 0.0225  max mem: 1378\n",
            "Test: [Task 2] Total time: 0:00:04 (0.2067 s / it)\n",
            "* Acc@task 61.587 Acc@1 66.806 Acc@5 84.551 loss 1.496\n",
            "Test: [Task 3]  [ 0/21]  eta: 0:00:16  Loss: 0.9898 (0.9898)  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  Acc@task: 75.0000 (75.0000)  time: 0.7792  data: 0.6116  max mem: 1378\n",
            "Test: [Task 3]  [10/21]  eta: 0:00:02  Loss: 2.2189 (2.0866)  Acc@1: 62.5000 (59.0909)  Acc@5: 79.1667 (78.0303)  Acc@task: 70.8333 (68.9394)  time: 0.2329  data: 0.0561  max mem: 1378\n",
            "Test: [Task 3]  [20/21]  eta: 0:00:00  Loss: 2.0858 (2.0492)  Acc@1: 62.5000 (60.5634)  Acc@5: 79.1667 (79.6781)  Acc@task: 66.6667 (66.8008)  time: 0.1754  data: 0.0004  max mem: 1378\n",
            "Test: [Task 3] Total time: 0:00:04 (0.2087 s / it)\n",
            "* Acc@task 66.801 Acc@1 60.563 Acc@5 79.678 loss 2.049\n",
            "Test: [Task 4]  [ 0/20]  eta: 0:00:11  Loss: 1.6115 (1.6115)  Acc@1: 66.6667 (66.6667)  Acc@5: 79.1667 (79.1667)  Acc@task: 79.1667 (79.1667)  time: 0.5736  data: 0.4092  max mem: 1378\n",
            "Test: [Task 4]  [10/20]  eta: 0:00:02  Loss: 1.6573 (1.7711)  Acc@1: 66.6667 (62.1212)  Acc@5: 79.1667 (82.5758)  Acc@task: 70.8333 (67.0455)  time: 0.2160  data: 0.0379  max mem: 1378\n",
            "Test: [Task 4]  [19/20]  eta: 0:00:00  Loss: 1.6631 (1.7981)  Acc@1: 58.3333 (61.3588)  Acc@5: 83.3333 (82.5902)  Acc@task: 66.6667 (66.2420)  time: 0.1955  data: 0.0209  max mem: 1378\n",
            "Test: [Task 4] Total time: 0:00:04 (0.2003 s / it)\n",
            "* Acc@task 66.242 Acc@1 61.359 Acc@5 82.590 loss 1.798\n",
            "Test: [Task 5]  [ 0/21]  eta: 0:00:10  Loss: 3.8018 (3.8018)  Acc@1: 20.8333 (20.8333)  Acc@5: 62.5000 (62.5000)  Acc@task: 54.1667 (54.1667)  time: 0.4986  data: 0.3173  max mem: 1378\n",
            "Test: [Task 5]  [10/21]  eta: 0:00:02  Loss: 4.3474 (4.3317)  Acc@1: 8.3333 (8.3333)  Acc@5: 54.1667 (60.2273)  Acc@task: 54.1667 (56.8182)  time: 0.2088  data: 0.0304  max mem: 1378\n",
            "Test: [Task 5]  [20/21]  eta: 0:00:00  Loss: 4.3915 (4.4659)  Acc@1: 8.3333 (8.7576)  Acc@5: 54.1667 (56.2118)  Acc@task: 54.1667 (55.8045)  time: 0.1741  data: 0.0011  max mem: 1378\n",
            "Test: [Task 5] Total time: 0:00:04 (0.1965 s / it)\n",
            "* Acc@task 55.804 Acc@1 8.758 Acc@5 56.212 loss 4.466\n",
            "[Average accuracy till task5]\tAcc@task: 64.3778\tAcc@1: 54.3852\tAcc@5: 79.3003\tLoss: 2.1606\tForgetting: 0.2515\tBackward: 50.5585\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:35  Loss: 0.3030 (0.3030)  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.8449  data: 0.6681  max mem: 1378\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:07  Loss: 0.6047 (0.7854)  Acc@1: 75.0000 (77.2727)  Acc@5: 100.0000 (97.3485)  Acc@task: 83.3333 (81.8182)  time: 0.2405  data: 0.0613  max mem: 1378\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.6047 (0.7250)  Acc@1: 79.1667 (79.9603)  Acc@5: 95.8333 (97.0238)  Acc@task: 83.3333 (82.1429)  time: 0.1799  data: 0.0005  max mem: 1378\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.5299 (0.6652)  Acc@1: 83.3333 (81.7204)  Acc@5: 100.0000 (97.5806)  Acc@task: 83.3333 (83.0645)  time: 0.1800  data: 0.0003  max mem: 1378\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.4917 (0.6321)  Acc@1: 83.3333 (82.5203)  Acc@5: 100.0000 (98.0691)  Acc@task: 87.5000 (84.3496)  time: 0.1807  data: 0.0002  max mem: 1378\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.4174 (0.6215)  Acc@1: 91.6667 (82.7000)  Acc@5: 100.0000 (98.1000)  Acc@task: 87.5000 (84.6000)  time: 0.1774  data: 0.0002  max mem: 1378\n",
            "Test: [Task 1] Total time: 0:00:08 (0.1969 s / it)\n",
            "* Acc@task 84.600 Acc@1 82.700 Acc@5 98.100 loss 0.622\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:24  Loss: 0.7407 (0.7407)  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  Acc@task: 75.0000 (75.0000)  time: 0.5805  data: 0.4150  max mem: 1378\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.7336 (0.7559)  Acc@1: 83.3333 (82.5758)  Acc@5: 100.0000 (96.2121)  Acc@task: 75.0000 (75.7576)  time: 0.2154  data: 0.0380  max mem: 1378\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.6008 (0.7982)  Acc@1: 83.3333 (81.3492)  Acc@5: 95.8333 (95.6349)  Acc@task: 75.0000 (76.5873)  time: 0.1794  data: 0.0005  max mem: 1378\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.6008 (0.7575)  Acc@1: 79.1667 (81.7204)  Acc@5: 95.8333 (95.9677)  Acc@task: 79.1667 (76.6129)  time: 0.1805  data: 0.0011  max mem: 1378\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.5806 (0.7181)  Acc@1: 83.3333 (82.5203)  Acc@5: 95.8333 (96.3415)  Acc@task: 75.0000 (76.7276)  time: 0.1818  data: 0.0009  max mem: 1378\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.5568 (0.7095)  Acc@1: 83.3333 (82.6000)  Acc@5: 95.8333 (96.4000)  Acc@task: 75.0000 (76.7000)  time: 0.1782  data: 0.0008  max mem: 1378\n",
            "Test: [Task 2] Total time: 0:00:08 (0.1913 s / it)\n",
            "* Acc@task 76.700 Acc@1 82.600 Acc@5 96.400 loss 0.709\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:20  Loss: 0.1433 (0.1433)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  Acc@task: 100.0000 (100.0000)  time: 0.4975  data: 0.3131  max mem: 1378\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 0.5571 (0.6647)  Acc@1: 83.3333 (82.9545)  Acc@5: 95.8333 (97.7273)  Acc@task: 79.1667 (82.1970)  time: 0.2100  data: 0.0291  max mem: 1378\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.5400 (0.6204)  Acc@1: 83.3333 (84.1270)  Acc@5: 95.8333 (97.2222)  Acc@task: 79.1667 (81.3492)  time: 0.1806  data: 0.0005  max mem: 1378\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.5304 (0.6066)  Acc@1: 83.3333 (84.2742)  Acc@5: 95.8333 (97.3118)  Acc@task: 83.3333 (81.8548)  time: 0.1810  data: 0.0003  max mem: 1378\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.5680 (0.6168)  Acc@1: 83.3333 (84.4512)  Acc@5: 95.8333 (96.8496)  Acc@task: 79.1667 (81.5041)  time: 0.1822  data: 0.0003  max mem: 1378\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.5680 (0.6165)  Acc@1: 83.3333 (84.3000)  Acc@5: 95.8333 (96.8000)  Acc@task: 79.1667 (81.2000)  time: 0.1787  data: 0.0003  max mem: 1378\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1897 s / it)\n",
            "* Acc@task 81.200 Acc@1 84.300 Acc@5 96.800 loss 0.616\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:20  Loss: 0.6188 (0.6188)  Acc@1: 83.3333 (83.3333)  Acc@5: 91.6667 (91.6667)  Acc@task: 70.8333 (70.8333)  time: 0.4987  data: 0.3215  max mem: 1378\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:06  Loss: 0.5978 (0.5901)  Acc@1: 83.3333 (84.0909)  Acc@5: 95.8333 (96.9697)  Acc@task: 79.1667 (78.4091)  time: 0.2092  data: 0.0299  max mem: 1378\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.5577 (0.6188)  Acc@1: 83.3333 (83.9286)  Acc@5: 95.8333 (97.2222)  Acc@task: 79.1667 (79.9603)  time: 0.1805  data: 0.0007  max mem: 1378\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.4443 (0.5902)  Acc@1: 83.3333 (84.6774)  Acc@5: 95.8333 (97.1774)  Acc@task: 79.1667 (80.1075)  time: 0.1816  data: 0.0005  max mem: 1378\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.5317 (0.6216)  Acc@1: 83.3333 (83.6382)  Acc@5: 95.8333 (96.5447)  Acc@task: 79.1667 (79.7764)  time: 0.1816  data: 0.0002  max mem: 1378\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.4598 (0.6077)  Acc@1: 83.3333 (83.9000)  Acc@5: 95.8333 (96.6000)  Acc@task: 79.1667 (80.0000)  time: 0.1782  data: 0.0002  max mem: 1378\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1893 s / it)\n",
            "* Acc@task 80.000 Acc@1 83.900 Acc@5 96.600 loss 0.608\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:23  Loss: 3.2226 (3.2226)  Acc@1: 25.0000 (25.0000)  Acc@5: 79.1667 (79.1667)  Acc@task: 62.5000 (62.5000)  time: 0.5529  data: 0.3815  max mem: 1378\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 4.0996 (4.0656)  Acc@1: 16.6667 (18.5606)  Acc@5: 75.0000 (76.8939)  Acc@task: 70.8333 (73.1061)  time: 0.2159  data: 0.0350  max mem: 1378\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 4.0648 (4.0795)  Acc@1: 12.5000 (17.6587)  Acc@5: 75.0000 (76.9841)  Acc@task: 70.8333 (71.8254)  time: 0.1807  data: 0.0003  max mem: 1378\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 4.2787 (4.1594)  Acc@1: 12.5000 (15.9946)  Acc@5: 75.0000 (74.8656)  Acc@task: 66.6667 (70.4301)  time: 0.1798  data: 0.0003  max mem: 1378\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 4.2825 (4.2045)  Acc@1: 12.5000 (14.9390)  Acc@5: 70.8333 (72.9675)  Acc@task: 62.5000 (67.9878)  time: 0.1805  data: 0.0003  max mem: 1378\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 4.2825 (4.2345)  Acc@1: 12.5000 (15.0000)  Acc@5: 66.6667 (72.6000)  Acc@task: 62.5000 (67.6000)  time: 0.1769  data: 0.0003  max mem: 1378\n",
            "Test: [Task 5] Total time: 0:00:08 (0.1910 s / it)\n",
            "* Acc@task 67.600 Acc@1 15.000 Acc@5 72.600 loss 4.234\n",
            "[Average accuracy till task5]\tAcc@task: 78.0200\tAcc@1: 69.7000\tAcc@5: 92.1000\tLoss: 1.3579\tForgetting: 0.1250\tBackward: 62.5750\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:30  Loss: 0.3030 (0.3030)  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.7364  data: 0.5441  max mem: 1378\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:07  Loss: 0.6047 (0.7854)  Acc@1: 75.0000 (77.2727)  Acc@5: 100.0000 (97.3485)  Acc@task: 83.3333 (81.8182)  time: 0.2312  data: 0.0510  max mem: 1378\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.6047 (0.7250)  Acc@1: 79.1667 (79.9603)  Acc@5: 95.8333 (97.0238)  Acc@task: 83.3333 (82.1429)  time: 0.1802  data: 0.0010  max mem: 1378\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.5299 (0.6652)  Acc@1: 83.3333 (81.7204)  Acc@5: 100.0000 (97.5806)  Acc@task: 83.3333 (83.0645)  time: 0.1800  data: 0.0004  max mem: 1378\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.4917 (0.6321)  Acc@1: 83.3333 (82.5203)  Acc@5: 100.0000 (98.0691)  Acc@task: 87.5000 (84.3496)  time: 0.1800  data: 0.0004  max mem: 1378\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.4174 (0.6215)  Acc@1: 91.6667 (82.7000)  Acc@5: 100.0000 (98.1000)  Acc@task: 87.5000 (84.6000)  time: 0.1768  data: 0.0004  max mem: 1378\n",
            "Test: [Task 1] Total time: 0:00:08 (0.1941 s / it)\n",
            "* Acc@task 84.600 Acc@1 82.700 Acc@5 98.100 loss 0.622\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:30  Loss: 0.7407 (0.7407)  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  Acc@task: 75.0000 (75.0000)  time: 0.7310  data: 0.5510  max mem: 1378\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:07  Loss: 0.7336 (0.7559)  Acc@1: 83.3333 (82.5758)  Acc@5: 100.0000 (96.2121)  Acc@task: 75.0000 (75.7576)  time: 0.2294  data: 0.0515  max mem: 1378\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.6008 (0.7982)  Acc@1: 83.3333 (81.3492)  Acc@5: 95.8333 (95.6349)  Acc@task: 75.0000 (76.5873)  time: 0.1788  data: 0.0015  max mem: 1378\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.6008 (0.7575)  Acc@1: 79.1667 (81.7204)  Acc@5: 95.8333 (95.9677)  Acc@task: 79.1667 (76.6129)  time: 0.1793  data: 0.0012  max mem: 1378\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.5806 (0.7181)  Acc@1: 83.3333 (82.5203)  Acc@5: 95.8333 (96.3415)  Acc@task: 75.0000 (76.7276)  time: 0.1805  data: 0.0006  max mem: 1378\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.5568 (0.7095)  Acc@1: 83.3333 (82.6000)  Acc@5: 95.8333 (96.4000)  Acc@task: 75.0000 (76.7000)  time: 0.1772  data: 0.0005  max mem: 1378\n",
            "Test: [Task 2] Total time: 0:00:08 (0.1939 s / it)\n",
            "* Acc@task 76.700 Acc@1 82.600 Acc@5 96.400 loss 0.709\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:20  Loss: 0.1433 (0.1433)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  Acc@task: 100.0000 (100.0000)  time: 0.4771  data: 0.3006  max mem: 1378\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 0.5571 (0.6647)  Acc@1: 83.3333 (82.9545)  Acc@5: 95.8333 (97.7273)  Acc@task: 79.1667 (82.1970)  time: 0.2052  data: 0.0280  max mem: 1378\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.5400 (0.6204)  Acc@1: 83.3333 (84.1270)  Acc@5: 95.8333 (97.2222)  Acc@task: 79.1667 (81.3492)  time: 0.1783  data: 0.0005  max mem: 1378\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.5304 (0.6066)  Acc@1: 83.3333 (84.2742)  Acc@5: 95.8333 (97.3118)  Acc@task: 83.3333 (81.8548)  time: 0.1791  data: 0.0003  max mem: 1378\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.5680 (0.6168)  Acc@1: 83.3333 (84.4512)  Acc@5: 95.8333 (96.8496)  Acc@task: 79.1667 (81.5041)  time: 0.1800  data: 0.0003  max mem: 1378\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.5680 (0.6165)  Acc@1: 83.3333 (84.3000)  Acc@5: 95.8333 (96.8000)  Acc@task: 79.1667 (81.2000)  time: 0.1767  data: 0.0003  max mem: 1378\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1872 s / it)\n",
            "* Acc@task 81.200 Acc@1 84.300 Acc@5 96.800 loss 0.616\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:19  Loss: 0.6188 (0.6188)  Acc@1: 83.3333 (83.3333)  Acc@5: 91.6667 (91.6667)  Acc@task: 70.8333 (70.8333)  time: 0.4575  data: 0.2887  max mem: 1378\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:06  Loss: 0.5978 (0.5901)  Acc@1: 83.3333 (84.0909)  Acc@5: 95.8333 (96.9697)  Acc@task: 79.1667 (78.4091)  time: 0.2068  data: 0.0337  max mem: 1378\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.5577 (0.6188)  Acc@1: 83.3333 (83.9286)  Acc@5: 95.8333 (97.2222)  Acc@task: 79.1667 (79.9603)  time: 0.1808  data: 0.0050  max mem: 1378\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.4443 (0.5902)  Acc@1: 83.3333 (84.6774)  Acc@5: 95.8333 (97.1774)  Acc@task: 79.1667 (80.1075)  time: 0.1797  data: 0.0012  max mem: 1378\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.5317 (0.6216)  Acc@1: 83.3333 (83.6382)  Acc@5: 95.8333 (96.5447)  Acc@task: 79.1667 (79.7764)  time: 0.1795  data: 0.0003  max mem: 1378\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.4598 (0.6077)  Acc@1: 83.3333 (83.9000)  Acc@5: 95.8333 (96.6000)  Acc@task: 79.1667 (80.0000)  time: 0.1767  data: 0.0002  max mem: 1378\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1878 s / it)\n",
            "* Acc@task 80.000 Acc@1 83.900 Acc@5 96.600 loss 0.608\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:19  Loss: 3.2226 (3.2226)  Acc@1: 25.0000 (25.0000)  Acc@5: 79.1667 (79.1667)  Acc@task: 62.5000 (62.5000)  time: 0.4641  data: 0.2940  max mem: 1378\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 4.0996 (4.0656)  Acc@1: 16.6667 (18.5606)  Acc@5: 75.0000 (76.8939)  Acc@task: 70.8333 (73.1061)  time: 0.2052  data: 0.0290  max mem: 1378\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 4.0648 (4.0795)  Acc@1: 12.5000 (17.6587)  Acc@5: 75.0000 (76.9841)  Acc@task: 70.8333 (71.8254)  time: 0.1789  data: 0.0014  max mem: 1378\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 4.2787 (4.1594)  Acc@1: 12.5000 (15.9946)  Acc@5: 75.0000 (74.8656)  Acc@task: 66.6667 (70.4301)  time: 0.1788  data: 0.0003  max mem: 1378\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 4.2825 (4.2045)  Acc@1: 12.5000 (14.9390)  Acc@5: 70.8333 (72.9675)  Acc@task: 62.5000 (67.9878)  time: 0.1796  data: 0.0003  max mem: 1378\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 4.2825 (4.2345)  Acc@1: 12.5000 (15.0000)  Acc@5: 66.6667 (72.6000)  Acc@task: 62.5000 (67.6000)  time: 0.1761  data: 0.0002  max mem: 1378\n",
            "Test: [Task 5] Total time: 0:00:07 (0.1884 s / it)\n",
            "* Acc@task 67.600 Acc@1 15.000 Acc@5 72.600 loss 4.234\n",
            "[Average accuracy till task5]\tAcc@task: 78.0200\tAcc@1: 69.7000\tAcc@5: 92.1000\tLoss: 1.3579\tForgetting: 2.8250\tBackward: 52.1000\n",
            "torch.Size([52560, 384])\n",
            "Averaged stats: Lr: 0.005000  Loss: 0.0214  Acc@1: 99.1667 (94.8542)  Acc@5: 100.0000 (99.7292)\n",
            "torch.Size([52560, 384])\n",
            "Averaged stats: Lr: 0.004878  Loss: 0.0114  Acc@1: 100.0000 (99.4792)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([52560, 384])\n",
            "Averaged stats: Lr: 0.004523  Loss: 0.0219  Acc@1: 100.0000 (99.7292)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([52560, 384])\n",
            "Averaged stats: Lr: 0.003969  Loss: 0.0129  Acc@1: 100.0000 (99.8542)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([52560, 384])\n",
            "Averaged stats: Lr: 0.003273  Loss: 0.0094  Acc@1: 100.0000 (99.7708)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([52560, 384])\n",
            "Averaged stats: Lr: 0.002500  Loss: 0.0068  Acc@1: 100.0000 (99.8958)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([52560, 384])\n",
            "Averaged stats: Lr: 0.001727  Loss: 0.0070  Acc@1: 100.0000 (99.9167)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([52560, 384])\n",
            "Averaged stats: Lr: 0.001031  Loss: 0.0050  Acc@1: 100.0000 (99.8542)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([52560, 384])\n",
            "Averaged stats: Lr: 0.000477  Loss: 0.0197  Acc@1: 100.0000 (99.8750)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([52560, 384])\n",
            "Averaged stats: Lr: 0.000122  Loss: 0.0098  Acc@1: 100.0000 (99.9792)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [Task 1]  [ 0/23]  eta: 0:00:12  Loss: 0.9631 (0.9631)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  Acc@task: 79.1667 (79.1667)  time: 0.5350  data: 0.3605  max mem: 1378\n",
            "Test: [Task 1]  [10/23]  eta: 0:00:02  Loss: 0.8756 (0.9386)  Acc@1: 75.0000 (78.0303)  Acc@5: 91.6667 (92.0455)  Acc@task: 75.0000 (72.7273)  time: 0.2091  data: 0.0333  max mem: 1378\n",
            "Test: [Task 1]  [20/23]  eta: 0:00:00  Loss: 0.7508 (0.8784)  Acc@1: 75.0000 (77.9762)  Acc@5: 95.8333 (94.4444)  Acc@task: 75.0000 (73.2143)  time: 0.1769  data: 0.0006  max mem: 1378\n",
            "Test: [Task 1]  [22/23]  eta: 0:00:00  Loss: 0.7394 (0.8814)  Acc@1: 75.0000 (77.7985)  Acc@5: 95.8333 (94.5896)  Acc@task: 75.0000 (73.5075)  time: 0.1727  data: 0.0006  max mem: 1378\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1940 s / it)\n",
            "* Acc@task 73.507 Acc@1 77.799 Acc@5 94.590 loss 0.881\n",
            "Test: [Task 2]  [ 0/20]  eta: 0:00:14  Loss: 1.6480 (1.6480)  Acc@1: 45.8333 (45.8333)  Acc@5: 83.3333 (83.3333)  Acc@task: 58.3333 (58.3333)  time: 0.7076  data: 0.5398  max mem: 1378\n",
            "Test: [Task 2]  [10/20]  eta: 0:00:02  Loss: 1.6700 (1.7648)  Acc@1: 62.5000 (62.1212)  Acc@5: 79.1667 (82.1970)  Acc@task: 58.3333 (59.8485)  time: 0.2266  data: 0.0493  max mem: 1378\n",
            "Test: [Task 2]  [19/20]  eta: 0:00:00  Loss: 1.6480 (1.7595)  Acc@1: 62.5000 (61.3779)  Acc@5: 83.3333 (83.0898)  Acc@task: 58.3333 (60.1253)  time: 0.2041  data: 0.0272  max mem: 1378\n",
            "Test: [Task 2] Total time: 0:00:04 (0.2086 s / it)\n",
            "* Acc@task 60.125 Acc@1 61.378 Acc@5 83.090 loss 1.759\n",
            "Test: [Task 3]  [ 0/21]  eta: 0:00:12  Loss: 2.4774 (2.4774)  Acc@1: 58.3333 (58.3333)  Acc@5: 70.8333 (70.8333)  Acc@task: 79.1667 (79.1667)  time: 0.6087  data: 0.4426  max mem: 1378\n",
            "Test: [Task 3]  [10/21]  eta: 0:00:02  Loss: 2.0304 (2.0615)  Acc@1: 58.3333 (58.3333)  Acc@5: 83.3333 (82.1970)  Acc@task: 75.0000 (69.6970)  time: 0.2181  data: 0.0407  max mem: 1378\n",
            "Test: [Task 3]  [20/21]  eta: 0:00:00  Loss: 2.0304 (2.1901)  Acc@1: 58.3333 (57.3441)  Acc@5: 82.3529 (80.0805)  Acc@task: 66.6667 (68.2093)  time: 0.1760  data: 0.0003  max mem: 1378\n",
            "Test: [Task 3] Total time: 0:00:04 (0.2010 s / it)\n",
            "* Acc@task 68.209 Acc@1 57.344 Acc@5 80.080 loss 2.190\n",
            "Test: [Task 4]  [ 0/20]  eta: 0:00:11  Loss: 2.7762 (2.7762)  Acc@1: 54.1667 (54.1667)  Acc@5: 79.1667 (79.1667)  Acc@task: 58.3333 (58.3333)  time: 0.5666  data: 0.3928  max mem: 1378\n",
            "Test: [Task 4]  [10/20]  eta: 0:00:02  Loss: 2.1620 (2.1754)  Acc@1: 54.1667 (56.8182)  Acc@5: 79.1667 (79.5455)  Acc@task: 66.6667 (62.5000)  time: 0.2147  data: 0.0361  max mem: 1378\n",
            "Test: [Task 4]  [19/20]  eta: 0:00:00  Loss: 1.8840 (2.0835)  Acc@1: 58.3333 (60.0849)  Acc@5: 79.1667 (80.8917)  Acc@task: 66.6667 (64.7558)  time: 0.1955  data: 0.0200  max mem: 1378\n",
            "Test: [Task 4] Total time: 0:00:04 (0.2043 s / it)\n",
            "* Acc@task 64.756 Acc@1 60.085 Acc@5 80.892 loss 2.084\n",
            "Test: [Task 5]  [ 0/21]  eta: 0:00:17  Loss: 2.2195 (2.2195)  Acc@1: 66.6667 (66.6667)  Acc@5: 79.1667 (79.1667)  Acc@task: 58.3333 (58.3333)  time: 0.8514  data: 0.6620  max mem: 1378\n",
            "Test: [Task 5]  [10/21]  eta: 0:00:02  Loss: 1.7888 (1.9400)  Acc@1: 62.5000 (60.6061)  Acc@5: 79.1667 (79.9242)  Acc@task: 54.1667 (54.5455)  time: 0.2416  data: 0.0608  max mem: 1378\n",
            "Test: [Task 5]  [20/21]  eta: 0:00:00  Loss: 1.8003 (1.9800)  Acc@1: 58.3333 (59.0631)  Acc@5: 79.1667 (79.8371)  Acc@task: 54.1667 (55.8045)  time: 0.1751  data: 0.0004  max mem: 1378\n",
            "Test: [Task 5] Total time: 0:00:04 (0.2119 s / it)\n",
            "* Acc@task 55.804 Acc@1 59.063 Acc@5 79.837 loss 1.980\n",
            "[Average accuracy till task5]\tAcc@task: 64.4805\tAcc@1: 63.1337\tAcc@5: 83.6977\tLoss: 1.7789\tForgetting: 0.0000\tBackward: 64.1513\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:17  Loss: 0.5102 (0.5102)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.4080  data: 0.2429  max mem: 1378\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.6054 (0.8232)  Acc@1: 75.0000 (77.2727)  Acc@5: 100.0000 (96.5909)  Acc@task: 83.3333 (81.8182)  time: 0.2088  data: 0.0314  max mem: 1378\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.6170 (0.7542)  Acc@1: 75.0000 (79.5635)  Acc@5: 95.8333 (96.2302)  Acc@task: 83.3333 (82.1429)  time: 0.1858  data: 0.0053  max mem: 1378\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.6104 (0.6986)  Acc@1: 83.3333 (81.3172)  Acc@5: 95.8333 (96.9086)  Acc@task: 83.3333 (83.0645)  time: 0.1832  data: 0.0005  max mem: 1378\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.5234 (0.6626)  Acc@1: 83.3333 (82.0122)  Acc@5: 100.0000 (97.5610)  Acc@task: 87.5000 (84.3496)  time: 0.1831  data: 0.0005  max mem: 1378\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.4915 (0.6532)  Acc@1: 83.3333 (82.2000)  Acc@5: 100.0000 (97.6000)  Acc@task: 87.5000 (84.6000)  time: 0.1796  data: 0.0005  max mem: 1378\n",
            "Test: [Task 1] Total time: 0:00:08 (0.1918 s / it)\n",
            "* Acc@task 84.600 Acc@1 82.200 Acc@5 97.600 loss 0.653\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:36  Loss: 0.8697 (0.8697)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 75.0000 (75.0000)  time: 0.8730  data: 0.6889  max mem: 1378\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:07  Loss: 0.7908 (0.8443)  Acc@1: 79.1667 (81.4394)  Acc@5: 95.8333 (95.8333)  Acc@task: 75.0000 (75.7576)  time: 0.2453  data: 0.0632  max mem: 1378\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.7540 (0.9359)  Acc@1: 79.1667 (78.7698)  Acc@5: 95.8333 (94.8413)  Acc@task: 75.0000 (76.5873)  time: 0.1824  data: 0.0005  max mem: 1378\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.7524 (0.8780)  Acc@1: 79.1667 (79.0323)  Acc@5: 95.8333 (95.4301)  Acc@task: 79.1667 (76.6129)  time: 0.1822  data: 0.0003  max mem: 1378\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.6139 (0.8243)  Acc@1: 79.1667 (79.8781)  Acc@5: 95.8333 (95.5285)  Acc@task: 75.0000 (76.7276)  time: 0.1827  data: 0.0003  max mem: 1378\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.5859 (0.8155)  Acc@1: 79.1667 (79.9000)  Acc@5: 95.8333 (95.6000)  Acc@task: 75.0000 (76.7000)  time: 0.1794  data: 0.0003  max mem: 1378\n",
            "Test: [Task 2] Total time: 0:00:08 (0.1999 s / it)\n",
            "* Acc@task 76.700 Acc@1 79.900 Acc@5 95.600 loss 0.816\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:19  Loss: 0.1943 (0.1943)  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)  Acc@task: 100.0000 (100.0000)  time: 0.4715  data: 0.2997  max mem: 1378\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 0.7718 (0.7500)  Acc@1: 83.3333 (82.1970)  Acc@5: 95.8333 (96.2121)  Acc@task: 79.1667 (82.1970)  time: 0.2081  data: 0.0278  max mem: 1378\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.6839 (0.7313)  Acc@1: 83.3333 (82.3413)  Acc@5: 95.8333 (96.0317)  Acc@task: 79.1667 (81.3492)  time: 0.1817  data: 0.0007  max mem: 1378\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.5681 (0.6995)  Acc@1: 79.1667 (82.1237)  Acc@5: 95.8333 (96.6398)  Acc@task: 83.3333 (81.8548)  time: 0.1824  data: 0.0009  max mem: 1378\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.6150 (0.6992)  Acc@1: 83.3333 (82.6220)  Acc@5: 95.8333 (96.3415)  Acc@task: 79.1667 (81.5041)  time: 0.1830  data: 0.0007  max mem: 1378\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.6150 (0.7006)  Acc@1: 83.3333 (82.6000)  Acc@5: 95.8333 (96.4000)  Acc@task: 79.1667 (81.2000)  time: 0.1793  data: 0.0006  max mem: 1378\n",
            "Test: [Task 3] Total time: 0:00:08 (0.1908 s / it)\n",
            "* Acc@task 81.200 Acc@1 82.600 Acc@5 96.400 loss 0.701\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:35  Loss: 0.6208 (0.6208)  Acc@1: 83.3333 (83.3333)  Acc@5: 91.6667 (91.6667)  Acc@task: 70.8333 (70.8333)  time: 0.8380  data: 0.6420  max mem: 1378\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:07  Loss: 0.6839 (0.6702)  Acc@1: 83.3333 (82.9545)  Acc@5: 95.8333 (95.0758)  Acc@task: 79.1667 (78.4091)  time: 0.2413  data: 0.0590  max mem: 1378\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.6839 (0.6990)  Acc@1: 83.3333 (82.9365)  Acc@5: 95.8333 (96.0317)  Acc@task: 79.1667 (79.9603)  time: 0.1817  data: 0.0006  max mem: 1378\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.5396 (0.6685)  Acc@1: 83.3333 (82.9301)  Acc@5: 95.8333 (96.2366)  Acc@task: 79.1667 (80.1075)  time: 0.1817  data: 0.0004  max mem: 1378\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7131 (0.7237)  Acc@1: 83.3333 (81.8089)  Acc@5: 95.8333 (95.8333)  Acc@task: 79.1667 (79.7764)  time: 0.1818  data: 0.0004  max mem: 1378\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.5795 (0.7086)  Acc@1: 83.3333 (82.0000)  Acc@5: 95.8333 (95.9000)  Acc@task: 79.1667 (80.0000)  time: 0.1785  data: 0.0003  max mem: 1378\n",
            "Test: [Task 4] Total time: 0:00:08 (0.1981 s / it)\n",
            "* Acc@task 80.000 Acc@1 82.000 Acc@5 95.900 loss 0.709\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:17  Loss: 0.2570 (0.2570)  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)  Acc@task: 62.5000 (62.5000)  time: 0.4234  data: 0.2496  max mem: 1378\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 0.5192 (0.6003)  Acc@1: 83.3333 (82.9545)  Acc@5: 100.0000 (97.7273)  Acc@task: 70.8333 (73.1061)  time: 0.2101  data: 0.0330  max mem: 1378\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.5192 (0.6475)  Acc@1: 79.1667 (82.5397)  Acc@5: 95.8333 (97.0238)  Acc@task: 70.8333 (71.8254)  time: 0.1849  data: 0.0063  max mem: 1378\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.6316 (0.6493)  Acc@1: 79.1667 (82.1237)  Acc@5: 95.8333 (97.1774)  Acc@task: 66.6667 (70.4301)  time: 0.1815  data: 0.0010  max mem: 1378\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.6609 (0.6706)  Acc@1: 79.1667 (81.9106)  Acc@5: 95.8333 (96.9512)  Acc@task: 62.5000 (67.9878)  time: 0.1816  data: 0.0005  max mem: 1378\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.6667 (0.7226)  Acc@1: 79.1667 (81.3000)  Acc@5: 95.8333 (96.7000)  Acc@task: 62.5000 (67.6000)  time: 0.1782  data: 0.0004  max mem: 1378\n",
            "Test: [Task 5] Total time: 0:00:07 (0.1896 s / it)\n",
            "* Acc@task 67.600 Acc@1 81.300 Acc@5 96.700 loss 0.723\n",
            "[Average accuracy till task5]\tAcc@task: 78.0200\tAcc@1: 81.6000\tAcc@5: 96.4400\tLoss: 0.7201\tForgetting: 0.0000\tBackward: 81.6750\n",
            "Loading checkpoint from: ./output/cifar100_full_dino_5epoch_10pct/checkpoint/task6_checkpoint.pth\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/engines/hide_promtp_wtp_and_tap_engine.py:574: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  loss = torch.nn.functional.cross_entropy(sim, torch.range(0, sim.shape[0] - 1).long().to(device))\n",
            "Train: Epoch[ 1/10]  [ 0/20]  eta: 0:00:16  Lr: 0.002812  Loss: 3.7726  Acc@1: 8.3333 (8.3333)  Acc@5: 50.0000 (50.0000)  time: 0.8240  data: 0.5371  max mem: 1378\n",
            "Train: Epoch[ 1/10]  [10/20]  eta: 0:00:03  Lr: 0.002812  Loss: 3.0770  Acc@1: 12.5000 (10.2273)  Acc@5: 62.5000 (58.3333)  time: 0.3292  data: 0.0491  max mem: 1378\n",
            "Train: Epoch[ 1/10]  [19/20]  eta: 0:00:00  Lr: 0.002812  Loss: 1.8648  Acc@1: 12.5000 (15.4167)  Acc@5: 66.6667 (64.7917)  time: 0.3063  data: 0.0270  max mem: 1378\n",
            "Train: Epoch[ 1/10] Total time: 0:00:06 (0.3113 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.8648  Acc@1: 12.5000 (15.4167)  Acc@5: 66.6667 (64.7917)\n",
            "Train: Epoch[ 2/10]  [ 0/20]  eta: 0:00:20  Lr: 0.002812  Loss: 2.1512  Acc@1: 29.1667 (29.1667)  Acc@5: 70.8333 (70.8333)  time: 1.0412  data: 0.6959  max mem: 1378\n",
            "Train: Epoch[ 2/10]  [10/20]  eta: 0:00:03  Lr: 0.002812  Loss: 2.0366  Acc@1: 37.5000 (35.6061)  Acc@5: 83.3333 (82.1970)  time: 0.3481  data: 0.0647  max mem: 1378\n",
            "Train: Epoch[ 2/10]  [19/20]  eta: 0:00:00  Lr: 0.002812  Loss: 1.6698  Acc@1: 37.5000 (37.7083)  Acc@5: 87.5000 (84.5833)  time: 0.3173  data: 0.0357  max mem: 1378\n",
            "Train: Epoch[ 2/10] Total time: 0:00:06 (0.3224 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.6698  Acc@1: 37.5000 (37.7083)  Acc@5: 87.5000 (84.5833)\n",
            "Train: Epoch[ 3/10]  [ 0/20]  eta: 0:00:12  Lr: 0.002812  Loss: 1.3719  Acc@1: 50.0000 (50.0000)  Acc@5: 95.8333 (95.8333)  time: 0.6102  data: 0.3072  max mem: 1378\n",
            "Train: Epoch[ 3/10]  [10/20]  eta: 0:00:03  Lr: 0.002812  Loss: 1.1029  Acc@1: 54.1667 (53.7879)  Acc@5: 95.8333 (93.5606)  time: 0.3082  data: 0.0284  max mem: 1378\n",
            "Train: Epoch[ 3/10]  [19/20]  eta: 0:00:00  Lr: 0.002812  Loss: 1.1005  Acc@1: 54.1667 (53.9583)  Acc@5: 95.8333 (94.1667)  time: 0.2948  data: 0.0157  max mem: 1378\n",
            "Train: Epoch[ 3/10] Total time: 0:00:05 (0.2993 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.1005  Acc@1: 54.1667 (53.9583)  Acc@5: 95.8333 (94.1667)\n",
            "Train: Epoch[ 4/10]  [ 0/20]  eta: 0:00:12  Lr: 0.002812  Loss: 1.2820  Acc@1: 54.1667 (54.1667)  Acc@5: 91.6667 (91.6667)  time: 0.6151  data: 0.3432  max mem: 1378\n",
            "Train: Epoch[ 4/10]  [10/20]  eta: 0:00:03  Lr: 0.002812  Loss: 1.0440  Acc@1: 66.6667 (64.0152)  Acc@5: 95.8333 (96.5909)  time: 0.3083  data: 0.0323  max mem: 1378\n",
            "Train: Epoch[ 4/10]  [19/20]  eta: 0:00:00  Lr: 0.002812  Loss: 1.1252  Acc@1: 66.6667 (64.5833)  Acc@5: 95.8333 (96.8750)  time: 0.2944  data: 0.0179  max mem: 1378\n",
            "Train: Epoch[ 4/10] Total time: 0:00:05 (0.2991 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.1252  Acc@1: 66.6667 (64.5833)  Acc@5: 95.8333 (96.8750)\n",
            "Train: Epoch[ 5/10]  [ 0/20]  eta: 0:00:16  Lr: 0.002812  Loss: 0.7160  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.8232  data: 0.5458  max mem: 1378\n",
            "Train: Epoch[ 5/10]  [10/20]  eta: 0:00:03  Lr: 0.002812  Loss: 0.8484  Acc@1: 70.8333 (73.1061)  Acc@5: 100.0000 (98.1061)  time: 0.3278  data: 0.0501  max mem: 1378\n",
            "Train: Epoch[ 5/10]  [19/20]  eta: 0:00:00  Lr: 0.002812  Loss: 0.9296  Acc@1: 75.0000 (75.4167)  Acc@5: 100.0000 (97.9167)  time: 0.3057  data: 0.0276  max mem: 1378\n",
            "Train: Epoch[ 5/10] Total time: 0:00:06 (0.3101 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.9296  Acc@1: 75.0000 (75.4167)  Acc@5: 100.0000 (97.9167)\n",
            "Train: Epoch[ 6/10]  [ 0/20]  eta: 0:00:13  Lr: 0.002812  Loss: 0.6850  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.6939  data: 0.4165  max mem: 1378\n",
            "Train: Epoch[ 6/10]  [10/20]  eta: 0:00:03  Lr: 0.002812  Loss: 0.7237  Acc@1: 75.0000 (76.1364)  Acc@5: 100.0000 (98.1061)  time: 0.3155  data: 0.0389  max mem: 1378\n",
            "Train: Epoch[ 6/10]  [19/20]  eta: 0:00:00  Lr: 0.002812  Loss: 0.7512  Acc@1: 75.0000 (74.7917)  Acc@5: 100.0000 (97.9167)  time: 0.2986  data: 0.0216  max mem: 1378\n",
            "Train: Epoch[ 6/10] Total time: 0:00:06 (0.3034 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.7512  Acc@1: 75.0000 (74.7917)  Acc@5: 100.0000 (97.9167)\n",
            "Train: Epoch[ 7/10]  [ 0/20]  eta: 0:00:11  Lr: 0.002812  Loss: 0.7558  Acc@1: 70.8333 (70.8333)  Acc@5: 100.0000 (100.0000)  time: 0.5684  data: 0.2755  max mem: 1378\n",
            "Train: Epoch[ 7/10]  [10/20]  eta: 0:00:03  Lr: 0.002812  Loss: 0.5137  Acc@1: 75.0000 (76.5152)  Acc@5: 100.0000 (99.6212)  time: 0.3061  data: 0.0259  max mem: 1378\n",
            "Train: Epoch[ 7/10]  [19/20]  eta: 0:00:00  Lr: 0.002812  Loss: 0.9342  Acc@1: 75.0000 (78.3333)  Acc@5: 100.0000 (99.1667)  time: 0.2946  data: 0.0143  max mem: 1378\n",
            "Train: Epoch[ 7/10] Total time: 0:00:05 (0.2989 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.9342  Acc@1: 75.0000 (78.3333)  Acc@5: 100.0000 (99.1667)\n",
            "Train: Epoch[ 8/10]  [ 0/20]  eta: 0:00:11  Lr: 0.002812  Loss: 0.7801  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  time: 0.5725  data: 0.2781  max mem: 1378\n",
            "Train: Epoch[ 8/10]  [10/20]  eta: 0:00:03  Lr: 0.002812  Loss: 0.5037  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (99.2424)  time: 0.3082  data: 0.0258  max mem: 1378\n",
            "Train: Epoch[ 8/10]  [19/20]  eta: 0:00:00  Lr: 0.002812  Loss: 0.7057  Acc@1: 83.3333 (82.5000)  Acc@5: 100.0000 (99.1667)  time: 0.2960  data: 0.0144  max mem: 1378\n",
            "Train: Epoch[ 8/10] Total time: 0:00:06 (0.3015 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.7057  Acc@1: 83.3333 (82.5000)  Acc@5: 100.0000 (99.1667)\n",
            "Train: Epoch[ 9/10]  [ 0/20]  eta: 0:00:13  Lr: 0.002812  Loss: 0.5833  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.6982  data: 0.4216  max mem: 1378\n",
            "Train: Epoch[ 9/10]  [10/20]  eta: 0:00:03  Lr: 0.002812  Loss: 0.4426  Acc@1: 83.3333 (81.8182)  Acc@5: 100.0000 (99.2424)  time: 0.3196  data: 0.0387  max mem: 1378\n",
            "Train: Epoch[ 9/10]  [19/20]  eta: 0:00:00  Lr: 0.002812  Loss: 0.5560  Acc@1: 83.3333 (82.9167)  Acc@5: 100.0000 (99.3750)  time: 0.3019  data: 0.0214  max mem: 1378\n",
            "Train: Epoch[ 9/10] Total time: 0:00:06 (0.3062 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.5560  Acc@1: 83.3333 (82.9167)  Acc@5: 100.0000 (99.3750)\n",
            "Train: Epoch[10/10]  [ 0/20]  eta: 0:00:17  Lr: 0.002812  Loss: 0.5042  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.8855  data: 0.5964  max mem: 1378\n",
            "Train: Epoch[10/10]  [10/20]  eta: 0:00:03  Lr: 0.002812  Loss: 0.7983  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (99.2424)  time: 0.3355  data: 0.0545  max mem: 1378\n",
            "Train: Epoch[10/10]  [19/20]  eta: 0:00:00  Lr: 0.002812  Loss: 0.4937  Acc@1: 83.3333 (84.1667)  Acc@5: 100.0000 (99.1667)  time: 0.3108  data: 0.0300  max mem: 1378\n",
            "Train: Epoch[10/10] Total time: 0:00:06 (0.3175 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.4937  Acc@1: 83.3333 (84.1667)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([5, 2, 5, 6, 64])\n",
            "torch.Size([5, 2, 1, 5, 6, 64])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/23]  eta: 0:00:11  Loss: 2.4571 (2.4571)  Acc@1: 58.3333 (58.3333)  Acc@5: 83.3333 (83.3333)  Acc@task: 70.8333 (70.8333)  time: 0.4998  data: 0.3311  max mem: 1378\n",
            "Test: [Task 1]  [10/23]  eta: 0:00:02  Loss: 1.2261 (1.3133)  Acc@1: 70.8333 (70.4545)  Acc@5: 91.6667 (91.2879)  Acc@task: 66.6667 (68.1818)  time: 0.2085  data: 0.0305  max mem: 1378\n",
            "Test: [Task 1]  [20/23]  eta: 0:00:00  Loss: 0.9496 (1.0953)  Acc@1: 75.0000 (73.0159)  Acc@5: 91.6667 (93.4524)  Acc@task: 62.5000 (66.0714)  time: 0.1794  data: 0.0003  max mem: 1378\n",
            "Test: [Task 1]  [22/23]  eta: 0:00:00  Loss: 0.9495 (1.0697)  Acc@1: 75.0000 (73.5075)  Acc@5: 95.8333 (93.6567)  Acc@task: 66.6667 (66.7910)  time: 0.1747  data: 0.0002  max mem: 1378\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1930 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@task 66.791 Acc@1 73.507 Acc@5 93.657 loss 1.070\n",
            "Test: [Task 2]  [ 0/20]  eta: 0:00:12  Loss: 2.4005 (2.4005)  Acc@1: 50.0000 (50.0000)  Acc@5: 79.1667 (79.1667)  Acc@task: 58.3333 (58.3333)  time: 0.6343  data: 0.4670  max mem: 1378\n",
            "Test: [Task 2]  [10/20]  eta: 0:00:02  Loss: 1.2986 (1.5014)  Acc@1: 66.6667 (66.2879)  Acc@5: 87.5000 (88.2576)  Acc@task: 62.5000 (61.7424)  time: 0.2227  data: 0.0433  max mem: 1378\n",
            "Test: [Task 2]  [19/20]  eta: 0:00:00  Loss: 1.4523 (1.5237)  Acc@1: 66.6667 (65.1357)  Acc@5: 87.5000 (87.0564)  Acc@task: 62.5000 (63.6743)  time: 0.2027  data: 0.0240  max mem: 1378\n",
            "Test: [Task 2] Total time: 0:00:04 (0.2097 s / it)\n",
            "* Acc@task 63.674 Acc@1 65.136 Acc@5 87.056 loss 1.524\n",
            "Test: [Task 3]  [ 0/21]  eta: 0:00:14  Loss: 3.3661 (3.3661)  Acc@1: 45.8333 (45.8333)  Acc@5: 66.6667 (66.6667)  Acc@task: 54.1667 (54.1667)  time: 0.6814  data: 0.5076  max mem: 1378\n",
            "Test: [Task 3]  [10/21]  eta: 0:00:02  Loss: 1.8664 (2.0478)  Acc@1: 62.5000 (60.2273)  Acc@5: 83.3333 (81.0606)  Acc@task: 66.6667 (67.0455)  time: 0.2252  data: 0.0470  max mem: 1378\n",
            "Test: [Task 3]  [20/21]  eta: 0:00:00  Loss: 1.7542 (1.9671)  Acc@1: 62.5000 (60.1610)  Acc@5: 83.3333 (81.2877)  Acc@task: 70.8333 (68.2093)  time: 0.1773  data: 0.0005  max mem: 1378\n",
            "Test: [Task 3] Total time: 0:00:04 (0.2055 s / it)\n",
            "* Acc@task 68.209 Acc@1 60.161 Acc@5 81.288 loss 1.967\n",
            "Test: [Task 4]  [ 0/20]  eta: 0:00:09  Loss: 2.4083 (2.4083)  Acc@1: 54.1667 (54.1667)  Acc@5: 70.8333 (70.8333)  Acc@task: 66.6667 (66.6667)  time: 0.4782  data: 0.3117  max mem: 1378\n",
            "Test: [Task 4]  [10/20]  eta: 0:00:02  Loss: 2.1909 (2.0920)  Acc@1: 58.3333 (58.3333)  Acc@5: 83.3333 (82.1970)  Acc@task: 66.6667 (64.0152)  time: 0.2117  data: 0.0349  max mem: 1378\n",
            "Test: [Task 4]  [19/20]  eta: 0:00:00  Loss: 2.2397 (2.2159)  Acc@1: 58.3333 (55.2017)  Acc@5: 79.1667 (79.8302)  Acc@task: 62.5000 (61.9958)  time: 0.1947  data: 0.0193  max mem: 1378\n",
            "Test: [Task 4] Total time: 0:00:03 (0.1997 s / it)\n",
            "* Acc@task 61.996 Acc@1 55.202 Acc@5 79.830 loss 2.216\n",
            "Test: [Task 5]  [ 0/21]  eta: 0:00:09  Loss: 1.9286 (1.9286)  Acc@1: 58.3333 (58.3333)  Acc@5: 79.1667 (79.1667)  Acc@task: 45.8333 (45.8333)  time: 0.4552  data: 0.2885  max mem: 1378\n",
            "Test: [Task 5]  [10/21]  eta: 0:00:02  Loss: 1.9738 (1.8607)  Acc@1: 58.3333 (61.3636)  Acc@5: 83.3333 (83.3333)  Acc@task: 54.1667 (57.9545)  time: 0.2104  data: 0.0337  max mem: 1378\n",
            "Test: [Task 5]  [20/21]  eta: 0:00:00  Loss: 1.9738 (1.9143)  Acc@1: 58.3333 (58.8595)  Acc@5: 83.3333 (83.9104)  Acc@task: 58.3333 (58.4521)  time: 0.1792  data: 0.0044  max mem: 1378\n",
            "Test: [Task 5] Total time: 0:00:04 (0.1991 s / it)\n",
            "* Acc@task 58.452 Acc@1 58.859 Acc@5 83.910 loss 1.914\n",
            "Test: [Task 6]  [ 0/20]  eta: 0:00:17  Loss: 5.2184 (5.2184)  Acc@1: 8.3333 (8.3333)  Acc@5: 62.5000 (62.5000)  Acc@task: 54.1667 (54.1667)  time: 0.8679  data: 0.6654  max mem: 1378\n",
            "Test: [Task 6]  [10/20]  eta: 0:00:02  Loss: 5.3906 (5.4630)  Acc@1: 4.1667 (3.7879)  Acc@5: 50.0000 (50.0000)  Acc@task: 58.3333 (56.8182)  time: 0.2446  data: 0.0609  max mem: 1378\n",
            "Test: [Task 6]  [19/20]  eta: 0:00:00  Loss: 5.4170 (5.5331)  Acc@1: 4.1667 (4.3750)  Acc@5: 50.0000 (49.7917)  Acc@task: 58.3333 (57.7083)  time: 0.2156  data: 0.0336  max mem: 1378\n",
            "Test: [Task 6] Total time: 0:00:04 (0.2203 s / it)\n",
            "* Acc@task 57.708 Acc@1 4.375 Acc@5 49.792 loss 5.533\n",
            "[Average accuracy till task6]\tAcc@task: 62.8051\tAcc@1: 52.8734\tAcc@5: 79.2555\tLoss: 2.3706\tForgetting: 1.8756\tBackward: 50.7604\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:19  Loss: 0.5063 (0.5063)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  Acc@task: 79.1667 (79.1667)  time: 0.4712  data: 0.2979  max mem: 1378\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.6531 (0.8300)  Acc@1: 75.0000 (76.8939)  Acc@5: 100.0000 (96.2121)  Acc@task: 79.1667 (80.3030)  time: 0.2062  data: 0.0273  max mem: 1378\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.6928 (0.7596)  Acc@1: 75.0000 (79.3651)  Acc@5: 95.8333 (96.2302)  Acc@task: 79.1667 (79.9603)  time: 0.1804  data: 0.0003  max mem: 1378\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.6063 (0.7025)  Acc@1: 83.3333 (81.0484)  Acc@5: 95.8333 (96.9086)  Acc@task: 75.0000 (79.8387)  time: 0.1813  data: 0.0004  max mem: 1378\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.5306 (0.6659)  Acc@1: 83.3333 (81.8089)  Acc@5: 100.0000 (97.5610)  Acc@task: 83.3333 (81.0976)  time: 0.1815  data: 0.0003  max mem: 1378\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.4824 (0.6564)  Acc@1: 83.3333 (82.0000)  Acc@5: 100.0000 (97.6000)  Acc@task: 83.3333 (81.4000)  time: 0.1781  data: 0.0003  max mem: 1378\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1898 s / it)\n",
            "* Acc@task 81.400 Acc@1 82.000 Acc@5 97.600 loss 0.656\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:34  Loss: 0.8723 (0.8723)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 70.8333 (70.8333)  time: 0.8147  data: 0.6011  max mem: 1378\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:07  Loss: 0.7895 (0.8370)  Acc@1: 79.1667 (81.4394)  Acc@5: 95.8333 (95.8333)  Acc@task: 70.8333 (73.1061)  time: 0.2378  data: 0.0552  max mem: 1378\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.7604 (0.9313)  Acc@1: 79.1667 (78.7698)  Acc@5: 95.8333 (95.0397)  Acc@task: 75.0000 (74.4048)  time: 0.1806  data: 0.0005  max mem: 1378\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.7472 (0.8763)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.5645)  Acc@task: 75.0000 (74.7312)  time: 0.1811  data: 0.0003  max mem: 1378\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.6337 (0.8269)  Acc@1: 79.1667 (79.8781)  Acc@5: 95.8333 (95.6301)  Acc@task: 75.0000 (74.8984)  time: 0.1810  data: 0.0002  max mem: 1378\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.6135 (0.8181)  Acc@1: 79.1667 (79.9000)  Acc@5: 95.8333 (95.7000)  Acc@task: 75.0000 (74.9000)  time: 0.1776  data: 0.0002  max mem: 1378\n",
            "Test: [Task 2] Total time: 0:00:08 (0.1967 s / it)\n",
            "* Acc@task 74.900 Acc@1 79.900 Acc@5 95.700 loss 0.818\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:18  Loss: 0.1852 (0.1852)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  Acc@task: 87.5000 (87.5000)  time: 0.4412  data: 0.2736  max mem: 1378\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 0.7703 (0.7476)  Acc@1: 83.3333 (82.9545)  Acc@5: 95.8333 (96.5909)  Acc@task: 79.1667 (79.5455)  time: 0.2044  data: 0.0254  max mem: 1378\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.6896 (0.7253)  Acc@1: 83.3333 (82.7381)  Acc@5: 95.8333 (96.2302)  Acc@task: 79.1667 (79.3651)  time: 0.1799  data: 0.0005  max mem: 1378\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.5705 (0.6947)  Acc@1: 79.1667 (82.5269)  Acc@5: 95.8333 (96.7742)  Acc@task: 79.1667 (79.8387)  time: 0.1800  data: 0.0008  max mem: 1378\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.5726 (0.6905)  Acc@1: 83.3333 (82.9268)  Acc@5: 95.8333 (96.4431)  Acc@task: 79.1667 (79.7764)  time: 0.1813  data: 0.0008  max mem: 1378\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.5726 (0.6899)  Acc@1: 83.3333 (82.9000)  Acc@5: 95.8333 (96.5000)  Acc@task: 79.1667 (79.6000)  time: 0.1778  data: 0.0007  max mem: 1378\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1877 s / it)\n",
            "* Acc@task 79.600 Acc@1 82.900 Acc@5 96.500 loss 0.690\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:20  Loss: 0.6327 (0.6327)  Acc@1: 83.3333 (83.3333)  Acc@5: 91.6667 (91.6667)  Acc@task: 62.5000 (62.5000)  time: 0.4853  data: 0.2808  max mem: 1378\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:06  Loss: 0.6912 (0.6726)  Acc@1: 83.3333 (82.9545)  Acc@5: 95.8333 (95.4545)  Acc@task: 79.1667 (77.6515)  time: 0.2104  data: 0.0303  max mem: 1378\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.6912 (0.6986)  Acc@1: 83.3333 (82.7381)  Acc@5: 95.8333 (96.2302)  Acc@task: 79.1667 (79.3651)  time: 0.1813  data: 0.0028  max mem: 1378\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.5794 (0.6689)  Acc@1: 83.3333 (82.7957)  Acc@5: 95.8333 (96.3710)  Acc@task: 83.3333 (79.4355)  time: 0.1802  data: 0.0003  max mem: 1378\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7005 (0.7251)  Acc@1: 83.3333 (81.7073)  Acc@5: 95.8333 (95.8333)  Acc@task: 79.1667 (79.2683)  time: 0.1811  data: 0.0003  max mem: 1378\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.5948 (0.7100)  Acc@1: 83.3333 (81.9000)  Acc@5: 95.8333 (95.9000)  Acc@task: 79.1667 (79.5000)  time: 0.1778  data: 0.0003  max mem: 1378\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1893 s / it)\n",
            "* Acc@task 79.500 Acc@1 81.900 Acc@5 95.900 loss 0.710\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:20  Loss: 0.2300 (0.2300)  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)  Acc@task: 75.0000 (75.0000)  time: 0.4821  data: 0.3055  max mem: 1378\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 0.4772 (0.5931)  Acc@1: 83.3333 (82.9545)  Acc@5: 100.0000 (98.1061)  Acc@task: 75.0000 (76.1364)  time: 0.2070  data: 0.0284  max mem: 1378\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.4772 (0.6327)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (97.2222)  Acc@task: 75.0000 (75.1984)  time: 0.1801  data: 0.0009  max mem: 1378\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.6005 (0.6318)  Acc@1: 79.1667 (82.7957)  Acc@5: 95.8333 (97.3118)  Acc@task: 75.0000 (75.2688)  time: 0.1807  data: 0.0009  max mem: 1378\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.6217 (0.6493)  Acc@1: 79.1667 (82.6220)  Acc@5: 95.8333 (97.0528)  Acc@task: 70.8333 (73.1707)  time: 0.1810  data: 0.0004  max mem: 1378\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.6306 (0.7007)  Acc@1: 79.1667 (82.1000)  Acc@5: 95.8333 (96.8000)  Acc@task: 70.8333 (73.0000)  time: 0.1776  data: 0.0004  max mem: 1378\n",
            "Test: [Task 5] Total time: 0:00:07 (0.1884 s / it)\n",
            "* Acc@task 73.000 Acc@1 82.100 Acc@5 96.800 loss 0.701\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:19  Loss: 5.1484 (5.1484)  Acc@1: 25.0000 (25.0000)  Acc@5: 75.0000 (75.0000)  Acc@task: 79.1667 (79.1667)  time: 0.4664  data: 0.2901  max mem: 1378\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:06  Loss: 5.5855 (5.6444)  Acc@1: 4.1667 (8.7121)  Acc@5: 66.6667 (65.1515)  Acc@task: 70.8333 (68.9394)  time: 0.2060  data: 0.0270  max mem: 1378\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:04  Loss: 5.5855 (5.6066)  Acc@1: 4.1667 (7.7381)  Acc@5: 62.5000 (64.2857)  Acc@task: 66.6667 (67.6587)  time: 0.1800  data: 0.0005  max mem: 1378\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 5.5148 (5.6711)  Acc@1: 4.1667 (7.1237)  Acc@5: 62.5000 (64.1129)  Acc@task: 66.6667 (65.9946)  time: 0.1804  data: 0.0003  max mem: 1378\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 5.6950 (5.6943)  Acc@1: 4.1667 (6.9106)  Acc@5: 62.5000 (63.1098)  Acc@task: 62.5000 (65.5488)  time: 0.1814  data: 0.0002  max mem: 1378\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 5.6950 (5.7015)  Acc@1: 4.1667 (6.9000)  Acc@5: 58.3333 (63.0000)  Acc@task: 62.5000 (65.5000)  time: 0.1780  data: 0.0002  max mem: 1378\n",
            "Test: [Task 6] Total time: 0:00:07 (0.1890 s / it)\n",
            "* Acc@task 65.500 Acc@1 6.900 Acc@5 63.000 loss 5.702\n",
            "[Average accuracy till task6]\tAcc@task: 75.6500\tAcc@1: 69.2833\tAcc@5: 90.9167\tLoss: 1.5461\tForgetting: 0.0600\tBackward: 65.5000\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:37  Loss: 0.5063 (0.5063)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  Acc@task: 79.1667 (79.1667)  time: 0.8883  data: 0.7018  max mem: 1378\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:07  Loss: 0.6531 (0.8300)  Acc@1: 75.0000 (76.8939)  Acc@5: 100.0000 (96.2121)  Acc@task: 79.1667 (80.3030)  time: 0.2440  data: 0.0645  max mem: 1378\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.6928 (0.7596)  Acc@1: 75.0000 (79.3651)  Acc@5: 95.8333 (96.2302)  Acc@task: 79.1667 (79.9603)  time: 0.1799  data: 0.0005  max mem: 1378\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.6063 (0.7025)  Acc@1: 83.3333 (81.0484)  Acc@5: 95.8333 (96.9086)  Acc@task: 75.0000 (79.8387)  time: 0.1804  data: 0.0003  max mem: 1378\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.5306 (0.6659)  Acc@1: 83.3333 (81.8089)  Acc@5: 100.0000 (97.5610)  Acc@task: 83.3333 (81.0976)  time: 0.1817  data: 0.0003  max mem: 1378\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.4824 (0.6564)  Acc@1: 83.3333 (82.0000)  Acc@5: 100.0000 (97.6000)  Acc@task: 83.3333 (81.4000)  time: 0.1783  data: 0.0003  max mem: 1378\n",
            "Test: [Task 1] Total time: 0:00:08 (0.1983 s / it)\n",
            "* Acc@task 81.400 Acc@1 82.000 Acc@5 97.600 loss 0.656\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:18  Loss: 0.8723 (0.8723)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 70.8333 (70.8333)  time: 0.4358  data: 0.2633  max mem: 1378\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.7895 (0.8370)  Acc@1: 79.1667 (81.4394)  Acc@5: 95.8333 (95.8333)  Acc@task: 70.8333 (73.1061)  time: 0.2071  data: 0.0289  max mem: 1378\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.7604 (0.9313)  Acc@1: 79.1667 (78.7698)  Acc@5: 95.8333 (95.0397)  Acc@task: 75.0000 (74.4048)  time: 0.1817  data: 0.0029  max mem: 1378\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.7472 (0.8763)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.5645)  Acc@task: 75.0000 (74.7312)  time: 0.1802  data: 0.0007  max mem: 1378\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.6337 (0.8269)  Acc@1: 79.1667 (79.8781)  Acc@5: 95.8333 (95.6301)  Acc@task: 75.0000 (74.8984)  time: 0.1808  data: 0.0008  max mem: 1378\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.6135 (0.8181)  Acc@1: 79.1667 (79.9000)  Acc@5: 95.8333 (95.7000)  Acc@task: 75.0000 (74.9000)  time: 0.1775  data: 0.0008  max mem: 1378\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1883 s / it)\n",
            "* Acc@task 74.900 Acc@1 79.900 Acc@5 95.700 loss 0.818\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:20  Loss: 0.1852 (0.1852)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  Acc@task: 87.5000 (87.5000)  time: 0.4954  data: 0.3190  max mem: 1378\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 0.7703 (0.7476)  Acc@1: 83.3333 (82.9545)  Acc@5: 95.8333 (96.5909)  Acc@task: 79.1667 (79.5455)  time: 0.2100  data: 0.0310  max mem: 1378\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.6896 (0.7253)  Acc@1: 83.3333 (82.7381)  Acc@5: 95.8333 (96.2302)  Acc@task: 79.1667 (79.3651)  time: 0.1803  data: 0.0013  max mem: 1378\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.5705 (0.6947)  Acc@1: 79.1667 (82.5269)  Acc@5: 95.8333 (96.7742)  Acc@task: 79.1667 (79.8387)  time: 0.1797  data: 0.0003  max mem: 1378\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.5726 (0.6905)  Acc@1: 83.3333 (82.9268)  Acc@5: 95.8333 (96.4431)  Acc@task: 79.1667 (79.7764)  time: 0.1809  data: 0.0002  max mem: 1378\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.5726 (0.6899)  Acc@1: 83.3333 (82.9000)  Acc@5: 95.8333 (96.5000)  Acc@task: 79.1667 (79.6000)  time: 0.1775  data: 0.0002  max mem: 1378\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1892 s / it)\n",
            "* Acc@task 79.600 Acc@1 82.900 Acc@5 96.500 loss 0.690\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:18  Loss: 0.6327 (0.6327)  Acc@1: 83.3333 (83.3333)  Acc@5: 91.6667 (91.6667)  Acc@task: 62.5000 (62.5000)  time: 0.4494  data: 0.2757  max mem: 1378\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:06  Loss: 0.6912 (0.6726)  Acc@1: 83.3333 (82.9545)  Acc@5: 95.8333 (95.4545)  Acc@task: 79.1667 (77.6515)  time: 0.2042  data: 0.0277  max mem: 1378\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.6912 (0.6986)  Acc@1: 83.3333 (82.7381)  Acc@5: 95.8333 (96.2302)  Acc@task: 79.1667 (79.3651)  time: 0.1798  data: 0.0018  max mem: 1378\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.5794 (0.6689)  Acc@1: 83.3333 (82.7957)  Acc@5: 95.8333 (96.3710)  Acc@task: 83.3333 (79.4355)  time: 0.1807  data: 0.0007  max mem: 1378\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7005 (0.7251)  Acc@1: 83.3333 (81.7073)  Acc@5: 95.8333 (95.8333)  Acc@task: 79.1667 (79.2683)  time: 0.1812  data: 0.0004  max mem: 1378\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.5948 (0.7100)  Acc@1: 83.3333 (81.9000)  Acc@5: 95.8333 (95.9000)  Acc@task: 79.1667 (79.5000)  time: 0.1779  data: 0.0004  max mem: 1378\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1878 s / it)\n",
            "* Acc@task 79.500 Acc@1 81.900 Acc@5 95.900 loss 0.710\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:24  Loss: 0.2300 (0.2300)  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)  Acc@task: 75.0000 (75.0000)  time: 0.5893  data: 0.4273  max mem: 1378\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 0.4772 (0.5931)  Acc@1: 83.3333 (82.9545)  Acc@5: 100.0000 (98.1061)  Acc@task: 75.0000 (76.1364)  time: 0.2162  data: 0.0391  max mem: 1378\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.4772 (0.6327)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (97.2222)  Acc@task: 75.0000 (75.1984)  time: 0.1794  data: 0.0003  max mem: 1378\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.6005 (0.6318)  Acc@1: 79.1667 (82.7957)  Acc@5: 95.8333 (97.3118)  Acc@task: 75.0000 (75.2688)  time: 0.1801  data: 0.0003  max mem: 1378\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.6217 (0.6493)  Acc@1: 79.1667 (82.6220)  Acc@5: 95.8333 (97.0528)  Acc@task: 70.8333 (73.1707)  time: 0.1807  data: 0.0002  max mem: 1378\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.6306 (0.7007)  Acc@1: 79.1667 (82.1000)  Acc@5: 95.8333 (96.8000)  Acc@task: 70.8333 (73.0000)  time: 0.1770  data: 0.0002  max mem: 1378\n",
            "Test: [Task 5] Total time: 0:00:08 (0.1914 s / it)\n",
            "* Acc@task 73.000 Acc@1 82.100 Acc@5 96.800 loss 0.701\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:36  Loss: 5.1484 (5.1484)  Acc@1: 25.0000 (25.0000)  Acc@5: 75.0000 (75.0000)  Acc@task: 79.1667 (79.1667)  time: 0.8800  data: 0.6997  max mem: 1378\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:07  Loss: 5.5855 (5.6444)  Acc@1: 4.1667 (8.7121)  Acc@5: 66.6667 (65.1515)  Acc@task: 70.8333 (68.9394)  time: 0.2438  data: 0.0645  max mem: 1378\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:04  Loss: 5.5855 (5.6066)  Acc@1: 4.1667 (7.7381)  Acc@5: 62.5000 (64.2857)  Acc@task: 66.6667 (67.6587)  time: 0.1797  data: 0.0006  max mem: 1378\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 5.5148 (5.6711)  Acc@1: 4.1667 (7.1237)  Acc@5: 62.5000 (64.1129)  Acc@task: 66.6667 (65.9946)  time: 0.1805  data: 0.0004  max mem: 1378\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 5.6950 (5.6943)  Acc@1: 4.1667 (6.9106)  Acc@5: 62.5000 (63.1098)  Acc@task: 62.5000 (65.5488)  time: 0.1813  data: 0.0003  max mem: 1378\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 5.6950 (5.7015)  Acc@1: 4.1667 (6.9000)  Acc@5: 58.3333 (63.0000)  Acc@task: 62.5000 (65.5000)  time: 0.1779  data: 0.0003  max mem: 1378\n",
            "Test: [Task 6] Total time: 0:00:08 (0.1979 s / it)\n",
            "* Acc@task 65.500 Acc@1 6.900 Acc@5 63.000 loss 5.702\n",
            "[Average accuracy till task6]\tAcc@task: 75.6500\tAcc@1: 69.2833\tAcc@5: 90.9167\tLoss: 1.5461\tForgetting: 3.6200\tBackward: 53.7400\n",
            "torch.Size([62400, 384])\n",
            "Averaged stats: Lr: 0.005000  Loss: 0.0530  Acc@1: 98.3333 (95.2167)  Acc@5: 100.0000 (99.3833)\n",
            "torch.Size([62400, 384])\n",
            "Averaged stats: Lr: 0.004878  Loss: 0.0196  Acc@1: 100.0000 (99.3833)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([62400, 384])\n",
            "Averaged stats: Lr: 0.004523  Loss: 0.0170  Acc@1: 100.0000 (99.6000)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([62400, 384])\n",
            "Averaged stats: Lr: 0.003969  Loss: 0.0160  Acc@1: 100.0000 (99.6667)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([62400, 384])\n",
            "Averaged stats: Lr: 0.003273  Loss: 0.0088  Acc@1: 100.0000 (99.7000)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([62400, 384])\n",
            "Averaged stats: Lr: 0.002500  Loss: 0.0068  Acc@1: 100.0000 (99.6833)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([62400, 384])\n",
            "Averaged stats: Lr: 0.001727  Loss: 0.0074  Acc@1: 100.0000 (99.8000)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([62400, 384])\n",
            "Averaged stats: Lr: 0.001031  Loss: 0.0295  Acc@1: 100.0000 (99.8167)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([62400, 384])\n",
            "Averaged stats: Lr: 0.000477  Loss: 0.0070  Acc@1: 100.0000 (99.7667)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([62400, 384])\n",
            "Averaged stats: Lr: 0.000122  Loss: 0.0202  Acc@1: 100.0000 (99.8833)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [Task 1]  [ 0/23]  eta: 0:00:10  Loss: 0.9183 (0.9183)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  Acc@task: 79.1667 (79.1667)  time: 0.4638  data: 0.2887  max mem: 1378\n",
            "Test: [Task 1]  [10/23]  eta: 0:00:02  Loss: 1.0678 (1.2093)  Acc@1: 75.0000 (73.1061)  Acc@5: 91.6667 (92.0455)  Acc@task: 66.6667 (66.2879)  time: 0.2067  data: 0.0306  max mem: 1378\n",
            "Test: [Task 1]  [20/23]  eta: 0:00:00  Loss: 0.9598 (1.0699)  Acc@1: 75.0000 (74.2064)  Acc@5: 91.6667 (93.0556)  Acc@task: 66.6667 (67.8571)  time: 0.1790  data: 0.0025  max mem: 1378\n",
            "Test: [Task 1]  [22/23]  eta: 0:00:00  Loss: 0.9590 (1.1213)  Acc@1: 75.0000 (73.8806)  Acc@5: 91.6667 (92.7239)  Acc@task: 66.6667 (67.7239)  time: 0.1722  data: 0.0002  max mem: 1378\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1907 s / it)\n",
            "* Acc@task 67.724 Acc@1 73.881 Acc@5 92.724 loss 1.121\n",
            "Test: [Task 2]  [ 0/20]  eta: 0:00:17  Loss: 2.0323 (2.0323)  Acc@1: 54.1667 (54.1667)  Acc@5: 87.5000 (87.5000)  Acc@task: 70.8333 (70.8333)  time: 0.8944  data: 0.6787  max mem: 1378\n",
            "Test: [Task 2]  [10/20]  eta: 0:00:02  Loss: 1.8688 (1.7611)  Acc@1: 58.3333 (60.6061)  Acc@5: 83.3333 (84.4697)  Acc@task: 62.5000 (60.6061)  time: 0.2414  data: 0.0637  max mem: 1378\n",
            "Test: [Task 2]  [19/20]  eta: 0:00:00  Loss: 1.6008 (1.7629)  Acc@1: 62.5000 (61.1691)  Acc@5: 83.3333 (84.9687)  Acc@task: 58.3333 (58.0376)  time: 0.2125  data: 0.0352  max mem: 1378\n",
            "Test: [Task 2] Total time: 0:00:04 (0.2176 s / it)\n",
            "* Acc@task 58.038 Acc@1 61.169 Acc@5 84.969 loss 1.763\n",
            "Test: [Task 3]  [ 0/21]  eta: 0:00:14  Loss: 2.1991 (2.1991)  Acc@1: 66.6667 (66.6667)  Acc@5: 75.0000 (75.0000)  Acc@task: 66.6667 (66.6667)  time: 0.7136  data: 0.5435  max mem: 1378\n",
            "Test: [Task 3]  [10/21]  eta: 0:00:02  Loss: 2.2629 (2.5223)  Acc@1: 54.1667 (56.8182)  Acc@5: 79.1667 (76.8939)  Acc@task: 66.6667 (64.3939)  time: 0.2269  data: 0.0497  max mem: 1378\n",
            "Test: [Task 3]  [20/21]  eta: 0:00:00  Loss: 2.2629 (2.4154)  Acc@1: 54.1667 (56.9417)  Acc@5: 79.1667 (78.6720)  Acc@task: 66.6667 (65.1911)  time: 0.1757  data: 0.0002  max mem: 1378\n",
            "Test: [Task 3] Total time: 0:00:04 (0.2065 s / it)\n",
            "* Acc@task 65.191 Acc@1 56.942 Acc@5 78.672 loss 2.415\n",
            "Test: [Task 4]  [ 0/20]  eta: 0:00:11  Loss: 2.5415 (2.5415)  Acc@1: 50.0000 (50.0000)  Acc@5: 79.1667 (79.1667)  Acc@task: 58.3333 (58.3333)  time: 0.5554  data: 0.3845  max mem: 1378\n",
            "Test: [Task 4]  [10/20]  eta: 0:00:02  Loss: 2.1503 (2.1512)  Acc@1: 54.1667 (57.1970)  Acc@5: 83.3333 (81.0606)  Acc@task: 58.3333 (60.6061)  time: 0.2123  data: 0.0354  max mem: 1378\n",
            "Test: [Task 4]  [19/20]  eta: 0:00:00  Loss: 2.1503 (2.2589)  Acc@1: 58.3333 (56.9002)  Acc@5: 80.0000 (79.8302)  Acc@task: 62.5000 (61.9958)  time: 0.1946  data: 0.0196  max mem: 1378\n",
            "Test: [Task 4] Total time: 0:00:03 (0.1994 s / it)\n",
            "* Acc@task 61.996 Acc@1 56.900 Acc@5 79.830 loss 2.259\n",
            "Test: [Task 5]  [ 0/21]  eta: 0:00:10  Loss: 1.6990 (1.6990)  Acc@1: 58.3333 (58.3333)  Acc@5: 91.6667 (91.6667)  Acc@task: 75.0000 (75.0000)  time: 0.4992  data: 0.3289  max mem: 1378\n",
            "Test: [Task 5]  [10/21]  eta: 0:00:02  Loss: 2.1794 (2.1333)  Acc@1: 54.1667 (55.6818)  Acc@5: 83.3333 (82.5758)  Acc@task: 58.3333 (60.6061)  time: 0.2108  data: 0.0326  max mem: 1378\n",
            "Test: [Task 5]  [20/21]  eta: 0:00:00  Loss: 2.2183 (2.3174)  Acc@1: 54.1667 (55.8045)  Acc@5: 79.1667 (80.0407)  Acc@task: 54.5455 (59.4705)  time: 0.1758  data: 0.0017  max mem: 1378\n",
            "Test: [Task 5] Total time: 0:00:04 (0.1975 s / it)\n",
            "* Acc@task 59.470 Acc@1 55.804 Acc@5 80.041 loss 2.317\n",
            "Test: [Task 6]  [ 0/20]  eta: 0:00:17  Loss: 1.7399 (1.7399)  Acc@1: 58.3333 (58.3333)  Acc@5: 79.1667 (79.1667)  Acc@task: 58.3333 (58.3333)  time: 0.8835  data: 0.7158  max mem: 1378\n",
            "Test: [Task 6]  [10/20]  eta: 0:00:02  Loss: 1.7399 (1.8320)  Acc@1: 62.5000 (60.6061)  Acc@5: 83.3333 (84.8485)  Acc@task: 58.3333 (59.4697)  time: 0.2438  data: 0.0656  max mem: 1378\n",
            "Test: [Task 6]  [19/20]  eta: 0:00:00  Loss: 1.8776 (1.9915)  Acc@1: 58.3333 (57.2917)  Acc@5: 83.3333 (84.3750)  Acc@task: 58.3333 (57.9167)  time: 0.2151  data: 0.0362  max mem: 1378\n",
            "Test: [Task 6] Total time: 0:00:04 (0.2195 s / it)\n",
            "* Acc@task 57.917 Acc@1 57.292 Acc@5 84.375 loss 1.992\n",
            "[Average accuracy till task6]\tAcc@task: 61.7226\tAcc@1: 60.3313\tAcc@5: 83.4351\tLoss: 1.9779\tForgetting: 0.0000\tBackward: 60.9392\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:19  Loss: 0.6612 (0.6612)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  Acc@task: 79.1667 (79.1667)  time: 0.4546  data: 0.2797  max mem: 1378\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.7178 (0.8603)  Acc@1: 79.1667 (76.8939)  Acc@5: 100.0000 (96.5909)  Acc@task: 79.1667 (80.3030)  time: 0.2082  data: 0.0288  max mem: 1378\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.7178 (0.7858)  Acc@1: 79.1667 (79.3651)  Acc@5: 95.8333 (96.4286)  Acc@task: 79.1667 (79.9603)  time: 0.1826  data: 0.0020  max mem: 1378\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.6395 (0.7352)  Acc@1: 83.3333 (80.9140)  Acc@5: 95.8333 (96.9086)  Acc@task: 75.0000 (79.8387)  time: 0.1820  data: 0.0004  max mem: 1378\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.5366 (0.6852)  Acc@1: 83.3333 (81.7073)  Acc@5: 100.0000 (97.3577)  Acc@task: 83.3333 (81.0976)  time: 0.1825  data: 0.0004  max mem: 1378\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.5085 (0.6764)  Acc@1: 83.3333 (81.8000)  Acc@5: 100.0000 (97.4000)  Acc@task: 83.3333 (81.4000)  time: 0.1791  data: 0.0003  max mem: 1378\n",
            "Test: [Task 1] Total time: 0:00:08 (0.1908 s / it)\n",
            "* Acc@task 81.400 Acc@1 81.800 Acc@5 97.400 loss 0.676\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:35  Loss: 0.8921 (0.8921)  Acc@1: 83.3333 (83.3333)  Acc@5: 91.6667 (91.6667)  Acc@task: 70.8333 (70.8333)  time: 0.8559  data: 0.6115  max mem: 1378\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:07  Loss: 0.7363 (0.8680)  Acc@1: 83.3333 (81.8182)  Acc@5: 95.8333 (95.8333)  Acc@task: 70.8333 (73.1061)  time: 0.2439  data: 0.0561  max mem: 1378\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.7445 (0.9541)  Acc@1: 79.1667 (78.9683)  Acc@5: 95.8333 (94.8413)  Acc@task: 75.0000 (74.4048)  time: 0.1818  data: 0.0006  max mem: 1378\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.7953 (0.9145)  Acc@1: 79.1667 (78.6290)  Acc@5: 95.8333 (95.5645)  Acc@task: 75.0000 (74.7312)  time: 0.1820  data: 0.0005  max mem: 1378\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.6500 (0.8525)  Acc@1: 79.1667 (79.3699)  Acc@5: 95.8333 (95.5285)  Acc@task: 75.0000 (74.8984)  time: 0.1832  data: 0.0003  max mem: 1378\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.6274 (0.8471)  Acc@1: 79.1667 (79.5000)  Acc@5: 95.8333 (95.6000)  Acc@task: 75.0000 (74.9000)  time: 0.1798  data: 0.0003  max mem: 1378\n",
            "Test: [Task 2] Total time: 0:00:08 (0.1995 s / it)\n",
            "* Acc@task 74.900 Acc@1 79.500 Acc@5 95.600 loss 0.847\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:24  Loss: 0.1905 (0.1905)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  Acc@task: 87.5000 (87.5000)  time: 0.5879  data: 0.4026  max mem: 1378\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:07  Loss: 0.8833 (0.8131)  Acc@1: 83.3333 (81.8182)  Acc@5: 95.8333 (95.4545)  Acc@task: 79.1667 (79.5455)  time: 0.2191  data: 0.0372  max mem: 1378\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.8366 (0.8102)  Acc@1: 83.3333 (81.9444)  Acc@5: 95.8333 (95.4365)  Acc@task: 79.1667 (79.3651)  time: 0.1818  data: 0.0007  max mem: 1378\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.6790 (0.7784)  Acc@1: 79.1667 (81.4516)  Acc@5: 95.8333 (96.2366)  Acc@task: 79.1667 (79.8387)  time: 0.1820  data: 0.0007  max mem: 1378\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.6671 (0.7667)  Acc@1: 83.3333 (82.1138)  Acc@5: 95.8333 (95.9350)  Acc@task: 79.1667 (79.7764)  time: 0.1828  data: 0.0006  max mem: 1378\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.6671 (0.7659)  Acc@1: 83.3333 (82.1000)  Acc@5: 95.8333 (96.0000)  Acc@task: 79.1667 (79.6000)  time: 0.1791  data: 0.0006  max mem: 1378\n",
            "Test: [Task 3] Total time: 0:00:08 (0.1937 s / it)\n",
            "* Acc@task 79.600 Acc@1 82.100 Acc@5 96.000 loss 0.766\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:36  Loss: 0.7457 (0.7457)  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  Acc@task: 62.5000 (62.5000)  time: 0.8693  data: 0.6646  max mem: 1378\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:07  Loss: 0.7457 (0.7385)  Acc@1: 83.3333 (80.6818)  Acc@5: 95.8333 (95.4545)  Acc@task: 79.1667 (77.6515)  time: 0.2444  data: 0.0610  max mem: 1378\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.6998 (0.7808)  Acc@1: 79.1667 (80.9524)  Acc@5: 95.8333 (96.2302)  Acc@task: 79.1667 (79.3651)  time: 0.1809  data: 0.0005  max mem: 1378\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.6768 (0.7427)  Acc@1: 79.1667 (81.0484)  Acc@5: 95.8333 (96.5054)  Acc@task: 83.3333 (79.4355)  time: 0.1810  data: 0.0003  max mem: 1378\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7242 (0.8053)  Acc@1: 79.1667 (79.9797)  Acc@5: 95.8333 (96.0366)  Acc@task: 79.1667 (79.2683)  time: 0.1822  data: 0.0003  max mem: 1378\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.6834 (0.7906)  Acc@1: 79.1667 (80.2000)  Acc@5: 95.8333 (96.1000)  Acc@task: 79.1667 (79.5000)  time: 0.1789  data: 0.0003  max mem: 1378\n",
            "Test: [Task 4] Total time: 0:00:08 (0.1989 s / it)\n",
            "* Acc@task 79.500 Acc@1 80.200 Acc@5 96.100 loss 0.791\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:19  Loss: 0.4861 (0.4861)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  Acc@task: 75.0000 (75.0000)  time: 0.4550  data: 0.2783  max mem: 1378\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 0.5789 (0.7143)  Acc@1: 79.1667 (80.3030)  Acc@5: 95.8333 (97.3485)  Acc@task: 75.0000 (76.1364)  time: 0.2076  data: 0.0301  max mem: 1378\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.6454 (0.7434)  Acc@1: 79.1667 (81.1508)  Acc@5: 95.8333 (96.4286)  Acc@task: 75.0000 (75.1984)  time: 0.1822  data: 0.0032  max mem: 1378\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.7743 (0.7468)  Acc@1: 79.1667 (80.3763)  Acc@5: 95.8333 (96.1022)  Acc@task: 75.0000 (75.2688)  time: 0.1811  data: 0.0008  max mem: 1378\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.8131 (0.7976)  Acc@1: 75.0000 (79.7764)  Acc@5: 95.8333 (95.8333)  Acc@task: 70.8333 (73.1707)  time: 0.1809  data: 0.0003  max mem: 1378\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.8306 (0.8512)  Acc@1: 75.0000 (79.3000)  Acc@5: 95.8333 (95.6000)  Acc@task: 70.8333 (73.0000)  time: 0.1778  data: 0.0003  max mem: 1378\n",
            "Test: [Task 5] Total time: 0:00:07 (0.1889 s / it)\n",
            "* Acc@task 73.000 Acc@1 79.300 Acc@5 95.600 loss 0.851\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:21  Loss: 0.4076 (0.4076)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  Acc@task: 79.1667 (79.1667)  time: 0.5148  data: 0.3046  max mem: 1378\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:06  Loss: 0.7181 (0.7343)  Acc@1: 79.1667 (78.7879)  Acc@5: 100.0000 (97.3485)  Acc@task: 70.8333 (68.9394)  time: 0.2099  data: 0.0282  max mem: 1378\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:04  Loss: 0.7456 (0.7681)  Acc@1: 79.1667 (78.5714)  Acc@5: 95.8333 (97.0238)  Acc@task: 66.6667 (67.6587)  time: 0.1791  data: 0.0005  max mem: 1378\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 0.9020 (0.8080)  Acc@1: 75.0000 (77.8226)  Acc@5: 95.8333 (97.0430)  Acc@task: 66.6667 (65.9946)  time: 0.1793  data: 0.0004  max mem: 1378\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 0.8899 (0.8372)  Acc@1: 75.0000 (77.3374)  Acc@5: 95.8333 (97.1545)  Acc@task: 62.5000 (65.5488)  time: 0.1798  data: 0.0003  max mem: 1378\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 0.8899 (0.8499)  Acc@1: 75.0000 (77.2000)  Acc@5: 100.0000 (97.2000)  Acc@task: 62.5000 (65.5000)  time: 0.1765  data: 0.0003  max mem: 1378\n",
            "Test: [Task 6] Total time: 0:00:07 (0.1895 s / it)\n",
            "* Acc@task 65.500 Acc@1 77.200 Acc@5 97.200 loss 0.850\n",
            "[Average accuracy till task6]\tAcc@task: 75.6500\tAcc@1: 80.0167\tAcc@5: 96.3167\tLoss: 0.7968\tForgetting: 0.0000\tBackward: 80.5800\n",
            "Loading checkpoint from: ./output/cifar100_full_dino_5epoch_10pct/checkpoint/task7_checkpoint.pth\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/engines/hide_promtp_wtp_and_tap_engine.py:574: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  loss = torch.nn.functional.cross_entropy(sim, torch.range(0, sim.shape[0] - 1).long().to(device))\n",
            "Train: Epoch[ 1/10]  [ 0/23]  eta: 0:00:21  Lr: 0.002812  Loss: 3.7472  Acc@1: 4.1667 (4.1667)  Acc@5: 54.1667 (54.1667)  time: 0.9142  data: 0.6382  max mem: 1378\n",
            "Train: Epoch[ 1/10]  [10/23]  eta: 0:00:04  Lr: 0.002812  Loss: 2.9970  Acc@1: 8.3333 (10.9848)  Acc@5: 54.1667 (53.0303)  time: 0.3374  data: 0.0583  max mem: 1378\n",
            "Train: Epoch[ 1/10]  [20/23]  eta: 0:00:00  Lr: 0.002812  Loss: 2.3734  Acc@1: 16.6667 (17.2619)  Acc@5: 58.3333 (59.9206)  time: 0.2794  data: 0.0002  max mem: 1378\n",
            "Train: Epoch[ 1/10]  [22/23]  eta: 0:00:00  Lr: 0.002812  Loss: 2.0739  Acc@1: 20.8333 (18.1818)  Acc@5: 62.5000 (61.0909)  time: 0.2794  data: 0.0002  max mem: 1378\n",
            "Train: Epoch[ 1/10] Total time: 0:00:07 (0.3102 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 2.0739  Acc@1: 20.8333 (18.1818)  Acc@5: 62.5000 (61.0909)\n",
            "Train: Epoch[ 2/10]  [ 0/23]  eta: 0:00:15  Lr: 0.002812  Loss: 2.2709  Acc@1: 25.0000 (25.0000)  Acc@5: 87.5000 (87.5000)  time: 0.6626  data: 0.3462  max mem: 1378\n",
            "Train: Epoch[ 2/10]  [10/23]  eta: 0:00:04  Lr: 0.002812  Loss: 1.5468  Acc@1: 37.5000 (36.3636)  Acc@5: 87.5000 (85.9849)  time: 0.3149  data: 0.0321  max mem: 1378\n",
            "Train: Epoch[ 2/10]  [20/23]  eta: 0:00:00  Lr: 0.002812  Loss: 1.4829  Acc@1: 41.6667 (42.2619)  Acc@5: 87.5000 (87.3016)  time: 0.2800  data: 0.0006  max mem: 1378\n",
            "Train: Epoch[ 2/10]  [22/23]  eta: 0:00:00  Lr: 0.002812  Loss: 1.8067  Acc@1: 41.6667 (43.6364)  Acc@5: 87.5000 (86.9091)  time: 0.2797  data: 0.0004  max mem: 1378\n",
            "Train: Epoch[ 2/10] Total time: 0:00:06 (0.3000 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.8067  Acc@1: 41.6667 (43.6364)  Acc@5: 87.5000 (86.9091)\n",
            "Train: Epoch[ 3/10]  [ 0/23]  eta: 0:00:13  Lr: 0.002812  Loss: 1.4089  Acc@1: 58.3333 (58.3333)  Acc@5: 95.8333 (95.8333)  time: 0.5832  data: 0.2964  max mem: 1378\n",
            "Train: Epoch[ 3/10]  [10/23]  eta: 0:00:03  Lr: 0.002812  Loss: 1.1074  Acc@1: 62.5000 (62.1212)  Acc@5: 95.8333 (94.6970)  time: 0.3060  data: 0.0272  max mem: 1378\n",
            "Train: Epoch[ 3/10]  [20/23]  eta: 0:00:00  Lr: 0.002812  Loss: 1.2972  Acc@1: 66.6667 (65.8730)  Acc@5: 95.8333 (94.4444)  time: 0.2790  data: 0.0002  max mem: 1378\n",
            "Train: Epoch[ 3/10]  [22/23]  eta: 0:00:00  Lr: 0.002812  Loss: 1.0298  Acc@1: 66.6667 (65.8182)  Acc@5: 95.8333 (94.0000)  time: 0.2788  data: 0.0002  max mem: 1378\n",
            "Train: Epoch[ 3/10] Total time: 0:00:06 (0.2953 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.0298  Acc@1: 66.6667 (65.8182)  Acc@5: 95.8333 (94.0000)\n",
            "Train: Epoch[ 4/10]  [ 0/23]  eta: 0:00:14  Lr: 0.002812  Loss: 0.7761  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.6263  data: 0.3436  max mem: 1378\n",
            "Train: Epoch[ 4/10]  [10/23]  eta: 0:00:04  Lr: 0.002812  Loss: 0.7179  Acc@1: 75.0000 (76.1364)  Acc@5: 100.0000 (98.4848)  time: 0.3114  data: 0.0316  max mem: 1378\n",
            "Train: Epoch[ 4/10]  [20/23]  eta: 0:00:00  Lr: 0.002812  Loss: 0.9170  Acc@1: 70.8333 (73.0159)  Acc@5: 100.0000 (97.8175)  time: 0.2796  data: 0.0006  max mem: 1378\n",
            "Train: Epoch[ 4/10]  [22/23]  eta: 0:00:00  Lr: 0.002812  Loss: 0.8145  Acc@1: 70.8333 (72.7273)  Acc@5: 95.8333 (97.2727)  time: 0.2796  data: 0.0005  max mem: 1378\n",
            "Train: Epoch[ 4/10] Total time: 0:00:06 (0.2980 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.8145  Acc@1: 70.8333 (72.7273)  Acc@5: 95.8333 (97.2727)\n",
            "Train: Epoch[ 5/10]  [ 0/23]  eta: 0:00:15  Lr: 0.002812  Loss: 0.6960  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  time: 0.6790  data: 0.3788  max mem: 1378\n",
            "Train: Epoch[ 5/10]  [10/23]  eta: 0:00:04  Lr: 0.002812  Loss: 0.5644  Acc@1: 70.8333 (70.0758)  Acc@5: 95.8333 (96.2121)  time: 0.3149  data: 0.0347  max mem: 1378\n",
            "Train: Epoch[ 5/10]  [20/23]  eta: 0:00:00  Lr: 0.002812  Loss: 0.9821  Acc@1: 70.8333 (72.8175)  Acc@5: 95.8333 (96.6270)  time: 0.2789  data: 0.0003  max mem: 1378\n",
            "Train: Epoch[ 5/10]  [22/23]  eta: 0:00:00  Lr: 0.002812  Loss: 0.6685  Acc@1: 70.8333 (73.2727)  Acc@5: 95.8333 (96.7273)  time: 0.2789  data: 0.0002  max mem: 1378\n",
            "Train: Epoch[ 5/10] Total time: 0:00:06 (0.2995 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.6685  Acc@1: 70.8333 (73.2727)  Acc@5: 95.8333 (96.7273)\n",
            "Train: Epoch[ 6/10]  [ 0/23]  eta: 0:00:17  Lr: 0.002812  Loss: 0.2559  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  time: 0.7682  data: 0.4873  max mem: 1378\n",
            "Train: Epoch[ 6/10]  [10/23]  eta: 0:00:04  Lr: 0.002812  Loss: 0.7253  Acc@1: 75.0000 (80.6818)  Acc@5: 100.0000 (98.8636)  time: 0.3242  data: 0.0445  max mem: 1378\n",
            "Train: Epoch[ 6/10]  [20/23]  eta: 0:00:00  Lr: 0.002812  Loss: 0.5567  Acc@1: 79.1667 (81.1508)  Acc@5: 100.0000 (98.2143)  time: 0.2802  data: 0.0003  max mem: 1378\n",
            "Train: Epoch[ 6/10]  [22/23]  eta: 0:00:00  Lr: 0.002812  Loss: 0.7573  Acc@1: 79.1667 (81.2727)  Acc@5: 100.0000 (98.3636)  time: 0.2806  data: 0.0003  max mem: 1378\n",
            "Train: Epoch[ 6/10] Total time: 0:00:07 (0.3047 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.7573  Acc@1: 79.1667 (81.2727)  Acc@5: 100.0000 (98.3636)\n",
            "Train: Epoch[ 7/10]  [ 0/23]  eta: 0:00:13  Lr: 0.002812  Loss: 0.5562  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.5886  data: 0.3023  max mem: 1378\n",
            "Train: Epoch[ 7/10]  [10/23]  eta: 0:00:03  Lr: 0.002812  Loss: 0.5011  Acc@1: 83.3333 (81.0606)  Acc@5: 100.0000 (98.4848)  time: 0.3076  data: 0.0289  max mem: 1378\n",
            "Train: Epoch[ 7/10]  [20/23]  eta: 0:00:00  Lr: 0.002812  Loss: 0.8884  Acc@1: 83.3333 (81.3492)  Acc@5: 100.0000 (98.6111)  time: 0.2802  data: 0.0009  max mem: 1378\n",
            "Train: Epoch[ 7/10]  [22/23]  eta: 0:00:00  Lr: 0.002812  Loss: 0.4248  Acc@1: 79.1667 (81.2727)  Acc@5: 100.0000 (98.5455)  time: 0.2803  data: 0.0002  max mem: 1378\n",
            "Train: Epoch[ 7/10] Total time: 0:00:06 (0.2973 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.4248  Acc@1: 79.1667 (81.2727)  Acc@5: 100.0000 (98.5455)\n",
            "Train: Epoch[ 8/10]  [ 0/23]  eta: 0:00:15  Lr: 0.002812  Loss: 0.5945  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.6824  data: 0.3889  max mem: 1378\n",
            "Train: Epoch[ 8/10]  [10/23]  eta: 0:00:04  Lr: 0.002812  Loss: 0.4722  Acc@1: 83.3333 (85.2273)  Acc@5: 100.0000 (99.2424)  time: 0.3181  data: 0.0366  max mem: 1378\n",
            "Train: Epoch[ 8/10]  [20/23]  eta: 0:00:00  Lr: 0.002812  Loss: 0.3698  Acc@1: 87.5000 (86.1111)  Acc@5: 100.0000 (98.6111)  time: 0.2816  data: 0.0009  max mem: 1378\n",
            "Train: Epoch[ 8/10]  [22/23]  eta: 0:00:00  Lr: 0.002812  Loss: 0.5537  Acc@1: 87.5000 (85.8182)  Acc@5: 100.0000 (98.7273)  time: 0.2813  data: 0.0008  max mem: 1378\n",
            "Train: Epoch[ 8/10] Total time: 0:00:06 (0.3020 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.5537  Acc@1: 87.5000 (85.8182)  Acc@5: 100.0000 (98.7273)\n",
            "Train: Epoch[ 9/10]  [ 0/23]  eta: 0:00:15  Lr: 0.002812  Loss: 0.7341  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.6727  data: 0.3920  max mem: 1378\n",
            "Train: Epoch[ 9/10]  [10/23]  eta: 0:00:04  Lr: 0.002812  Loss: 0.7809  Acc@1: 87.5000 (84.8485)  Acc@5: 100.0000 (97.7273)  time: 0.3172  data: 0.0362  max mem: 1378\n",
            "Train: Epoch[ 9/10]  [20/23]  eta: 0:00:00  Lr: 0.002812  Loss: 0.4166  Acc@1: 87.5000 (83.9286)  Acc@5: 100.0000 (98.4127)  time: 0.2812  data: 0.0004  max mem: 1378\n",
            "Train: Epoch[ 9/10]  [22/23]  eta: 0:00:00  Lr: 0.002812  Loss: 0.4779  Acc@1: 87.5000 (84.3636)  Acc@5: 100.0000 (98.5455)  time: 0.2815  data: 0.0004  max mem: 1378\n",
            "Train: Epoch[ 9/10] Total time: 0:00:06 (0.3032 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.4779  Acc@1: 87.5000 (84.3636)  Acc@5: 100.0000 (98.5455)\n",
            "Train: Epoch[10/10]  [ 0/23]  eta: 0:00:25  Lr: 0.002812  Loss: 0.6110  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 1.1140  data: 0.7535  max mem: 1378\n",
            "Train: Epoch[10/10]  [10/23]  eta: 0:00:04  Lr: 0.002812  Loss: 0.4145  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (98.4848)  time: 0.3569  data: 0.0697  max mem: 1378\n",
            "Train: Epoch[10/10]  [20/23]  eta: 0:00:00  Lr: 0.002812  Loss: 0.3022  Acc@1: 87.5000 (88.0952)  Acc@5: 100.0000 (98.6111)  time: 0.2821  data: 0.0009  max mem: 1378\n",
            "Train: Epoch[10/10]  [22/23]  eta: 0:00:00  Lr: 0.002812  Loss: 0.7110  Acc@1: 91.6667 (88.1818)  Acc@5: 100.0000 (98.7273)  time: 0.2817  data: 0.0009  max mem: 1378\n",
            "Train: Epoch[10/10] Total time: 0:00:07 (0.3216 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.7110  Acc@1: 91.6667 (88.1818)  Acc@5: 100.0000 (98.7273)\n",
            "torch.Size([5, 2, 5, 6, 64])\n",
            "torch.Size([5, 2, 1, 5, 6, 64])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/23]  eta: 0:00:13  Loss: 1.6939 (1.6939)  Acc@1: 58.3333 (58.3333)  Acc@5: 83.3333 (83.3333)  Acc@task: 62.5000 (62.5000)  time: 0.5955  data: 0.4218  max mem: 1378\n",
            "Test: [Task 1]  [10/23]  eta: 0:00:02  Loss: 1.1533 (1.0818)  Acc@1: 75.0000 (73.4849)  Acc@5: 91.6667 (92.8030)  Acc@task: 66.6667 (69.3182)  time: 0.2171  data: 0.0390  max mem: 1378\n",
            "Test: [Task 1]  [20/23]  eta: 0:00:00  Loss: 0.9136 (1.0409)  Acc@1: 75.0000 (74.4048)  Acc@5: 91.6667 (93.4524)  Acc@task: 66.6667 (68.6508)  time: 0.1788  data: 0.0006  max mem: 1378\n",
            "Test: [Task 1]  [22/23]  eta: 0:00:00  Loss: 0.9136 (0.9895)  Acc@1: 75.0000 (75.1866)  Acc@5: 95.8333 (93.8433)  Acc@task: 66.6667 (69.0299)  time: 0.1744  data: 0.0005  max mem: 1378\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1984 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@task 69.030 Acc@1 75.187 Acc@5 93.843 loss 0.990\n",
            "Test: [Task 2]  [ 0/20]  eta: 0:00:19  Loss: 3.2770 (3.2770)  Acc@1: 41.6667 (41.6667)  Acc@5: 70.8333 (70.8333)  Acc@task: 50.0000 (50.0000)  time: 0.9631  data: 0.7888  max mem: 1378\n",
            "Test: [Task 2]  [10/20]  eta: 0:00:02  Loss: 1.8706 (1.8986)  Acc@1: 58.3333 (60.2273)  Acc@5: 83.3333 (85.6061)  Acc@task: 54.1667 (57.1970)  time: 0.2511  data: 0.0726  max mem: 1378\n",
            "Test: [Task 2]  [19/20]  eta: 0:00:00  Loss: 1.5838 (1.8136)  Acc@1: 62.5000 (61.3779)  Acc@5: 87.5000 (85.5950)  Acc@task: 54.1667 (56.9937)  time: 0.2190  data: 0.0400  max mem: 1378\n",
            "Test: [Task 2] Total time: 0:00:04 (0.2240 s / it)\n",
            "* Acc@task 56.994 Acc@1 61.378 Acc@5 85.595 loss 1.814\n",
            "Test: [Task 3]  [ 0/21]  eta: 0:00:11  Loss: 2.7603 (2.7603)  Acc@1: 45.8333 (45.8333)  Acc@5: 75.0000 (75.0000)  Acc@task: 50.0000 (50.0000)  time: 0.5359  data: 0.3613  max mem: 1378\n",
            "Test: [Task 3]  [10/21]  eta: 0:00:02  Loss: 2.3483 (2.2734)  Acc@1: 54.1667 (57.1970)  Acc@5: 79.1667 (78.7879)  Acc@task: 70.8333 (65.1515)  time: 0.2122  data: 0.0337  max mem: 1378\n",
            "Test: [Task 3]  [20/21]  eta: 0:00:00  Loss: 2.0958 (2.2524)  Acc@1: 58.3333 (58.1489)  Acc@5: 79.1667 (78.8732)  Acc@task: 66.6667 (65.5936)  time: 0.1776  data: 0.0006  max mem: 1378\n",
            "Test: [Task 3] Total time: 0:00:04 (0.1992 s / it)\n",
            "* Acc@task 65.594 Acc@1 58.149 Acc@5 78.873 loss 2.252\n",
            "Test: [Task 4]  [ 0/20]  eta: 0:00:10  Loss: 2.2319 (2.2319)  Acc@1: 66.6667 (66.6667)  Acc@5: 79.1667 (79.1667)  Acc@task: 62.5000 (62.5000)  time: 0.5056  data: 0.3298  max mem: 1378\n",
            "Test: [Task 4]  [10/20]  eta: 0:00:02  Loss: 2.4769 (2.3909)  Acc@1: 54.1667 (57.1970)  Acc@5: 79.1667 (79.1667)  Acc@task: 58.3333 (59.0909)  time: 0.2117  data: 0.0347  max mem: 1378\n",
            "Test: [Task 4]  [19/20]  eta: 0:00:00  Loss: 2.4769 (2.4063)  Acc@1: 54.1667 (57.5372)  Acc@5: 79.1667 (79.6178)  Acc@task: 58.3333 (60.0849)  time: 0.1954  data: 0.0192  max mem: 1378\n",
            "Test: [Task 4] Total time: 0:00:04 (0.2021 s / it)\n",
            "* Acc@task 60.085 Acc@1 57.537 Acc@5 79.618 loss 2.406\n",
            "Test: [Task 5]  [ 0/21]  eta: 0:00:19  Loss: 2.2003 (2.2003)  Acc@1: 66.6667 (66.6667)  Acc@5: 79.1667 (79.1667)  Acc@task: 66.6667 (66.6667)  time: 0.9171  data: 0.7401  max mem: 1378\n",
            "Test: [Task 5]  [10/21]  eta: 0:00:02  Loss: 2.5809 (2.5089)  Acc@1: 54.1667 (53.4091)  Acc@5: 79.1667 (79.9242)  Acc@task: 62.5000 (57.5758)  time: 0.2489  data: 0.0683  max mem: 1378\n",
            "Test: [Task 5]  [20/21]  eta: 0:00:00  Loss: 2.5283 (2.5358)  Acc@1: 54.1667 (52.7495)  Acc@5: 79.1667 (79.0224)  Acc@task: 58.3333 (55.6008)  time: 0.1768  data: 0.0007  max mem: 1378\n",
            "Test: [Task 5] Total time: 0:00:04 (0.2168 s / it)\n",
            "* Acc@task 55.601 Acc@1 52.749 Acc@5 79.022 loss 2.536\n",
            "Test: [Task 6]  [ 0/20]  eta: 0:00:13  Loss: 1.9075 (1.9075)  Acc@1: 62.5000 (62.5000)  Acc@5: 79.1667 (79.1667)  Acc@task: 66.6667 (66.6667)  time: 0.6552  data: 0.4896  max mem: 1378\n",
            "Test: [Task 6]  [10/20]  eta: 0:00:02  Loss: 1.9644 (2.2220)  Acc@1: 54.1667 (55.6818)  Acc@5: 83.3333 (81.8182)  Acc@task: 54.1667 (57.1970)  time: 0.2244  data: 0.0448  max mem: 1378\n",
            "Test: [Task 6]  [19/20]  eta: 0:00:00  Loss: 1.9053 (2.0764)  Acc@1: 58.3333 (58.1250)  Acc@5: 83.3333 (83.9583)  Acc@task: 58.3333 (57.9167)  time: 0.2040  data: 0.0247  max mem: 1378\n",
            "Test: [Task 6] Total time: 0:00:04 (0.2085 s / it)\n",
            "* Acc@task 57.917 Acc@1 58.125 Acc@5 83.958 loss 2.076\n",
            "Test: [Task 7]  [ 0/23]  eta: 0:00:12  Loss: 5.5210 (5.5210)  Acc@1: 4.1667 (4.1667)  Acc@5: 25.0000 (25.0000)  Acc@task: 37.5000 (37.5000)  time: 0.5419  data: 0.3737  max mem: 1378\n",
            "Test: [Task 7]  [10/23]  eta: 0:00:02  Loss: 4.9884 (5.0396)  Acc@1: 8.3333 (6.8182)  Acc@5: 45.8333 (47.3485)  Acc@task: 62.5000 (60.6061)  time: 0.2135  data: 0.0351  max mem: 1378\n",
            "Test: [Task 7]  [20/23]  eta: 0:00:00  Loss: 4.8524 (5.1167)  Acc@1: 4.1667 (5.9524)  Acc@5: 50.0000 (48.2143)  Acc@task: 62.5000 (61.3095)  time: 0.1806  data: 0.0007  max mem: 1378\n",
            "Test: [Task 7]  [22/23]  eta: 0:00:00  Loss: 4.8074 (5.1398)  Acc@1: 4.1667 (5.8182)  Acc@5: 50.0000 (48.9091)  Acc@task: 62.5000 (60.1818)  time: 0.1811  data: 0.0003  max mem: 1378\n",
            "Test: [Task 7] Total time: 0:00:04 (0.2027 s / it)\n",
            "* Acc@task 60.182 Acc@1 5.818 Acc@5 48.909 loss 5.140\n",
            "[Average accuracy till task7]\tAcc@task: 60.7716\tAcc@1: 52.7062\tAcc@5: 78.5456\tLoss: 2.4591\tForgetting: 0.5092\tBackward: 50.9722\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:36  Loss: 0.6614 (0.6614)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  Acc@task: 79.1667 (79.1667)  time: 0.8755  data: 0.6702  max mem: 1378\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:07  Loss: 0.7027 (0.8595)  Acc@1: 79.1667 (76.5152)  Acc@5: 100.0000 (96.5909)  Acc@task: 79.1667 (79.5455)  time: 0.2434  data: 0.0621  max mem: 1378\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.7027 (0.7831)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (96.4286)  Acc@task: 79.1667 (80.7540)  time: 0.1808  data: 0.0008  max mem: 1378\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.6392 (0.7367)  Acc@1: 83.3333 (80.9140)  Acc@5: 95.8333 (96.9086)  Acc@task: 79.1667 (80.6452)  time: 0.1815  data: 0.0004  max mem: 1378\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.5376 (0.6865)  Acc@1: 83.3333 (81.7073)  Acc@5: 100.0000 (97.3577)  Acc@task: 83.3333 (81.6057)  time: 0.1817  data: 0.0003  max mem: 1378\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.5082 (0.6776)  Acc@1: 83.3333 (81.8000)  Acc@5: 100.0000 (97.4000)  Acc@task: 83.3333 (81.9000)  time: 0.1783  data: 0.0003  max mem: 1378\n",
            "Test: [Task 1] Total time: 0:00:08 (0.1987 s / it)\n",
            "* Acc@task 81.900 Acc@1 81.800 Acc@5 97.400 loss 0.678\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:18  Loss: 0.8922 (0.8922)  Acc@1: 83.3333 (83.3333)  Acc@5: 91.6667 (91.6667)  Acc@task: 62.5000 (62.5000)  time: 0.4366  data: 0.2690  max mem: 1378\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.8105 (0.8743)  Acc@1: 83.3333 (81.8182)  Acc@5: 95.8333 (95.4545)  Acc@task: 70.8333 (70.0758)  time: 0.2096  data: 0.0336  max mem: 1378\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.7642 (0.9638)  Acc@1: 79.1667 (78.9683)  Acc@5: 95.8333 (94.6429)  Acc@task: 70.8333 (71.4286)  time: 0.1848  data: 0.0053  max mem: 1378\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.7775 (0.9206)  Acc@1: 79.1667 (78.6290)  Acc@5: 95.8333 (95.4301)  Acc@task: 70.8333 (71.1022)  time: 0.1819  data: 0.0006  max mem: 1378\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.6492 (0.8569)  Acc@1: 79.1667 (79.3699)  Acc@5: 95.8333 (95.4268)  Acc@task: 70.8333 (71.7480)  time: 0.1814  data: 0.0005  max mem: 1378\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.6253 (0.8514)  Acc@1: 79.1667 (79.5000)  Acc@5: 95.8333 (95.5000)  Acc@task: 70.8333 (71.7000)  time: 0.1784  data: 0.0005  max mem: 1378\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1900 s / it)\n",
            "* Acc@task 71.700 Acc@1 79.500 Acc@5 95.500 loss 0.851\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:25  Loss: 0.2004 (0.2004)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.6106  data: 0.4430  max mem: 1378\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:07  Loss: 0.8828 (0.8075)  Acc@1: 83.3333 (81.8182)  Acc@5: 95.8333 (95.4545)  Acc@task: 79.1667 (78.7879)  time: 0.2199  data: 0.0405  max mem: 1378\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.8365 (0.8051)  Acc@1: 83.3333 (81.9444)  Acc@5: 95.8333 (95.4365)  Acc@task: 79.1667 (78.7698)  time: 0.1800  data: 0.0003  max mem: 1378\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.6290 (0.7734)  Acc@1: 79.1667 (81.4516)  Acc@5: 95.8333 (96.2366)  Acc@task: 75.0000 (78.6290)  time: 0.1803  data: 0.0003  max mem: 1378\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.6290 (0.7660)  Acc@1: 79.1667 (82.0122)  Acc@5: 95.8333 (95.8333)  Acc@task: 75.0000 (78.5569)  time: 0.1815  data: 0.0003  max mem: 1378\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.6290 (0.7654)  Acc@1: 81.2500 (82.0000)  Acc@5: 95.8333 (95.9000)  Acc@task: 75.0000 (78.5000)  time: 0.1779  data: 0.0003  max mem: 1378\n",
            "Test: [Task 3] Total time: 0:00:08 (0.1918 s / it)\n",
            "* Acc@task 78.500 Acc@1 82.000 Acc@5 95.900 loss 0.765\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:21  Loss: 0.7423 (0.7423)  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  Acc@task: 62.5000 (62.5000)  time: 0.5017  data: 0.3251  max mem: 1378\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:06  Loss: 0.7487 (0.7496)  Acc@1: 83.3333 (80.6818)  Acc@5: 95.8333 (95.4545)  Acc@task: 75.0000 (75.7576)  time: 0.2087  data: 0.0311  max mem: 1378\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.7487 (0.7839)  Acc@1: 79.1667 (80.7540)  Acc@5: 95.8333 (96.2302)  Acc@task: 79.1667 (76.9841)  time: 0.1796  data: 0.0012  max mem: 1378\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.6700 (0.7419)  Acc@1: 79.1667 (80.9140)  Acc@5: 95.8333 (96.5054)  Acc@task: 79.1667 (76.7473)  time: 0.1803  data: 0.0006  max mem: 1378\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7366 (0.8049)  Acc@1: 79.1667 (79.8781)  Acc@5: 95.8333 (96.0366)  Acc@task: 75.0000 (76.6260)  time: 0.1804  data: 0.0004  max mem: 1378\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.7065 (0.7902)  Acc@1: 79.1667 (80.1000)  Acc@5: 95.8333 (96.1000)  Acc@task: 75.0000 (76.9000)  time: 0.1769  data: 0.0003  max mem: 1378\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1886 s / it)\n",
            "* Acc@task 76.900 Acc@1 80.100 Acc@5 96.100 loss 0.790\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:19  Loss: 0.4957 (0.4957)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  Acc@task: 66.6667 (66.6667)  time: 0.4636  data: 0.2996  max mem: 1378\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 0.6005 (0.7071)  Acc@1: 79.1667 (80.6818)  Acc@5: 95.8333 (97.3485)  Acc@task: 70.8333 (74.6212)  time: 0.2064  data: 0.0279  max mem: 1378\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.6393 (0.7460)  Acc@1: 79.1667 (81.1508)  Acc@5: 95.8333 (96.4286)  Acc@task: 70.8333 (73.2143)  time: 0.1793  data: 0.0006  max mem: 1378\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.7488 (0.7429)  Acc@1: 79.1667 (80.5108)  Acc@5: 95.8333 (96.1022)  Acc@task: 75.0000 (73.9247)  time: 0.1791  data: 0.0004  max mem: 1378\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.7743 (0.7947)  Acc@1: 79.1667 (79.9797)  Acc@5: 95.8333 (95.8333)  Acc@task: 70.8333 (72.8659)  time: 0.1805  data: 0.0003  max mem: 1378\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.8133 (0.8483)  Acc@1: 79.1667 (79.5000)  Acc@5: 95.8333 (95.6000)  Acc@task: 70.8333 (72.7000)  time: 0.1770  data: 0.0003  max mem: 1378\n",
            "Test: [Task 5] Total time: 0:00:07 (0.1889 s / it)\n",
            "* Acc@task 72.700 Acc@1 79.500 Acc@5 95.600 loss 0.848\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:39  Loss: 0.3918 (0.3918)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  Acc@task: 79.1667 (79.1667)  time: 0.9389  data: 0.7345  max mem: 1378\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:07  Loss: 0.7095 (0.6911)  Acc@1: 79.1667 (80.6818)  Acc@5: 100.0000 (98.1061)  Acc@task: 75.0000 (75.0000)  time: 0.2482  data: 0.0670  max mem: 1378\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:04  Loss: 0.7184 (0.7378)  Acc@1: 79.1667 (79.9603)  Acc@5: 95.8333 (97.4206)  Acc@task: 75.0000 (74.6032)  time: 0.1790  data: 0.0003  max mem: 1378\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 0.8521 (0.7783)  Acc@1: 79.1667 (78.7634)  Acc@5: 95.8333 (97.3118)  Acc@task: 75.0000 (74.3280)  time: 0.1795  data: 0.0005  max mem: 1378\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 0.8317 (0.8024)  Acc@1: 75.0000 (78.4553)  Acc@5: 95.8333 (97.4594)  Acc@task: 70.8333 (73.7805)  time: 0.1801  data: 0.0004  max mem: 1378\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 0.8317 (0.8130)  Acc@1: 75.0000 (78.2000)  Acc@5: 100.0000 (97.5000)  Acc@task: 70.8333 (73.8000)  time: 0.1768  data: 0.0003  max mem: 1378\n",
            "Test: [Task 6] Total time: 0:00:08 (0.1990 s / it)\n",
            "* Acc@task 73.800 Acc@1 78.200 Acc@5 97.500 loss 0.813\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:26  Loss: 5.6467 (5.6467)  Acc@1: 4.1667 (4.1667)  Acc@5: 58.3333 (58.3333)  Acc@task: 54.1667 (54.1667)  time: 0.6219  data: 0.4502  max mem: 1378\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:07  Loss: 5.7236 (5.5672)  Acc@1: 4.1667 (6.4394)  Acc@5: 62.5000 (59.8485)  Acc@task: 58.3333 (60.9849)  time: 0.2192  data: 0.0415  max mem: 1378\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:04  Loss: 5.7236 (5.4740)  Acc@1: 4.1667 (6.7460)  Acc@5: 58.3333 (57.9365)  Acc@task: 62.5000 (61.3095)  time: 0.1791  data: 0.0005  max mem: 1378\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:02  Loss: 5.6925 (5.5385)  Acc@1: 8.3333 (7.3925)  Acc@5: 54.1667 (56.0484)  Acc@task: 62.5000 (62.3656)  time: 0.1798  data: 0.0011  max mem: 1378\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 5.5374 (5.5234)  Acc@1: 4.1667 (6.8089)  Acc@5: 54.1667 (55.7927)  Acc@task: 66.6667 (63.6179)  time: 0.1808  data: 0.0010  max mem: 1378\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 5.5079 (5.4915)  Acc@1: 4.1667 (6.9000)  Acc@5: 54.1667 (55.8000)  Acc@task: 66.6667 (63.9000)  time: 0.1772  data: 0.0010  max mem: 1378\n",
            "Test: [Task 7] Total time: 0:00:08 (0.1912 s / it)\n",
            "* Acc@task 63.900 Acc@1 6.900 Acc@5 55.800 loss 5.491\n",
            "[Average accuracy till task7]\tAcc@task: 74.2000\tAcc@1: 69.7143\tAcc@5: 90.5429\tLoss: 1.4625\tForgetting: 0.0333\tBackward: 67.3167\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:22  Loss: 0.6614 (0.6614)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  Acc@task: 79.1667 (79.1667)  time: 0.5455  data: 0.3800  max mem: 1378\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.7027 (0.8595)  Acc@1: 79.1667 (76.5152)  Acc@5: 100.0000 (96.5909)  Acc@task: 79.1667 (79.5455)  time: 0.2136  data: 0.0349  max mem: 1378\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.7027 (0.7831)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (96.4286)  Acc@task: 79.1667 (80.7540)  time: 0.1799  data: 0.0004  max mem: 1378\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.6392 (0.7367)  Acc@1: 83.3333 (80.9140)  Acc@5: 95.8333 (96.9086)  Acc@task: 79.1667 (80.6452)  time: 0.1797  data: 0.0005  max mem: 1378\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.5376 (0.6865)  Acc@1: 83.3333 (81.7073)  Acc@5: 100.0000 (97.3577)  Acc@task: 83.3333 (81.6057)  time: 0.1801  data: 0.0005  max mem: 1378\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.5082 (0.6776)  Acc@1: 83.3333 (81.8000)  Acc@5: 100.0000 (97.4000)  Acc@task: 83.3333 (81.9000)  time: 0.1763  data: 0.0005  max mem: 1378\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1895 s / it)\n",
            "* Acc@task 81.900 Acc@1 81.800 Acc@5 97.400 loss 0.678\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:21  Loss: 0.8922 (0.8922)  Acc@1: 83.3333 (83.3333)  Acc@5: 91.6667 (91.6667)  Acc@task: 62.5000 (62.5000)  time: 0.5124  data: 0.3377  max mem: 1378\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.8105 (0.8743)  Acc@1: 83.3333 (81.8182)  Acc@5: 95.8333 (95.4545)  Acc@task: 70.8333 (70.0758)  time: 0.2092  data: 0.0322  max mem: 1378\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.7642 (0.9638)  Acc@1: 79.1667 (78.9683)  Acc@5: 95.8333 (94.6429)  Acc@task: 70.8333 (71.4286)  time: 0.1791  data: 0.0015  max mem: 1378\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.7775 (0.9206)  Acc@1: 79.1667 (78.6290)  Acc@5: 95.8333 (95.4301)  Acc@task: 70.8333 (71.1022)  time: 0.1801  data: 0.0008  max mem: 1378\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.6492 (0.8569)  Acc@1: 79.1667 (79.3699)  Acc@5: 95.8333 (95.4268)  Acc@task: 70.8333 (71.7480)  time: 0.1808  data: 0.0003  max mem: 1378\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.6253 (0.8514)  Acc@1: 79.1667 (79.5000)  Acc@5: 95.8333 (95.5000)  Acc@task: 70.8333 (71.7000)  time: 0.1776  data: 0.0003  max mem: 1378\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1888 s / it)\n",
            "* Acc@task 71.700 Acc@1 79.500 Acc@5 95.500 loss 0.851\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:25  Loss: 0.2004 (0.2004)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.6018  data: 0.4355  max mem: 1378\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:07  Loss: 0.8828 (0.8075)  Acc@1: 83.3333 (81.8182)  Acc@5: 95.8333 (95.4545)  Acc@task: 79.1667 (78.7879)  time: 0.2196  data: 0.0399  max mem: 1378\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.8365 (0.8051)  Acc@1: 83.3333 (81.9444)  Acc@5: 95.8333 (95.4365)  Acc@task: 79.1667 (78.7698)  time: 0.1802  data: 0.0003  max mem: 1378\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.6290 (0.7734)  Acc@1: 79.1667 (81.4516)  Acc@5: 95.8333 (96.2366)  Acc@task: 75.0000 (78.6290)  time: 0.1797  data: 0.0004  max mem: 1378\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.6290 (0.7660)  Acc@1: 79.1667 (82.0122)  Acc@5: 95.8333 (95.8333)  Acc@task: 75.0000 (78.5569)  time: 0.1803  data: 0.0003  max mem: 1378\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.6290 (0.7654)  Acc@1: 81.2500 (82.0000)  Acc@5: 95.8333 (95.9000)  Acc@task: 75.0000 (78.5000)  time: 0.1767  data: 0.0003  max mem: 1378\n",
            "Test: [Task 3] Total time: 0:00:08 (0.1922 s / it)\n",
            "* Acc@task 78.500 Acc@1 82.000 Acc@5 95.900 loss 0.765\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:33  Loss: 0.7423 (0.7423)  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  Acc@task: 62.5000 (62.5000)  time: 0.7881  data: 0.6005  max mem: 1378\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:07  Loss: 0.7487 (0.7496)  Acc@1: 83.3333 (80.6818)  Acc@5: 95.8333 (95.4545)  Acc@task: 75.0000 (75.7576)  time: 0.2343  data: 0.0553  max mem: 1378\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.7487 (0.7839)  Acc@1: 79.1667 (80.7540)  Acc@5: 95.8333 (96.2302)  Acc@task: 79.1667 (76.9841)  time: 0.1789  data: 0.0006  max mem: 1378\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.6700 (0.7419)  Acc@1: 79.1667 (80.9140)  Acc@5: 95.8333 (96.5054)  Acc@task: 79.1667 (76.7473)  time: 0.1799  data: 0.0004  max mem: 1378\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7366 (0.8049)  Acc@1: 79.1667 (79.8781)  Acc@5: 95.8333 (96.0366)  Acc@task: 75.0000 (76.6260)  time: 0.1807  data: 0.0003  max mem: 1378\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.7065 (0.7902)  Acc@1: 79.1667 (80.1000)  Acc@5: 95.8333 (96.1000)  Acc@task: 75.0000 (76.9000)  time: 0.1771  data: 0.0003  max mem: 1378\n",
            "Test: [Task 4] Total time: 0:00:08 (0.1952 s / it)\n",
            "* Acc@task 76.900 Acc@1 80.100 Acc@5 96.100 loss 0.790\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:19  Loss: 0.4957 (0.4957)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  Acc@task: 66.6667 (66.6667)  time: 0.4712  data: 0.3053  max mem: 1378\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 0.6005 (0.7071)  Acc@1: 79.1667 (80.6818)  Acc@5: 95.8333 (97.3485)  Acc@task: 70.8333 (74.6212)  time: 0.2053  data: 0.0283  max mem: 1378\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.6393 (0.7460)  Acc@1: 79.1667 (81.1508)  Acc@5: 95.8333 (96.4286)  Acc@task: 70.8333 (73.2143)  time: 0.1790  data: 0.0007  max mem: 1378\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.7488 (0.7429)  Acc@1: 79.1667 (80.5108)  Acc@5: 95.8333 (96.1022)  Acc@task: 75.0000 (73.9247)  time: 0.1795  data: 0.0009  max mem: 1378\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.7743 (0.7947)  Acc@1: 79.1667 (79.9797)  Acc@5: 95.8333 (95.8333)  Acc@task: 70.8333 (72.8659)  time: 0.1803  data: 0.0006  max mem: 1378\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.8133 (0.8483)  Acc@1: 79.1667 (79.5000)  Acc@5: 95.8333 (95.6000)  Acc@task: 70.8333 (72.7000)  time: 0.1766  data: 0.0006  max mem: 1378\n",
            "Test: [Task 5] Total time: 0:00:07 (0.1872 s / it)\n",
            "* Acc@task 72.700 Acc@1 79.500 Acc@5 95.600 loss 0.848\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:26  Loss: 0.3918 (0.3918)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  Acc@task: 79.1667 (79.1667)  time: 0.6225  data: 0.4546  max mem: 1378\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:07  Loss: 0.7095 (0.6911)  Acc@1: 79.1667 (80.6818)  Acc@5: 100.0000 (98.1061)  Acc@task: 75.0000 (75.0000)  time: 0.2201  data: 0.0417  max mem: 1378\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:04  Loss: 0.7184 (0.7378)  Acc@1: 79.1667 (79.9603)  Acc@5: 95.8333 (97.4206)  Acc@task: 75.0000 (74.6032)  time: 0.1789  data: 0.0004  max mem: 1378\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 0.8521 (0.7783)  Acc@1: 79.1667 (78.7634)  Acc@5: 95.8333 (97.3118)  Acc@task: 75.0000 (74.3280)  time: 0.1790  data: 0.0003  max mem: 1378\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 0.8317 (0.8024)  Acc@1: 75.0000 (78.4553)  Acc@5: 95.8333 (97.4594)  Acc@task: 70.8333 (73.7805)  time: 0.1800  data: 0.0002  max mem: 1378\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 0.8317 (0.8130)  Acc@1: 75.0000 (78.2000)  Acc@5: 100.0000 (97.5000)  Acc@task: 70.8333 (73.8000)  time: 0.1764  data: 0.0002  max mem: 1378\n",
            "Test: [Task 6] Total time: 0:00:08 (0.1906 s / it)\n",
            "* Acc@task 73.800 Acc@1 78.200 Acc@5 97.500 loss 0.813\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:20  Loss: 5.6467 (5.6467)  Acc@1: 4.1667 (4.1667)  Acc@5: 58.3333 (58.3333)  Acc@task: 54.1667 (54.1667)  time: 0.4763  data: 0.3139  max mem: 1378\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:06  Loss: 5.7236 (5.5672)  Acc@1: 4.1667 (6.4394)  Acc@5: 62.5000 (59.8485)  Acc@task: 58.3333 (60.9849)  time: 0.2062  data: 0.0292  max mem: 1378\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:04  Loss: 5.7236 (5.4740)  Acc@1: 4.1667 (6.7460)  Acc@5: 58.3333 (57.9365)  Acc@task: 62.5000 (61.3095)  time: 0.1788  data: 0.0007  max mem: 1378\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:02  Loss: 5.6925 (5.5385)  Acc@1: 8.3333 (7.3925)  Acc@5: 54.1667 (56.0484)  Acc@task: 62.5000 (62.3656)  time: 0.1790  data: 0.0005  max mem: 1378\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 5.5374 (5.5234)  Acc@1: 4.1667 (6.8089)  Acc@5: 54.1667 (55.7927)  Acc@task: 66.6667 (63.6179)  time: 0.1800  data: 0.0003  max mem: 1378\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 5.5079 (5.4915)  Acc@1: 4.1667 (6.9000)  Acc@5: 54.1667 (55.8000)  Acc@task: 66.6667 (63.9000)  time: 0.1763  data: 0.0003  max mem: 1378\n",
            "Test: [Task 7] Total time: 0:00:07 (0.1873 s / it)\n",
            "* Acc@task 63.900 Acc@1 6.900 Acc@5 55.800 loss 5.491\n",
            "[Average accuracy till task7]\tAcc@task: 74.2000\tAcc@1: 69.7143\tAcc@5: 90.5429\tLoss: 1.4625\tForgetting: 4.0000\tBackward: 55.6833\n",
            "torch.Size([72600, 384])\n",
            "Averaged stats: Lr: 0.005000  Loss: 0.0754  Acc@1: 99.1667 (97.1250)  Acc@5: 100.0000 (99.6250)\n",
            "torch.Size([72600, 384])\n",
            "Averaged stats: Lr: 0.004878  Loss: 0.0322  Acc@1: 100.0000 (99.5556)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([72600, 384])\n",
            "Averaged stats: Lr: 0.004523  Loss: 0.0178  Acc@1: 100.0000 (99.7778)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([72600, 384])\n",
            "Averaged stats: Lr: 0.003969  Loss: 0.0158  Acc@1: 100.0000 (99.8195)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([72600, 384])\n",
            "Averaged stats: Lr: 0.003273  Loss: 0.0147  Acc@1: 100.0000 (99.8472)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([72600, 384])\n",
            "Averaged stats: Lr: 0.002500  Loss: 0.0142  Acc@1: 100.0000 (99.8472)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([72600, 384])\n",
            "Averaged stats: Lr: 0.001727  Loss: 0.0060  Acc@1: 100.0000 (99.8472)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([72600, 384])\n",
            "Averaged stats: Lr: 0.001031  Loss: 0.0057  Acc@1: 100.0000 (99.8750)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([72600, 384])\n",
            "Averaged stats: Lr: 0.000477  Loss: 0.0140  Acc@1: 100.0000 (99.8750)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([72600, 384])\n",
            "Averaged stats: Lr: 0.000122  Loss: 0.0080  Acc@1: 100.0000 (99.8750)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [Task 1]  [ 0/23]  eta: 0:00:12  Loss: 1.6070 (1.6070)  Acc@1: 66.6667 (66.6667)  Acc@5: 87.5000 (87.5000)  Acc@task: 66.6667 (66.6667)  time: 0.5234  data: 0.3548  max mem: 1378\n",
            "Test: [Task 1]  [10/23]  eta: 0:00:02  Loss: 1.0429 (1.0605)  Acc@1: 70.8333 (74.2424)  Acc@5: 91.6667 (91.6667)  Acc@task: 66.6667 (67.8030)  time: 0.2082  data: 0.0327  max mem: 1378\n",
            "Test: [Task 1]  [20/23]  eta: 0:00:00  Loss: 1.0429 (1.0983)  Acc@1: 70.8333 (72.2222)  Acc@5: 91.6667 (91.0714)  Acc@task: 62.5000 (65.0794)  time: 0.1762  data: 0.0004  max mem: 1378\n",
            "Test: [Task 1]  [22/23]  eta: 0:00:00  Loss: 0.9521 (1.0833)  Acc@1: 70.8333 (72.3881)  Acc@5: 91.6667 (91.2313)  Acc@task: 66.6667 (65.6716)  time: 0.1718  data: 0.0003  max mem: 1378\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1911 s / it)\n",
            "* Acc@task 65.672 Acc@1 72.388 Acc@5 91.231 loss 1.083\n",
            "Test: [Task 2]  [ 0/20]  eta: 0:00:14  Loss: 1.9656 (1.9656)  Acc@1: 66.6667 (66.6667)  Acc@5: 75.0000 (75.0000)  Acc@task: 58.3333 (58.3333)  time: 0.7038  data: 0.4707  max mem: 1378\n",
            "Test: [Task 2]  [10/20]  eta: 0:00:02  Loss: 2.2901 (2.2004)  Acc@1: 54.1667 (56.8182)  Acc@5: 79.1667 (79.1667)  Acc@task: 54.1667 (52.2727)  time: 0.2248  data: 0.0442  max mem: 1378\n",
            "Test: [Task 2]  [19/20]  eta: 0:00:00  Loss: 1.9729 (2.0607)  Acc@1: 54.1667 (55.9499)  Acc@5: 83.3333 (82.2547)  Acc@task: 54.1667 (52.8184)  time: 0.2037  data: 0.0244  max mem: 1378\n",
            "Test: [Task 2] Total time: 0:00:04 (0.2119 s / it)\n",
            "* Acc@task 52.818 Acc@1 55.950 Acc@5 82.255 loss 2.061\n",
            "Test: [Task 3]  [ 0/21]  eta: 0:00:13  Loss: 3.6602 (3.6602)  Acc@1: 33.3333 (33.3333)  Acc@5: 66.6667 (66.6667)  Acc@task: 33.3333 (33.3333)  time: 0.6357  data: 0.4698  max mem: 1378\n",
            "Test: [Task 3]  [10/21]  eta: 0:00:02  Loss: 2.7624 (2.5832)  Acc@1: 58.3333 (56.4394)  Acc@5: 70.8333 (75.7576)  Acc@task: 62.5000 (59.0909)  time: 0.2206  data: 0.0430  max mem: 1378\n",
            "Test: [Task 3]  [20/21]  eta: 0:00:00  Loss: 2.3377 (2.5006)  Acc@1: 58.3333 (57.3441)  Acc@5: 75.0000 (77.6660)  Acc@task: 62.5000 (60.9658)  time: 0.1764  data: 0.0002  max mem: 1378\n",
            "Test: [Task 3] Total time: 0:00:04 (0.2027 s / it)\n",
            "* Acc@task 60.966 Acc@1 57.344 Acc@5 77.666 loss 2.501\n",
            "Test: [Task 4]  [ 0/20]  eta: 0:00:11  Loss: 3.7307 (3.7307)  Acc@1: 45.8333 (45.8333)  Acc@5: 62.5000 (62.5000)  Acc@task: 45.8333 (45.8333)  time: 0.5562  data: 0.3904  max mem: 1378\n",
            "Test: [Task 4]  [10/20]  eta: 0:00:02  Loss: 2.6428 (2.7824)  Acc@1: 45.8333 (48.1061)  Acc@5: 79.1667 (73.4848)  Acc@task: 62.5000 (59.0909)  time: 0.2137  data: 0.0360  max mem: 1378\n",
            "Test: [Task 4]  [19/20]  eta: 0:00:00  Loss: 2.4884 (2.5633)  Acc@1: 54.1667 (52.4416)  Acc@5: 79.1667 (76.4331)  Acc@task: 60.0000 (61.3588)  time: 0.1955  data: 0.0199  max mem: 1378\n",
            "Test: [Task 4] Total time: 0:00:04 (0.2001 s / it)\n",
            "* Acc@task 61.359 Acc@1 52.442 Acc@5 76.433 loss 2.563\n",
            "Test: [Task 5]  [ 0/21]  eta: 0:00:10  Loss: 2.5991 (2.5991)  Acc@1: 58.3333 (58.3333)  Acc@5: 75.0000 (75.0000)  Acc@task: 58.3333 (58.3333)  time: 0.5120  data: 0.3425  max mem: 1378\n",
            "Test: [Task 5]  [10/21]  eta: 0:00:02  Loss: 2.7016 (2.8278)  Acc@1: 50.0000 (48.8636)  Acc@5: 75.0000 (73.8636)  Acc@task: 54.1667 (55.6818)  time: 0.2125  data: 0.0327  max mem: 1378\n",
            "Test: [Task 5]  [20/21]  eta: 0:00:00  Loss: 2.6377 (2.7457)  Acc@1: 50.0000 (50.9165)  Acc@5: 75.0000 (75.1528)  Acc@task: 54.1667 (56.2118)  time: 0.1777  data: 0.0010  max mem: 1378\n",
            "Test: [Task 5] Total time: 0:00:04 (0.2017 s / it)\n",
            "* Acc@task 56.212 Acc@1 50.916 Acc@5 75.153 loss 2.746\n",
            "Test: [Task 6]  [ 0/20]  eta: 0:00:16  Loss: 1.4746 (1.4746)  Acc@1: 66.6667 (66.6667)  Acc@5: 95.8333 (95.8333)  Acc@task: 70.8333 (70.8333)  time: 0.8223  data: 0.5651  max mem: 1378\n",
            "Test: [Task 6]  [10/20]  eta: 0:00:02  Loss: 2.6698 (2.3768)  Acc@1: 58.3333 (58.7121)  Acc@5: 79.1667 (80.3030)  Acc@task: 58.3333 (55.6818)  time: 0.2413  data: 0.0516  max mem: 1378\n",
            "Test: [Task 6]  [19/20]  eta: 0:00:00  Loss: 2.4284 (2.4470)  Acc@1: 54.1667 (56.2500)  Acc@5: 79.1667 (79.7917)  Acc@task: 58.3333 (56.4583)  time: 0.2144  data: 0.0285  max mem: 1378\n",
            "Test: [Task 6] Total time: 0:00:04 (0.2191 s / it)\n",
            "* Acc@task 56.458 Acc@1 56.250 Acc@5 79.792 loss 2.447\n",
            "Test: [Task 7]  [ 0/23]  eta: 0:00:15  Loss: 1.1552 (1.1552)  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  Acc@task: 62.5000 (62.5000)  time: 0.6527  data: 0.4678  max mem: 1378\n",
            "Test: [Task 7]  [10/23]  eta: 0:00:02  Loss: 1.6917 (1.5752)  Acc@1: 70.8333 (66.6667)  Acc@5: 83.3333 (88.2576)  Acc@task: 58.3333 (59.4697)  time: 0.2273  data: 0.0428  max mem: 1378\n",
            "Test: [Task 7]  [20/23]  eta: 0:00:00  Loss: 1.5473 (1.5498)  Acc@1: 70.8333 (67.8571)  Acc@5: 83.3333 (86.9048)  Acc@task: 58.3333 (61.1111)  time: 0.1838  data: 0.0003  max mem: 1378\n",
            "Test: [Task 7]  [22/23]  eta: 0:00:00  Loss: 1.4024 (1.5609)  Acc@1: 70.8333 (67.0909)  Acc@5: 87.5000 (86.7273)  Acc@task: 59.0909 (60.7273)  time: 0.1845  data: 0.0003  max mem: 1378\n",
            "Test: [Task 7] Total time: 0:00:04 (0.2082 s / it)\n",
            "* Acc@task 60.727 Acc@1 67.091 Acc@5 86.727 loss 1.561\n",
            "[Average accuracy till task7]\tAcc@task: 59.1732\tAcc@1: 58.9116\tAcc@5: 81.3224\tLoss: 2.1374\tForgetting: 0.0000\tBackward: 57.5484\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:19  Loss: 0.6726 (0.6726)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  Acc@task: 79.1667 (79.1667)  time: 0.4642  data: 0.2897  max mem: 1378\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.8295 (0.9302)  Acc@1: 70.8333 (75.7576)  Acc@5: 95.8333 (96.5909)  Acc@task: 79.1667 (79.5455)  time: 0.2110  data: 0.0289  max mem: 1378\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.8816 (0.8749)  Acc@1: 70.8333 (77.5794)  Acc@5: 95.8333 (96.0317)  Acc@task: 79.1667 (80.7540)  time: 0.1849  data: 0.0020  max mem: 1378\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.6932 (0.8212)  Acc@1: 83.3333 (79.0323)  Acc@5: 95.8333 (96.6398)  Acc@task: 79.1667 (80.6452)  time: 0.1844  data: 0.0012  max mem: 1378\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.5731 (0.7580)  Acc@1: 83.3333 (80.2846)  Acc@5: 100.0000 (97.1545)  Acc@task: 83.3333 (81.6057)  time: 0.1846  data: 0.0007  max mem: 1378\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.5355 (0.7472)  Acc@1: 83.3333 (80.4000)  Acc@5: 100.0000 (97.2000)  Acc@task: 83.3333 (81.9000)  time: 0.1810  data: 0.0005  max mem: 1378\n",
            "Test: [Task 1] Total time: 0:00:08 (0.1923 s / it)\n",
            "* Acc@task 81.900 Acc@1 80.400 Acc@5 97.200 loss 0.747\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:19  Loss: 0.9511 (0.9511)  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  Acc@task: 62.5000 (62.5000)  time: 0.4725  data: 0.2947  max mem: 1378\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.8437 (0.9522)  Acc@1: 79.1667 (78.7879)  Acc@5: 95.8333 (95.4545)  Acc@task: 70.8333 (70.0758)  time: 0.2117  data: 0.0305  max mem: 1378\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.8237 (1.0421)  Acc@1: 75.0000 (76.7857)  Acc@5: 95.8333 (94.6429)  Acc@task: 70.8333 (71.4286)  time: 0.1845  data: 0.0022  max mem: 1378\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.9061 (1.0055)  Acc@1: 79.1667 (77.4194)  Acc@5: 95.8333 (95.2957)  Acc@task: 70.8333 (71.1022)  time: 0.1834  data: 0.0004  max mem: 1378\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.7670 (0.9416)  Acc@1: 83.3333 (77.9472)  Acc@5: 95.8333 (95.3252)  Acc@task: 70.8333 (71.7480)  time: 0.1832  data: 0.0003  max mem: 1378\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.6630 (0.9350)  Acc@1: 81.2500 (78.0000)  Acc@5: 95.8333 (95.4000)  Acc@task: 70.8333 (71.7000)  time: 0.1794  data: 0.0003  max mem: 1378\n",
            "Test: [Task 2] Total time: 0:00:08 (0.1926 s / it)\n",
            "* Acc@task 71.700 Acc@1 78.000 Acc@5 95.400 loss 0.935\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:37  Loss: 0.4116 (0.4116)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.8987  data: 0.7038  max mem: 1378\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:07  Loss: 0.9205 (0.9205)  Acc@1: 83.3333 (80.6818)  Acc@5: 95.8333 (95.4545)  Acc@task: 79.1667 (78.7879)  time: 0.2454  data: 0.0649  max mem: 1378\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.8281 (0.8943)  Acc@1: 79.1667 (80.7540)  Acc@5: 95.8333 (95.2381)  Acc@task: 79.1667 (78.7698)  time: 0.1805  data: 0.0007  max mem: 1378\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.6862 (0.8522)  Acc@1: 79.1667 (80.3763)  Acc@5: 95.8333 (95.5645)  Acc@task: 75.0000 (78.6290)  time: 0.1813  data: 0.0004  max mem: 1378\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.6946 (0.8376)  Acc@1: 79.1667 (80.4878)  Acc@5: 95.8333 (95.4268)  Acc@task: 75.0000 (78.5569)  time: 0.1812  data: 0.0003  max mem: 1378\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.6946 (0.8343)  Acc@1: 79.1667 (80.4000)  Acc@5: 95.8333 (95.4000)  Acc@task: 75.0000 (78.5000)  time: 0.1778  data: 0.0003  max mem: 1378\n",
            "Test: [Task 3] Total time: 0:00:08 (0.1988 s / it)\n",
            "* Acc@task 78.500 Acc@1 80.400 Acc@5 95.400 loss 0.834\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:20  Loss: 0.8032 (0.8032)  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  Acc@task: 62.5000 (62.5000)  time: 0.4911  data: 0.3167  max mem: 1378\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:06  Loss: 0.7878 (0.7881)  Acc@1: 79.1667 (80.6818)  Acc@5: 95.8333 (95.4545)  Acc@task: 75.0000 (75.7576)  time: 0.2087  data: 0.0292  max mem: 1378\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.7878 (0.8604)  Acc@1: 79.1667 (78.7698)  Acc@5: 95.8333 (95.6349)  Acc@task: 79.1667 (76.9841)  time: 0.1796  data: 0.0004  max mem: 1378\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.7220 (0.8027)  Acc@1: 79.1667 (79.5699)  Acc@5: 95.8333 (96.1022)  Acc@task: 79.1667 (76.7473)  time: 0.1794  data: 0.0007  max mem: 1378\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7626 (0.8678)  Acc@1: 79.1667 (78.5569)  Acc@5: 95.8333 (95.5285)  Acc@task: 75.0000 (76.6260)  time: 0.1798  data: 0.0012  max mem: 1378\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.7220 (0.8528)  Acc@1: 79.1667 (78.8000)  Acc@5: 95.8333 (95.6000)  Acc@task: 75.0000 (76.9000)  time: 0.1762  data: 0.0012  max mem: 1378\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1880 s / it)\n",
            "* Acc@task 76.900 Acc@1 78.800 Acc@5 95.600 loss 0.853\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:18  Loss: 0.6022 (0.6022)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 66.6667 (66.6667)  time: 0.4477  data: 0.2817  max mem: 1378\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 0.6022 (0.7616)  Acc@1: 79.1667 (78.7879)  Acc@5: 95.8333 (96.9697)  Acc@task: 70.8333 (74.6212)  time: 0.2057  data: 0.0290  max mem: 1378\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.8187 (0.7961)  Acc@1: 79.1667 (79.7619)  Acc@5: 95.8333 (96.6270)  Acc@task: 70.8333 (73.2143)  time: 0.1799  data: 0.0021  max mem: 1378\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.8121 (0.7970)  Acc@1: 79.1667 (79.8387)  Acc@5: 95.8333 (96.1022)  Acc@task: 75.0000 (73.9247)  time: 0.1787  data: 0.0004  max mem: 1378\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.8248 (0.8516)  Acc@1: 79.1667 (79.0650)  Acc@5: 95.8333 (95.6301)  Acc@task: 70.8333 (72.8659)  time: 0.1794  data: 0.0002  max mem: 1378\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.9001 (0.9074)  Acc@1: 79.1667 (78.6000)  Acc@5: 95.8333 (95.3000)  Acc@task: 70.8333 (72.7000)  time: 0.1759  data: 0.0002  max mem: 1378\n",
            "Test: [Task 5] Total time: 0:00:07 (0.1868 s / it)\n",
            "* Acc@task 72.700 Acc@1 78.600 Acc@5 95.300 loss 0.907\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:24  Loss: 0.6904 (0.6904)  Acc@1: 70.8333 (70.8333)  Acc@5: 100.0000 (100.0000)  Acc@task: 79.1667 (79.1667)  time: 0.5745  data: 0.4027  max mem: 1378\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:06  Loss: 0.8142 (0.8070)  Acc@1: 79.1667 (77.6515)  Acc@5: 95.8333 (97.3485)  Acc@task: 75.0000 (75.0000)  time: 0.2152  data: 0.0379  max mem: 1378\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:04  Loss: 0.8209 (0.8417)  Acc@1: 79.1667 (78.1746)  Acc@5: 95.8333 (97.0238)  Acc@task: 75.0000 (74.6032)  time: 0.1783  data: 0.0012  max mem: 1378\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 1.0388 (0.8732)  Acc@1: 75.0000 (77.4194)  Acc@5: 95.8333 (96.9086)  Acc@task: 75.0000 (74.3280)  time: 0.1781  data: 0.0008  max mem: 1378\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.0388 (0.8999)  Acc@1: 75.0000 (77.2358)  Acc@5: 95.8333 (96.7480)  Acc@task: 70.8333 (73.7805)  time: 0.1787  data: 0.0005  max mem: 1378\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.0388 (0.9077)  Acc@1: 75.0000 (76.9000)  Acc@5: 95.8333 (96.7000)  Acc@task: 70.8333 (73.8000)  time: 0.1754  data: 0.0005  max mem: 1378\n",
            "Test: [Task 6] Total time: 0:00:07 (0.1888 s / it)\n",
            "* Acc@task 73.800 Acc@1 76.900 Acc@5 96.700 loss 0.908\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:19  Loss: 1.4888 (1.4888)  Acc@1: 79.1667 (79.1667)  Acc@5: 87.5000 (87.5000)  Acc@task: 54.1667 (54.1667)  time: 0.4698  data: 0.2987  max mem: 1378\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:06  Loss: 0.5153 (0.7718)  Acc@1: 83.3333 (81.0606)  Acc@5: 100.0000 (96.5909)  Acc@task: 58.3333 (60.9849)  time: 0.2047  data: 0.0275  max mem: 1378\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:04  Loss: 0.6196 (0.8094)  Acc@1: 79.1667 (78.9683)  Acc@5: 95.8333 (96.6270)  Acc@task: 62.5000 (61.3095)  time: 0.1776  data: 0.0005  max mem: 1378\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:02  Loss: 0.8219 (0.8727)  Acc@1: 79.1667 (77.5538)  Acc@5: 95.8333 (96.5054)  Acc@task: 62.5000 (62.3656)  time: 0.1783  data: 0.0006  max mem: 1378\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 0.8956 (0.8994)  Acc@1: 70.8333 (76.6260)  Acc@5: 95.8333 (96.2398)  Acc@task: 66.6667 (63.6179)  time: 0.1794  data: 0.0004  max mem: 1378\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 0.8956 (0.8921)  Acc@1: 75.0000 (76.6000)  Acc@5: 95.8333 (96.3000)  Acc@task: 66.6667 (63.9000)  time: 0.1756  data: 0.0004  max mem: 1378\n",
            "Test: [Task 7] Total time: 0:00:07 (0.1873 s / it)\n",
            "* Acc@task 63.900 Acc@1 76.600 Acc@5 96.300 loss 0.892\n",
            "[Average accuracy till task7]\tAcc@task: 74.2000\tAcc@1: 78.5286\tAcc@5: 95.9857\tLoss: 0.8681\tForgetting: 0.0000\tBackward: 78.8500\n",
            "Loading checkpoint from: ./output/cifar100_full_dino_5epoch_10pct/checkpoint/task8_checkpoint.pth\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/engines/hide_promtp_wtp_and_tap_engine.py:574: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  loss = torch.nn.functional.cross_entropy(sim, torch.range(0, sim.shape[0] - 1).long().to(device))\n",
            "Train: Epoch[ 1/10]  [ 0/22]  eta: 0:00:24  Lr: 0.002812  Loss: 3.7844  Acc@1: 8.3333 (8.3333)  Acc@5: 50.0000 (50.0000)  time: 1.1307  data: 0.8420  max mem: 1378\n",
            "Train: Epoch[ 1/10]  [10/22]  eta: 0:00:04  Lr: 0.002812  Loss: 3.2521  Acc@1: 12.5000 (13.6364)  Acc@5: 50.0000 (54.1667)  time: 0.3621  data: 0.0768  max mem: 1378\n",
            "Train: Epoch[ 1/10]  [20/22]  eta: 0:00:00  Lr: 0.002812  Loss: 2.2947  Acc@1: 16.6667 (18.0556)  Acc@5: 62.5000 (61.7064)  time: 0.2832  data: 0.0002  max mem: 1378\n",
            "Train: Epoch[ 1/10]  [21/22]  eta: 0:00:00  Lr: 0.002812  Loss: 2.6892  Acc@1: 16.6667 (18.3953)  Acc@5: 66.6667 (61.8395)  time: 0.2715  data: 0.0002  max mem: 1378\n",
            "Train: Epoch[ 1/10] Total time: 0:00:07 (0.3190 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 2.6892  Acc@1: 16.6667 (18.3953)  Acc@5: 66.6667 (61.8395)\n",
            "Train: Epoch[ 2/10]  [ 0/22]  eta: 0:00:23  Lr: 0.002812  Loss: 2.3515  Acc@1: 16.6667 (16.6667)  Acc@5: 75.0000 (75.0000)  time: 1.0687  data: 0.7108  max mem: 1378\n",
            "Train: Epoch[ 2/10]  [10/22]  eta: 0:00:04  Lr: 0.002812  Loss: 1.6913  Acc@1: 29.1667 (32.9545)  Acc@5: 79.1667 (78.7879)  time: 0.3537  data: 0.0649  max mem: 1378\n",
            "Train: Epoch[ 2/10]  [20/22]  eta: 0:00:00  Lr: 0.002812  Loss: 1.5347  Acc@1: 37.5000 (38.2937)  Acc@5: 83.3333 (83.3333)  time: 0.2827  data: 0.0003  max mem: 1378\n",
            "Train: Epoch[ 2/10]  [21/22]  eta: 0:00:00  Lr: 0.002812  Loss: 1.1318  Acc@1: 37.5000 (38.5519)  Acc@5: 87.5000 (83.5616)  time: 0.2734  data: 0.0003  max mem: 1378\n",
            "Train: Epoch[ 2/10] Total time: 0:00:06 (0.3165 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.1318  Acc@1: 37.5000 (38.5519)  Acc@5: 87.5000 (83.5616)\n",
            "Train: Epoch[ 3/10]  [ 0/22]  eta: 0:00:19  Lr: 0.002812  Loss: 1.8948  Acc@1: 50.0000 (50.0000)  Acc@5: 87.5000 (87.5000)  time: 0.8999  data: 0.5839  max mem: 1378\n",
            "Train: Epoch[ 3/10]  [10/22]  eta: 0:00:04  Lr: 0.002812  Loss: 1.0872  Acc@1: 54.1667 (56.4394)  Acc@5: 91.6667 (92.4242)  time: 0.3381  data: 0.0533  max mem: 1378\n",
            "Train: Epoch[ 3/10]  [20/22]  eta: 0:00:00  Lr: 0.002812  Loss: 0.7848  Acc@1: 58.3333 (61.5079)  Acc@5: 91.6667 (92.6587)  time: 0.2821  data: 0.0002  max mem: 1378\n",
            "Train: Epoch[ 3/10]  [21/22]  eta: 0:00:00  Lr: 0.002812  Loss: 0.3220  Acc@1: 58.3333 (62.0352)  Acc@5: 95.8333 (92.7593)  time: 0.2734  data: 0.0002  max mem: 1378\n",
            "Train: Epoch[ 3/10] Total time: 0:00:06 (0.3061 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.3220  Acc@1: 58.3333 (62.0352)  Acc@5: 95.8333 (92.7593)\n",
            "Train: Epoch[ 4/10]  [ 0/22]  eta: 0:00:14  Lr: 0.002812  Loss: 1.2333  Acc@1: 66.6667 (66.6667)  Acc@5: 95.8333 (95.8333)  time: 0.6417  data: 0.3623  max mem: 1378\n",
            "Train: Epoch[ 4/10]  [10/22]  eta: 0:00:03  Lr: 0.002812  Loss: 0.7707  Acc@1: 75.0000 (73.4849)  Acc@5: 95.8333 (96.5909)  time: 0.3161  data: 0.0332  max mem: 1378\n",
            "Train: Epoch[ 4/10]  [20/22]  eta: 0:00:00  Lr: 0.002812  Loss: 0.5347  Acc@1: 70.8333 (71.4286)  Acc@5: 95.8333 (95.2381)  time: 0.2831  data: 0.0003  max mem: 1378\n",
            "Train: Epoch[ 4/10]  [21/22]  eta: 0:00:00  Lr: 0.002812  Loss: 0.5664  Acc@1: 70.8333 (71.6243)  Acc@5: 95.8333 (95.1076)  time: 0.2740  data: 0.0003  max mem: 1378\n",
            "Train: Epoch[ 4/10] Total time: 0:00:06 (0.2983 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.5664  Acc@1: 70.8333 (71.6243)  Acc@5: 95.8333 (95.1076)\n",
            "Train: Epoch[ 5/10]  [ 0/22]  eta: 0:00:17  Lr: 0.002812  Loss: 1.0411  Acc@1: 66.6667 (66.6667)  Acc@5: 91.6667 (91.6667)  time: 0.7872  data: 0.4686  max mem: 1378\n",
            "Train: Epoch[ 5/10]  [10/22]  eta: 0:00:03  Lr: 0.002812  Loss: 0.9060  Acc@1: 75.0000 (73.4849)  Acc@5: 95.8333 (96.5909)  time: 0.3294  data: 0.0435  max mem: 1378\n",
            "Train: Epoch[ 5/10]  [20/22]  eta: 0:00:00  Lr: 0.002812  Loss: 0.5595  Acc@1: 79.1667 (76.7857)  Acc@5: 100.0000 (97.4206)  time: 0.2832  data: 0.0006  max mem: 1378\n",
            "Train: Epoch[ 5/10]  [21/22]  eta: 0:00:00  Lr: 0.002812  Loss: 0.5513  Acc@1: 75.0000 (76.7123)  Acc@5: 100.0000 (97.4560)  time: 0.2739  data: 0.0003  max mem: 1378\n",
            "Train: Epoch[ 5/10] Total time: 0:00:06 (0.3017 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.5513  Acc@1: 75.0000 (76.7123)  Acc@5: 100.0000 (97.4560)\n",
            "Train: Epoch[ 6/10]  [ 0/22]  eta: 0:00:13  Lr: 0.002812  Loss: 0.6818  Acc@1: 70.8333 (70.8333)  Acc@5: 100.0000 (100.0000)  time: 0.6040  data: 0.3122  max mem: 1378\n",
            "Train: Epoch[ 6/10]  [10/22]  eta: 0:00:03  Lr: 0.002812  Loss: 0.3148  Acc@1: 83.3333 (81.0606)  Acc@5: 100.0000 (98.4848)  time: 0.3129  data: 0.0287  max mem: 1378\n",
            "Train: Epoch[ 6/10]  [20/22]  eta: 0:00:00  Lr: 0.002812  Loss: 0.3615  Acc@1: 83.3333 (82.5397)  Acc@5: 100.0000 (97.8175)  time: 0.2832  data: 0.0003  max mem: 1378\n",
            "Train: Epoch[ 6/10]  [21/22]  eta: 0:00:00  Lr: 0.002812  Loss: 0.3633  Acc@1: 83.3333 (82.7789)  Acc@5: 100.0000 (97.8474)  time: 0.2741  data: 0.0003  max mem: 1378\n",
            "Train: Epoch[ 6/10] Total time: 0:00:06 (0.2965 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.3633  Acc@1: 83.3333 (82.7789)  Acc@5: 100.0000 (97.8474)\n",
            "Train: Epoch[ 7/10]  [ 0/22]  eta: 0:00:21  Lr: 0.002812  Loss: 0.7436  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  time: 0.9579  data: 0.6241  max mem: 1378\n",
            "Train: Epoch[ 7/10]  [10/22]  eta: 0:00:04  Lr: 0.002812  Loss: 0.4338  Acc@1: 87.5000 (84.4697)  Acc@5: 100.0000 (99.2424)  time: 0.3454  data: 0.0570  max mem: 1378\n",
            "Train: Epoch[ 7/10]  [20/22]  eta: 0:00:00  Lr: 0.002812  Loss: 0.4216  Acc@1: 87.5000 (84.5238)  Acc@5: 100.0000 (99.2063)  time: 0.2832  data: 0.0002  max mem: 1378\n",
            "Train: Epoch[ 7/10]  [21/22]  eta: 0:00:00  Lr: 0.002812  Loss: 0.1665  Acc@1: 87.5000 (84.7358)  Acc@5: 100.0000 (99.2172)  time: 0.2743  data: 0.0002  max mem: 1378\n",
            "Train: Epoch[ 7/10] Total time: 0:00:06 (0.3095 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.1665  Acc@1: 87.5000 (84.7358)  Acc@5: 100.0000 (99.2172)\n",
            "Train: Epoch[ 8/10]  [ 0/22]  eta: 0:00:14  Lr: 0.002812  Loss: 0.5663  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.6473  data: 0.3642  max mem: 1378\n",
            "Train: Epoch[ 8/10]  [10/22]  eta: 0:00:03  Lr: 0.002812  Loss: 0.7058  Acc@1: 83.3333 (85.6061)  Acc@5: 100.0000 (98.4848)  time: 0.3162  data: 0.0334  max mem: 1378\n",
            "Train: Epoch[ 8/10]  [20/22]  eta: 0:00:00  Lr: 0.002812  Loss: 0.5018  Acc@1: 87.5000 (86.5079)  Acc@5: 100.0000 (98.8095)  time: 0.2828  data: 0.0002  max mem: 1378\n",
            "Train: Epoch[ 8/10]  [21/22]  eta: 0:00:00  Lr: 0.002812  Loss: 0.4029  Acc@1: 87.5000 (86.4971)  Acc@5: 100.0000 (98.8258)  time: 0.2737  data: 0.0002  max mem: 1378\n",
            "Train: Epoch[ 8/10] Total time: 0:00:06 (0.2970 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.4029  Acc@1: 87.5000 (86.4971)  Acc@5: 100.0000 (98.8258)\n",
            "Train: Epoch[ 9/10]  [ 0/22]  eta: 0:00:20  Lr: 0.002812  Loss: 0.3999  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  time: 0.9486  data: 0.6565  max mem: 1378\n",
            "Train: Epoch[ 9/10]  [10/22]  eta: 0:00:04  Lr: 0.002812  Loss: 0.3628  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (98.4848)  time: 0.3443  data: 0.0600  max mem: 1378\n",
            "Train: Epoch[ 9/10]  [20/22]  eta: 0:00:00  Lr: 0.002812  Loss: 0.5312  Acc@1: 87.5000 (87.6984)  Acc@5: 100.0000 (98.6111)  time: 0.2830  data: 0.0003  max mem: 1378\n",
            "Train: Epoch[ 9/10]  [21/22]  eta: 0:00:00  Lr: 0.002812  Loss: 0.8319  Acc@1: 87.5000 (87.4755)  Acc@5: 100.0000 (98.6301)  time: 0.2739  data: 0.0002  max mem: 1378\n",
            "Train: Epoch[ 9/10] Total time: 0:00:06 (0.3088 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.8319  Acc@1: 87.5000 (87.4755)  Acc@5: 100.0000 (98.6301)\n",
            "Train: Epoch[10/10]  [ 0/22]  eta: 0:00:16  Lr: 0.002812  Loss: 0.2696  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  time: 0.7636  data: 0.4932  max mem: 1378\n",
            "Train: Epoch[10/10]  [10/22]  eta: 0:00:03  Lr: 0.002812  Loss: 0.3331  Acc@1: 87.5000 (89.3939)  Acc@5: 100.0000 (98.4848)  time: 0.3265  data: 0.0452  max mem: 1378\n",
            "Train: Epoch[10/10]  [20/22]  eta: 0:00:00  Lr: 0.002812  Loss: 0.2126  Acc@1: 87.5000 (88.0952)  Acc@5: 100.0000 (98.6111)  time: 0.2829  data: 0.0003  max mem: 1378\n",
            "Train: Epoch[10/10]  [21/22]  eta: 0:00:00  Lr: 0.002812  Loss: 0.2347  Acc@1: 87.5000 (88.2583)  Acc@5: 100.0000 (98.6301)  time: 0.2739  data: 0.0003  max mem: 1378\n",
            "Train: Epoch[10/10] Total time: 0:00:06 (0.3024 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.2347  Acc@1: 87.5000 (88.2583)  Acc@5: 100.0000 (98.6301)\n",
            "torch.Size([5, 2, 5, 6, 64])\n",
            "torch.Size([5, 2, 1, 5, 6, 64])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/23]  eta: 0:00:12  Loss: 1.0692 (1.0692)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  Acc@task: 66.6667 (66.6667)  time: 0.5264  data: 0.3586  max mem: 1378\n",
            "Test: [Task 1]  [10/23]  eta: 0:00:02  Loss: 1.0692 (1.0970)  Acc@1: 75.0000 (76.5152)  Acc@5: 91.6667 (92.0455)  Acc@task: 66.6667 (63.6364)  time: 0.2099  data: 0.0329  max mem: 1378\n",
            "Test: [Task 1]  [20/23]  eta: 0:00:00  Loss: 1.0521 (1.1065)  Acc@1: 70.8333 (74.0079)  Acc@5: 91.6667 (91.4683)  Acc@task: 62.5000 (63.6905)  time: 0.1778  data: 0.0003  max mem: 1378\n",
            "Test: [Task 1]  [22/23]  eta: 0:00:00  Loss: 1.0521 (1.1034)  Acc@1: 70.8333 (73.8806)  Acc@5: 91.6667 (91.2313)  Acc@task: 62.5000 (62.8731)  time: 0.1729  data: 0.0003  max mem: 1378\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1925 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@task 62.873 Acc@1 73.881 Acc@5 91.231 loss 1.103\n",
            "Test: [Task 2]  [ 0/20]  eta: 0:00:11  Loss: 1.1021 (1.1021)  Acc@1: 62.5000 (62.5000)  Acc@5: 95.8333 (95.8333)  Acc@task: 54.1667 (54.1667)  time: 0.5964  data: 0.4171  max mem: 1378\n",
            "Test: [Task 2]  [10/20]  eta: 0:00:02  Loss: 2.0249 (1.9143)  Acc@1: 54.1667 (57.5758)  Acc@5: 83.3333 (82.9545)  Acc@task: 54.1667 (53.4091)  time: 0.2159  data: 0.0388  max mem: 1378\n",
            "Test: [Task 2]  [19/20]  eta: 0:00:00  Loss: 2.0249 (1.9188)  Acc@1: 58.3333 (58.2463)  Acc@5: 83.3333 (83.2985)  Acc@task: 54.1667 (53.0271)  time: 0.1991  data: 0.0214  max mem: 1378\n",
            "Test: [Task 2] Total time: 0:00:04 (0.2055 s / it)\n",
            "* Acc@task 53.027 Acc@1 58.246 Acc@5 83.299 loss 1.919\n",
            "Test: [Task 3]  [ 0/21]  eta: 0:00:15  Loss: 1.3860 (1.3860)  Acc@1: 70.8333 (70.8333)  Acc@5: 87.5000 (87.5000)  Acc@task: 70.8333 (70.8333)  time: 0.7155  data: 0.5451  max mem: 1378\n",
            "Test: [Task 3]  [10/21]  eta: 0:00:02  Loss: 2.6421 (2.5434)  Acc@1: 50.0000 (52.2727)  Acc@5: 79.1667 (79.1667)  Acc@task: 54.1667 (56.4394)  time: 0.2273  data: 0.0498  max mem: 1378\n",
            "Test: [Task 3]  [20/21]  eta: 0:00:00  Loss: 2.2612 (2.2820)  Acc@1: 54.1667 (55.5332)  Acc@5: 83.3333 (81.0865)  Acc@task: 58.3333 (60.3622)  time: 0.1756  data: 0.0002  max mem: 1378\n",
            "Test: [Task 3] Total time: 0:00:04 (0.2060 s / it)\n",
            "* Acc@task 60.362 Acc@1 55.533 Acc@5 81.087 loss 2.282\n",
            "Test: [Task 4]  [ 0/20]  eta: 0:00:10  Loss: 2.0077 (2.0077)  Acc@1: 62.5000 (62.5000)  Acc@5: 79.1667 (79.1667)  Acc@task: 70.8333 (70.8333)  time: 0.5339  data: 0.3741  max mem: 1378\n",
            "Test: [Task 4]  [10/20]  eta: 0:00:02  Loss: 2.4651 (2.6923)  Acc@1: 54.1667 (53.7879)  Acc@5: 79.1667 (75.3788)  Acc@task: 54.1667 (55.3030)  time: 0.2105  data: 0.0343  max mem: 1378\n",
            "Test: [Task 4]  [19/20]  eta: 0:00:00  Loss: 2.3990 (2.4485)  Acc@1: 58.3333 (57.7495)  Acc@5: 79.1667 (77.7070)  Acc@task: 58.3333 (59.4480)  time: 0.1927  data: 0.0189  max mem: 1378\n",
            "Test: [Task 4] Total time: 0:00:03 (0.1972 s / it)\n",
            "* Acc@task 59.448 Acc@1 57.749 Acc@5 77.707 loss 2.448\n",
            "Test: [Task 5]  [ 0/21]  eta: 0:00:13  Loss: 2.0951 (2.0951)  Acc@1: 58.3333 (58.3333)  Acc@5: 87.5000 (87.5000)  Acc@task: 45.8333 (45.8333)  time: 0.6320  data: 0.4610  max mem: 1378\n",
            "Test: [Task 5]  [10/21]  eta: 0:00:02  Loss: 2.7829 (2.6292)  Acc@1: 58.3333 (54.9242)  Acc@5: 75.0000 (76.8939)  Acc@task: 54.1667 (54.9242)  time: 0.2208  data: 0.0430  max mem: 1378\n",
            "Test: [Task 5]  [20/21]  eta: 0:00:00  Loss: 2.7993 (2.7098)  Acc@1: 50.0000 (53.9715)  Acc@5: 75.0000 (77.1894)  Acc@task: 54.1667 (56.2118)  time: 0.1744  data: 0.0007  max mem: 1378\n",
            "Test: [Task 5] Total time: 0:00:04 (0.2035 s / it)\n",
            "* Acc@task 56.212 Acc@1 53.971 Acc@5 77.189 loss 2.710\n",
            "Test: [Task 6]  [ 0/20]  eta: 0:00:18  Loss: 3.2114 (3.2114)  Acc@1: 45.8333 (45.8333)  Acc@5: 58.3333 (58.3333)  Acc@task: 41.6667 (41.6667)  time: 0.9226  data: 0.7289  max mem: 1378\n",
            "Test: [Task 6]  [10/20]  eta: 0:00:02  Loss: 2.1945 (2.4762)  Acc@1: 54.1667 (56.0606)  Acc@5: 79.1667 (77.6515)  Acc@task: 58.3333 (57.5758)  time: 0.2468  data: 0.0672  max mem: 1378\n",
            "Test: [Task 6]  [19/20]  eta: 0:00:00  Loss: 2.1513 (2.3559)  Acc@1: 54.1667 (56.8750)  Acc@5: 79.1667 (80.0000)  Acc@task: 58.3333 (59.3750)  time: 0.2164  data: 0.0370  max mem: 1378\n",
            "Test: [Task 6] Total time: 0:00:04 (0.2218 s / it)\n",
            "* Acc@task 59.375 Acc@1 56.875 Acc@5 80.000 loss 2.356\n",
            "Test: [Task 7]  [ 0/23]  eta: 0:00:11  Loss: 2.0136 (2.0136)  Acc@1: 58.3333 (58.3333)  Acc@5: 91.6667 (91.6667)  Acc@task: 62.5000 (62.5000)  time: 0.5181  data: 0.3462  max mem: 1378\n",
            "Test: [Task 7]  [10/23]  eta: 0:00:02  Loss: 1.8469 (1.7678)  Acc@1: 58.3333 (62.5000)  Acc@5: 83.3333 (86.7424)  Acc@task: 58.3333 (57.1970)  time: 0.2101  data: 0.0331  max mem: 1378\n",
            "Test: [Task 7]  [20/23]  eta: 0:00:00  Loss: 1.8063 (1.7823)  Acc@1: 62.5000 (63.0952)  Acc@5: 83.3333 (85.3175)  Acc@task: 58.3333 (57.3413)  time: 0.1787  data: 0.0012  max mem: 1378\n",
            "Test: [Task 7]  [22/23]  eta: 0:00:00  Loss: 1.8067 (1.8108)  Acc@1: 62.5000 (62.5455)  Acc@5: 83.3333 (85.0909)  Acc@task: 58.3333 (57.4545)  time: 0.1794  data: 0.0009  max mem: 1378\n",
            "Test: [Task 7] Total time: 0:00:04 (0.1998 s / it)\n",
            "* Acc@task 57.455 Acc@1 62.545 Acc@5 85.091 loss 1.811\n",
            "Test: [Task 8]  [ 0/22]  eta: 0:00:13  Loss: 6.3649 (6.3649)  Acc@1: 0.0000 (0.0000)  Acc@5: 33.3333 (33.3333)  Acc@task: 58.3333 (58.3333)  time: 0.5912  data: 0.4143  max mem: 1378\n",
            "Test: [Task 8]  [10/22]  eta: 0:00:02  Loss: 6.0079 (6.0073)  Acc@1: 4.1667 (3.0303)  Acc@5: 41.6667 (39.7727)  Acc@task: 54.1667 (56.8182)  time: 0.2159  data: 0.0379  max mem: 1378\n",
            "Test: [Task 8]  [20/22]  eta: 0:00:00  Loss: 5.9227 (6.0917)  Acc@1: 0.0000 (3.3730)  Acc@5: 37.5000 (38.4921)  Acc@task: 54.1667 (55.7540)  time: 0.1786  data: 0.0002  max mem: 1378\n",
            "Test: [Task 8]  [21/22]  eta: 0:00:00  Loss: 5.9227 (6.0535)  Acc@1: 0.0000 (3.3268)  Acc@5: 37.5000 (39.1389)  Acc@task: 54.1667 (55.9687)  time: 0.1730  data: 0.0002  max mem: 1378\n",
            "Test: [Task 8] Total time: 0:00:04 (0.1982 s / it)\n",
            "* Acc@task 55.969 Acc@1 3.327 Acc@5 39.139 loss 6.054\n",
            "[Average accuracy till task8]\tAcc@task: 58.0901\tAcc@1: 52.7660\tAcc@5: 76.8428\tLoss: 2.5853\tForgetting: 0.9080\tBackward: 50.2444\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:35  Loss: 0.6726 (0.6726)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  Acc@task: 79.1667 (79.1667)  time: 0.8484  data: 0.6599  max mem: 1378\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:07  Loss: 0.8333 (0.9666)  Acc@1: 70.8333 (75.3788)  Acc@5: 95.8333 (96.2121)  Acc@task: 75.0000 (76.5152)  time: 0.2408  data: 0.0604  max mem: 1378\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.8823 (0.9014)  Acc@1: 70.8333 (76.7857)  Acc@5: 95.8333 (95.8333)  Acc@task: 79.1667 (78.7698)  time: 0.1801  data: 0.0004  max mem: 1378\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.6999 (0.8416)  Acc@1: 79.1667 (78.4946)  Acc@5: 95.8333 (96.5054)  Acc@task: 79.1667 (79.0323)  time: 0.1804  data: 0.0004  max mem: 1378\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6014 (0.7749)  Acc@1: 83.3333 (79.8781)  Acc@5: 100.0000 (96.9512)  Acc@task: 83.3333 (80.4878)  time: 0.1808  data: 0.0003  max mem: 1378\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.5732 (0.7637)  Acc@1: 83.3333 (80.0000)  Acc@5: 100.0000 (97.0000)  Acc@task: 83.3333 (80.8000)  time: 0.1772  data: 0.0003  max mem: 1378\n",
            "Test: [Task 1] Total time: 0:00:08 (0.1973 s / it)\n",
            "* Acc@task 80.800 Acc@1 80.000 Acc@5 97.000 loss 0.764\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:20  Loss: 0.9762 (0.9762)  Acc@1: 79.1667 (79.1667)  Acc@5: 87.5000 (87.5000)  Acc@task: 62.5000 (62.5000)  time: 0.4894  data: 0.3083  max mem: 1378\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.8513 (0.9693)  Acc@1: 79.1667 (78.4091)  Acc@5: 95.8333 (95.0758)  Acc@task: 70.8333 (68.1818)  time: 0.2076  data: 0.0285  max mem: 1378\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.8513 (1.0581)  Acc@1: 75.0000 (76.1905)  Acc@5: 95.8333 (94.4444)  Acc@task: 70.8333 (70.4365)  time: 0.1796  data: 0.0006  max mem: 1378\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.9061 (1.0162)  Acc@1: 79.1667 (77.0161)  Acc@5: 95.8333 (95.0269)  Acc@task: 70.8333 (70.4301)  time: 0.1802  data: 0.0007  max mem: 1378\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.7578 (0.9475)  Acc@1: 79.1667 (77.6423)  Acc@5: 95.8333 (95.1220)  Acc@task: 70.8333 (70.8333)  time: 0.1814  data: 0.0005  max mem: 1378\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.6628 (0.9407)  Acc@1: 79.1667 (77.7000)  Acc@5: 95.8333 (95.2000)  Acc@task: 70.8333 (70.8000)  time: 0.1782  data: 0.0005  max mem: 1378\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1887 s / it)\n",
            "* Acc@task 70.800 Acc@1 77.700 Acc@5 95.200 loss 0.941\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:23  Loss: 0.4046 (0.4046)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.5684  data: 0.4013  max mem: 1378\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 0.9190 (0.9225)  Acc@1: 83.3333 (80.6818)  Acc@5: 95.8333 (95.4545)  Acc@task: 79.1667 (79.1667)  time: 0.2162  data: 0.0368  max mem: 1378\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.8381 (0.8969)  Acc@1: 79.1667 (80.7540)  Acc@5: 95.8333 (95.2381)  Acc@task: 79.1667 (78.9683)  time: 0.1806  data: 0.0005  max mem: 1378\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.6998 (0.8567)  Acc@1: 79.1667 (80.3763)  Acc@5: 95.8333 (95.5645)  Acc@task: 79.1667 (78.4946)  time: 0.1807  data: 0.0005  max mem: 1378\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.7013 (0.8404)  Acc@1: 79.1667 (80.4878)  Acc@5: 95.8333 (95.4268)  Acc@task: 75.0000 (78.3537)  time: 0.1813  data: 0.0002  max mem: 1378\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.6995 (0.8371)  Acc@1: 79.1667 (80.4000)  Acc@5: 95.8333 (95.4000)  Acc@task: 75.0000 (78.2000)  time: 0.1778  data: 0.0002  max mem: 1378\n",
            "Test: [Task 3] Total time: 0:00:08 (0.1909 s / it)\n",
            "* Acc@task 78.200 Acc@1 80.400 Acc@5 95.400 loss 0.837\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:27  Loss: 0.8034 (0.8034)  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  Acc@task: 66.6667 (66.6667)  time: 0.6509  data: 0.4315  max mem: 1378\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:07  Loss: 0.7848 (0.7733)  Acc@1: 79.1667 (80.6818)  Acc@5: 95.8333 (95.4545)  Acc@task: 70.8333 (73.4849)  time: 0.2216  data: 0.0407  max mem: 1378\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.7848 (0.8667)  Acc@1: 79.1667 (78.5714)  Acc@5: 95.8333 (95.4365)  Acc@task: 75.0000 (74.6032)  time: 0.1797  data: 0.0015  max mem: 1378\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.7705 (0.8126)  Acc@1: 79.1667 (79.4355)  Acc@5: 95.8333 (95.9677)  Acc@task: 75.0000 (74.5968)  time: 0.1814  data: 0.0009  max mem: 1378\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7989 (0.8753)  Acc@1: 79.1667 (78.5569)  Acc@5: 95.8333 (95.4268)  Acc@task: 75.0000 (73.9837)  time: 0.1820  data: 0.0003  max mem: 1378\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.7705 (0.8601)  Acc@1: 79.1667 (78.8000)  Acc@5: 95.8333 (95.5000)  Acc@task: 75.0000 (74.3000)  time: 0.1785  data: 0.0003  max mem: 1378\n",
            "Test: [Task 4] Total time: 0:00:08 (0.1930 s / it)\n",
            "* Acc@task 74.300 Acc@1 78.800 Acc@5 95.500 loss 0.860\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:23  Loss: 0.5928 (0.5928)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 75.0000 (75.0000)  time: 0.5546  data: 0.3561  max mem: 1378\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 0.5928 (0.7559)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (97.3485)  Acc@task: 75.0000 (76.1364)  time: 0.2136  data: 0.0327  max mem: 1378\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.8153 (0.7847)  Acc@1: 79.1667 (79.9603)  Acc@5: 95.8333 (96.8254)  Acc@task: 75.0000 (75.9921)  time: 0.1795  data: 0.0004  max mem: 1378\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.8121 (0.7863)  Acc@1: 79.1667 (80.1075)  Acc@5: 95.8333 (96.5054)  Acc@task: 75.0000 (76.0753)  time: 0.1799  data: 0.0005  max mem: 1378\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.8401 (0.8377)  Acc@1: 79.1667 (79.3699)  Acc@5: 95.8333 (95.9350)  Acc@task: 70.8333 (74.5935)  time: 0.1807  data: 0.0004  max mem: 1378\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.8898 (0.8938)  Acc@1: 79.1667 (78.9000)  Acc@5: 95.8333 (95.6000)  Acc@task: 70.8333 (74.4000)  time: 0.1773  data: 0.0004  max mem: 1378\n",
            "Test: [Task 5] Total time: 0:00:08 (0.1913 s / it)\n",
            "* Acc@task 74.400 Acc@1 78.900 Acc@5 95.600 loss 0.894\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:37  Loss: 0.6904 (0.6904)  Acc@1: 70.8333 (70.8333)  Acc@5: 100.0000 (100.0000)  Acc@task: 75.0000 (75.0000)  time: 0.8813  data: 0.6788  max mem: 1378\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:07  Loss: 0.8158 (0.8013)  Acc@1: 79.1667 (78.4091)  Acc@5: 95.8333 (96.9697)  Acc@task: 66.6667 (69.3182)  time: 0.2440  data: 0.0625  max mem: 1378\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:04  Loss: 0.8180 (0.8316)  Acc@1: 79.1667 (78.9683)  Acc@5: 95.8333 (96.8254)  Acc@task: 70.8333 (70.8333)  time: 0.1797  data: 0.0006  max mem: 1378\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 1.0717 (0.8675)  Acc@1: 75.0000 (77.9570)  Acc@5: 95.8333 (96.7742)  Acc@task: 70.8333 (69.4893)  time: 0.1792  data: 0.0003  max mem: 1378\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.0866 (0.8960)  Acc@1: 75.0000 (77.7439)  Acc@5: 95.8333 (96.6463)  Acc@task: 70.8333 (69.7155)  time: 0.1800  data: 0.0002  max mem: 1378\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.0866 (0.9027)  Acc@1: 75.0000 (77.5000)  Acc@5: 95.8333 (96.7000)  Acc@task: 70.8333 (69.8000)  time: 0.1767  data: 0.0002  max mem: 1378\n",
            "Test: [Task 6] Total time: 0:00:08 (0.1976 s / it)\n",
            "* Acc@task 69.800 Acc@1 77.500 Acc@5 96.700 loss 0.903\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:25  Loss: 1.4348 (1.4348)  Acc@1: 79.1667 (79.1667)  Acc@5: 87.5000 (87.5000)  Acc@task: 58.3333 (58.3333)  time: 0.5966  data: 0.4304  max mem: 1378\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:06  Loss: 0.5044 (0.7337)  Acc@1: 83.3333 (81.8182)  Acc@5: 100.0000 (97.3485)  Acc@task: 66.6667 (64.3939)  time: 0.2163  data: 0.0394  max mem: 1378\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:04  Loss: 0.6196 (0.7748)  Acc@1: 79.1667 (79.5635)  Acc@5: 95.8333 (97.0238)  Acc@task: 66.6667 (64.6825)  time: 0.1788  data: 0.0005  max mem: 1378\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:02  Loss: 0.8260 (0.8455)  Acc@1: 79.1667 (77.9570)  Acc@5: 95.8333 (96.6398)  Acc@task: 66.6667 (65.5914)  time: 0.1797  data: 0.0012  max mem: 1378\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 0.8984 (0.8772)  Acc@1: 75.0000 (76.9309)  Acc@5: 95.8333 (96.3415)  Acc@task: 70.8333 (66.5650)  time: 0.1801  data: 0.0011  max mem: 1378\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 0.8984 (0.8704)  Acc@1: 75.0000 (76.9000)  Acc@5: 95.8333 (96.4000)  Acc@task: 70.8333 (66.8000)  time: 0.1766  data: 0.0011  max mem: 1378\n",
            "Test: [Task 7] Total time: 0:00:07 (0.1902 s / it)\n",
            "* Acc@task 66.800 Acc@1 76.900 Acc@5 96.400 loss 0.870\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:25  Loss: 6.4684 (6.4684)  Acc@1: 0.0000 (0.0000)  Acc@5: 41.6667 (41.6667)  Acc@task: 41.6667 (41.6667)  time: 0.6112  data: 0.4399  max mem: 1378\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:06  Loss: 6.0121 (5.8462)  Acc@1: 4.1667 (4.5455)  Acc@5: 41.6667 (43.5606)  Acc@task: 54.1667 (57.1970)  time: 0.2186  data: 0.0403  max mem: 1378\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:04  Loss: 5.9779 (5.8987)  Acc@1: 4.1667 (6.1508)  Acc@5: 45.8333 (46.6270)  Acc@task: 58.3333 (58.3333)  time: 0.1787  data: 0.0005  max mem: 1378\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:02  Loss: 5.9779 (5.9048)  Acc@1: 4.1667 (5.6452)  Acc@5: 45.8333 (45.6989)  Acc@task: 58.3333 (58.0645)  time: 0.1793  data: 0.0005  max mem: 1378\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 6.0379 (5.8924)  Acc@1: 4.1667 (5.8943)  Acc@5: 45.8333 (45.5285)  Acc@task: 54.1667 (57.5203)  time: 0.1804  data: 0.0003  max mem: 1378\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 6.0379 (5.8839)  Acc@1: 4.1667 (6.0000)  Acc@5: 45.8333 (45.6000)  Acc@task: 54.1667 (57.7000)  time: 0.1768  data: 0.0002  max mem: 1378\n",
            "Test: [Task 8] Total time: 0:00:08 (0.1906 s / it)\n",
            "* Acc@task 57.700 Acc@1 6.000 Acc@5 45.600 loss 5.884\n",
            "[Average accuracy till task8]\tAcc@task: 71.6000\tAcc@1: 69.5250\tAcc@5: 89.6750\tLoss: 1.4941\tForgetting: 0.1000\tBackward: 67.6571\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:19  Loss: 0.6726 (0.6726)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  Acc@task: 79.1667 (79.1667)  time: 0.4651  data: 0.2977  max mem: 1378\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.8333 (0.9666)  Acc@1: 70.8333 (75.3788)  Acc@5: 95.8333 (96.2121)  Acc@task: 75.0000 (76.5152)  time: 0.2069  data: 0.0311  max mem: 1378\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.8823 (0.9014)  Acc@1: 70.8333 (76.7857)  Acc@5: 95.8333 (95.8333)  Acc@task: 79.1667 (78.7698)  time: 0.1803  data: 0.0031  max mem: 1378\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.6999 (0.8416)  Acc@1: 79.1667 (78.4946)  Acc@5: 95.8333 (96.5054)  Acc@task: 79.1667 (79.0323)  time: 0.1798  data: 0.0010  max mem: 1378\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6014 (0.7749)  Acc@1: 83.3333 (79.8781)  Acc@5: 100.0000 (96.9512)  Acc@task: 83.3333 (80.4878)  time: 0.1800  data: 0.0003  max mem: 1378\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.5732 (0.7637)  Acc@1: 83.3333 (80.0000)  Acc@5: 100.0000 (97.0000)  Acc@task: 83.3333 (80.8000)  time: 0.1766  data: 0.0003  max mem: 1378\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1878 s / it)\n",
            "* Acc@task 80.800 Acc@1 80.000 Acc@5 97.000 loss 0.764\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:18  Loss: 0.9762 (0.9762)  Acc@1: 79.1667 (79.1667)  Acc@5: 87.5000 (87.5000)  Acc@task: 62.5000 (62.5000)  time: 0.4418  data: 0.2666  max mem: 1378\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.8513 (0.9693)  Acc@1: 79.1667 (78.4091)  Acc@5: 95.8333 (95.0758)  Acc@task: 70.8333 (68.1818)  time: 0.2057  data: 0.0287  max mem: 1378\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.8513 (1.0581)  Acc@1: 75.0000 (76.1905)  Acc@5: 95.8333 (94.4444)  Acc@task: 70.8333 (70.4365)  time: 0.1804  data: 0.0027  max mem: 1378\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.9061 (1.0162)  Acc@1: 79.1667 (77.0161)  Acc@5: 95.8333 (95.0269)  Acc@task: 70.8333 (70.4301)  time: 0.1796  data: 0.0003  max mem: 1378\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.7578 (0.9475)  Acc@1: 79.1667 (77.6423)  Acc@5: 95.8333 (95.1220)  Acc@task: 70.8333 (70.8333)  time: 0.1803  data: 0.0003  max mem: 1378\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.6628 (0.9407)  Acc@1: 79.1667 (77.7000)  Acc@5: 95.8333 (95.2000)  Acc@task: 70.8333 (70.8000)  time: 0.1770  data: 0.0003  max mem: 1378\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1883 s / it)\n",
            "* Acc@task 70.800 Acc@1 77.700 Acc@5 95.200 loss 0.941\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:34  Loss: 0.4046 (0.4046)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.8216  data: 0.6440  max mem: 1378\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:07  Loss: 0.9190 (0.9225)  Acc@1: 83.3333 (80.6818)  Acc@5: 95.8333 (95.4545)  Acc@task: 79.1667 (79.1667)  time: 0.2373  data: 0.0596  max mem: 1378\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.8381 (0.8969)  Acc@1: 79.1667 (80.7540)  Acc@5: 95.8333 (95.2381)  Acc@task: 79.1667 (78.9683)  time: 0.1788  data: 0.0008  max mem: 1378\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.6998 (0.8567)  Acc@1: 79.1667 (80.3763)  Acc@5: 95.8333 (95.5645)  Acc@task: 79.1667 (78.4946)  time: 0.1788  data: 0.0004  max mem: 1378\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.7013 (0.8404)  Acc@1: 79.1667 (80.4878)  Acc@5: 95.8333 (95.4268)  Acc@task: 75.0000 (78.3537)  time: 0.1785  data: 0.0003  max mem: 1378\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.6995 (0.8371)  Acc@1: 79.1667 (80.4000)  Acc@5: 95.8333 (95.4000)  Acc@task: 75.0000 (78.2000)  time: 0.1750  data: 0.0003  max mem: 1378\n",
            "Test: [Task 3] Total time: 0:00:08 (0.1950 s / it)\n",
            "* Acc@task 78.200 Acc@1 80.400 Acc@5 95.400 loss 0.837\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:25  Loss: 0.8034 (0.8034)  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  Acc@task: 66.6667 (66.6667)  time: 0.6059  data: 0.4166  max mem: 1378\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:06  Loss: 0.7848 (0.7733)  Acc@1: 79.1667 (80.6818)  Acc@5: 95.8333 (95.4545)  Acc@task: 70.8333 (73.4849)  time: 0.2160  data: 0.0384  max mem: 1378\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.7848 (0.8667)  Acc@1: 79.1667 (78.5714)  Acc@5: 95.8333 (95.4365)  Acc@task: 75.0000 (74.6032)  time: 0.1773  data: 0.0005  max mem: 1378\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.7705 (0.8126)  Acc@1: 79.1667 (79.4355)  Acc@5: 95.8333 (95.9677)  Acc@task: 75.0000 (74.5968)  time: 0.1783  data: 0.0007  max mem: 1378\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7989 (0.8753)  Acc@1: 79.1667 (78.5569)  Acc@5: 95.8333 (95.4268)  Acc@task: 75.0000 (73.9837)  time: 0.1790  data: 0.0009  max mem: 1378\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.7705 (0.8601)  Acc@1: 79.1667 (78.8000)  Acc@5: 95.8333 (95.5000)  Acc@task: 75.0000 (74.3000)  time: 0.1756  data: 0.0009  max mem: 1378\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1891 s / it)\n",
            "* Acc@task 74.300 Acc@1 78.800 Acc@5 95.500 loss 0.860\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:19  Loss: 0.5928 (0.5928)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 75.0000 (75.0000)  time: 0.4692  data: 0.3010  max mem: 1378\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 0.5928 (0.7559)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (97.3485)  Acc@task: 75.0000 (76.1364)  time: 0.2079  data: 0.0328  max mem: 1378\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.8153 (0.7847)  Acc@1: 79.1667 (79.9603)  Acc@5: 95.8333 (96.8254)  Acc@task: 75.0000 (75.9921)  time: 0.1801  data: 0.0032  max mem: 1378\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.8121 (0.7863)  Acc@1: 79.1667 (80.1075)  Acc@5: 95.8333 (96.5054)  Acc@task: 75.0000 (76.0753)  time: 0.1786  data: 0.0004  max mem: 1378\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.8401 (0.8377)  Acc@1: 79.1667 (79.3699)  Acc@5: 95.8333 (95.9350)  Acc@task: 70.8333 (74.5935)  time: 0.1789  data: 0.0003  max mem: 1378\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.8898 (0.8938)  Acc@1: 79.1667 (78.9000)  Acc@5: 95.8333 (95.6000)  Acc@task: 70.8333 (74.4000)  time: 0.1756  data: 0.0003  max mem: 1378\n",
            "Test: [Task 5] Total time: 0:00:07 (0.1872 s / it)\n",
            "* Acc@task 74.400 Acc@1 78.900 Acc@5 95.600 loss 0.894\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:21  Loss: 0.6904 (0.6904)  Acc@1: 70.8333 (70.8333)  Acc@5: 100.0000 (100.0000)  Acc@task: 75.0000 (75.0000)  time: 0.5126  data: 0.3449  max mem: 1378\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:06  Loss: 0.8158 (0.8013)  Acc@1: 79.1667 (78.4091)  Acc@5: 95.8333 (96.9697)  Acc@task: 66.6667 (69.3182)  time: 0.2092  data: 0.0324  max mem: 1378\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:04  Loss: 0.8180 (0.8316)  Acc@1: 79.1667 (78.9683)  Acc@5: 95.8333 (96.8254)  Acc@task: 70.8333 (70.8333)  time: 0.1787  data: 0.0011  max mem: 1378\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 1.0717 (0.8675)  Acc@1: 75.0000 (77.9570)  Acc@5: 95.8333 (96.7742)  Acc@task: 70.8333 (69.4893)  time: 0.1794  data: 0.0010  max mem: 1378\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.0866 (0.8960)  Acc@1: 75.0000 (77.7439)  Acc@5: 95.8333 (96.6463)  Acc@task: 70.8333 (69.7155)  time: 0.1806  data: 0.0005  max mem: 1378\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.0866 (0.9027)  Acc@1: 75.0000 (77.5000)  Acc@5: 95.8333 (96.7000)  Acc@task: 70.8333 (69.8000)  time: 0.1772  data: 0.0004  max mem: 1378\n",
            "Test: [Task 6] Total time: 0:00:07 (0.1885 s / it)\n",
            "* Acc@task 69.800 Acc@1 77.500 Acc@5 96.700 loss 0.903\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:20  Loss: 1.4348 (1.4348)  Acc@1: 79.1667 (79.1667)  Acc@5: 87.5000 (87.5000)  Acc@task: 58.3333 (58.3333)  time: 0.4898  data: 0.3221  max mem: 1378\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:06  Loss: 0.5044 (0.7337)  Acc@1: 83.3333 (81.8182)  Acc@5: 100.0000 (97.3485)  Acc@task: 66.6667 (64.3939)  time: 0.2078  data: 0.0298  max mem: 1378\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:04  Loss: 0.6196 (0.7748)  Acc@1: 79.1667 (79.5635)  Acc@5: 95.8333 (97.0238)  Acc@task: 66.6667 (64.6825)  time: 0.1792  data: 0.0005  max mem: 1378\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:02  Loss: 0.8260 (0.8455)  Acc@1: 79.1667 (77.9570)  Acc@5: 95.8333 (96.6398)  Acc@task: 66.6667 (65.5914)  time: 0.1801  data: 0.0004  max mem: 1378\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 0.8984 (0.8772)  Acc@1: 75.0000 (76.9309)  Acc@5: 95.8333 (96.3415)  Acc@task: 70.8333 (66.5650)  time: 0.1814  data: 0.0003  max mem: 1378\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 0.8984 (0.8704)  Acc@1: 75.0000 (76.9000)  Acc@5: 95.8333 (96.4000)  Acc@task: 70.8333 (66.8000)  time: 0.1781  data: 0.0003  max mem: 1378\n",
            "Test: [Task 7] Total time: 0:00:07 (0.1893 s / it)\n",
            "* Acc@task 66.800 Acc@1 76.900 Acc@5 96.400 loss 0.870\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:33  Loss: 6.4684 (6.4684)  Acc@1: 0.0000 (0.0000)  Acc@5: 41.6667 (41.6667)  Acc@task: 41.6667 (41.6667)  time: 0.7894  data: 0.6079  max mem: 1378\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:07  Loss: 6.0121 (5.8462)  Acc@1: 4.1667 (4.5455)  Acc@5: 41.6667 (43.5606)  Acc@task: 54.1667 (57.1970)  time: 0.2359  data: 0.0562  max mem: 1378\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:04  Loss: 5.9779 (5.8987)  Acc@1: 4.1667 (6.1508)  Acc@5: 45.8333 (46.6270)  Acc@task: 58.3333 (58.3333)  time: 0.1799  data: 0.0008  max mem: 1378\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:02  Loss: 5.9779 (5.9048)  Acc@1: 4.1667 (5.6452)  Acc@5: 45.8333 (45.6989)  Acc@task: 58.3333 (58.0645)  time: 0.1805  data: 0.0004  max mem: 1378\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 6.0379 (5.8924)  Acc@1: 4.1667 (5.8943)  Acc@5: 45.8333 (45.5285)  Acc@task: 54.1667 (57.5203)  time: 0.1821  data: 0.0002  max mem: 1378\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 6.0379 (5.8839)  Acc@1: 4.1667 (6.0000)  Acc@5: 45.8333 (45.6000)  Acc@task: 54.1667 (57.7000)  time: 0.1785  data: 0.0002  max mem: 1378\n",
            "Test: [Task 8] Total time: 0:00:08 (0.1964 s / it)\n",
            "* Acc@task 57.700 Acc@1 6.000 Acc@5 45.600 loss 5.884\n",
            "[Average accuracy till task8]\tAcc@task: 71.6000\tAcc@1: 69.5250\tAcc@5: 89.6750\tLoss: 1.4941\tForgetting: 4.5429\tBackward: 56.6143\n",
            "torch.Size([83040, 384])\n",
            "Averaged stats: Lr: 0.005000  Loss: 0.0135  Acc@1: 99.1667 (97.4286)  Acc@5: 100.0000 (99.6191)\n",
            "torch.Size([83040, 384])\n",
            "Averaged stats: Lr: 0.004878  Loss: 0.0244  Acc@1: 100.0000 (99.4762)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([83040, 384])\n",
            "Averaged stats: Lr: 0.004523  Loss: 0.0075  Acc@1: 100.0000 (99.6786)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([83040, 384])\n",
            "Averaged stats: Lr: 0.003969  Loss: 0.0128  Acc@1: 100.0000 (99.8214)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([83040, 384])\n",
            "Averaged stats: Lr: 0.003273  Loss: 0.0143  Acc@1: 100.0000 (99.7738)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([83040, 384])\n",
            "Averaged stats: Lr: 0.002500  Loss: 0.0050  Acc@1: 100.0000 (99.8214)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([83040, 384])\n",
            "Averaged stats: Lr: 0.001727  Loss: 0.0134  Acc@1: 100.0000 (99.7976)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([83040, 384])\n",
            "Averaged stats: Lr: 0.001031  Loss: 0.0094  Acc@1: 100.0000 (99.8572)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([83040, 384])\n",
            "Averaged stats: Lr: 0.000477  Loss: 0.0130  Acc@1: 100.0000 (99.8929)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([83040, 384])\n",
            "Averaged stats: Lr: 0.000122  Loss: 0.0082  Acc@1: 100.0000 (99.8810)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [Task 1]  [ 0/23]  eta: 0:00:13  Loss: 2.5001 (2.5001)  Acc@1: 45.8333 (45.8333)  Acc@5: 91.6667 (91.6667)  Acc@task: 54.1667 (54.1667)  time: 0.5829  data: 0.3856  max mem: 1378\n",
            "Test: [Task 1]  [10/23]  eta: 0:00:02  Loss: 1.4221 (1.4066)  Acc@1: 70.8333 (69.3182)  Acc@5: 91.6667 (90.1515)  Acc@task: 58.3333 (58.3333)  time: 0.2163  data: 0.0362  max mem: 1378\n",
            "Test: [Task 1]  [20/23]  eta: 0:00:00  Loss: 1.3539 (1.3208)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (90.2778)  Acc@task: 66.6667 (64.4841)  time: 0.1797  data: 0.0009  max mem: 1378\n",
            "Test: [Task 1]  [22/23]  eta: 0:00:00  Loss: 1.3598 (1.3768)  Acc@1: 70.8333 (70.5224)  Acc@5: 87.5000 (89.5522)  Acc@task: 66.6667 (64.1791)  time: 0.1753  data: 0.0006  max mem: 1378\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1969 s / it)\n",
            "* Acc@task 64.179 Acc@1 70.522 Acc@5 89.552 loss 1.377\n",
            "Test: [Task 2]  [ 0/20]  eta: 0:00:11  Loss: 2.4336 (2.4336)  Acc@1: 54.1667 (54.1667)  Acc@5: 79.1667 (79.1667)  Acc@task: 62.5000 (62.5000)  time: 0.5544  data: 0.3877  max mem: 1378\n",
            "Test: [Task 2]  [10/20]  eta: 0:00:02  Loss: 2.1183 (2.0651)  Acc@1: 54.1667 (57.1970)  Acc@5: 87.5000 (86.3636)  Acc@task: 58.3333 (57.1970)  time: 0.2140  data: 0.0358  max mem: 1378\n",
            "Test: [Task 2]  [19/20]  eta: 0:00:00  Loss: 2.0486 (1.9790)  Acc@1: 54.1667 (57.8288)  Acc@5: 87.5000 (85.1775)  Acc@task: 58.3333 (56.9937)  time: 0.1984  data: 0.0198  max mem: 1378\n",
            "Test: [Task 2] Total time: 0:00:04 (0.2030 s / it)\n",
            "* Acc@task 56.994 Acc@1 57.829 Acc@5 85.177 loss 1.979\n",
            "Test: [Task 3]  [ 0/21]  eta: 0:00:11  Loss: 3.1619 (3.1619)  Acc@1: 58.3333 (58.3333)  Acc@5: 75.0000 (75.0000)  Acc@task: 58.3333 (58.3333)  time: 0.5372  data: 0.3474  max mem: 1378\n",
            "Test: [Task 3]  [10/21]  eta: 0:00:02  Loss: 2.8313 (2.8508)  Acc@1: 58.3333 (56.8182)  Acc@5: 79.1667 (78.0303)  Acc@task: 62.5000 (63.2576)  time: 0.2129  data: 0.0324  max mem: 1378\n",
            "Test: [Task 3]  [20/21]  eta: 0:00:00  Loss: 2.7338 (2.7699)  Acc@1: 58.3333 (56.1368)  Acc@5: 76.4706 (77.8672)  Acc@task: 64.7059 (63.5815)  time: 0.1782  data: 0.0005  max mem: 1378\n",
            "Test: [Task 3] Total time: 0:00:04 (0.2000 s / it)\n",
            "* Acc@task 63.581 Acc@1 56.137 Acc@5 77.867 loss 2.770\n",
            "Test: [Task 4]  [ 0/20]  eta: 0:00:10  Loss: 3.4169 (3.4169)  Acc@1: 54.1667 (54.1667)  Acc@5: 66.6667 (66.6667)  Acc@task: 62.5000 (62.5000)  time: 0.5089  data: 0.3343  max mem: 1378\n",
            "Test: [Task 4]  [10/20]  eta: 0:00:02  Loss: 3.0676 (2.6968)  Acc@1: 50.0000 (51.8939)  Acc@5: 75.0000 (75.7576)  Acc@task: 58.3333 (57.5758)  time: 0.2119  data: 0.0337  max mem: 1378\n",
            "Test: [Task 4]  [19/20]  eta: 0:00:00  Loss: 2.2952 (2.8047)  Acc@1: 54.1667 (51.5924)  Acc@5: 75.0000 (76.8578)  Acc@task: 58.3333 (57.1125)  time: 0.1956  data: 0.0186  max mem: 1378\n",
            "Test: [Task 4] Total time: 0:00:04 (0.2023 s / it)\n",
            "* Acc@task 57.113 Acc@1 51.592 Acc@5 76.858 loss 2.805\n",
            "Test: [Task 5]  [ 0/21]  eta: 0:00:19  Loss: 3.9468 (3.9468)  Acc@1: 33.3333 (33.3333)  Acc@5: 70.8333 (70.8333)  Acc@task: 50.0000 (50.0000)  time: 0.9196  data: 0.7019  max mem: 1378\n",
            "Test: [Task 5]  [10/21]  eta: 0:00:02  Loss: 3.3159 (3.2496)  Acc@1: 50.0000 (48.4848)  Acc@5: 70.8333 (71.9697)  Acc@task: 50.0000 (52.6515)  time: 0.2472  data: 0.0641  max mem: 1378\n",
            "Test: [Task 5]  [20/21]  eta: 0:00:00  Loss: 3.0020 (2.9754)  Acc@1: 50.0000 (50.9165)  Acc@5: 70.8333 (73.3198)  Acc@task: 50.0000 (55.1935)  time: 0.1759  data: 0.0003  max mem: 1378\n",
            "Test: [Task 5] Total time: 0:00:04 (0.2159 s / it)\n",
            "* Acc@task 55.193 Acc@1 50.916 Acc@5 73.320 loss 2.975\n",
            "Test: [Task 6]  [ 0/20]  eta: 0:00:13  Loss: 3.8098 (3.8098)  Acc@1: 41.6667 (41.6667)  Acc@5: 58.3333 (58.3333)  Acc@task: 45.8333 (45.8333)  time: 0.6890  data: 0.5286  max mem: 1378\n",
            "Test: [Task 6]  [10/20]  eta: 0:00:02  Loss: 2.9573 (2.7194)  Acc@1: 54.1667 (54.1667)  Acc@5: 79.1667 (78.0303)  Acc@task: 54.1667 (55.3030)  time: 0.2281  data: 0.0486  max mem: 1378\n",
            "Test: [Task 6]  [19/20]  eta: 0:00:00  Loss: 2.9573 (2.8860)  Acc@1: 54.1667 (53.3333)  Acc@5: 75.0000 (75.6250)  Acc@task: 54.1667 (56.4583)  time: 0.2067  data: 0.0268  max mem: 1378\n",
            "Test: [Task 6] Total time: 0:00:04 (0.2115 s / it)\n",
            "* Acc@task 56.458 Acc@1 53.333 Acc@5 75.625 loss 2.886\n",
            "Test: [Task 7]  [ 0/23]  eta: 0:00:14  Loss: 2.0087 (2.0087)  Acc@1: 66.6667 (66.6667)  Acc@5: 83.3333 (83.3333)  Acc@task: 54.1667 (54.1667)  time: 0.6379  data: 0.4597  max mem: 1378\n",
            "Test: [Task 7]  [10/23]  eta: 0:00:02  Loss: 2.2638 (2.3557)  Acc@1: 62.5000 (60.9849)  Acc@5: 79.1667 (79.5455)  Acc@task: 58.3333 (56.8182)  time: 0.2235  data: 0.0421  max mem: 1378\n",
            "Test: [Task 7]  [20/23]  eta: 0:00:00  Loss: 2.2601 (2.2397)  Acc@1: 58.3333 (58.9286)  Acc@5: 83.3333 (83.5317)  Acc@task: 54.1667 (53.7698)  time: 0.1817  data: 0.0003  max mem: 1378\n",
            "Test: [Task 7]  [22/23]  eta: 0:00:00  Loss: 2.2230 (2.2255)  Acc@1: 58.3333 (59.2727)  Acc@5: 83.3333 (83.4545)  Acc@task: 54.1667 (54.3636)  time: 0.1826  data: 0.0003  max mem: 1378\n",
            "Test: [Task 7] Total time: 0:00:04 (0.2079 s / it)\n",
            "* Acc@task 54.364 Acc@1 59.273 Acc@5 83.455 loss 2.226\n",
            "Test: [Task 8]  [ 0/22]  eta: 0:00:17  Loss: 2.2512 (2.2512)  Acc@1: 58.3333 (58.3333)  Acc@5: 83.3333 (83.3333)  Acc@task: 66.6667 (66.6667)  time: 0.8164  data: 0.6353  max mem: 1378\n",
            "Test: [Task 8]  [10/22]  eta: 0:00:02  Loss: 2.1506 (2.0973)  Acc@1: 62.5000 (59.4697)  Acc@5: 83.3333 (83.3333)  Acc@task: 50.0000 (51.1364)  time: 0.2374  data: 0.0586  max mem: 1378\n",
            "Test: [Task 8]  [20/22]  eta: 0:00:00  Loss: 2.0055 (2.1247)  Acc@1: 58.3333 (57.1429)  Acc@5: 83.3333 (82.7381)  Acc@task: 54.1667 (53.1746)  time: 0.1803  data: 0.0006  max mem: 1378\n",
            "Test: [Task 8]  [21/22]  eta: 0:00:00  Loss: 1.9104 (2.1034)  Acc@1: 57.1429 (57.1429)  Acc@5: 83.3333 (82.9746)  Acc@task: 54.1667 (53.2290)  time: 0.1749  data: 0.0006  max mem: 1378\n",
            "Test: [Task 8] Total time: 0:00:04 (0.2086 s / it)\n",
            "* Acc@task 53.229 Acc@1 57.143 Acc@5 82.975 loss 2.103\n",
            "[Average accuracy till task8]\tAcc@task: 57.6389\tAcc@1: 57.0932\tAcc@5: 80.6036\tLoss: 2.3901\tForgetting: 0.0000\tBackward: 57.0861\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:22  Loss: 0.6818 (0.6818)  Acc@1: 87.5000 (87.5000)  Acc@5: 95.8333 (95.8333)  Acc@task: 79.1667 (79.1667)  time: 0.5438  data: 0.3533  max mem: 1378\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 1.0159 (1.0192)  Acc@1: 75.0000 (75.3788)  Acc@5: 95.8333 (95.4545)  Acc@task: 75.0000 (76.5152)  time: 0.2134  data: 0.0324  max mem: 1378\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 1.0019 (0.9644)  Acc@1: 75.0000 (76.9841)  Acc@5: 95.8333 (95.2381)  Acc@task: 79.1667 (78.7698)  time: 0.1802  data: 0.0003  max mem: 1378\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.7825 (0.8948)  Acc@1: 83.3333 (78.3602)  Acc@5: 95.8333 (95.8333)  Acc@task: 79.1667 (79.0323)  time: 0.1807  data: 0.0003  max mem: 1378\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6118 (0.8259)  Acc@1: 83.3333 (79.6748)  Acc@5: 100.0000 (96.5447)  Acc@task: 83.3333 (80.4878)  time: 0.1817  data: 0.0003  max mem: 1378\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6040 (0.8152)  Acc@1: 83.3333 (79.8000)  Acc@5: 100.0000 (96.6000)  Acc@task: 83.3333 (80.8000)  time: 0.1781  data: 0.0003  max mem: 1378\n",
            "Test: [Task 1] Total time: 0:00:08 (0.1911 s / it)\n",
            "* Acc@task 80.800 Acc@1 79.800 Acc@5 96.600 loss 0.815\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:33  Loss: 1.0445 (1.0445)  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  Acc@task: 62.5000 (62.5000)  time: 0.8004  data: 0.5927  max mem: 1378\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:07  Loss: 0.9171 (1.0193)  Acc@1: 83.3333 (81.0606)  Acc@5: 95.8333 (95.0758)  Acc@task: 70.8333 (68.1818)  time: 0.2373  data: 0.0549  max mem: 1378\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.9074 (1.1380)  Acc@1: 79.1667 (77.3810)  Acc@5: 95.8333 (93.8492)  Acc@task: 70.8333 (70.4365)  time: 0.1808  data: 0.0008  max mem: 1378\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.9211 (1.0895)  Acc@1: 79.1667 (77.5538)  Acc@5: 95.8333 (94.3548)  Acc@task: 70.8333 (70.4301)  time: 0.1812  data: 0.0005  max mem: 1378\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.7828 (1.0124)  Acc@1: 79.1667 (77.6423)  Acc@5: 95.8333 (94.6138)  Acc@task: 70.8333 (70.8333)  time: 0.1814  data: 0.0002  max mem: 1378\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.7828 (1.0078)  Acc@1: 75.0000 (77.6000)  Acc@5: 95.8333 (94.7000)  Acc@task: 70.8333 (70.8000)  time: 0.1780  data: 0.0002  max mem: 1378\n",
            "Test: [Task 2] Total time: 0:00:08 (0.1971 s / it)\n",
            "* Acc@task 70.800 Acc@1 77.600 Acc@5 94.700 loss 1.008\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:21  Loss: 0.4862 (0.4862)  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.5160  data: 0.3350  max mem: 1378\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 0.9420 (0.9488)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.0758)  Acc@task: 79.1667 (79.1667)  time: 0.2105  data: 0.0308  max mem: 1378\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.9066 (0.9321)  Acc@1: 79.1667 (79.3651)  Acc@5: 95.8333 (94.4444)  Acc@task: 79.1667 (78.9683)  time: 0.1801  data: 0.0003  max mem: 1378\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.8696 (0.9070)  Acc@1: 79.1667 (79.0323)  Acc@5: 95.8333 (94.7581)  Acc@task: 79.1667 (78.4946)  time: 0.1802  data: 0.0009  max mem: 1378\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.7138 (0.8941)  Acc@1: 79.1667 (79.0650)  Acc@5: 95.8333 (94.8171)  Acc@task: 75.0000 (78.3537)  time: 0.1802  data: 0.0009  max mem: 1378\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.7138 (0.8949)  Acc@1: 79.1667 (79.0000)  Acc@5: 95.8333 (94.9000)  Acc@task: 75.0000 (78.2000)  time: 0.1768  data: 0.0009  max mem: 1378\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1890 s / it)\n",
            "* Acc@task 78.200 Acc@1 79.000 Acc@5 94.900 loss 0.895\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:19  Loss: 0.9834 (0.9834)  Acc@1: 75.0000 (75.0000)  Acc@5: 87.5000 (87.5000)  Acc@task: 66.6667 (66.6667)  time: 0.4578  data: 0.2792  max mem: 1378\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:06  Loss: 0.8770 (0.9091)  Acc@1: 83.3333 (79.9242)  Acc@5: 95.8333 (94.6970)  Acc@task: 70.8333 (73.4849)  time: 0.2057  data: 0.0266  max mem: 1378\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.8342 (0.9821)  Acc@1: 79.1667 (78.1746)  Acc@5: 95.8333 (95.0397)  Acc@task: 75.0000 (74.6032)  time: 0.1799  data: 0.0009  max mem: 1378\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.7925 (0.8979)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 75.0000 (74.5968)  time: 0.1802  data: 0.0004  max mem: 1378\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7767 (0.9763)  Acc@1: 79.1667 (77.9472)  Acc@5: 95.8333 (95.0203)  Acc@task: 75.0000 (73.9837)  time: 0.1809  data: 0.0002  max mem: 1378\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.7767 (0.9569)  Acc@1: 79.1667 (78.2000)  Acc@5: 95.8333 (95.1000)  Acc@task: 75.0000 (74.3000)  time: 0.1776  data: 0.0002  max mem: 1378\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1881 s / it)\n",
            "* Acc@task 74.300 Acc@1 78.200 Acc@5 95.100 loss 0.957\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:23  Loss: 0.5889 (0.5889)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  Acc@task: 75.0000 (75.0000)  time: 0.5498  data: 0.3759  max mem: 1378\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 0.6954 (0.8129)  Acc@1: 79.1667 (79.5455)  Acc@5: 100.0000 (97.3485)  Acc@task: 75.0000 (76.1364)  time: 0.2135  data: 0.0350  max mem: 1378\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.8400 (0.8419)  Acc@1: 79.1667 (79.9603)  Acc@5: 95.8333 (96.2302)  Acc@task: 75.0000 (75.9921)  time: 0.1796  data: 0.0009  max mem: 1378\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.8400 (0.8283)  Acc@1: 79.1667 (80.2419)  Acc@5: 95.8333 (95.8333)  Acc@task: 75.0000 (76.0753)  time: 0.1798  data: 0.0007  max mem: 1378\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.8469 (0.8844)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.3252)  Acc@task: 70.8333 (74.5935)  time: 0.1804  data: 0.0003  max mem: 1378\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.8915 (0.9398)  Acc@1: 75.0000 (78.7000)  Acc@5: 95.8333 (95.1000)  Acc@task: 70.8333 (74.4000)  time: 0.1769  data: 0.0003  max mem: 1378\n",
            "Test: [Task 5] Total time: 0:00:07 (0.1895 s / it)\n",
            "* Acc@task 74.400 Acc@1 78.700 Acc@5 95.100 loss 0.940\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:21  Loss: 0.7432 (0.7432)  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  Acc@task: 75.0000 (75.0000)  time: 0.5170  data: 0.3397  max mem: 1378\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:06  Loss: 0.7924 (0.8870)  Acc@1: 79.1667 (79.5455)  Acc@5: 95.8333 (96.9697)  Acc@task: 66.6667 (69.3182)  time: 0.2102  data: 0.0315  max mem: 1378\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:04  Loss: 0.9783 (0.9488)  Acc@1: 79.1667 (77.7778)  Acc@5: 95.8333 (96.4286)  Acc@task: 70.8333 (70.8333)  time: 0.1793  data: 0.0005  max mem: 1378\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 1.1139 (0.9767)  Acc@1: 75.0000 (76.2097)  Acc@5: 95.8333 (96.2366)  Acc@task: 70.8333 (69.4893)  time: 0.1795  data: 0.0003  max mem: 1378\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.0540 (1.0116)  Acc@1: 70.8333 (76.1179)  Acc@5: 95.8333 (95.9350)  Acc@task: 70.8333 (69.7155)  time: 0.1802  data: 0.0003  max mem: 1378\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.1548 (1.0160)  Acc@1: 70.8333 (76.0000)  Acc@5: 95.8333 (96.0000)  Acc@task: 70.8333 (69.8000)  time: 0.1768  data: 0.0003  max mem: 1378\n",
            "Test: [Task 6] Total time: 0:00:07 (0.1900 s / it)\n",
            "* Acc@task 69.800 Acc@1 76.000 Acc@5 96.000 loss 1.016\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:33  Loss: 1.5685 (1.5685)  Acc@1: 70.8333 (70.8333)  Acc@5: 87.5000 (87.5000)  Acc@task: 58.3333 (58.3333)  time: 0.7978  data: 0.5951  max mem: 1378\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:07  Loss: 0.7831 (0.9474)  Acc@1: 75.0000 (76.5152)  Acc@5: 100.0000 (95.4545)  Acc@task: 66.6667 (64.3939)  time: 0.2362  data: 0.0550  max mem: 1378\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:04  Loss: 0.7831 (0.9790)  Acc@1: 75.0000 (75.3968)  Acc@5: 95.8333 (95.2381)  Acc@task: 66.6667 (64.6825)  time: 0.1791  data: 0.0007  max mem: 1378\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:02  Loss: 1.0046 (1.0569)  Acc@1: 75.0000 (74.8656)  Acc@5: 95.8333 (94.6237)  Acc@task: 66.6667 (65.5914)  time: 0.1793  data: 0.0003  max mem: 1378\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 1.1048 (1.0809)  Acc@1: 70.8333 (74.2886)  Acc@5: 95.8333 (94.5122)  Acc@task: 70.8333 (66.5650)  time: 0.1800  data: 0.0002  max mem: 1378\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 1.1048 (1.0693)  Acc@1: 70.8333 (74.5000)  Acc@5: 93.7500 (94.5000)  Acc@task: 70.8333 (66.8000)  time: 0.1768  data: 0.0002  max mem: 1378\n",
            "Test: [Task 7] Total time: 0:00:08 (0.1951 s / it)\n",
            "* Acc@task 66.800 Acc@1 74.500 Acc@5 94.500 loss 1.069\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:19  Loss: 1.2615 (1.2615)  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.8333)  Acc@task: 41.6667 (41.6667)  time: 0.4569  data: 0.2923  max mem: 1378\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:06  Loss: 0.8933 (1.0553)  Acc@1: 79.1667 (76.8939)  Acc@5: 95.8333 (93.9394)  Acc@task: 54.1667 (57.1970)  time: 0.2056  data: 0.0271  max mem: 1378\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:04  Loss: 0.8720 (0.9985)  Acc@1: 75.0000 (75.1984)  Acc@5: 95.8333 (95.4365)  Acc@task: 58.3333 (58.3333)  time: 0.1797  data: 0.0005  max mem: 1378\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:02  Loss: 0.8661 (0.9873)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.6989)  Acc@task: 58.3333 (58.0645)  time: 0.1797  data: 0.0008  max mem: 1378\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 1.0291 (1.0043)  Acc@1: 75.0000 (75.4065)  Acc@5: 95.8333 (95.3252)  Acc@task: 54.1667 (57.5203)  time: 0.1810  data: 0.0010  max mem: 1378\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 1.0291 (1.0022)  Acc@1: 75.0000 (75.5000)  Acc@5: 95.8333 (95.3000)  Acc@task: 54.1667 (57.7000)  time: 0.1772  data: 0.0010  max mem: 1378\n",
            "Test: [Task 8] Total time: 0:00:07 (0.1877 s / it)\n",
            "* Acc@task 57.700 Acc@1 75.500 Acc@5 95.300 loss 1.002\n",
            "[Average accuracy till task8]\tAcc@task: 71.6000\tAcc@1: 77.4125\tAcc@5: 95.2750\tLoss: 0.9628\tForgetting: 0.0000\tBackward: 77.6857\n",
            "Loading checkpoint from: ./output/cifar100_full_dino_5epoch_10pct/checkpoint/task9_checkpoint.pth\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/engines/hide_promtp_wtp_and_tap_engine.py:574: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  loss = torch.nn.functional.cross_entropy(sim, torch.range(0, sim.shape[0] - 1).long().to(device))\n",
            "Train: Epoch[ 1/10]  [ 0/21]  eta: 0:00:15  Lr: 0.002812  Loss: 4.1283  Acc@1: 8.3333 (8.3333)  Acc@5: 29.1667 (29.1667)  time: 0.7400  data: 0.4445  max mem: 1378\n",
            "Train: Epoch[ 1/10]  [10/21]  eta: 0:00:03  Lr: 0.002812  Loss: 2.0833  Acc@1: 8.3333 (14.3939)  Acc@5: 62.5000 (60.2273)  time: 0.3253  data: 0.0410  max mem: 1380\n",
            "Train: Epoch[ 1/10]  [20/21]  eta: 0:00:00  Lr: 0.002812  Loss: 1.9587  Acc@1: 20.8333 (22.0441)  Acc@5: 70.8333 (68.3367)  time: 0.2808  data: 0.0004  max mem: 1380\n",
            "Train: Epoch[ 1/10] Total time: 0:00:06 (0.3075 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.9587  Acc@1: 20.8333 (22.0441)  Acc@5: 70.8333 (68.3367)\n",
            "Train: Epoch[ 2/10]  [ 0/21]  eta: 0:00:13  Lr: 0.002812  Loss: 1.4577  Acc@1: 54.1667 (54.1667)  Acc@5: 87.5000 (87.5000)  time: 0.6274  data: 0.3517  max mem: 1380\n",
            "Train: Epoch[ 2/10]  [10/21]  eta: 0:00:03  Lr: 0.002812  Loss: 1.5114  Acc@1: 45.8333 (44.3182)  Acc@5: 83.3333 (84.0909)  time: 0.3183  data: 0.0348  max mem: 1380\n",
            "Train: Epoch[ 2/10]  [20/21]  eta: 0:00:00  Lr: 0.002812  Loss: 0.7813  Acc@1: 50.0000 (50.9018)  Acc@5: 87.5000 (87.5752)  time: 0.2836  data: 0.0017  max mem: 1380\n",
            "Train: Epoch[ 2/10] Total time: 0:00:06 (0.3088 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.7813  Acc@1: 50.0000 (50.9018)  Acc@5: 87.5000 (87.5752)\n",
            "Train: Epoch[ 3/10]  [ 0/21]  eta: 0:00:23  Lr: 0.002812  Loss: 1.1447  Acc@1: 58.3333 (58.3333)  Acc@5: 95.8333 (95.8333)  time: 1.1047  data: 0.8128  max mem: 1380\n",
            "Train: Epoch[ 3/10]  [10/21]  eta: 0:00:03  Lr: 0.002812  Loss: 0.9485  Acc@1: 62.5000 (64.0152)  Acc@5: 95.8333 (93.5606)  time: 0.3580  data: 0.0745  max mem: 1380\n",
            "Train: Epoch[ 3/10]  [20/21]  eta: 0:00:00  Lr: 0.002812  Loss: 0.7927  Acc@1: 70.8333 (70.1403)  Acc@5: 95.8333 (94.5892)  time: 0.2810  data: 0.0004  max mem: 1380\n",
            "Train: Epoch[ 3/10] Total time: 0:00:06 (0.3247 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.7927  Acc@1: 70.8333 (70.1403)  Acc@5: 95.8333 (94.5892)\n",
            "Train: Epoch[ 4/10]  [ 0/21]  eta: 0:00:12  Lr: 0.002812  Loss: 1.0688  Acc@1: 54.1667 (54.1667)  Acc@5: 91.6667 (91.6667)  time: 0.5949  data: 0.3047  max mem: 1380\n",
            "Train: Epoch[ 4/10]  [10/21]  eta: 0:00:03  Lr: 0.002812  Loss: 0.6381  Acc@1: 75.0000 (74.6212)  Acc@5: 100.0000 (97.7273)  time: 0.3124  data: 0.0280  max mem: 1380\n",
            "Train: Epoch[ 4/10]  [20/21]  eta: 0:00:00  Lr: 0.002812  Loss: 0.2861  Acc@1: 75.0000 (76.1523)  Acc@5: 100.0000 (97.5952)  time: 0.2817  data: 0.0003  max mem: 1380\n",
            "Train: Epoch[ 4/10] Total time: 0:00:06 (0.3025 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.2861  Acc@1: 75.0000 (76.1523)  Acc@5: 100.0000 (97.5952)\n",
            "Train: Epoch[ 5/10]  [ 0/21]  eta: 0:00:22  Lr: 0.002812  Loss: 0.6366  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 1.0584  data: 0.6575  max mem: 1380\n",
            "Train: Epoch[ 5/10]  [10/21]  eta: 0:00:03  Lr: 0.002812  Loss: 0.7633  Acc@1: 83.3333 (79.9242)  Acc@5: 95.8333 (96.5909)  time: 0.3549  data: 0.0601  max mem: 1380\n",
            "Train: Epoch[ 5/10]  [20/21]  eta: 0:00:00  Lr: 0.002812  Loss: 0.3887  Acc@1: 83.3333 (80.5611)  Acc@5: 95.8333 (97.3948)  time: 0.2813  data: 0.0003  max mem: 1380\n",
            "Train: Epoch[ 5/10] Total time: 0:00:06 (0.3231 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.3887  Acc@1: 83.3333 (80.5611)  Acc@5: 95.8333 (97.3948)\n",
            "Train: Epoch[ 6/10]  [ 0/21]  eta: 0:00:16  Lr: 0.002812  Loss: 0.7152  Acc@1: 70.8333 (70.8333)  Acc@5: 100.0000 (100.0000)  time: 0.7684  data: 0.4921  max mem: 1380\n",
            "Train: Epoch[ 6/10]  [10/21]  eta: 0:00:03  Lr: 0.002812  Loss: 0.6354  Acc@1: 83.3333 (82.1970)  Acc@5: 100.0000 (100.0000)  time: 0.3289  data: 0.0450  max mem: 1380\n",
            "Train: Epoch[ 6/10]  [20/21]  eta: 0:00:00  Lr: 0.002812  Loss: 0.7799  Acc@1: 87.5000 (84.9699)  Acc@5: 100.0000 (99.3988)  time: 0.2820  data: 0.0003  max mem: 1380\n",
            "Train: Epoch[ 6/10] Total time: 0:00:06 (0.3134 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.7799  Acc@1: 87.5000 (84.9699)  Acc@5: 100.0000 (99.3988)\n",
            "Train: Epoch[ 7/10]  [ 0/21]  eta: 0:00:18  Lr: 0.002812  Loss: 0.1787  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  time: 0.8908  data: 0.4675  max mem: 1380\n",
            "Train: Epoch[ 7/10]  [10/21]  eta: 0:00:03  Lr: 0.002812  Loss: 0.4385  Acc@1: 87.5000 (89.3939)  Acc@5: 100.0000 (98.8636)  time: 0.3388  data: 0.0428  max mem: 1380\n",
            "Train: Epoch[ 7/10]  [20/21]  eta: 0:00:00  Lr: 0.002812  Loss: 0.2388  Acc@1: 87.5000 (87.9760)  Acc@5: 100.0000 (98.3968)  time: 0.2812  data: 0.0003  max mem: 1380\n",
            "Train: Epoch[ 7/10] Total time: 0:00:06 (0.3149 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.2388  Acc@1: 87.5000 (87.9760)  Acc@5: 100.0000 (98.3968)\n",
            "Train: Epoch[ 8/10]  [ 0/21]  eta: 0:00:14  Lr: 0.002812  Loss: 0.6097  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.6993  data: 0.4070  max mem: 1380\n",
            "Train: Epoch[ 8/10]  [10/21]  eta: 0:00:03  Lr: 0.002812  Loss: 0.5298  Acc@1: 83.3333 (83.7121)  Acc@5: 100.0000 (98.4848)  time: 0.3231  data: 0.0373  max mem: 1380\n",
            "Train: Epoch[ 8/10]  [20/21]  eta: 0:00:00  Lr: 0.002812  Loss: 0.3973  Acc@1: 83.3333 (85.1703)  Acc@5: 100.0000 (98.1964)  time: 0.2826  data: 0.0003  max mem: 1380\n",
            "Train: Epoch[ 8/10] Total time: 0:00:06 (0.3097 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.3973  Acc@1: 83.3333 (85.1703)  Acc@5: 100.0000 (98.1964)\n",
            "Train: Epoch[ 9/10]  [ 0/21]  eta: 0:00:23  Lr: 0.002812  Loss: 0.3840  Acc@1: 91.6667 (91.6667)  Acc@5: 95.8333 (95.8333)  time: 1.1309  data: 0.7875  max mem: 1380\n",
            "Train: Epoch[ 9/10]  [10/21]  eta: 0:00:03  Lr: 0.002812  Loss: 0.3500  Acc@1: 91.6667 (90.1515)  Acc@5: 100.0000 (99.2424)  time: 0.3630  data: 0.0722  max mem: 1380\n",
            "Train: Epoch[ 9/10]  [20/21]  eta: 0:00:00  Lr: 0.002812  Loss: 0.4050  Acc@1: 91.6667 (89.7796)  Acc@5: 100.0000 (99.3988)  time: 0.2828  data: 0.0005  max mem: 1380\n",
            "Train: Epoch[ 9/10] Total time: 0:00:06 (0.3274 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.4050  Acc@1: 91.6667 (89.7796)  Acc@5: 100.0000 (99.3988)\n",
            "Train: Epoch[10/10]  [ 0/21]  eta: 0:00:16  Lr: 0.002812  Loss: 0.3477  Acc@1: 91.6667 (91.6667)  Acc@5: 95.8333 (95.8333)  time: 0.7644  data: 0.4797  max mem: 1380\n",
            "Train: Epoch[10/10]  [10/21]  eta: 0:00:03  Lr: 0.002812  Loss: 0.3695  Acc@1: 87.5000 (87.1212)  Acc@5: 95.8333 (97.3485)  time: 0.3288  data: 0.0439  max mem: 1380\n",
            "Train: Epoch[10/10]  [20/21]  eta: 0:00:00  Lr: 0.002812  Loss: 0.1712  Acc@1: 87.5000 (88.9780)  Acc@5: 100.0000 (97.3948)  time: 0.2826  data: 0.0003  max mem: 1380\n",
            "Train: Epoch[10/10] Total time: 0:00:06 (0.3131 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.1712  Acc@1: 87.5000 (88.9780)  Acc@5: 100.0000 (97.3948)\n",
            "torch.Size([5, 2, 5, 6, 64])\n",
            "torch.Size([5, 2, 1, 5, 6, 64])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/23]  eta: 0:00:12  Loss: 0.6695 (0.6695)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  Acc@task: 70.8333 (70.8333)  time: 0.5328  data: 0.3568  max mem: 1380\n",
            "Test: [Task 1]  [10/23]  eta: 0:00:02  Loss: 0.7501 (0.8845)  Acc@1: 75.0000 (74.6212)  Acc@5: 95.8333 (93.5606)  Acc@task: 66.6667 (63.2576)  time: 0.2113  data: 0.0329  max mem: 1380\n",
            "Test: [Task 1]  [20/23]  eta: 0:00:00  Loss: 1.0444 (1.0462)  Acc@1: 70.8333 (72.8175)  Acc@5: 91.6667 (92.2619)  Acc@task: 62.5000 (61.9048)  time: 0.1782  data: 0.0004  max mem: 1380\n",
            "Test: [Task 1]  [22/23]  eta: 0:00:00  Loss: 1.0483 (1.0387)  Acc@1: 70.8333 (73.1343)  Acc@5: 91.6667 (92.5373)  Acc@task: 62.5000 (62.6866)  time: 0.1739  data: 0.0003  max mem: 1380\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1934 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@task 62.687 Acc@1 73.134 Acc@5 92.537 loss 1.039\n",
            "Test: [Task 2]  [ 0/20]  eta: 0:00:08  Loss: 1.9677 (1.9677)  Acc@1: 58.3333 (58.3333)  Acc@5: 87.5000 (87.5000)  Acc@task: 41.6667 (41.6667)  time: 0.4290  data: 0.2613  max mem: 1380\n",
            "Test: [Task 2]  [10/20]  eta: 0:00:02  Loss: 1.5846 (1.5355)  Acc@1: 62.5000 (64.3939)  Acc@5: 87.5000 (90.1515)  Acc@task: 54.1667 (54.1667)  time: 0.2091  data: 0.0351  max mem: 1380\n",
            "Test: [Task 2]  [19/20]  eta: 0:00:00  Loss: 1.6617 (1.7080)  Acc@1: 62.5000 (62.2129)  Acc@5: 87.5000 (86.8476)  Acc@task: 54.1667 (53.4447)  time: 0.1966  data: 0.0194  max mem: 1380\n",
            "Test: [Task 2] Total time: 0:00:04 (0.2038 s / it)\n",
            "* Acc@task 53.445 Acc@1 62.213 Acc@5 86.848 loss 1.708\n",
            "Test: [Task 3]  [ 0/21]  eta: 0:00:14  Loss: 2.6552 (2.6552)  Acc@1: 45.8333 (45.8333)  Acc@5: 83.3333 (83.3333)  Acc@task: 45.8333 (45.8333)  time: 0.7082  data: 0.5460  max mem: 1380\n",
            "Test: [Task 3]  [10/21]  eta: 0:00:02  Loss: 2.6480 (2.7238)  Acc@1: 50.0000 (53.7879)  Acc@5: 79.1667 (76.8939)  Acc@task: 58.3333 (58.7121)  time: 0.2278  data: 0.0499  max mem: 1380\n",
            "Test: [Task 3]  [20/21]  eta: 0:00:00  Loss: 2.6099 (2.6590)  Acc@1: 54.1667 (54.5272)  Acc@5: 79.1667 (76.6600)  Acc@task: 58.3333 (58.9537)  time: 0.1765  data: 0.0002  max mem: 1380\n",
            "Test: [Task 3] Total time: 0:00:04 (0.2062 s / it)\n",
            "* Acc@task 58.954 Acc@1 54.527 Acc@5 76.660 loss 2.659\n",
            "Test: [Task 4]  [ 0/20]  eta: 0:00:10  Loss: 2.6321 (2.6321)  Acc@1: 54.1667 (54.1667)  Acc@5: 79.1667 (79.1667)  Acc@task: 50.0000 (50.0000)  time: 0.5138  data: 0.3396  max mem: 1380\n",
            "Test: [Task 4]  [10/20]  eta: 0:00:02  Loss: 2.7199 (2.6854)  Acc@1: 50.0000 (52.2727)  Acc@5: 75.0000 (76.8939)  Acc@task: 54.1667 (54.1667)  time: 0.2091  data: 0.0319  max mem: 1380\n",
            "Test: [Task 4]  [19/20]  eta: 0:00:00  Loss: 2.8020 (2.7430)  Acc@1: 50.0000 (54.3524)  Acc@5: 75.0000 (77.2824)  Acc@task: 54.1667 (56.0510)  time: 0.1930  data: 0.0176  max mem: 1380\n",
            "Test: [Task 4] Total time: 0:00:03 (0.1978 s / it)\n",
            "* Acc@task 56.051 Acc@1 54.352 Acc@5 77.282 loss 2.743\n",
            "Test: [Task 5]  [ 0/21]  eta: 0:00:12  Loss: 2.0046 (2.0046)  Acc@1: 58.3333 (58.3333)  Acc@5: 83.3333 (83.3333)  Acc@task: 58.3333 (58.3333)  time: 0.5905  data: 0.4238  max mem: 1380\n",
            "Test: [Task 5]  [10/21]  eta: 0:00:02  Loss: 3.2656 (3.2256)  Acc@1: 50.0000 (48.8636)  Acc@5: 70.8333 (71.5909)  Acc@task: 50.0000 (50.7576)  time: 0.2172  data: 0.0392  max mem: 1380\n",
            "Test: [Task 5]  [20/21]  eta: 0:00:00  Loss: 3.2138 (3.1391)  Acc@1: 45.8333 (49.0835)  Acc@5: 70.8333 (72.3014)  Acc@task: 50.0000 (52.3422)  time: 0.1746  data: 0.0006  max mem: 1380\n",
            "Test: [Task 5] Total time: 0:00:04 (0.2026 s / it)\n",
            "* Acc@task 52.342 Acc@1 49.084 Acc@5 72.301 loss 3.139\n",
            "Test: [Task 6]  [ 0/20]  eta: 0:00:16  Loss: 2.6864 (2.6864)  Acc@1: 62.5000 (62.5000)  Acc@5: 75.0000 (75.0000)  Acc@task: 66.6667 (66.6667)  time: 0.8412  data: 0.6605  max mem: 1380\n",
            "Test: [Task 6]  [10/20]  eta: 0:00:02  Loss: 2.6864 (2.7094)  Acc@1: 54.1667 (53.4091)  Acc@5: 75.0000 (78.4091)  Acc@task: 54.1667 (56.0606)  time: 0.2408  data: 0.0614  max mem: 1380\n",
            "Test: [Task 6]  [19/20]  eta: 0:00:00  Loss: 2.2914 (2.4876)  Acc@1: 54.1667 (53.9583)  Acc@5: 79.1667 (81.2500)  Acc@task: 54.1667 (56.8750)  time: 0.2134  data: 0.0338  max mem: 1380\n",
            "Test: [Task 6] Total time: 0:00:04 (0.2193 s / it)\n",
            "* Acc@task 56.875 Acc@1 53.958 Acc@5 81.250 loss 2.488\n",
            "Test: [Task 7]  [ 0/23]  eta: 0:00:11  Loss: 2.3572 (2.3572)  Acc@1: 58.3333 (58.3333)  Acc@5: 75.0000 (75.0000)  Acc@task: 54.1667 (54.1667)  time: 0.5046  data: 0.3340  max mem: 1380\n",
            "Test: [Task 7]  [10/23]  eta: 0:00:02  Loss: 1.6110 (1.7864)  Acc@1: 66.6667 (64.3939)  Acc@5: 87.5000 (85.2273)  Acc@task: 62.5000 (58.3333)  time: 0.2100  data: 0.0308  max mem: 1380\n",
            "Test: [Task 7]  [20/23]  eta: 0:00:00  Loss: 1.7863 (1.9687)  Acc@1: 62.5000 (61.3095)  Acc@5: 87.5000 (83.9286)  Acc@task: 58.3333 (57.3413)  time: 0.1800  data: 0.0004  max mem: 1380\n",
            "Test: [Task 7]  [22/23]  eta: 0:00:00  Loss: 1.7764 (1.9330)  Acc@1: 62.5000 (62.0000)  Acc@5: 86.3636 (84.0000)  Acc@task: 58.3333 (57.2727)  time: 0.1807  data: 0.0003  max mem: 1380\n",
            "Test: [Task 7] Total time: 0:00:04 (0.1984 s / it)\n",
            "* Acc@task 57.273 Acc@1 62.000 Acc@5 84.000 loss 1.933\n",
            "Test: [Task 8]  [ 0/22]  eta: 0:00:10  Loss: 1.8491 (1.8491)  Acc@1: 41.6667 (41.6667)  Acc@5: 87.5000 (87.5000)  Acc@task: 45.8333 (45.8333)  time: 0.4935  data: 0.3137  max mem: 1380\n",
            "Test: [Task 8]  [10/22]  eta: 0:00:02  Loss: 1.9550 (2.1372)  Acc@1: 54.1667 (54.1667)  Acc@5: 87.5000 (84.8485)  Acc@task: 54.1667 (56.0606)  time: 0.2074  data: 0.0294  max mem: 1380\n",
            "Test: [Task 8]  [20/22]  eta: 0:00:00  Loss: 1.9550 (2.0343)  Acc@1: 54.1667 (55.3571)  Acc@5: 87.5000 (86.5079)  Acc@task: 58.3333 (58.5317)  time: 0.1795  data: 0.0007  max mem: 1380\n",
            "Test: [Task 8]  [21/22]  eta: 0:00:00  Loss: 1.9550 (1.9897)  Acc@1: 54.1667 (55.3816)  Acc@5: 87.5000 (86.6928)  Acc@task: 58.3333 (58.7084)  time: 0.1736  data: 0.0007  max mem: 1380\n",
            "Test: [Task 8] Total time: 0:00:04 (0.1956 s / it)\n",
            "* Acc@task 58.708 Acc@1 55.382 Acc@5 86.693 loss 1.990\n",
            "Test: [Task 9]  [ 0/21]  eta: 0:00:20  Loss: 5.9558 (5.9558)  Acc@1: 4.1667 (4.1667)  Acc@5: 66.6667 (66.6667)  Acc@task: 83.3333 (83.3333)  time: 0.9985  data: 0.7975  max mem: 1380\n",
            "Test: [Task 9]  [10/21]  eta: 0:00:02  Loss: 5.5864 (5.6803)  Acc@1: 4.1667 (2.2727)  Acc@5: 50.0000 (48.1061)  Acc@task: 50.0000 (54.1667)  time: 0.2553  data: 0.0751  max mem: 1380\n",
            "Test: [Task 9]  [20/21]  eta: 0:00:00  Loss: 5.5390 (5.6946)  Acc@1: 0.0000 (2.0040)  Acc@5: 50.0000 (48.8978)  Acc@task: 50.0000 (52.3046)  time: 0.1796  data: 0.0019  max mem: 1380\n",
            "Test: [Task 9] Total time: 0:00:04 (0.2231 s / it)\n",
            "* Acc@task 52.305 Acc@1 2.004 Acc@5 48.898 loss 5.695\n",
            "[Average accuracy till task9]\tAcc@task: 56.5154\tAcc@1: 51.8505\tAcc@5: 78.4966\tLoss: 2.5992\tForgetting: 0.6505\tBackward: 50.9384\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:18  Loss: 0.7084 (0.7084)  Acc@1: 87.5000 (87.5000)  Acc@5: 95.8333 (95.8333)  Acc@task: 75.0000 (75.0000)  time: 0.4484  data: 0.2808  max mem: 1380\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.9149 (1.0147)  Acc@1: 75.0000 (75.3788)  Acc@5: 95.8333 (95.8333)  Acc@task: 75.0000 (75.3788)  time: 0.2075  data: 0.0296  max mem: 1380\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.9185 (0.9752)  Acc@1: 70.8333 (76.5873)  Acc@5: 95.8333 (95.2381)  Acc@task: 75.0000 (76.9841)  time: 0.1820  data: 0.0025  max mem: 1380\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.7823 (0.8970)  Acc@1: 83.3333 (78.0914)  Acc@5: 95.8333 (95.9677)  Acc@task: 79.1667 (77.8226)  time: 0.1811  data: 0.0006  max mem: 1380\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6040 (0.8292)  Acc@1: 83.3333 (79.4715)  Acc@5: 100.0000 (96.5447)  Acc@task: 79.1667 (78.8618)  time: 0.1819  data: 0.0004  max mem: 1380\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.5790 (0.8185)  Acc@1: 83.3333 (79.6000)  Acc@5: 100.0000 (96.6000)  Acc@task: 79.1667 (79.2000)  time: 0.1783  data: 0.0004  max mem: 1380\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1889 s / it)\n",
            "* Acc@task 79.200 Acc@1 79.600 Acc@5 96.600 loss 0.818\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:21  Loss: 1.0459 (1.0459)  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  Acc@task: 62.5000 (62.5000)  time: 0.5054  data: 0.3389  max mem: 1380\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.9178 (1.0200)  Acc@1: 83.3333 (80.6818)  Acc@5: 95.8333 (95.0758)  Acc@task: 66.6667 (68.1818)  time: 0.2119  data: 0.0316  max mem: 1380\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.9058 (1.1447)  Acc@1: 79.1667 (76.5873)  Acc@5: 95.8333 (93.8492)  Acc@task: 66.6667 (69.6429)  time: 0.1817  data: 0.0014  max mem: 1380\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.9199 (1.0924)  Acc@1: 79.1667 (77.0161)  Acc@5: 95.8333 (94.3548)  Acc@task: 70.8333 (71.1022)  time: 0.1813  data: 0.0012  max mem: 1380\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.7865 (1.0129)  Acc@1: 79.1667 (77.3374)  Acc@5: 95.8333 (94.7154)  Acc@task: 75.0000 (71.3415)  time: 0.1822  data: 0.0003  max mem: 1380\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.7865 (1.0083)  Acc@1: 75.0000 (77.3000)  Acc@5: 95.8333 (94.8000)  Acc@task: 75.0000 (71.4000)  time: 0.1786  data: 0.0002  max mem: 1380\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1904 s / it)\n",
            "* Acc@task 71.400 Acc@1 77.300 Acc@5 94.800 loss 1.008\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:19  Loss: 0.4863 (0.4863)  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.4529  data: 0.2881  max mem: 1380\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 0.9478 (0.9403)  Acc@1: 79.1667 (79.5455)  Acc@5: 95.8333 (95.0758)  Acc@task: 79.1667 (79.1667)  time: 0.2072  data: 0.0284  max mem: 1380\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.9052 (0.9309)  Acc@1: 79.1667 (79.5635)  Acc@5: 95.8333 (94.4444)  Acc@task: 79.1667 (78.7698)  time: 0.1816  data: 0.0014  max mem: 1380\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.8508 (0.9030)  Acc@1: 79.1667 (79.3011)  Acc@5: 95.8333 (94.7581)  Acc@task: 75.0000 (78.2258)  time: 0.1810  data: 0.0004  max mem: 1380\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.7153 (0.8905)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (94.8171)  Acc@task: 79.1667 (78.1504)  time: 0.1816  data: 0.0003  max mem: 1380\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.7153 (0.8913)  Acc@1: 79.1667 (79.1000)  Acc@5: 95.8333 (94.9000)  Acc@task: 79.1667 (78.0000)  time: 0.1779  data: 0.0003  max mem: 1380\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1903 s / it)\n",
            "* Acc@task 78.000 Acc@1 79.100 Acc@5 94.900 loss 0.891\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:32  Loss: 0.9597 (0.9597)  Acc@1: 75.0000 (75.0000)  Acc@5: 87.5000 (87.5000)  Acc@task: 66.6667 (66.6667)  time: 0.7687  data: 0.5369  max mem: 1380\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:07  Loss: 0.8730 (0.9137)  Acc@1: 83.3333 (80.3030)  Acc@5: 95.8333 (94.6970)  Acc@task: 70.8333 (72.7273)  time: 0.2349  data: 0.0495  max mem: 1380\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.8523 (0.9747)  Acc@1: 79.1667 (78.7698)  Acc@5: 95.8333 (95.0397)  Acc@task: 75.0000 (73.2143)  time: 0.1803  data: 0.0006  max mem: 1380\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.7376 (0.8894)  Acc@1: 79.1667 (79.5699)  Acc@5: 95.8333 (95.8333)  Acc@task: 79.1667 (74.5968)  time: 0.1804  data: 0.0004  max mem: 1380\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7740 (0.9667)  Acc@1: 79.1667 (78.3537)  Acc@5: 95.8333 (95.0203)  Acc@task: 70.8333 (74.0854)  time: 0.1811  data: 0.0003  max mem: 1380\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.7740 (0.9475)  Acc@1: 79.1667 (78.6000)  Acc@5: 95.8333 (95.1000)  Acc@task: 70.8333 (74.4000)  time: 0.1775  data: 0.0003  max mem: 1380\n",
            "Test: [Task 4] Total time: 0:00:08 (0.1954 s / it)\n",
            "* Acc@task 74.400 Acc@1 78.600 Acc@5 95.100 loss 0.948\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:20  Loss: 0.6103 (0.6103)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  Acc@task: 70.8333 (70.8333)  time: 0.4775  data: 0.3022  max mem: 1380\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 0.6850 (0.8069)  Acc@1: 79.1667 (79.5455)  Acc@5: 100.0000 (97.3485)  Acc@task: 70.8333 (75.0000)  time: 0.2064  data: 0.0277  max mem: 1380\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.9009 (0.8415)  Acc@1: 79.1667 (80.1587)  Acc@5: 95.8333 (96.2302)  Acc@task: 70.8333 (74.2064)  time: 0.1795  data: 0.0003  max mem: 1380\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.8594 (0.8299)  Acc@1: 79.1667 (80.3763)  Acc@5: 95.8333 (95.8333)  Acc@task: 75.0000 (74.0591)  time: 0.1800  data: 0.0009  max mem: 1380\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.8594 (0.8868)  Acc@1: 79.1667 (79.3699)  Acc@5: 95.8333 (95.4268)  Acc@task: 70.8333 (72.5610)  time: 0.1806  data: 0.0009  max mem: 1380\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.8717 (0.9421)  Acc@1: 75.0000 (78.9000)  Acc@5: 95.8333 (95.2000)  Acc@task: 70.8333 (72.4000)  time: 0.1773  data: 0.0008  max mem: 1380\n",
            "Test: [Task 5] Total time: 0:00:07 (0.1880 s / it)\n",
            "* Acc@task 72.400 Acc@1 78.900 Acc@5 95.200 loss 0.942\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:17  Loss: 0.6749 (0.6749)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.4226  data: 0.2432  max mem: 1380\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:06  Loss: 0.7922 (0.8777)  Acc@1: 83.3333 (79.5455)  Acc@5: 95.8333 (96.9697)  Acc@task: 75.0000 (74.2424)  time: 0.2032  data: 0.0254  max mem: 1380\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:04  Loss: 0.9201 (0.9425)  Acc@1: 79.1667 (77.7778)  Acc@5: 95.8333 (96.2302)  Acc@task: 75.0000 (75.0000)  time: 0.1803  data: 0.0020  max mem: 1380\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 1.0572 (0.9614)  Acc@1: 75.0000 (76.4785)  Acc@5: 95.8333 (96.1022)  Acc@task: 75.0000 (74.3280)  time: 0.1798  data: 0.0004  max mem: 1380\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.0487 (0.9900)  Acc@1: 70.8333 (76.5244)  Acc@5: 95.8333 (95.8333)  Acc@task: 70.8333 (73.7805)  time: 0.1801  data: 0.0003  max mem: 1380\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.0572 (0.9950)  Acc@1: 70.8333 (76.4000)  Acc@5: 95.8333 (95.9000)  Acc@task: 70.8333 (73.8000)  time: 0.1766  data: 0.0003  max mem: 1380\n",
            "Test: [Task 6] Total time: 0:00:07 (0.1868 s / it)\n",
            "* Acc@task 73.800 Acc@1 76.400 Acc@5 95.900 loss 0.995\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:17  Loss: 1.5453 (1.5453)  Acc@1: 70.8333 (70.8333)  Acc@5: 87.5000 (87.5000)  Acc@task: 54.1667 (54.1667)  time: 0.4122  data: 0.2463  max mem: 1380\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:06  Loss: 0.8093 (0.9619)  Acc@1: 75.0000 (76.8939)  Acc@5: 100.0000 (95.8333)  Acc@task: 62.5000 (62.5000)  time: 0.2022  data: 0.0265  max mem: 1380\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:04  Loss: 0.8093 (0.9897)  Acc@1: 75.0000 (75.1984)  Acc@5: 95.8333 (95.4365)  Acc@task: 66.6667 (62.6984)  time: 0.1799  data: 0.0031  max mem: 1380\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:02  Loss: 0.9567 (1.0530)  Acc@1: 75.0000 (74.8656)  Acc@5: 91.6667 (94.8925)  Acc@task: 66.6667 (63.8441)  time: 0.1790  data: 0.0012  max mem: 1380\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 1.1181 (1.0813)  Acc@1: 75.0000 (74.3902)  Acc@5: 91.6667 (94.7154)  Acc@task: 66.6667 (64.2276)  time: 0.1795  data: 0.0005  max mem: 1380\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 1.1181 (1.0697)  Acc@1: 75.0000 (74.6000)  Acc@5: 91.6667 (94.7000)  Acc@task: 66.6667 (64.5000)  time: 0.1762  data: 0.0005  max mem: 1380\n",
            "Test: [Task 7] Total time: 0:00:07 (0.1860 s / it)\n",
            "* Acc@task 64.500 Acc@1 74.600 Acc@5 94.700 loss 1.070\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:23  Loss: 1.2230 (1.2230)  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.8333)  Acc@task: 62.5000 (62.5000)  time: 0.5649  data: 0.3956  max mem: 1380\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:06  Loss: 0.9376 (1.0435)  Acc@1: 79.1667 (77.2727)  Acc@5: 95.8333 (94.6970)  Acc@task: 66.6667 (66.6667)  time: 0.2141  data: 0.0363  max mem: 1380\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:04  Loss: 0.7839 (0.9698)  Acc@1: 75.0000 (76.1905)  Acc@5: 95.8333 (96.2302)  Acc@task: 66.6667 (67.8571)  time: 0.1789  data: 0.0003  max mem: 1380\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:02  Loss: 0.8138 (0.9586)  Acc@1: 75.0000 (75.9409)  Acc@5: 95.8333 (96.2366)  Acc@task: 70.8333 (68.9516)  time: 0.1788  data: 0.0003  max mem: 1380\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 1.0116 (0.9751)  Acc@1: 75.0000 (76.2195)  Acc@5: 95.8333 (95.7317)  Acc@task: 66.6667 (68.1911)  time: 0.1803  data: 0.0003  max mem: 1380\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 1.0116 (0.9737)  Acc@1: 75.0000 (76.3000)  Acc@5: 95.8333 (95.7000)  Acc@task: 66.6667 (68.3000)  time: 0.1766  data: 0.0003  max mem: 1380\n",
            "Test: [Task 8] Total time: 0:00:07 (0.1902 s / it)\n",
            "* Acc@task 68.300 Acc@1 76.300 Acc@5 95.700 loss 0.974\n",
            "Test: [Task 9]  [ 0/42]  eta: 0:00:34  Loss: 5.5830 (5.5830)  Acc@1: 12.5000 (12.5000)  Acc@5: 79.1667 (79.1667)  Acc@task: 58.3333 (58.3333)  time: 0.8322  data: 0.6554  max mem: 1380\n",
            "Test: [Task 9]  [10/42]  eta: 0:00:07  Loss: 5.5830 (5.6439)  Acc@1: 4.1667 (6.4394)  Acc@5: 62.5000 (65.1515)  Acc@task: 58.3333 (56.0606)  time: 0.2379  data: 0.0609  max mem: 1380\n",
            "Test: [Task 9]  [20/42]  eta: 0:00:04  Loss: 5.5382 (5.5420)  Acc@1: 4.1667 (6.5476)  Acc@5: 62.5000 (64.6825)  Acc@task: 58.3333 (57.9365)  time: 0.1786  data: 0.0008  max mem: 1380\n",
            "Test: [Task 9]  [30/42]  eta: 0:00:02  Loss: 5.4979 (5.6095)  Acc@1: 4.1667 (5.7796)  Acc@5: 58.3333 (61.9624)  Acc@task: 58.3333 (56.9892)  time: 0.1796  data: 0.0003  max mem: 1380\n",
            "Test: [Task 9]  [40/42]  eta: 0:00:00  Loss: 5.5314 (5.5785)  Acc@1: 4.1667 (5.5894)  Acc@5: 62.5000 (62.5000)  Acc@task: 58.3333 (58.2317)  time: 0.1800  data: 0.0002  max mem: 1380\n",
            "Test: [Task 9]  [41/42]  eta: 0:00:00  Loss: 5.5314 (5.5630)  Acc@1: 4.1667 (5.6000)  Acc@5: 62.5000 (62.6000)  Acc@task: 58.3333 (58.4000)  time: 0.1765  data: 0.0002  max mem: 1380\n",
            "Test: [Task 9] Total time: 0:00:08 (0.1960 s / it)\n",
            "* Acc@task 58.400 Acc@1 5.600 Acc@5 62.600 loss 5.563\n",
            "[Average accuracy till task9]\tAcc@task: 71.1556\tAcc@1: 69.6000\tAcc@5: 91.7222\tLoss: 1.4677\tForgetting: 0.0625\tBackward: 68.1625\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:23  Loss: 0.7084 (0.7084)  Acc@1: 87.5000 (87.5000)  Acc@5: 95.8333 (95.8333)  Acc@task: 75.0000 (75.0000)  time: 0.5496  data: 0.3846  max mem: 1380\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.9149 (1.0147)  Acc@1: 75.0000 (75.3788)  Acc@5: 95.8333 (95.8333)  Acc@task: 75.0000 (75.3788)  time: 0.2128  data: 0.0352  max mem: 1380\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.9185 (0.9752)  Acc@1: 70.8333 (76.5873)  Acc@5: 95.8333 (95.2381)  Acc@task: 75.0000 (76.9841)  time: 0.1784  data: 0.0003  max mem: 1380\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.7823 (0.8970)  Acc@1: 83.3333 (78.0914)  Acc@5: 95.8333 (95.9677)  Acc@task: 79.1667 (77.8226)  time: 0.1781  data: 0.0006  max mem: 1380\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6040 (0.8292)  Acc@1: 83.3333 (79.4715)  Acc@5: 100.0000 (96.5447)  Acc@task: 79.1667 (78.8618)  time: 0.1792  data: 0.0006  max mem: 1380\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.5790 (0.8185)  Acc@1: 83.3333 (79.6000)  Acc@5: 100.0000 (96.6000)  Acc@task: 79.1667 (79.2000)  time: 0.1757  data: 0.0006  max mem: 1380\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1884 s / it)\n",
            "* Acc@task 79.200 Acc@1 79.600 Acc@5 96.600 loss 0.818\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:20  Loss: 1.0459 (1.0459)  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  Acc@task: 62.5000 (62.5000)  time: 0.4931  data: 0.3202  max mem: 1380\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.9178 (1.0200)  Acc@1: 83.3333 (80.6818)  Acc@5: 95.8333 (95.0758)  Acc@task: 66.6667 (68.1818)  time: 0.2073  data: 0.0295  max mem: 1380\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.9058 (1.1447)  Acc@1: 79.1667 (76.5873)  Acc@5: 95.8333 (93.8492)  Acc@task: 66.6667 (69.6429)  time: 0.1784  data: 0.0004  max mem: 1380\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.9199 (1.0924)  Acc@1: 79.1667 (77.0161)  Acc@5: 95.8333 (94.3548)  Acc@task: 70.8333 (71.1022)  time: 0.1792  data: 0.0003  max mem: 1380\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.7865 (1.0129)  Acc@1: 79.1667 (77.3374)  Acc@5: 95.8333 (94.7154)  Acc@task: 75.0000 (71.3415)  time: 0.1801  data: 0.0003  max mem: 1380\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.7865 (1.0083)  Acc@1: 75.0000 (77.3000)  Acc@5: 95.8333 (94.8000)  Acc@task: 75.0000 (71.4000)  time: 0.1764  data: 0.0003  max mem: 1380\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1880 s / it)\n",
            "* Acc@task 71.400 Acc@1 77.300 Acc@5 94.800 loss 1.008\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:24  Loss: 0.4863 (0.4863)  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.5862  data: 0.4165  max mem: 1380\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 0.9478 (0.9403)  Acc@1: 79.1667 (79.5455)  Acc@5: 95.8333 (95.0758)  Acc@task: 79.1667 (79.1667)  time: 0.2168  data: 0.0383  max mem: 1380\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.9052 (0.9309)  Acc@1: 79.1667 (79.5635)  Acc@5: 95.8333 (94.4444)  Acc@task: 79.1667 (78.7698)  time: 0.1790  data: 0.0010  max mem: 1380\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.8508 (0.9030)  Acc@1: 79.1667 (79.3011)  Acc@5: 95.8333 (94.7581)  Acc@task: 75.0000 (78.2258)  time: 0.1785  data: 0.0012  max mem: 1380\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.7153 (0.8905)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (94.8171)  Acc@task: 79.1667 (78.1504)  time: 0.1796  data: 0.0005  max mem: 1380\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.7153 (0.8913)  Acc@1: 79.1667 (79.1000)  Acc@5: 95.8333 (94.9000)  Acc@task: 79.1667 (78.0000)  time: 0.1760  data: 0.0004  max mem: 1380\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1897 s / it)\n",
            "* Acc@task 78.000 Acc@1 79.100 Acc@5 94.900 loss 0.891\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:22  Loss: 0.9597 (0.9597)  Acc@1: 75.0000 (75.0000)  Acc@5: 87.5000 (87.5000)  Acc@task: 66.6667 (66.6667)  time: 0.5284  data: 0.3575  max mem: 1380\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:06  Loss: 0.8730 (0.9137)  Acc@1: 83.3333 (80.3030)  Acc@5: 95.8333 (94.6970)  Acc@task: 70.8333 (72.7273)  time: 0.2105  data: 0.0328  max mem: 1380\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.8523 (0.9747)  Acc@1: 79.1667 (78.7698)  Acc@5: 95.8333 (95.0397)  Acc@task: 75.0000 (73.2143)  time: 0.1784  data: 0.0003  max mem: 1380\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.7376 (0.8894)  Acc@1: 79.1667 (79.5699)  Acc@5: 95.8333 (95.8333)  Acc@task: 79.1667 (74.5968)  time: 0.1788  data: 0.0004  max mem: 1380\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7740 (0.9667)  Acc@1: 79.1667 (78.3537)  Acc@5: 95.8333 (95.0203)  Acc@task: 70.8333 (74.0854)  time: 0.1797  data: 0.0003  max mem: 1380\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.7740 (0.9475)  Acc@1: 79.1667 (78.6000)  Acc@5: 95.8333 (95.1000)  Acc@task: 70.8333 (74.4000)  time: 0.1764  data: 0.0003  max mem: 1380\n",
            "Test: [Task 4] Total time: 0:00:08 (0.1905 s / it)\n",
            "* Acc@task 74.400 Acc@1 78.600 Acc@5 95.100 loss 0.948\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:30  Loss: 0.6103 (0.6103)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  Acc@task: 70.8333 (70.8333)  time: 0.7328  data: 0.5323  max mem: 1380\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:07  Loss: 0.6850 (0.8069)  Acc@1: 79.1667 (79.5455)  Acc@5: 100.0000 (97.3485)  Acc@task: 70.8333 (75.0000)  time: 0.2295  data: 0.0498  max mem: 1380\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.9009 (0.8415)  Acc@1: 79.1667 (80.1587)  Acc@5: 95.8333 (96.2302)  Acc@task: 70.8333 (74.2064)  time: 0.1789  data: 0.0010  max mem: 1380\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.8594 (0.8299)  Acc@1: 79.1667 (80.3763)  Acc@5: 95.8333 (95.8333)  Acc@task: 75.0000 (74.0591)  time: 0.1791  data: 0.0005  max mem: 1380\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.8594 (0.8868)  Acc@1: 79.1667 (79.3699)  Acc@5: 95.8333 (95.4268)  Acc@task: 70.8333 (72.5610)  time: 0.1800  data: 0.0003  max mem: 1380\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.8717 (0.9421)  Acc@1: 75.0000 (78.9000)  Acc@5: 95.8333 (95.2000)  Acc@task: 70.8333 (72.4000)  time: 0.1765  data: 0.0003  max mem: 1380\n",
            "Test: [Task 5] Total time: 0:00:08 (0.1934 s / it)\n",
            "* Acc@task 72.400 Acc@1 78.900 Acc@5 95.200 loss 0.942\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:19  Loss: 0.6749 (0.6749)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.4525  data: 0.2856  max mem: 1380\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:06  Loss: 0.7922 (0.8777)  Acc@1: 83.3333 (79.5455)  Acc@5: 95.8333 (96.9697)  Acc@task: 75.0000 (74.2424)  time: 0.2032  data: 0.0268  max mem: 1380\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:04  Loss: 0.9201 (0.9425)  Acc@1: 79.1667 (77.7778)  Acc@5: 95.8333 (96.2302)  Acc@task: 75.0000 (75.0000)  time: 0.1783  data: 0.0006  max mem: 1380\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 1.0572 (0.9614)  Acc@1: 75.0000 (76.4785)  Acc@5: 95.8333 (96.1022)  Acc@task: 75.0000 (74.3280)  time: 0.1790  data: 0.0003  max mem: 1380\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.0487 (0.9900)  Acc@1: 70.8333 (76.5244)  Acc@5: 95.8333 (95.8333)  Acc@task: 70.8333 (73.7805)  time: 0.1800  data: 0.0002  max mem: 1380\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.0572 (0.9950)  Acc@1: 70.8333 (76.4000)  Acc@5: 95.8333 (95.9000)  Acc@task: 70.8333 (73.8000)  time: 0.1766  data: 0.0002  max mem: 1380\n",
            "Test: [Task 6] Total time: 0:00:07 (0.1874 s / it)\n",
            "* Acc@task 73.800 Acc@1 76.400 Acc@5 95.900 loss 0.995\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:22  Loss: 1.5453 (1.5453)  Acc@1: 70.8333 (70.8333)  Acc@5: 87.5000 (87.5000)  Acc@task: 54.1667 (54.1667)  time: 0.5472  data: 0.3816  max mem: 1380\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:06  Loss: 0.8093 (0.9619)  Acc@1: 75.0000 (76.8939)  Acc@5: 100.0000 (95.8333)  Acc@task: 62.5000 (62.5000)  time: 0.2117  data: 0.0352  max mem: 1380\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:04  Loss: 0.8093 (0.9897)  Acc@1: 75.0000 (75.1984)  Acc@5: 95.8333 (95.4365)  Acc@task: 66.6667 (62.6984)  time: 0.1785  data: 0.0004  max mem: 1380\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:02  Loss: 0.9567 (1.0530)  Acc@1: 75.0000 (74.8656)  Acc@5: 91.6667 (94.8925)  Acc@task: 66.6667 (63.8441)  time: 0.1795  data: 0.0003  max mem: 1380\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 1.1181 (1.0813)  Acc@1: 75.0000 (74.3902)  Acc@5: 91.6667 (94.7154)  Acc@task: 66.6667 (64.2276)  time: 0.1803  data: 0.0003  max mem: 1380\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 1.1181 (1.0697)  Acc@1: 75.0000 (74.6000)  Acc@5: 91.6667 (94.7000)  Acc@task: 66.6667 (64.5000)  time: 0.1767  data: 0.0003  max mem: 1380\n",
            "Test: [Task 7] Total time: 0:00:07 (0.1888 s / it)\n",
            "* Acc@task 64.500 Acc@1 74.600 Acc@5 94.700 loss 1.070\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:25  Loss: 1.2230 (1.2230)  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.8333)  Acc@task: 62.5000 (62.5000)  time: 0.6050  data: 0.4404  max mem: 1380\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:07  Loss: 0.9376 (1.0435)  Acc@1: 79.1667 (77.2727)  Acc@5: 95.8333 (94.6970)  Acc@task: 66.6667 (66.6667)  time: 0.2194  data: 0.0406  max mem: 1380\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:04  Loss: 0.7839 (0.9698)  Acc@1: 75.0000 (76.1905)  Acc@5: 95.8333 (96.2302)  Acc@task: 66.6667 (67.8571)  time: 0.1794  data: 0.0006  max mem: 1380\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:02  Loss: 0.8138 (0.9586)  Acc@1: 75.0000 (75.9409)  Acc@5: 95.8333 (96.2366)  Acc@task: 70.8333 (68.9516)  time: 0.1792  data: 0.0011  max mem: 1380\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 1.0116 (0.9751)  Acc@1: 75.0000 (76.2195)  Acc@5: 95.8333 (95.7317)  Acc@task: 66.6667 (68.1911)  time: 0.1808  data: 0.0009  max mem: 1380\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 1.0116 (0.9737)  Acc@1: 75.0000 (76.3000)  Acc@5: 95.8333 (95.7000)  Acc@task: 66.6667 (68.3000)  time: 0.1772  data: 0.0009  max mem: 1380\n",
            "Test: [Task 8] Total time: 0:00:08 (0.1910 s / it)\n",
            "* Acc@task 68.300 Acc@1 76.300 Acc@5 95.700 loss 0.974\n",
            "Test: [Task 9]  [ 0/42]  eta: 0:00:23  Loss: 5.5830 (5.5830)  Acc@1: 12.5000 (12.5000)  Acc@5: 79.1667 (79.1667)  Acc@task: 58.3333 (58.3333)  time: 0.5494  data: 0.3766  max mem: 1380\n",
            "Test: [Task 9]  [10/42]  eta: 0:00:06  Loss: 5.5830 (5.6439)  Acc@1: 4.1667 (6.4394)  Acc@5: 62.5000 (65.1515)  Acc@task: 58.3333 (56.0606)  time: 0.2136  data: 0.0347  max mem: 1380\n",
            "Test: [Task 9]  [20/42]  eta: 0:00:04  Loss: 5.5382 (5.5420)  Acc@1: 4.1667 (6.5476)  Acc@5: 62.5000 (64.6825)  Acc@task: 58.3333 (57.9365)  time: 0.1793  data: 0.0005  max mem: 1380\n",
            "Test: [Task 9]  [30/42]  eta: 0:00:02  Loss: 5.4979 (5.6095)  Acc@1: 4.1667 (5.7796)  Acc@5: 58.3333 (61.9624)  Acc@task: 58.3333 (56.9892)  time: 0.1798  data: 0.0004  max mem: 1380\n",
            "Test: [Task 9]  [40/42]  eta: 0:00:00  Loss: 5.5314 (5.5785)  Acc@1: 4.1667 (5.5894)  Acc@5: 62.5000 (62.5000)  Acc@task: 58.3333 (58.2317)  time: 0.1808  data: 0.0002  max mem: 1380\n",
            "Test: [Task 9]  [41/42]  eta: 0:00:00  Loss: 5.5314 (5.5630)  Acc@1: 4.1667 (5.6000)  Acc@5: 62.5000 (62.6000)  Acc@task: 58.3333 (58.4000)  time: 0.1771  data: 0.0002  max mem: 1380\n",
            "Test: [Task 9] Total time: 0:00:08 (0.1912 s / it)\n",
            "* Acc@task 58.400 Acc@1 5.600 Acc@5 62.600 loss 5.563\n",
            "[Average accuracy till task9]\tAcc@task: 71.1556\tAcc@1: 69.6000\tAcc@5: 91.7222\tLoss: 1.4677\tForgetting: 4.6875\tBackward: 57.6125\n",
            "torch.Size([93480, 384])\n",
            "Averaged stats: Lr: 0.005000  Loss: 0.0185  Acc@1: 100.0000 (98.0938)  Acc@5: 100.0000 (99.8229)\n",
            "torch.Size([93480, 384])\n",
            "Averaged stats: Lr: 0.004878  Loss: 0.0136  Acc@1: 100.0000 (99.8021)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([93480, 384])\n",
            "Averaged stats: Lr: 0.004523  Loss: 0.0143  Acc@1: 100.0000 (99.8333)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([93480, 384])\n",
            "Averaged stats: Lr: 0.003969  Loss: 0.0109  Acc@1: 100.0000 (99.8438)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([93480, 384])\n",
            "Averaged stats: Lr: 0.003273  Loss: 0.0053  Acc@1: 100.0000 (99.8438)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([93480, 384])\n",
            "Averaged stats: Lr: 0.002500  Loss: 0.0067  Acc@1: 100.0000 (99.8646)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([93480, 384])\n",
            "Averaged stats: Lr: 0.001727  Loss: 0.0114  Acc@1: 100.0000 (99.8542)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([93480, 384])\n",
            "Averaged stats: Lr: 0.001031  Loss: 0.0083  Acc@1: 100.0000 (99.9167)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([93480, 384])\n",
            "Averaged stats: Lr: 0.000477  Loss: 0.0049  Acc@1: 100.0000 (99.8958)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([93480, 384])\n",
            "Averaged stats: Lr: 0.000122  Loss: 0.0093  Acc@1: 100.0000 (99.8750)  Acc@5: 100.0000 (100.0000)\n",
            "Test: [Task 1]  [ 0/23]  eta: 0:00:14  Loss: 2.0861 (2.0861)  Acc@1: 58.3333 (58.3333)  Acc@5: 79.1667 (79.1667)  Acc@task: 58.3333 (58.3333)  time: 0.6487  data: 0.4844  max mem: 1380\n",
            "Test: [Task 1]  [10/23]  eta: 0:00:02  Loss: 1.0137 (1.0914)  Acc@1: 75.0000 (72.3485)  Acc@5: 91.6667 (92.8030)  Acc@task: 62.5000 (66.6667)  time: 0.2207  data: 0.0445  max mem: 1380\n",
            "Test: [Task 1]  [20/23]  eta: 0:00:00  Loss: 1.3803 (1.3324)  Acc@1: 66.6667 (68.4524)  Acc@5: 91.6667 (90.2778)  Acc@task: 62.5000 (63.8889)  time: 0.1776  data: 0.0004  max mem: 1380\n",
            "Test: [Task 1]  [22/23]  eta: 0:00:00  Loss: 1.2006 (1.3407)  Acc@1: 66.6667 (68.0970)  Acc@5: 91.6667 (90.2985)  Acc@task: 62.5000 (63.8060)  time: 0.1732  data: 0.0003  max mem: 1380\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1977 s / it)\n",
            "* Acc@task 63.806 Acc@1 68.097 Acc@5 90.299 loss 1.341\n",
            "Test: [Task 2]  [ 0/20]  eta: 0:00:10  Loss: 1.8312 (1.8312)  Acc@1: 54.1667 (54.1667)  Acc@5: 87.5000 (87.5000)  Acc@task: 62.5000 (62.5000)  time: 0.5373  data: 0.3665  max mem: 1380\n",
            "Test: [Task 2]  [10/20]  eta: 0:00:02  Loss: 1.8312 (1.8932)  Acc@1: 54.1667 (56.8182)  Acc@5: 87.5000 (85.9849)  Acc@task: 54.1667 (53.0303)  time: 0.2112  data: 0.0343  max mem: 1380\n",
            "Test: [Task 2]  [19/20]  eta: 0:00:00  Loss: 1.8312 (1.9920)  Acc@1: 54.1667 (56.9937)  Acc@5: 83.3333 (84.5512)  Acc@task: 54.1667 (53.6534)  time: 0.1964  data: 0.0190  max mem: 1380\n",
            "Test: [Task 2] Total time: 0:00:04 (0.2053 s / it)\n",
            "* Acc@task 53.653 Acc@1 56.994 Acc@5 84.551 loss 1.992\n",
            "Test: [Task 3]  [ 0/21]  eta: 0:00:20  Loss: 1.9223 (1.9223)  Acc@1: 62.5000 (62.5000)  Acc@5: 87.5000 (87.5000)  Acc@task: 50.0000 (50.0000)  time: 0.9861  data: 0.7995  max mem: 1380\n",
            "Test: [Task 3]  [10/21]  eta: 0:00:02  Loss: 2.0587 (2.4575)  Acc@1: 58.3333 (53.4091)  Acc@5: 79.1667 (79.1667)  Acc@task: 50.0000 (54.1667)  time: 0.2521  data: 0.0734  max mem: 1380\n",
            "Test: [Task 3]  [20/21]  eta: 0:00:00  Loss: 2.6327 (2.5857)  Acc@1: 58.3333 (54.3260)  Acc@5: 79.1667 (77.8672)  Acc@task: 54.1667 (57.5453)  time: 0.1773  data: 0.0005  max mem: 1380\n",
            "Test: [Task 3] Total time: 0:00:04 (0.2203 s / it)\n",
            "* Acc@task 57.545 Acc@1 54.326 Acc@5 77.867 loss 2.586\n",
            "Test: [Task 4]  [ 0/20]  eta: 0:00:12  Loss: 3.2229 (3.2229)  Acc@1: 54.1667 (54.1667)  Acc@5: 70.8333 (70.8333)  Acc@task: 54.1667 (54.1667)  time: 0.6184  data: 0.4469  max mem: 1380\n",
            "Test: [Task 4]  [10/20]  eta: 0:00:02  Loss: 3.1932 (3.0087)  Acc@1: 54.1667 (51.8939)  Acc@5: 70.8333 (70.4545)  Acc@task: 62.5000 (60.6061)  time: 0.2213  data: 0.0409  max mem: 1380\n",
            "Test: [Task 4]  [19/20]  eta: 0:00:00  Loss: 3.0871 (3.0345)  Acc@1: 50.0000 (49.4692)  Acc@5: 70.8333 (71.9745)  Acc@task: 54.1667 (56.0510)  time: 0.1996  data: 0.0226  max mem: 1380\n",
            "Test: [Task 4] Total time: 0:00:04 (0.2052 s / it)\n",
            "* Acc@task 56.051 Acc@1 49.469 Acc@5 71.975 loss 3.034\n",
            "Test: [Task 5]  [ 0/21]  eta: 0:00:12  Loss: 2.4394 (2.4394)  Acc@1: 62.5000 (62.5000)  Acc@5: 75.0000 (75.0000)  Acc@task: 75.0000 (75.0000)  time: 0.5959  data: 0.4290  max mem: 1380\n",
            "Test: [Task 5]  [10/21]  eta: 0:00:02  Loss: 3.4142 (3.2695)  Acc@1: 54.1667 (50.7576)  Acc@5: 75.0000 (73.1061)  Acc@task: 54.1667 (54.9242)  time: 0.2197  data: 0.0398  max mem: 1380\n",
            "Test: [Task 5]  [20/21]  eta: 0:00:00  Loss: 2.9720 (3.1470)  Acc@1: 50.0000 (49.4908)  Acc@5: 75.0000 (74.5418)  Acc@task: 50.0000 (52.9532)  time: 0.1766  data: 0.0006  max mem: 1380\n",
            "Test: [Task 5] Total time: 0:00:04 (0.2030 s / it)\n",
            "* Acc@task 52.953 Acc@1 49.491 Acc@5 74.542 loss 3.147\n",
            "Test: [Task 6]  [ 0/20]  eta: 0:00:19  Loss: 2.5839 (2.5839)  Acc@1: 45.8333 (45.8333)  Acc@5: 79.1667 (79.1667)  Acc@task: 45.8333 (45.8333)  time: 0.9601  data: 0.7814  max mem: 1380\n",
            "Test: [Task 6]  [10/20]  eta: 0:00:02  Loss: 3.0863 (3.1429)  Acc@1: 50.0000 (52.2727)  Acc@5: 70.8333 (74.2424)  Acc@task: 58.3333 (57.5758)  time: 0.2522  data: 0.0725  max mem: 1380\n",
            "Test: [Task 6]  [19/20]  eta: 0:00:00  Loss: 2.6575 (2.8052)  Acc@1: 50.0000 (52.9167)  Acc@5: 75.0000 (75.8333)  Acc@task: 58.3333 (58.1250)  time: 0.2202  data: 0.0399  max mem: 1380\n",
            "Test: [Task 6] Total time: 0:00:04 (0.2252 s / it)\n",
            "* Acc@task 58.125 Acc@1 52.917 Acc@5 75.833 loss 2.805\n",
            "Test: [Task 7]  [ 0/23]  eta: 0:00:15  Loss: 2.7818 (2.7818)  Acc@1: 58.3333 (58.3333)  Acc@5: 75.0000 (75.0000)  Acc@task: 54.1667 (54.1667)  time: 0.6623  data: 0.4976  max mem: 1380\n",
            "Test: [Task 7]  [10/23]  eta: 0:00:02  Loss: 2.0923 (2.2691)  Acc@1: 58.3333 (56.4394)  Acc@5: 83.3333 (81.4394)  Acc@task: 54.1667 (53.7879)  time: 0.2252  data: 0.0455  max mem: 1380\n",
            "Test: [Task 7]  [20/23]  eta: 0:00:00  Loss: 1.8650 (2.0870)  Acc@1: 58.3333 (59.9206)  Acc@5: 83.3333 (82.5397)  Acc@task: 58.3333 (54.9603)  time: 0.1817  data: 0.0003  max mem: 1380\n",
            "Test: [Task 7]  [22/23]  eta: 0:00:00  Loss: 1.9822 (2.0384)  Acc@1: 62.5000 (60.9091)  Acc@5: 83.3333 (82.9091)  Acc@task: 58.3333 (55.8182)  time: 0.1827  data: 0.0002  max mem: 1380\n",
            "Test: [Task 7] Total time: 0:00:04 (0.2068 s / it)\n",
            "* Acc@task 55.818 Acc@1 60.909 Acc@5 82.909 loss 2.038\n",
            "Test: [Task 8]  [ 0/22]  eta: 0:00:13  Loss: 1.2976 (1.2976)  Acc@1: 70.8333 (70.8333)  Acc@5: 87.5000 (87.5000)  Acc@task: 75.0000 (75.0000)  time: 0.6123  data: 0.4277  max mem: 1380\n",
            "Test: [Task 8]  [10/22]  eta: 0:00:02  Loss: 1.7631 (1.8166)  Acc@1: 62.5000 (62.1212)  Acc@5: 87.5000 (85.2273)  Acc@task: 62.5000 (61.3636)  time: 0.2210  data: 0.0392  max mem: 1380\n",
            "Test: [Task 8]  [20/22]  eta: 0:00:00  Loss: 2.1049 (1.9730)  Acc@1: 58.3333 (59.5238)  Acc@5: 83.3333 (83.7302)  Acc@task: 66.6667 (62.5000)  time: 0.1817  data: 0.0003  max mem: 1380\n",
            "Test: [Task 8]  [21/22]  eta: 0:00:00  Loss: 2.1049 (1.9163)  Acc@1: 58.3333 (59.6869)  Acc@5: 83.3333 (83.9530)  Acc@task: 66.6667 (62.2309)  time: 0.1760  data: 0.0003  max mem: 1380\n",
            "Test: [Task 8] Total time: 0:00:04 (0.2048 s / it)\n",
            "* Acc@task 62.231 Acc@1 59.687 Acc@5 83.953 loss 1.916\n",
            "Test: [Task 9]  [ 0/21]  eta: 0:00:19  Loss: 2.7808 (2.7808)  Acc@1: 37.5000 (37.5000)  Acc@5: 70.8333 (70.8333)  Acc@task: 45.8333 (45.8333)  time: 0.9418  data: 0.7399  max mem: 1380\n",
            "Test: [Task 9]  [10/21]  eta: 0:00:02  Loss: 1.8749 (1.8707)  Acc@1: 54.1667 (56.4394)  Acc@5: 83.3333 (83.7121)  Acc@task: 45.8333 (49.6212)  time: 0.2499  data: 0.0678  max mem: 1380\n",
            "Test: [Task 9]  [20/21]  eta: 0:00:00  Loss: 1.9917 (2.0518)  Acc@1: 54.1667 (55.3106)  Acc@5: 83.3333 (82.9659)  Acc@task: 50.0000 (50.1002)  time: 0.1791  data: 0.0004  max mem: 1380\n",
            "Test: [Task 9] Total time: 0:00:04 (0.2201 s / it)\n",
            "* Acc@task 50.100 Acc@1 55.311 Acc@5 82.966 loss 2.052\n",
            "[Average accuracy till task9]\tAcc@task: 56.6981\tAcc@1: 56.3556\tAcc@5: 80.5438\tLoss: 2.3235\tForgetting: 0.0000\tBackward: 56.4862\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:20  Loss: 1.1849 (1.1849)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 75.0000 (75.0000)  time: 0.4770  data: 0.2994  max mem: 1380\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 1.1262 (1.0557)  Acc@1: 70.8333 (75.3788)  Acc@5: 95.8333 (96.2121)  Acc@task: 75.0000 (75.3788)  time: 0.2069  data: 0.0280  max mem: 1380\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.9376 (1.0186)  Acc@1: 70.8333 (76.1905)  Acc@5: 95.8333 (94.8413)  Acc@task: 75.0000 (76.9841)  time: 0.1804  data: 0.0007  max mem: 1380\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.8169 (0.9243)  Acc@1: 79.1667 (77.9570)  Acc@5: 95.8333 (95.6989)  Acc@task: 79.1667 (77.8226)  time: 0.1810  data: 0.0005  max mem: 1380\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6206 (0.8557)  Acc@1: 83.3333 (79.1667)  Acc@5: 100.0000 (96.2398)  Acc@task: 79.1667 (78.8618)  time: 0.1808  data: 0.0003  max mem: 1380\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.5472 (0.8443)  Acc@1: 83.3333 (79.2000)  Acc@5: 100.0000 (96.3000)  Acc@task: 79.1667 (79.2000)  time: 0.1775  data: 0.0003  max mem: 1380\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1899 s / it)\n",
            "* Acc@task 79.200 Acc@1 79.200 Acc@5 96.300 loss 0.844\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:34  Loss: 1.0812 (1.0812)  Acc@1: 79.1667 (79.1667)  Acc@5: 87.5000 (87.5000)  Acc@task: 62.5000 (62.5000)  time: 0.8192  data: 0.6211  max mem: 1380\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:07  Loss: 0.9142 (1.0410)  Acc@1: 79.1667 (80.3030)  Acc@5: 95.8333 (94.6970)  Acc@task: 66.6667 (68.1818)  time: 0.2381  data: 0.0577  max mem: 1380\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.9007 (1.1561)  Acc@1: 79.1667 (76.5873)  Acc@5: 95.8333 (93.8492)  Acc@task: 66.6667 (69.6429)  time: 0.1795  data: 0.0008  max mem: 1380\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.8679 (1.1008)  Acc@1: 79.1667 (77.1505)  Acc@5: 95.8333 (94.6237)  Acc@task: 70.8333 (71.1022)  time: 0.1804  data: 0.0004  max mem: 1380\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.7962 (1.0194)  Acc@1: 79.1667 (77.2358)  Acc@5: 95.8333 (95.0203)  Acc@task: 75.0000 (71.3415)  time: 0.1812  data: 0.0003  max mem: 1380\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.7801 (1.0134)  Acc@1: 79.1667 (77.2000)  Acc@5: 95.8333 (95.1000)  Acc@task: 75.0000 (71.4000)  time: 0.1777  data: 0.0003  max mem: 1380\n",
            "Test: [Task 2] Total time: 0:00:08 (0.1962 s / it)\n",
            "* Acc@task 71.400 Acc@1 77.200 Acc@5 95.100 loss 1.013\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:22  Loss: 0.4767 (0.4767)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.5282  data: 0.3523  max mem: 1380\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 1.0632 (0.9997)  Acc@1: 79.1667 (78.4091)  Acc@5: 95.8333 (95.0758)  Acc@task: 79.1667 (79.1667)  time: 0.2125  data: 0.0349  max mem: 1380\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 1.0632 (1.0059)  Acc@1: 79.1667 (78.9683)  Acc@5: 95.8333 (94.4444)  Acc@task: 79.1667 (78.7698)  time: 0.1791  data: 0.0017  max mem: 1380\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.8788 (0.9668)  Acc@1: 79.1667 (78.6290)  Acc@5: 95.8333 (94.4893)  Acc@task: 75.0000 (78.2258)  time: 0.1777  data: 0.0007  max mem: 1380\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.8187 (0.9554)  Acc@1: 79.1667 (78.6585)  Acc@5: 95.8333 (94.2073)  Acc@task: 79.1667 (78.1504)  time: 0.1783  data: 0.0006  max mem: 1380\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.8187 (0.9557)  Acc@1: 79.1667 (78.6000)  Acc@5: 95.8333 (94.3000)  Acc@task: 79.1667 (78.0000)  time: 0.1750  data: 0.0006  max mem: 1380\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1879 s / it)\n",
            "* Acc@task 78.000 Acc@1 78.600 Acc@5 94.300 loss 0.956\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:20  Loss: 1.0507 (1.0507)  Acc@1: 79.1667 (79.1667)  Acc@5: 87.5000 (87.5000)  Acc@task: 66.6667 (66.6667)  time: 0.4783  data: 0.3047  max mem: 1380\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:06  Loss: 0.9167 (0.9561)  Acc@1: 79.1667 (78.0303)  Acc@5: 95.8333 (93.9394)  Acc@task: 70.8333 (72.7273)  time: 0.2055  data: 0.0285  max mem: 1380\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.9115 (1.0355)  Acc@1: 75.0000 (76.3889)  Acc@5: 95.8333 (94.6429)  Acc@task: 75.0000 (73.2143)  time: 0.1780  data: 0.0006  max mem: 1380\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.8288 (0.9581)  Acc@1: 75.0000 (77.6882)  Acc@5: 95.8333 (95.2957)  Acc@task: 79.1667 (74.5968)  time: 0.1786  data: 0.0003  max mem: 1380\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.8288 (1.0322)  Acc@1: 79.1667 (77.0325)  Acc@5: 95.8333 (94.4106)  Acc@task: 70.8333 (74.0854)  time: 0.1797  data: 0.0002  max mem: 1380\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.8288 (1.0132)  Acc@1: 79.1667 (77.2000)  Acc@5: 95.8333 (94.5000)  Acc@task: 70.8333 (74.4000)  time: 0.1759  data: 0.0002  max mem: 1380\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1868 s / it)\n",
            "* Acc@task 74.400 Acc@1 77.200 Acc@5 94.500 loss 1.013\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:25  Loss: 0.7423 (0.7423)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 70.8333 (70.8333)  time: 0.6155  data: 0.4413  max mem: 1380\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:07  Loss: 0.7423 (0.8587)  Acc@1: 79.1667 (79.5455)  Acc@5: 100.0000 (96.5909)  Acc@task: 70.8333 (75.0000)  time: 0.2199  data: 0.0408  max mem: 1380\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.9531 (0.9069)  Acc@1: 79.1667 (79.3651)  Acc@5: 95.8333 (95.6349)  Acc@task: 70.8333 (74.2064)  time: 0.1795  data: 0.0011  max mem: 1380\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.9760 (0.9221)  Acc@1: 75.0000 (79.0323)  Acc@5: 95.8333 (94.8925)  Acc@task: 75.0000 (74.0591)  time: 0.1794  data: 0.0011  max mem: 1380\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.9694 (0.9901)  Acc@1: 75.0000 (77.9472)  Acc@5: 95.8333 (94.5122)  Acc@task: 70.8333 (72.5610)  time: 0.1802  data: 0.0005  max mem: 1380\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 1.0006 (1.0519)  Acc@1: 75.0000 (77.4000)  Acc@5: 91.6667 (94.2000)  Acc@task: 70.8333 (72.4000)  time: 0.1769  data: 0.0004  max mem: 1380\n",
            "Test: [Task 5] Total time: 0:00:08 (0.1910 s / it)\n",
            "* Acc@task 72.400 Acc@1 77.400 Acc@5 94.200 loss 1.052\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:19  Loss: 0.6549 (0.6549)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.4575  data: 0.2910  max mem: 1380\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:06  Loss: 0.8172 (0.9200)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (96.5909)  Acc@task: 75.0000 (74.2424)  time: 0.2041  data: 0.0267  max mem: 1380\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:04  Loss: 0.9130 (0.9917)  Acc@1: 79.1667 (77.7778)  Acc@5: 95.8333 (96.0317)  Acc@task: 75.0000 (75.0000)  time: 0.1794  data: 0.0003  max mem: 1380\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 1.0204 (0.9895)  Acc@1: 75.0000 (76.2097)  Acc@5: 95.8333 (95.8333)  Acc@task: 75.0000 (74.3280)  time: 0.1802  data: 0.0004  max mem: 1380\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.0015 (1.0154)  Acc@1: 70.8333 (75.6098)  Acc@5: 95.8333 (95.9350)  Acc@task: 70.8333 (73.7805)  time: 0.1810  data: 0.0003  max mem: 1380\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.0204 (1.0203)  Acc@1: 70.8333 (75.6000)  Acc@5: 95.8333 (96.0000)  Acc@task: 70.8333 (73.8000)  time: 0.1777  data: 0.0003  max mem: 1380\n",
            "Test: [Task 6] Total time: 0:00:07 (0.1885 s / it)\n",
            "* Acc@task 73.800 Acc@1 75.600 Acc@5 96.000 loss 1.020\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:37  Loss: 1.4787 (1.4787)  Acc@1: 79.1667 (79.1667)  Acc@5: 87.5000 (87.5000)  Acc@task: 54.1667 (54.1667)  time: 0.8891  data: 0.6750  max mem: 1380\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:07  Loss: 0.8921 (1.0011)  Acc@1: 79.1667 (76.5152)  Acc@5: 100.0000 (95.8333)  Acc@task: 62.5000 (62.5000)  time: 0.2444  data: 0.0622  max mem: 1380\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:04  Loss: 0.8921 (1.0487)  Acc@1: 70.8333 (75.0000)  Acc@5: 95.8333 (95.0397)  Acc@task: 66.6667 (62.6984)  time: 0.1799  data: 0.0009  max mem: 1380\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:02  Loss: 1.0664 (1.1100)  Acc@1: 75.0000 (74.0591)  Acc@5: 91.6667 (94.3548)  Acc@task: 66.6667 (63.8441)  time: 0.1801  data: 0.0007  max mem: 1380\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 1.1664 (1.1437)  Acc@1: 70.8333 (73.2724)  Acc@5: 91.6667 (94.1057)  Acc@task: 66.6667 (64.2276)  time: 0.1807  data: 0.0003  max mem: 1380\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 1.1664 (1.1302)  Acc@1: 70.8333 (73.4000)  Acc@5: 91.6667 (94.1000)  Acc@task: 66.6667 (64.5000)  time: 0.1777  data: 0.0003  max mem: 1380\n",
            "Test: [Task 7] Total time: 0:00:08 (0.1981 s / it)\n",
            "* Acc@task 64.500 Acc@1 73.400 Acc@5 94.100 loss 1.130\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:26  Loss: 1.2550 (1.2550)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  Acc@task: 62.5000 (62.5000)  time: 0.6381  data: 0.4713  max mem: 1380\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:07  Loss: 0.9877 (1.1073)  Acc@1: 79.1667 (77.6515)  Acc@5: 95.8333 (93.1818)  Acc@task: 66.6667 (66.6667)  time: 0.2222  data: 0.0431  max mem: 1380\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:04  Loss: 0.7682 (1.0041)  Acc@1: 75.0000 (76.5873)  Acc@5: 95.8333 (94.6429)  Acc@task: 66.6667 (67.8571)  time: 0.1796  data: 0.0003  max mem: 1380\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:02  Loss: 0.8297 (0.9911)  Acc@1: 75.0000 (76.6129)  Acc@5: 95.8333 (94.7581)  Acc@task: 70.8333 (68.9516)  time: 0.1796  data: 0.0006  max mem: 1380\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 0.9673 (1.0145)  Acc@1: 75.0000 (76.8293)  Acc@5: 95.8333 (94.3089)  Acc@task: 66.6667 (68.1911)  time: 0.1814  data: 0.0008  max mem: 1380\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 0.9673 (1.0112)  Acc@1: 75.0000 (76.9000)  Acc@5: 93.7500 (94.3000)  Acc@task: 66.6667 (68.3000)  time: 0.1778  data: 0.0008  max mem: 1380\n",
            "Test: [Task 8] Total time: 0:00:08 (0.1921 s / it)\n",
            "* Acc@task 68.300 Acc@1 76.900 Acc@5 94.300 loss 1.011\n",
            "Test: [Task 9]  [ 0/42]  eta: 0:00:17  Loss: 0.6809 (0.6809)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  Acc@task: 58.3333 (58.3333)  time: 0.4242  data: 0.2597  max mem: 1380\n",
            "Test: [Task 9]  [10/42]  eta: 0:00:06  Loss: 0.8704 (0.8187)  Acc@1: 79.1667 (76.1364)  Acc@5: 100.0000 (98.1061)  Acc@task: 58.3333 (56.0606)  time: 0.2045  data: 0.0262  max mem: 1380\n",
            "Test: [Task 9]  [20/42]  eta: 0:00:04  Loss: 0.6632 (0.7565)  Acc@1: 79.1667 (78.7698)  Acc@5: 95.8333 (97.6190)  Acc@task: 58.3333 (57.9365)  time: 0.1815  data: 0.0016  max mem: 1380\n",
            "Test: [Task 9]  [30/42]  eta: 0:00:02  Loss: 0.6928 (0.8282)  Acc@1: 79.1667 (77.1505)  Acc@5: 95.8333 (96.3710)  Acc@task: 58.3333 (56.9892)  time: 0.1808  data: 0.0003  max mem: 1380\n",
            "Test: [Task 9]  [40/42]  eta: 0:00:00  Loss: 0.6095 (0.7587)  Acc@1: 79.1667 (78.7602)  Acc@5: 95.8333 (96.7480)  Acc@task: 58.3333 (58.2317)  time: 0.1818  data: 0.0002  max mem: 1380\n",
            "Test: [Task 9]  [41/42]  eta: 0:00:00  Loss: 0.6095 (0.7418)  Acc@1: 79.1667 (79.1000)  Acc@5: 95.8333 (96.8000)  Acc@task: 58.3333 (58.4000)  time: 0.1781  data: 0.0002  max mem: 1380\n",
            "Test: [Task 9] Total time: 0:00:07 (0.1882 s / it)\n",
            "* Acc@task 58.400 Acc@1 79.100 Acc@5 96.800 loss 0.742\n",
            "[Average accuracy till task9]\tAcc@task: 71.1556\tAcc@1: 77.1778\tAcc@5: 95.0667\tLoss: 0.9758\tForgetting: 0.0000\tBackward: 76.9375\n",
            "Loading checkpoint from: ./output/cifar100_full_dino_5epoch_10pct/checkpoint/task10_checkpoint.pth\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/engines/hide_promtp_wtp_and_tap_engine.py:574: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  loss = torch.nn.functional.cross_entropy(sim, torch.range(0, sim.shape[0] - 1).long().to(device))\n",
            "Train: Epoch[ 1/10]  [ 0/21]  eta: 0:00:14  Lr: 0.002812  Loss: 3.5868  Acc@1: 8.3333 (8.3333)  Acc@5: 41.6667 (41.6667)  time: 0.6840  data: 0.3987  max mem: 1380\n",
            "Train: Epoch[ 1/10]  [10/21]  eta: 0:00:03  Lr: 0.002812  Loss: 3.2343  Acc@1: 12.5000 (13.6364)  Acc@5: 62.5000 (57.5758)  time: 0.3236  data: 0.0374  max mem: 1380\n",
            "Train: Epoch[ 1/10]  [20/21]  eta: 0:00:00  Lr: 0.002812  Loss: 1.8571  Acc@1: 25.0000 (21.8107)  Acc@5: 66.6667 (67.2840)  time: 0.2775  data: 0.0007  max mem: 1380\n",
            "Train: Epoch[ 1/10] Total time: 0:00:06 (0.3018 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.8571  Acc@1: 25.0000 (21.8107)  Acc@5: 66.6667 (67.2840)\n",
            "Train: Epoch[ 2/10]  [ 0/21]  eta: 0:00:12  Lr: 0.002812  Loss: 1.8569  Acc@1: 45.8333 (45.8333)  Acc@5: 79.1667 (79.1667)  time: 0.6109  data: 0.3168  max mem: 1380\n",
            "Train: Epoch[ 2/10]  [10/21]  eta: 0:00:03  Lr: 0.002812  Loss: 1.4458  Acc@1: 50.0000 (50.3788)  Acc@5: 83.3333 (85.2273)  time: 0.3159  data: 0.0290  max mem: 1380\n",
            "Train: Epoch[ 2/10]  [20/21]  eta: 0:00:00  Lr: 0.002812  Loss: 1.0906  Acc@1: 54.1667 (54.1152)  Acc@5: 87.5000 (86.8313)  time: 0.2764  data: 0.0003  max mem: 1380\n",
            "Train: Epoch[ 2/10] Total time: 0:00:06 (0.2970 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.0906  Acc@1: 54.1667 (54.1152)  Acc@5: 87.5000 (86.8313)\n",
            "Train: Epoch[ 3/10]  [ 0/21]  eta: 0:00:14  Lr: 0.002812  Loss: 1.1945  Acc@1: 62.5000 (62.5000)  Acc@5: 95.8333 (95.8333)  time: 0.6712  data: 0.3821  max mem: 1380\n",
            "Train: Epoch[ 3/10]  [10/21]  eta: 0:00:03  Lr: 0.002812  Loss: 0.9188  Acc@1: 66.6667 (69.3182)  Acc@5: 95.8333 (94.6970)  time: 0.3218  data: 0.0354  max mem: 1380\n",
            "Train: Epoch[ 3/10]  [20/21]  eta: 0:00:00  Lr: 0.002812  Loss: 1.5083  Acc@1: 70.8333 (70.3704)  Acc@5: 95.8333 (95.6790)  time: 0.2770  data: 0.0007  max mem: 1380\n",
            "Train: Epoch[ 3/10] Total time: 0:00:06 (0.3010 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.5083  Acc@1: 70.8333 (70.3704)  Acc@5: 95.8333 (95.6790)\n",
            "Train: Epoch[ 4/10]  [ 0/21]  eta: 0:00:15  Lr: 0.002812  Loss: 0.8287  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  time: 0.7437  data: 0.4653  max mem: 1380\n",
            "Train: Epoch[ 4/10]  [10/21]  eta: 0:00:03  Lr: 0.002812  Loss: 0.5259  Acc@1: 87.5000 (84.0909)  Acc@5: 100.0000 (96.9697)  time: 0.3277  data: 0.0427  max mem: 1380\n",
            "Train: Epoch[ 4/10]  [20/21]  eta: 0:00:00  Lr: 0.002812  Loss: 0.6572  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (97.5309)  time: 0.2768  data: 0.0003  max mem: 1380\n",
            "Train: Epoch[ 4/10] Total time: 0:00:06 (0.3033 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.6572  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (97.5309)\n",
            "Train: Epoch[ 5/10]  [ 0/21]  eta: 0:00:14  Lr: 0.002812  Loss: 0.4931  Acc@1: 87.5000 (87.5000)  Acc@5: 95.8333 (95.8333)  time: 0.6753  data: 0.3824  max mem: 1380\n",
            "Train: Epoch[ 5/10]  [10/21]  eta: 0:00:03  Lr: 0.002812  Loss: 0.7704  Acc@1: 79.1667 (78.0303)  Acc@5: 100.0000 (97.7273)  time: 0.3233  data: 0.0352  max mem: 1380\n",
            "Train: Epoch[ 5/10]  [20/21]  eta: 0:00:00  Lr: 0.002812  Loss: 0.8287  Acc@1: 83.3333 (83.9506)  Acc@5: 100.0000 (98.3539)  time: 0.2773  data: 0.0003  max mem: 1380\n",
            "Train: Epoch[ 5/10] Total time: 0:00:06 (0.3006 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.8287  Acc@1: 83.3333 (83.9506)  Acc@5: 100.0000 (98.3539)\n",
            "Train: Epoch[ 6/10]  [ 0/21]  eta: 0:00:12  Lr: 0.002812  Loss: 0.4090  Acc@1: 91.6667 (91.6667)  Acc@5: 95.8333 (95.8333)  time: 0.6153  data: 0.3245  max mem: 1380\n",
            "Train: Epoch[ 6/10]  [10/21]  eta: 0:00:03  Lr: 0.002812  Loss: 0.3358  Acc@1: 91.6667 (90.5303)  Acc@5: 100.0000 (98.8636)  time: 0.3159  data: 0.0299  max mem: 1380\n",
            "Train: Epoch[ 6/10]  [20/21]  eta: 0:00:00  Lr: 0.002812  Loss: 0.5108  Acc@1: 87.5000 (89.0947)  Acc@5: 100.0000 (98.5597)  time: 0.2762  data: 0.0003  max mem: 1380\n",
            "Train: Epoch[ 6/10] Total time: 0:00:06 (0.2969 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.5108  Acc@1: 87.5000 (89.0947)  Acc@5: 100.0000 (98.5597)\n",
            "Train: Epoch[ 7/10]  [ 0/21]  eta: 0:00:15  Lr: 0.002812  Loss: 0.2340  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  time: 0.7151  data: 0.4320  max mem: 1380\n",
            "Train: Epoch[ 7/10]  [10/21]  eta: 0:00:03  Lr: 0.002812  Loss: 0.3927  Acc@1: 91.6667 (88.6364)  Acc@5: 100.0000 (98.8636)  time: 0.3263  data: 0.0401  max mem: 1380\n",
            "Train: Epoch[ 7/10]  [20/21]  eta: 0:00:00  Lr: 0.002812  Loss: 0.1424  Acc@1: 87.5000 (88.2716)  Acc@5: 100.0000 (98.9712)  time: 0.2770  data: 0.0006  max mem: 1380\n",
            "Train: Epoch[ 7/10] Total time: 0:00:06 (0.3023 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.1424  Acc@1: 87.5000 (88.2716)  Acc@5: 100.0000 (98.9712)\n",
            "Train: Epoch[ 8/10]  [ 0/21]  eta: 0:00:13  Lr: 0.002812  Loss: 0.3442  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  time: 0.6496  data: 0.3729  max mem: 1380\n",
            "Train: Epoch[ 8/10]  [10/21]  eta: 0:00:03  Lr: 0.002812  Loss: 0.4903  Acc@1: 95.8333 (92.8030)  Acc@5: 100.0000 (100.0000)  time: 0.3198  data: 0.0342  max mem: 1380\n",
            "Train: Epoch[ 8/10]  [20/21]  eta: 0:00:00  Lr: 0.002812  Loss: 0.2536  Acc@1: 95.8333 (93.0041)  Acc@5: 100.0000 (99.7942)  time: 0.2771  data: 0.0002  max mem: 1380\n",
            "Train: Epoch[ 8/10] Total time: 0:00:06 (0.2991 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.2536  Acc@1: 95.8333 (93.0041)  Acc@5: 100.0000 (99.7942)\n",
            "Train: Epoch[ 9/10]  [ 0/21]  eta: 0:00:15  Lr: 0.002812  Loss: 0.6259  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.7584  data: 0.4772  max mem: 1380\n",
            "Train: Epoch[ 9/10]  [10/21]  eta: 0:00:03  Lr: 0.002812  Loss: 0.5910  Acc@1: 87.5000 (89.3939)  Acc@5: 100.0000 (98.4848)  time: 0.3309  data: 0.0436  max mem: 1380\n",
            "Train: Epoch[ 9/10]  [20/21]  eta: 0:00:00  Lr: 0.002812  Loss: 0.1723  Acc@1: 91.6667 (92.1811)  Acc@5: 100.0000 (99.1770)  time: 0.2776  data: 0.0004  max mem: 1380\n",
            "Train: Epoch[ 9/10] Total time: 0:00:06 (0.3047 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.1723  Acc@1: 91.6667 (92.1811)  Acc@5: 100.0000 (99.1770)\n",
            "Train: Epoch[10/10]  [ 0/21]  eta: 0:00:12  Lr: 0.002812  Loss: 0.2977  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.6179  data: 0.3202  max mem: 1380\n",
            "Train: Epoch[10/10]  [10/21]  eta: 0:00:03  Lr: 0.002812  Loss: 0.3843  Acc@1: 91.6667 (91.2879)  Acc@5: 100.0000 (99.6212)  time: 0.3173  data: 0.0294  max mem: 1380\n",
            "Train: Epoch[10/10]  [20/21]  eta: 0:00:00  Lr: 0.002812  Loss: 0.3947  Acc@1: 91.6667 (92.1811)  Acc@5: 100.0000 (99.5885)  time: 0.2771  data: 0.0002  max mem: 1380\n",
            "Train: Epoch[10/10] Total time: 0:00:06 (0.2975 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.3947  Acc@1: 91.6667 (92.1811)  Acc@5: 100.0000 (99.5885)\n",
            "torch.Size([5, 2, 5, 6, 64])\n",
            "torch.Size([5, 2, 1, 5, 6, 64])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/23]  eta: 0:00:23  Loss: 0.7917 (0.7917)  Acc@1: 62.5000 (62.5000)  Acc@5: 95.8333 (95.8333)  Acc@task: 79.1667 (79.1667)  time: 1.0132  data: 0.8146  max mem: 1380\n",
            "Test: [Task 1]  [10/23]  eta: 0:00:03  Loss: 1.0202 (1.0248)  Acc@1: 70.8333 (73.8636)  Acc@5: 95.8333 (94.6970)  Acc@task: 58.3333 (64.7727)  time: 0.2535  data: 0.0747  max mem: 1380\n",
            "Test: [Task 1]  [20/23]  eta: 0:00:00  Loss: 1.0277 (1.0893)  Acc@1: 75.0000 (73.2143)  Acc@5: 91.6667 (91.6667)  Acc@task: 58.3333 (62.6984)  time: 0.1779  data: 0.0005  max mem: 1380\n",
            "Test: [Task 1]  [22/23]  eta: 0:00:00  Loss: 1.0983 (1.1062)  Acc@1: 75.0000 (72.7612)  Acc@5: 91.6667 (91.6045)  Acc@task: 58.3333 (62.1269)  time: 0.1729  data: 0.0004  max mem: 1380\n",
            "Test: [Task 1] Total time: 0:00:04 (0.2136 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@task 62.127 Acc@1 72.761 Acc@5 91.604 loss 1.106\n",
            "Test: [Task 2]  [ 0/20]  eta: 0:00:13  Loss: 1.2904 (1.2904)  Acc@1: 70.8333 (70.8333)  Acc@5: 87.5000 (87.5000)  Acc@task: 66.6667 (66.6667)  time: 0.6700  data: 0.5031  max mem: 1380\n",
            "Test: [Task 2]  [10/20]  eta: 0:00:02  Loss: 2.0904 (2.2087)  Acc@1: 58.3333 (59.4697)  Acc@5: 83.3333 (80.3030)  Acc@task: 54.1667 (53.7879)  time: 0.2231  data: 0.0471  max mem: 1380\n",
            "Test: [Task 2]  [19/20]  eta: 0:00:00  Loss: 1.9631 (2.0219)  Acc@1: 58.3333 (60.9603)  Acc@5: 83.3333 (82.6722)  Acc@task: 54.1667 (55.1148)  time: 0.2031  data: 0.0260  max mem: 1380\n",
            "Test: [Task 2] Total time: 0:00:04 (0.2094 s / it)\n",
            "* Acc@task 55.115 Acc@1 60.960 Acc@5 82.672 loss 2.022\n",
            "Test: [Task 3]  [ 0/21]  eta: 0:00:14  Loss: 2.6213 (2.6213)  Acc@1: 54.1667 (54.1667)  Acc@5: 75.0000 (75.0000)  Acc@task: 62.5000 (62.5000)  time: 0.7084  data: 0.5407  max mem: 1380\n",
            "Test: [Task 3]  [10/21]  eta: 0:00:02  Loss: 2.7345 (2.8834)  Acc@1: 58.3333 (55.6818)  Acc@5: 75.0000 (73.4849)  Acc@task: 54.1667 (57.9545)  time: 0.2277  data: 0.0496  max mem: 1380\n",
            "Test: [Task 3]  [20/21]  eta: 0:00:00  Loss: 2.4370 (2.6946)  Acc@1: 58.3333 (55.9356)  Acc@5: 76.4706 (75.2515)  Acc@task: 58.3333 (58.1489)  time: 0.1763  data: 0.0003  max mem: 1380\n",
            "Test: [Task 3] Total time: 0:00:04 (0.2085 s / it)\n",
            "* Acc@task 58.149 Acc@1 55.936 Acc@5 75.252 loss 2.695\n",
            "Test: [Task 4]  [ 0/20]  eta: 0:00:19  Loss: 1.7630 (1.7630)  Acc@1: 50.0000 (50.0000)  Acc@5: 87.5000 (87.5000)  Acc@task: 75.0000 (75.0000)  time: 0.9926  data: 0.7967  max mem: 1380\n",
            "Test: [Task 4]  [10/20]  eta: 0:00:02  Loss: 2.5348 (2.7335)  Acc@1: 50.0000 (53.4091)  Acc@5: 83.3333 (78.0303)  Acc@task: 54.1667 (56.8182)  time: 0.2516  data: 0.0731  max mem: 1380\n",
            "Test: [Task 4]  [19/20]  eta: 0:00:00  Loss: 2.5348 (2.7062)  Acc@1: 50.0000 (52.8662)  Acc@5: 75.0000 (76.6454)  Acc@task: 54.1667 (55.8386)  time: 0.2160  data: 0.0403  max mem: 1380\n",
            "Test: [Task 4] Total time: 0:00:04 (0.2209 s / it)\n",
            "* Acc@task 55.839 Acc@1 52.866 Acc@5 76.645 loss 2.706\n",
            "Test: [Task 5]  [ 0/21]  eta: 0:00:10  Loss: 3.3124 (3.3124)  Acc@1: 50.0000 (50.0000)  Acc@5: 66.6667 (66.6667)  Acc@task: 41.6667 (41.6667)  time: 0.5228  data: 0.3531  max mem: 1380\n",
            "Test: [Task 5]  [10/21]  eta: 0:00:02  Loss: 3.3124 (3.2387)  Acc@1: 50.0000 (48.1061)  Acc@5: 66.6667 (68.9394)  Acc@task: 50.0000 (49.6212)  time: 0.2095  data: 0.0328  max mem: 1380\n",
            "Test: [Task 5]  [20/21]  eta: 0:00:00  Loss: 3.0808 (3.1118)  Acc@1: 45.8333 (50.3055)  Acc@5: 70.8333 (69.8574)  Acc@task: 50.0000 (51.7312)  time: 0.1739  data: 0.0005  max mem: 1380\n",
            "Test: [Task 5] Total time: 0:00:04 (0.1951 s / it)\n",
            "* Acc@task 51.731 Acc@1 50.306 Acc@5 69.857 loss 3.112\n",
            "Test: [Task 6]  [ 0/20]  eta: 0:00:09  Loss: 1.1272 (1.1272)  Acc@1: 62.5000 (62.5000)  Acc@5: 91.6667 (91.6667)  Acc@task: 62.5000 (62.5000)  time: 0.4919  data: 0.3264  max mem: 1380\n",
            "Test: [Task 6]  [10/20]  eta: 0:00:02  Loss: 2.5217 (2.3811)  Acc@1: 54.1667 (53.0303)  Acc@5: 83.3333 (82.1970)  Acc@task: 50.0000 (48.4848)  time: 0.2088  data: 0.0303  max mem: 1380\n",
            "Test: [Task 6]  [19/20]  eta: 0:00:00  Loss: 2.5217 (2.5346)  Acc@1: 54.1667 (55.0000)  Acc@5: 79.1667 (79.7917)  Acc@task: 50.0000 (50.6250)  time: 0.1953  data: 0.0167  max mem: 1380\n",
            "Test: [Task 6] Total time: 0:00:04 (0.2017 s / it)\n",
            "* Acc@task 50.625 Acc@1 55.000 Acc@5 79.792 loss 2.535\n",
            "Test: [Task 7]  [ 0/23]  eta: 0:00:20  Loss: 1.8935 (1.8935)  Acc@1: 62.5000 (62.5000)  Acc@5: 87.5000 (87.5000)  Acc@task: 66.6667 (66.6667)  time: 0.9030  data: 0.6917  max mem: 1380\n",
            "Test: [Task 7]  [10/23]  eta: 0:00:03  Loss: 2.2733 (2.3385)  Acc@1: 62.5000 (59.0909)  Acc@5: 83.3333 (81.8182)  Acc@task: 54.1667 (54.1667)  time: 0.2460  data: 0.0635  max mem: 1380\n",
            "Test: [Task 7]  [20/23]  eta: 0:00:00  Loss: 2.0002 (2.1437)  Acc@1: 58.3333 (60.1190)  Acc@5: 83.3333 (81.5476)  Acc@task: 54.1667 (55.9524)  time: 0.1803  data: 0.0005  max mem: 1380\n",
            "Test: [Task 7]  [22/23]  eta: 0:00:00  Loss: 2.2051 (2.2302)  Acc@1: 58.3333 (59.2727)  Acc@5: 79.1667 (80.3636)  Acc@task: 50.0000 (55.0909)  time: 0.1801  data: 0.0004  max mem: 1380\n",
            "Test: [Task 7] Total time: 0:00:04 (0.2160 s / it)\n",
            "* Acc@task 55.091 Acc@1 59.273 Acc@5 80.364 loss 2.230\n",
            "Test: [Task 8]  [ 0/22]  eta: 0:00:15  Loss: 2.1991 (2.1991)  Acc@1: 50.0000 (50.0000)  Acc@5: 79.1667 (79.1667)  Acc@task: 54.1667 (54.1667)  time: 0.6846  data: 0.5168  max mem: 1380\n",
            "Test: [Task 8]  [10/22]  eta: 0:00:02  Loss: 2.1991 (2.2302)  Acc@1: 54.1667 (56.0606)  Acc@5: 87.5000 (84.8485)  Acc@task: 58.3333 (58.7121)  time: 0.2246  data: 0.0473  max mem: 1380\n",
            "Test: [Task 8]  [20/22]  eta: 0:00:00  Loss: 2.0239 (2.0214)  Acc@1: 58.3333 (60.5159)  Acc@5: 83.3333 (84.9206)  Acc@task: 62.5000 (61.5079)  time: 0.1794  data: 0.0002  max mem: 1380\n",
            "Test: [Task 8]  [21/22]  eta: 0:00:00  Loss: 1.9309 (1.9973)  Acc@1: 58.3333 (60.4697)  Acc@5: 83.3333 (85.1272)  Acc@task: 62.5000 (61.2524)  time: 0.1740  data: 0.0002  max mem: 1380\n",
            "Test: [Task 8] Total time: 0:00:04 (0.2010 s / it)\n",
            "* Acc@task 61.252 Acc@1 60.470 Acc@5 85.127 loss 1.997\n",
            "Test: [Task 9]  [ 0/21]  eta: 0:00:13  Loss: 1.9477 (1.9477)  Acc@1: 62.5000 (62.5000)  Acc@5: 83.3333 (83.3333)  Acc@task: 54.1667 (54.1667)  time: 0.6195  data: 0.4480  max mem: 1380\n",
            "Test: [Task 9]  [10/21]  eta: 0:00:02  Loss: 1.8754 (1.7744)  Acc@1: 62.5000 (60.9849)  Acc@5: 87.5000 (85.6061)  Acc@task: 58.3333 (58.7121)  time: 0.2197  data: 0.0414  max mem: 1380\n",
            "Test: [Task 9]  [20/21]  eta: 0:00:00  Loss: 1.6725 (1.8549)  Acc@1: 62.5000 (60.9218)  Acc@5: 87.5000 (85.1703)  Acc@task: 58.3333 (57.1142)  time: 0.1779  data: 0.0004  max mem: 1380\n",
            "Test: [Task 9] Total time: 0:00:04 (0.2052 s / it)\n",
            "* Acc@task 57.114 Acc@1 60.922 Acc@5 85.170 loss 1.855\n",
            "Test: [Task 10]  [ 0/21]  eta: 0:00:20  Loss: 6.7718 (6.7718)  Acc@1: 0.0000 (0.0000)  Acc@5: 25.0000 (25.0000)  Acc@task: 50.0000 (50.0000)  time: 0.9808  data: 0.7993  max mem: 1380\n",
            "Test: [Task 10]  [10/21]  eta: 0:00:02  Loss: 6.5068 (6.4499)  Acc@1: 0.0000 (3.0303)  Acc@5: 41.6667 (37.8788)  Acc@task: 41.6667 (45.8333)  time: 0.2521  data: 0.0735  max mem: 1380\n",
            "Test: [Task 10]  [20/21]  eta: 0:00:00  Loss: 6.4378 (6.5155)  Acc@1: 0.0000 (2.0576)  Acc@5: 37.5000 (39.0947)  Acc@task: 41.6667 (44.8560)  time: 0.1731  data: 0.0005  max mem: 1380\n",
            "Test: [Task 10] Total time: 0:00:04 (0.2162 s / it)\n",
            "* Acc@task 44.856 Acc@1 2.058 Acc@5 39.095 loss 6.516\n",
            "[Average accuracy till task10]\tAcc@task: 55.1899\tAcc@1: 53.0551\tAcc@5: 76.5579\tLoss: 2.6773\tForgetting: 0.1818\tBackward: 52.5758\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:24  Loss: 1.1820 (1.1820)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 75.0000 (75.0000)  time: 0.5825  data: 0.4119  max mem: 1380\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 1.0634 (1.0705)  Acc@1: 70.8333 (74.6212)  Acc@5: 95.8333 (96.5909)  Acc@task: 75.0000 (74.2424)  time: 0.2176  data: 0.0379  max mem: 1380\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.9613 (1.0230)  Acc@1: 70.8333 (75.9921)  Acc@5: 95.8333 (95.2381)  Acc@task: 75.0000 (76.3889)  time: 0.1808  data: 0.0004  max mem: 1380\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.8167 (0.9343)  Acc@1: 79.1667 (77.8226)  Acc@5: 95.8333 (95.8333)  Acc@task: 79.1667 (76.3441)  time: 0.1813  data: 0.0003  max mem: 1380\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6209 (0.8637)  Acc@1: 83.3333 (79.0650)  Acc@5: 100.0000 (96.3415)  Acc@task: 79.1667 (77.6423)  time: 0.1824  data: 0.0002  max mem: 1380\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.5472 (0.8520)  Acc@1: 83.3333 (79.1000)  Acc@5: 100.0000 (96.4000)  Acc@task: 79.1667 (78.0000)  time: 0.1788  data: 0.0002  max mem: 1380\n",
            "Test: [Task 1] Total time: 0:00:08 (0.1939 s / it)\n",
            "* Acc@task 78.000 Acc@1 79.100 Acc@5 96.400 loss 0.852\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:36  Loss: 1.0810 (1.0810)  Acc@1: 79.1667 (79.1667)  Acc@5: 87.5000 (87.5000)  Acc@task: 62.5000 (62.5000)  time: 0.8725  data: 0.6931  max mem: 1380\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:07  Loss: 0.9192 (1.0357)  Acc@1: 79.1667 (80.3030)  Acc@5: 95.8333 (94.6970)  Acc@task: 70.8333 (69.6970)  time: 0.2444  data: 0.0643  max mem: 1380\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.9192 (1.1765)  Acc@1: 79.1667 (76.7857)  Acc@5: 95.8333 (93.6508)  Acc@task: 66.6667 (68.8492)  time: 0.1818  data: 0.0009  max mem: 1380\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.9389 (1.1203)  Acc@1: 79.1667 (77.2849)  Acc@5: 95.8333 (94.3548)  Acc@task: 66.6667 (70.2957)  time: 0.1818  data: 0.0005  max mem: 1380\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.8022 (1.0442)  Acc@1: 79.1667 (77.2358)  Acc@5: 95.8333 (94.8171)  Acc@task: 66.6667 (69.6138)  time: 0.1823  data: 0.0004  max mem: 1380\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.7855 (1.0375)  Acc@1: 79.1667 (77.2000)  Acc@5: 95.8333 (94.9000)  Acc@task: 66.6667 (69.7000)  time: 0.1788  data: 0.0003  max mem: 1380\n",
            "Test: [Task 2] Total time: 0:00:08 (0.1992 s / it)\n",
            "* Acc@task 69.700 Acc@1 77.200 Acc@5 94.900 loss 1.038\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:20  Loss: 0.4768 (0.4768)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.4817  data: 0.2904  max mem: 1380\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 1.0634 (1.0045)  Acc@1: 79.1667 (78.4091)  Acc@5: 95.8333 (95.0758)  Acc@task: 79.1667 (79.5455)  time: 0.2093  data: 0.0288  max mem: 1380\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 1.0376 (0.9971)  Acc@1: 79.1667 (78.9683)  Acc@5: 95.8333 (94.4444)  Acc@task: 79.1667 (79.1667)  time: 0.1815  data: 0.0015  max mem: 1380\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.8565 (0.9607)  Acc@1: 79.1667 (78.4946)  Acc@5: 95.8333 (94.3548)  Acc@task: 79.1667 (78.4946)  time: 0.1812  data: 0.0007  max mem: 1380\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.8093 (0.9520)  Acc@1: 79.1667 (78.5569)  Acc@5: 95.8333 (94.2073)  Acc@task: 79.1667 (78.4553)  time: 0.1813  data: 0.0007  max mem: 1380\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.8093 (0.9523)  Acc@1: 79.1667 (78.5000)  Acc@5: 95.8333 (94.3000)  Acc@task: 79.1667 (78.3000)  time: 0.1778  data: 0.0007  max mem: 1380\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1895 s / it)\n",
            "* Acc@task 78.300 Acc@1 78.500 Acc@5 94.300 loss 0.952\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:23  Loss: 1.0508 (1.0508)  Acc@1: 79.1667 (79.1667)  Acc@5: 87.5000 (87.5000)  Acc@task: 66.6667 (66.6667)  time: 0.5667  data: 0.3983  max mem: 1380\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:06  Loss: 0.9167 (0.9521)  Acc@1: 79.1667 (78.0303)  Acc@5: 95.8333 (93.9394)  Acc@task: 66.6667 (70.4545)  time: 0.2147  data: 0.0365  max mem: 1380\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.9167 (1.0394)  Acc@1: 75.0000 (76.3889)  Acc@5: 95.8333 (94.4444)  Acc@task: 70.8333 (71.8254)  time: 0.1797  data: 0.0004  max mem: 1380\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.8426 (0.9655)  Acc@1: 75.0000 (77.4194)  Acc@5: 95.8333 (95.1613)  Acc@task: 70.8333 (72.7151)  time: 0.1803  data: 0.0004  max mem: 1380\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.8426 (1.0382)  Acc@1: 79.1667 (77.0325)  Acc@5: 95.8333 (94.3089)  Acc@task: 70.8333 (72.1545)  time: 0.1807  data: 0.0002  max mem: 1380\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.8312 (1.0190)  Acc@1: 79.1667 (77.2000)  Acc@5: 95.8333 (94.4000)  Acc@task: 70.8333 (72.5000)  time: 0.1772  data: 0.0002  max mem: 1380\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1901 s / it)\n",
            "* Acc@task 72.500 Acc@1 77.200 Acc@5 94.400 loss 1.019\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:25  Loss: 0.7967 (0.7967)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 75.0000 (75.0000)  time: 0.6077  data: 0.4466  max mem: 1380\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:07  Loss: 0.7967 (0.8707)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (96.2121)  Acc@task: 70.8333 (73.8636)  time: 0.2195  data: 0.0413  max mem: 1380\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.9469 (0.9109)  Acc@1: 79.1667 (79.5635)  Acc@5: 95.8333 (95.4365)  Acc@task: 70.8333 (73.4127)  time: 0.1799  data: 0.0012  max mem: 1380\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.9696 (0.9250)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (94.7581)  Acc@task: 70.8333 (73.1183)  time: 0.1797  data: 0.0012  max mem: 1380\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.9696 (0.9908)  Acc@1: 75.0000 (78.0488)  Acc@5: 95.8333 (94.4106)  Acc@task: 66.6667 (71.5447)  time: 0.1809  data: 0.0004  max mem: 1380\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 1.0175 (1.0531)  Acc@1: 75.0000 (77.5000)  Acc@5: 91.6667 (94.2000)  Acc@task: 66.6667 (71.2000)  time: 0.1773  data: 0.0004  max mem: 1380\n",
            "Test: [Task 5] Total time: 0:00:08 (0.1914 s / it)\n",
            "* Acc@task 71.200 Acc@1 77.500 Acc@5 94.200 loss 1.053\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:20  Loss: 0.6272 (0.6272)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 87.5000 (87.5000)  time: 0.4846  data: 0.3118  max mem: 1380\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:06  Loss: 0.8182 (0.9189)  Acc@1: 79.1667 (79.5455)  Acc@5: 95.8333 (96.2121)  Acc@task: 75.0000 (74.6212)  time: 0.2071  data: 0.0289  max mem: 1380\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:04  Loss: 0.9121 (0.9928)  Acc@1: 79.1667 (77.7778)  Acc@5: 95.8333 (95.8333)  Acc@task: 75.0000 (74.6032)  time: 0.1793  data: 0.0006  max mem: 1380\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 1.0055 (0.9880)  Acc@1: 75.0000 (76.3441)  Acc@5: 95.8333 (95.6989)  Acc@task: 70.8333 (72.7151)  time: 0.1801  data: 0.0004  max mem: 1380\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.0053 (1.0165)  Acc@1: 70.8333 (75.6098)  Acc@5: 95.8333 (95.9350)  Acc@task: 70.8333 (72.0528)  time: 0.1800  data: 0.0003  max mem: 1380\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.0055 (1.0171)  Acc@1: 70.8333 (75.5000)  Acc@5: 95.8333 (96.0000)  Acc@task: 70.8333 (72.2000)  time: 0.1767  data: 0.0002  max mem: 1380\n",
            "Test: [Task 6] Total time: 0:00:07 (0.1890 s / it)\n",
            "* Acc@task 72.200 Acc@1 75.500 Acc@5 96.000 loss 1.017\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:36  Loss: 1.4141 (1.4141)  Acc@1: 79.1667 (79.1667)  Acc@5: 87.5000 (87.5000)  Acc@task: 58.3333 (58.3333)  time: 0.8592  data: 0.6607  max mem: 1380\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:07  Loss: 0.8920 (1.0006)  Acc@1: 79.1667 (76.5152)  Acc@5: 100.0000 (95.4545)  Acc@task: 66.6667 (65.1515)  time: 0.2403  data: 0.0625  max mem: 1380\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:04  Loss: 0.8920 (1.0470)  Acc@1: 70.8333 (75.0000)  Acc@5: 95.8333 (94.8413)  Acc@task: 66.6667 (65.6746)  time: 0.1786  data: 0.0015  max mem: 1380\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:02  Loss: 1.0588 (1.1073)  Acc@1: 75.0000 (74.0591)  Acc@5: 91.6667 (94.3548)  Acc@task: 66.6667 (66.3979)  time: 0.1789  data: 0.0004  max mem: 1380\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 1.1800 (1.1434)  Acc@1: 75.0000 (73.5772)  Acc@5: 91.6667 (93.9024)  Acc@task: 70.8333 (66.6667)  time: 0.1787  data: 0.0003  max mem: 1380\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 1.1800 (1.1319)  Acc@1: 75.0000 (73.7000)  Acc@5: 91.6667 (93.9000)  Acc@task: 70.8333 (66.8000)  time: 0.1756  data: 0.0003  max mem: 1380\n",
            "Test: [Task 7] Total time: 0:00:08 (0.1956 s / it)\n",
            "* Acc@task 66.800 Acc@1 73.700 Acc@5 93.900 loss 1.132\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:20  Loss: 1.2699 (1.2699)  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.8333)  Acc@task: 62.5000 (62.5000)  time: 0.4908  data: 0.3170  max mem: 1380\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:06  Loss: 0.9772 (1.0742)  Acc@1: 83.3333 (77.6515)  Acc@5: 95.8333 (93.1818)  Acc@task: 70.8333 (69.6970)  time: 0.2066  data: 0.0291  max mem: 1380\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:04  Loss: 0.7220 (0.9660)  Acc@1: 75.0000 (77.3810)  Acc@5: 95.8333 (95.0397)  Acc@task: 70.8333 (70.4365)  time: 0.1773  data: 0.0003  max mem: 1380\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:02  Loss: 0.8055 (0.9627)  Acc@1: 75.0000 (77.0161)  Acc@5: 95.8333 (95.0269)  Acc@task: 70.8333 (70.6989)  time: 0.1775  data: 0.0005  max mem: 1380\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 0.9887 (0.9880)  Acc@1: 75.0000 (77.1341)  Acc@5: 95.8333 (94.7154)  Acc@task: 66.6667 (69.3089)  time: 0.1786  data: 0.0007  max mem: 1380\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 0.9887 (0.9851)  Acc@1: 75.0000 (77.2000)  Acc@5: 93.7500 (94.7000)  Acc@task: 66.6667 (69.4000)  time: 0.1749  data: 0.0007  max mem: 1380\n",
            "Test: [Task 8] Total time: 0:00:07 (0.1863 s / it)\n",
            "* Acc@task 69.400 Acc@1 77.200 Acc@5 94.700 loss 0.985\n",
            "Test: [Task 9]  [ 0/42]  eta: 0:00:23  Loss: 0.6256 (0.6256)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 75.0000 (75.0000)  time: 0.5607  data: 0.3838  max mem: 1380\n",
            "Test: [Task 9]  [10/42]  eta: 0:00:06  Loss: 0.6880 (0.7741)  Acc@1: 79.1667 (77.2727)  Acc@5: 100.0000 (98.1061)  Acc@task: 62.5000 (65.5303)  time: 0.2125  data: 0.0357  max mem: 1380\n",
            "Test: [Task 9]  [20/42]  eta: 0:00:04  Loss: 0.6141 (0.7025)  Acc@1: 79.1667 (79.9603)  Acc@5: 95.8333 (97.8175)  Acc@task: 62.5000 (68.2540)  time: 0.1778  data: 0.0006  max mem: 1380\n",
            "Test: [Task 9]  [30/42]  eta: 0:00:02  Loss: 0.6796 (0.7696)  Acc@1: 79.1667 (78.3602)  Acc@5: 95.8333 (96.6398)  Acc@task: 66.6667 (67.7419)  time: 0.1785  data: 0.0003  max mem: 1380\n",
            "Test: [Task 9]  [40/42]  eta: 0:00:00  Loss: 0.5587 (0.7082)  Acc@1: 83.3333 (79.7764)  Acc@5: 95.8333 (97.0528)  Acc@task: 70.8333 (69.2073)  time: 0.1794  data: 0.0002  max mem: 1380\n",
            "Test: [Task 9]  [41/42]  eta: 0:00:00  Loss: 0.5587 (0.6920)  Acc@1: 83.3333 (80.1000)  Acc@5: 95.8333 (97.1000)  Acc@task: 70.8333 (69.4000)  time: 0.1760  data: 0.0002  max mem: 1380\n",
            "Test: [Task 9] Total time: 0:00:07 (0.1884 s / it)\n",
            "* Acc@task 69.400 Acc@1 80.100 Acc@5 97.100 loss 0.692\n",
            "Test: [Task 10]  [ 0/42]  eta: 0:00:22  Loss: 6.6828 (6.6828)  Acc@1: 0.0000 (0.0000)  Acc@5: 41.6667 (41.6667)  Acc@task: 45.8333 (45.8333)  time: 0.5282  data: 0.3622  max mem: 1380\n",
            "Test: [Task 10]  [10/42]  eta: 0:00:06  Loss: 6.6564 (6.5551)  Acc@1: 4.1667 (3.4091)  Acc@5: 45.8333 (45.0758)  Acc@task: 45.8333 (45.4545)  time: 0.2105  data: 0.0340  max mem: 1380\n",
            "Test: [Task 10]  [20/42]  eta: 0:00:04  Loss: 6.5059 (6.4672)  Acc@1: 4.1667 (3.5714)  Acc@5: 45.8333 (48.2143)  Acc@task: 45.8333 (46.2302)  time: 0.1780  data: 0.0011  max mem: 1380\n",
            "Test: [Task 10]  [30/42]  eta: 0:00:02  Loss: 6.5139 (6.5508)  Acc@1: 4.1667 (3.3602)  Acc@5: 45.8333 (47.7151)  Acc@task: 50.0000 (47.1774)  time: 0.1788  data: 0.0009  max mem: 1380\n",
            "Test: [Task 10]  [40/42]  eta: 0:00:00  Loss: 6.8858 (6.5884)  Acc@1: 4.1667 (3.4553)  Acc@5: 45.8333 (47.8659)  Acc@task: 45.8333 (45.7317)  time: 0.1803  data: 0.0005  max mem: 1380\n",
            "Test: [Task 10]  [41/42]  eta: 0:00:00  Loss: 6.6143 (6.5890)  Acc@1: 4.1667 (3.4000)  Acc@5: 45.8333 (48.2000)  Acc@task: 45.8333 (45.9000)  time: 0.1767  data: 0.0004  max mem: 1380\n",
            "Test: [Task 10] Total time: 0:00:07 (0.1887 s / it)\n",
            "* Acc@task 45.900 Acc@1 3.400 Acc@5 48.200 loss 6.589\n",
            "[Average accuracy till task10]\tAcc@task: 69.3400\tAcc@1: 69.9400\tAcc@5: 90.4100\tLoss: 1.5329\tForgetting: 0.0333\tBackward: 68.5444\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:26  Loss: 1.1820 (1.1820)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 75.0000 (75.0000)  time: 0.6246  data: 0.4566  max mem: 1380\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:07  Loss: 1.0634 (1.0705)  Acc@1: 70.8333 (74.6212)  Acc@5: 95.8333 (96.5909)  Acc@task: 75.0000 (74.2424)  time: 0.2209  data: 0.0419  max mem: 1380\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.9613 (1.0230)  Acc@1: 70.8333 (75.9921)  Acc@5: 95.8333 (95.2381)  Acc@task: 75.0000 (76.3889)  time: 0.1796  data: 0.0005  max mem: 1380\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.8167 (0.9343)  Acc@1: 79.1667 (77.8226)  Acc@5: 95.8333 (95.8333)  Acc@task: 79.1667 (76.3441)  time: 0.1793  data: 0.0005  max mem: 1380\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6209 (0.8637)  Acc@1: 83.3333 (79.0650)  Acc@5: 100.0000 (96.3415)  Acc@task: 79.1667 (77.6423)  time: 0.1804  data: 0.0004  max mem: 1380\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.5472 (0.8520)  Acc@1: 83.3333 (79.1000)  Acc@5: 100.0000 (96.4000)  Acc@task: 79.1667 (78.0000)  time: 0.1767  data: 0.0004  max mem: 1380\n",
            "Test: [Task 1] Total time: 0:00:08 (0.1930 s / it)\n",
            "* Acc@task 78.000 Acc@1 79.100 Acc@5 96.400 loss 0.852\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:36  Loss: 1.0810 (1.0810)  Acc@1: 79.1667 (79.1667)  Acc@5: 87.5000 (87.5000)  Acc@task: 62.5000 (62.5000)  time: 0.8595  data: 0.6641  max mem: 1380\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:07  Loss: 0.9192 (1.0357)  Acc@1: 79.1667 (80.3030)  Acc@5: 95.8333 (94.6970)  Acc@task: 70.8333 (69.6970)  time: 0.2423  data: 0.0614  max mem: 1380\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.9192 (1.1765)  Acc@1: 79.1667 (76.7857)  Acc@5: 95.8333 (93.6508)  Acc@task: 66.6667 (68.8492)  time: 0.1807  data: 0.0007  max mem: 1380\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.9389 (1.1203)  Acc@1: 79.1667 (77.2849)  Acc@5: 95.8333 (94.3548)  Acc@task: 66.6667 (70.2957)  time: 0.1809  data: 0.0004  max mem: 1380\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.8022 (1.0442)  Acc@1: 79.1667 (77.2358)  Acc@5: 95.8333 (94.8171)  Acc@task: 66.6667 (69.6138)  time: 0.1814  data: 0.0003  max mem: 1380\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.7855 (1.0375)  Acc@1: 79.1667 (77.2000)  Acc@5: 95.8333 (94.9000)  Acc@task: 66.6667 (69.7000)  time: 0.1780  data: 0.0003  max mem: 1380\n",
            "Test: [Task 2] Total time: 0:00:08 (0.1981 s / it)\n",
            "* Acc@task 69.700 Acc@1 77.200 Acc@5 94.900 loss 1.038\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:23  Loss: 0.4768 (0.4768)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.5521  data: 0.3535  max mem: 1380\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 1.0634 (1.0045)  Acc@1: 79.1667 (78.4091)  Acc@5: 95.8333 (95.0758)  Acc@task: 79.1667 (79.5455)  time: 0.2141  data: 0.0325  max mem: 1380\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 1.0376 (0.9971)  Acc@1: 79.1667 (78.9683)  Acc@5: 95.8333 (94.4444)  Acc@task: 79.1667 (79.1667)  time: 0.1805  data: 0.0003  max mem: 1380\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.8565 (0.9607)  Acc@1: 79.1667 (78.4946)  Acc@5: 95.8333 (94.3548)  Acc@task: 79.1667 (78.4946)  time: 0.1810  data: 0.0006  max mem: 1380\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.8093 (0.9520)  Acc@1: 79.1667 (78.5569)  Acc@5: 95.8333 (94.2073)  Acc@task: 79.1667 (78.4553)  time: 0.1817  data: 0.0007  max mem: 1380\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.8093 (0.9523)  Acc@1: 79.1667 (78.5000)  Acc@5: 95.8333 (94.3000)  Acc@task: 79.1667 (78.3000)  time: 0.1783  data: 0.0006  max mem: 1380\n",
            "Test: [Task 3] Total time: 0:00:08 (0.1907 s / it)\n",
            "* Acc@task 78.300 Acc@1 78.500 Acc@5 94.300 loss 0.952\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:24  Loss: 1.0508 (1.0508)  Acc@1: 79.1667 (79.1667)  Acc@5: 87.5000 (87.5000)  Acc@task: 66.6667 (66.6667)  time: 0.5862  data: 0.4104  max mem: 1380\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:06  Loss: 0.9167 (0.9521)  Acc@1: 79.1667 (78.0303)  Acc@5: 95.8333 (93.9394)  Acc@task: 66.6667 (70.4545)  time: 0.2179  data: 0.0376  max mem: 1380\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.9167 (1.0394)  Acc@1: 75.0000 (76.3889)  Acc@5: 95.8333 (94.4444)  Acc@task: 70.8333 (71.8254)  time: 0.1805  data: 0.0003  max mem: 1380\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.8426 (0.9655)  Acc@1: 75.0000 (77.4194)  Acc@5: 95.8333 (95.1613)  Acc@task: 70.8333 (72.7151)  time: 0.1809  data: 0.0004  max mem: 1380\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.8426 (1.0382)  Acc@1: 79.1667 (77.0325)  Acc@5: 95.8333 (94.3089)  Acc@task: 70.8333 (72.1545)  time: 0.1824  data: 0.0003  max mem: 1380\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.8312 (1.0190)  Acc@1: 79.1667 (77.2000)  Acc@5: 95.8333 (94.4000)  Acc@task: 70.8333 (72.5000)  time: 0.1789  data: 0.0003  max mem: 1380\n",
            "Test: [Task 4] Total time: 0:00:08 (0.1917 s / it)\n",
            "* Acc@task 72.500 Acc@1 77.200 Acc@5 94.400 loss 1.019\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:22  Loss: 0.7967 (0.7967)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 75.0000 (75.0000)  time: 0.5284  data: 0.3488  max mem: 1380\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 0.7967 (0.8707)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (96.2121)  Acc@task: 70.8333 (73.8636)  time: 0.2121  data: 0.0326  max mem: 1380\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.9469 (0.9109)  Acc@1: 79.1667 (79.5635)  Acc@5: 95.8333 (95.4365)  Acc@task: 70.8333 (73.4127)  time: 0.1807  data: 0.0010  max mem: 1380\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.9696 (0.9250)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (94.7581)  Acc@task: 70.8333 (73.1183)  time: 0.1812  data: 0.0009  max mem: 1380\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.9696 (0.9908)  Acc@1: 75.0000 (78.0488)  Acc@5: 95.8333 (94.4106)  Acc@task: 66.6667 (71.5447)  time: 0.1816  data: 0.0005  max mem: 1380\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 1.0175 (1.0531)  Acc@1: 75.0000 (77.5000)  Acc@5: 91.6667 (94.2000)  Acc@task: 66.6667 (71.2000)  time: 0.1780  data: 0.0004  max mem: 1380\n",
            "Test: [Task 5] Total time: 0:00:07 (0.1901 s / it)\n",
            "* Acc@task 71.200 Acc@1 77.500 Acc@5 94.200 loss 1.053\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:17  Loss: 0.6272 (0.6272)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 87.5000 (87.5000)  time: 0.4232  data: 0.2427  max mem: 1380\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:06  Loss: 0.8182 (0.9189)  Acc@1: 79.1667 (79.5455)  Acc@5: 95.8333 (96.2121)  Acc@task: 75.0000 (74.6212)  time: 0.2080  data: 0.0311  max mem: 1380\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:04  Loss: 0.9121 (0.9928)  Acc@1: 79.1667 (77.7778)  Acc@5: 95.8333 (95.8333)  Acc@task: 75.0000 (74.6032)  time: 0.1842  data: 0.0053  max mem: 1380\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 1.0055 (0.9880)  Acc@1: 75.0000 (76.3441)  Acc@5: 95.8333 (95.6989)  Acc@task: 70.8333 (72.7151)  time: 0.1818  data: 0.0005  max mem: 1380\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.0053 (1.0165)  Acc@1: 70.8333 (75.6098)  Acc@5: 95.8333 (95.9350)  Acc@task: 70.8333 (72.0528)  time: 0.1812  data: 0.0003  max mem: 1380\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.0055 (1.0171)  Acc@1: 70.8333 (75.5000)  Acc@5: 95.8333 (96.0000)  Acc@task: 70.8333 (72.2000)  time: 0.1778  data: 0.0003  max mem: 1380\n",
            "Test: [Task 6] Total time: 0:00:07 (0.1903 s / it)\n",
            "* Acc@task 72.200 Acc@1 75.500 Acc@5 96.000 loss 1.017\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:37  Loss: 1.4141 (1.4141)  Acc@1: 79.1667 (79.1667)  Acc@5: 87.5000 (87.5000)  Acc@task: 58.3333 (58.3333)  time: 0.8914  data: 0.7001  max mem: 1380\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:07  Loss: 0.8920 (1.0006)  Acc@1: 79.1667 (76.5152)  Acc@5: 100.0000 (95.4545)  Acc@task: 66.6667 (65.1515)  time: 0.2438  data: 0.0643  max mem: 1380\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:04  Loss: 0.8920 (1.0470)  Acc@1: 70.8333 (75.0000)  Acc@5: 95.8333 (94.8413)  Acc@task: 66.6667 (65.6746)  time: 0.1787  data: 0.0005  max mem: 1380\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:02  Loss: 1.0588 (1.1073)  Acc@1: 75.0000 (74.0591)  Acc@5: 91.6667 (94.3548)  Acc@task: 66.6667 (66.3979)  time: 0.1793  data: 0.0003  max mem: 1380\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 1.1800 (1.1434)  Acc@1: 75.0000 (73.5772)  Acc@5: 91.6667 (93.9024)  Acc@task: 70.8333 (66.6667)  time: 0.1806  data: 0.0002  max mem: 1380\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 1.1800 (1.1319)  Acc@1: 75.0000 (73.7000)  Acc@5: 91.6667 (93.9000)  Acc@task: 70.8333 (66.8000)  time: 0.1772  data: 0.0002  max mem: 1380\n",
            "Test: [Task 7] Total time: 0:00:08 (0.1974 s / it)\n",
            "* Acc@task 66.800 Acc@1 73.700 Acc@5 93.900 loss 1.132\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:27  Loss: 1.2699 (1.2699)  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.8333)  Acc@task: 62.5000 (62.5000)  time: 0.6503  data: 0.4868  max mem: 1380\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:07  Loss: 0.9772 (1.0742)  Acc@1: 83.3333 (77.6515)  Acc@5: 95.8333 (93.1818)  Acc@task: 70.8333 (69.6970)  time: 0.2233  data: 0.0445  max mem: 1380\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:04  Loss: 0.7220 (0.9660)  Acc@1: 75.0000 (77.3810)  Acc@5: 95.8333 (95.0397)  Acc@task: 70.8333 (70.4365)  time: 0.1794  data: 0.0003  max mem: 1380\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:02  Loss: 0.8055 (0.9627)  Acc@1: 75.0000 (77.0161)  Acc@5: 95.8333 (95.0269)  Acc@task: 70.8333 (70.6989)  time: 0.1790  data: 0.0009  max mem: 1380\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 0.9887 (0.9880)  Acc@1: 75.0000 (77.1341)  Acc@5: 95.8333 (94.7154)  Acc@task: 66.6667 (69.3089)  time: 0.1802  data: 0.0009  max mem: 1380\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 0.9887 (0.9851)  Acc@1: 75.0000 (77.2000)  Acc@5: 93.7500 (94.7000)  Acc@task: 66.6667 (69.4000)  time: 0.1768  data: 0.0008  max mem: 1380\n",
            "Test: [Task 8] Total time: 0:00:08 (0.1917 s / it)\n",
            "* Acc@task 69.400 Acc@1 77.200 Acc@5 94.700 loss 0.985\n",
            "Test: [Task 9]  [ 0/42]  eta: 0:00:26  Loss: 0.6256 (0.6256)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 75.0000 (75.0000)  time: 0.6312  data: 0.4630  max mem: 1380\n",
            "Test: [Task 9]  [10/42]  eta: 0:00:07  Loss: 0.6880 (0.7741)  Acc@1: 79.1667 (77.2727)  Acc@5: 100.0000 (98.1061)  Acc@task: 62.5000 (65.5303)  time: 0.2209  data: 0.0424  max mem: 1380\n",
            "Test: [Task 9]  [20/42]  eta: 0:00:04  Loss: 0.6141 (0.7025)  Acc@1: 79.1667 (79.9603)  Acc@5: 95.8333 (97.8175)  Acc@task: 62.5000 (68.2540)  time: 0.1789  data: 0.0003  max mem: 1380\n",
            "Test: [Task 9]  [30/42]  eta: 0:00:02  Loss: 0.6796 (0.7696)  Acc@1: 79.1667 (78.3602)  Acc@5: 95.8333 (96.6398)  Acc@task: 66.6667 (67.7419)  time: 0.1788  data: 0.0003  max mem: 1380\n",
            "Test: [Task 9]  [40/42]  eta: 0:00:00  Loss: 0.5587 (0.7082)  Acc@1: 83.3333 (79.7764)  Acc@5: 95.8333 (97.0528)  Acc@task: 70.8333 (69.2073)  time: 0.1795  data: 0.0003  max mem: 1380\n",
            "Test: [Task 9]  [41/42]  eta: 0:00:00  Loss: 0.5587 (0.6920)  Acc@1: 83.3333 (80.1000)  Acc@5: 95.8333 (97.1000)  Acc@task: 70.8333 (69.4000)  time: 0.1759  data: 0.0003  max mem: 1380\n",
            "Test: [Task 9] Total time: 0:00:08 (0.1906 s / it)\n",
            "* Acc@task 69.400 Acc@1 80.100 Acc@5 97.100 loss 0.692\n",
            "Test: [Task 10]  [ 0/42]  eta: 0:00:18  Loss: 6.6828 (6.6828)  Acc@1: 0.0000 (0.0000)  Acc@5: 41.6667 (41.6667)  Acc@task: 45.8333 (45.8333)  time: 0.4500  data: 0.2704  max mem: 1380\n",
            "Test: [Task 10]  [10/42]  eta: 0:00:06  Loss: 6.6564 (6.5551)  Acc@1: 4.1667 (3.4091)  Acc@5: 45.8333 (45.0758)  Acc@task: 45.8333 (45.4545)  time: 0.2043  data: 0.0260  max mem: 1380\n",
            "Test: [Task 10]  [20/42]  eta: 0:00:04  Loss: 6.5059 (6.4672)  Acc@1: 4.1667 (3.5714)  Acc@5: 45.8333 (48.2143)  Acc@task: 45.8333 (46.2302)  time: 0.1789  data: 0.0018  max mem: 1380\n",
            "Test: [Task 10]  [30/42]  eta: 0:00:02  Loss: 6.5139 (6.5508)  Acc@1: 4.1667 (3.3602)  Acc@5: 45.8333 (47.7151)  Acc@task: 50.0000 (47.1774)  time: 0.1788  data: 0.0014  max mem: 1380\n",
            "Test: [Task 10]  [40/42]  eta: 0:00:00  Loss: 6.8858 (6.5884)  Acc@1: 4.1667 (3.4553)  Acc@5: 45.8333 (47.8659)  Acc@task: 45.8333 (45.7317)  time: 0.1798  data: 0.0004  max mem: 1380\n",
            "Test: [Task 10]  [41/42]  eta: 0:00:00  Loss: 6.6143 (6.5890)  Acc@1: 4.1667 (3.4000)  Acc@5: 45.8333 (48.2000)  Acc@task: 45.8333 (45.9000)  time: 0.1765  data: 0.0003  max mem: 1380\n",
            "Test: [Task 10] Total time: 0:00:07 (0.1865 s / it)\n",
            "* Acc@task 45.900 Acc@1 3.400 Acc@5 48.200 loss 6.589\n",
            "[Average accuracy till task10]\tAcc@task: 69.3400\tAcc@1: 69.9400\tAcc@5: 90.4100\tLoss: 1.5329\tForgetting: 4.8111\tBackward: 58.9444\n",
            "torch.Size([103560, 384])\n",
            "Averaged stats: Lr: 0.005000  Loss: 0.0207  Acc@1: 100.0000 (97.6389)  Acc@5: 100.0000 (99.7037)\n",
            "torch.Size([103560, 384])\n",
            "Averaged stats: Lr: 0.004878  Loss: 0.0098  Acc@1: 100.0000 (99.6574)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([103560, 384])\n",
            "Averaged stats: Lr: 0.004523  Loss: 0.0063  Acc@1: 100.0000 (99.7685)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([103560, 384])\n",
            "Averaged stats: Lr: 0.003969  Loss: 0.0053  Acc@1: 100.0000 (99.8241)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([103560, 384])\n",
            "Averaged stats: Lr: 0.003273  Loss: 0.0065  Acc@1: 100.0000 (99.8611)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([103560, 384])\n",
            "Averaged stats: Lr: 0.002500  Loss: 0.0073  Acc@1: 100.0000 (99.8704)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([103560, 384])\n",
            "Averaged stats: Lr: 0.001727  Loss: 0.0294  Acc@1: 100.0000 (99.8333)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([103560, 384])\n",
            "Averaged stats: Lr: 0.001031  Loss: 0.0062  Acc@1: 100.0000 (99.8333)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([103560, 384])\n",
            "Averaged stats: Lr: 0.000477  Loss: 0.0069  Acc@1: 100.0000 (99.8704)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([103560, 384])\n",
            "Averaged stats: Lr: 0.000122  Loss: 0.0055  Acc@1: 100.0000 (99.8889)  Acc@5: 100.0000 (99.9907)\n",
            "Test: [Task 1]  [ 0/23]  eta: 0:00:18  Loss: 0.7295 (0.7295)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 70.8333 (70.8333)  time: 0.8191  data: 0.6181  max mem: 1380\n",
            "Test: [Task 1]  [10/23]  eta: 0:00:03  Loss: 1.5122 (1.4405)  Acc@1: 70.8333 (69.6970)  Acc@5: 91.6667 (90.9091)  Acc@task: 58.3333 (59.4697)  time: 0.2358  data: 0.0576  max mem: 1380\n",
            "Test: [Task 1]  [20/23]  eta: 0:00:00  Loss: 1.4330 (1.3809)  Acc@1: 66.6667 (69.4444)  Acc@5: 91.6667 (89.8810)  Acc@task: 58.3333 (60.1190)  time: 0.1777  data: 0.0010  max mem: 1380\n",
            "Test: [Task 1]  [22/23]  eta: 0:00:00  Loss: 1.3984 (1.3533)  Acc@1: 70.8333 (69.0299)  Acc@5: 91.6667 (90.1119)  Acc@task: 58.3333 (59.5149)  time: 0.1724  data: 0.0009  max mem: 1380\n",
            "Test: [Task 1] Total time: 0:00:04 (0.2060 s / it)\n",
            "* Acc@task 59.515 Acc@1 69.030 Acc@5 90.112 loss 1.353\n",
            "Test: [Task 2]  [ 0/20]  eta: 0:00:10  Loss: 1.5063 (1.5063)  Acc@1: 58.3333 (58.3333)  Acc@5: 95.8333 (95.8333)  Acc@task: 54.1667 (54.1667)  time: 0.5213  data: 0.3579  max mem: 1380\n",
            "Test: [Task 2]  [10/20]  eta: 0:00:02  Loss: 1.8643 (1.9033)  Acc@1: 58.3333 (57.9545)  Acc@5: 87.5000 (85.9849)  Acc@task: 58.3333 (56.0606)  time: 0.2114  data: 0.0330  max mem: 1380\n",
            "Test: [Task 2]  [19/20]  eta: 0:00:00  Loss: 2.0921 (2.0334)  Acc@1: 58.3333 (58.4551)  Acc@5: 83.3333 (83.9248)  Acc@task: 54.1667 (53.2359)  time: 0.1965  data: 0.0182  max mem: 1380\n",
            "Test: [Task 2] Total time: 0:00:04 (0.2013 s / it)\n",
            "* Acc@task 53.236 Acc@1 58.455 Acc@5 83.925 loss 2.033\n",
            "Test: [Task 3]  [ 0/21]  eta: 0:00:10  Loss: 2.9286 (2.9286)  Acc@1: 54.1667 (54.1667)  Acc@5: 58.3333 (58.3333)  Acc@task: 54.1667 (54.1667)  time: 0.5065  data: 0.3349  max mem: 1380\n",
            "Test: [Task 3]  [10/21]  eta: 0:00:02  Loss: 2.8714 (2.7498)  Acc@1: 54.1667 (54.9242)  Acc@5: 70.8333 (72.3485)  Acc@task: 62.5000 (60.9848)  time: 0.2113  data: 0.0309  max mem: 1380\n",
            "Test: [Task 3]  [20/21]  eta: 0:00:00  Loss: 2.5676 (2.6594)  Acc@1: 58.3333 (55.1308)  Acc@5: 75.0000 (75.0503)  Acc@task: 62.5000 (59.9598)  time: 0.1793  data: 0.0003  max mem: 1380\n",
            "Test: [Task 3] Total time: 0:00:04 (0.2011 s / it)\n",
            "* Acc@task 59.960 Acc@1 55.131 Acc@5 75.050 loss 2.659\n",
            "Test: [Task 4]  [ 0/20]  eta: 0:00:20  Loss: 2.3476 (2.3476)  Acc@1: 58.3333 (58.3333)  Acc@5: 79.1667 (79.1667)  Acc@task: 66.6667 (66.6667)  time: 1.0248  data: 0.8006  max mem: 1380\n",
            "Test: [Task 4]  [10/20]  eta: 0:00:02  Loss: 3.2028 (2.9081)  Acc@1: 50.0000 (50.7576)  Acc@5: 75.0000 (75.3788)  Acc@task: 54.1667 (53.4091)  time: 0.2584  data: 0.0746  max mem: 1380\n",
            "Test: [Task 4]  [19/20]  eta: 0:00:00  Loss: 3.1373 (3.0041)  Acc@1: 50.0000 (51.1677)  Acc@5: 70.8333 (75.1592)  Acc@task: 54.1667 (52.4416)  time: 0.2210  data: 0.0412  max mem: 1380\n",
            "Test: [Task 4] Total time: 0:00:04 (0.2264 s / it)\n",
            "* Acc@task 52.442 Acc@1 51.168 Acc@5 75.159 loss 3.004\n",
            "Test: [Task 5]  [ 0/21]  eta: 0:00:12  Loss: 3.5441 (3.5441)  Acc@1: 41.6667 (41.6667)  Acc@5: 66.6667 (66.6667)  Acc@task: 54.1667 (54.1667)  time: 0.5752  data: 0.4008  max mem: 1380\n",
            "Test: [Task 5]  [10/21]  eta: 0:00:02  Loss: 3.2080 (3.1145)  Acc@1: 50.0000 (50.3788)  Acc@5: 70.8333 (73.4849)  Acc@task: 58.3333 (59.0909)  time: 0.2194  data: 0.0367  max mem: 1380\n",
            "Test: [Task 5]  [20/21]  eta: 0:00:00  Loss: 3.0674 (3.1225)  Acc@1: 50.0000 (49.6945)  Acc@5: 70.8333 (71.4868)  Acc@task: 54.1667 (55.1935)  time: 0.1784  data: 0.0002  max mem: 1380\n",
            "Test: [Task 5] Total time: 0:00:04 (0.2023 s / it)\n",
            "* Acc@task 55.193 Acc@1 49.695 Acc@5 71.487 loss 3.122\n",
            "Test: [Task 6]  [ 0/20]  eta: 0:00:10  Loss: 2.5446 (2.5446)  Acc@1: 58.3333 (58.3333)  Acc@5: 87.5000 (87.5000)  Acc@task: 66.6667 (66.6667)  time: 0.5080  data: 0.3310  max mem: 1380\n",
            "Test: [Task 6]  [10/20]  eta: 0:00:02  Loss: 2.4356 (2.6006)  Acc@1: 58.3333 (53.7879)  Acc@5: 75.0000 (77.2727)  Acc@task: 45.8333 (52.2727)  time: 0.2161  data: 0.0331  max mem: 1380\n",
            "Test: [Task 6]  [19/20]  eta: 0:00:00  Loss: 2.4951 (2.6982)  Acc@1: 50.0000 (52.0833)  Acc@5: 75.0000 (77.0833)  Acc@task: 45.8333 (50.2083)  time: 0.2014  data: 0.0183  max mem: 1380\n",
            "Test: [Task 6] Total time: 0:00:04 (0.2068 s / it)\n",
            "* Acc@task 50.208 Acc@1 52.083 Acc@5 77.083 loss 2.698\n",
            "Test: [Task 7]  [ 0/23]  eta: 0:00:11  Loss: 1.9856 (1.9856)  Acc@1: 58.3333 (58.3333)  Acc@5: 83.3333 (83.3333)  Acc@task: 50.0000 (50.0000)  time: 0.5082  data: 0.3371  max mem: 1380\n",
            "Test: [Task 7]  [10/23]  eta: 0:00:02  Loss: 1.9856 (2.2312)  Acc@1: 58.3333 (57.5758)  Acc@5: 83.3333 (80.6818)  Acc@task: 50.0000 (50.3788)  time: 0.2140  data: 0.0324  max mem: 1380\n",
            "Test: [Task 7]  [20/23]  eta: 0:00:00  Loss: 2.0920 (2.2022)  Acc@1: 58.3333 (58.9286)  Acc@5: 79.1667 (81.1508)  Acc@task: 54.1667 (52.3810)  time: 0.1838  data: 0.0014  max mem: 1380\n",
            "Test: [Task 7]  [22/23]  eta: 0:00:00  Loss: 2.3020 (2.2582)  Acc@1: 58.3333 (58.7273)  Acc@5: 77.2727 (80.3636)  Acc@task: 54.1667 (52.5455)  time: 0.1839  data: 0.0012  max mem: 1380\n",
            "Test: [Task 7] Total time: 0:00:04 (0.2019 s / it)\n",
            "* Acc@task 52.545 Acc@1 58.727 Acc@5 80.364 loss 2.258\n",
            "Test: [Task 8]  [ 0/22]  eta: 0:00:13  Loss: 2.0985 (2.0985)  Acc@1: 62.5000 (62.5000)  Acc@5: 83.3333 (83.3333)  Acc@task: 58.3333 (58.3333)  time: 0.6195  data: 0.4456  max mem: 1380\n",
            "Test: [Task 8]  [10/22]  eta: 0:00:02  Loss: 2.0985 (2.2072)  Acc@1: 58.3333 (57.9545)  Acc@5: 83.3333 (81.8182)  Acc@task: 58.3333 (59.8485)  time: 0.2225  data: 0.0408  max mem: 1380\n",
            "Test: [Task 8]  [20/22]  eta: 0:00:00  Loss: 2.3986 (2.4153)  Acc@1: 58.3333 (55.9524)  Acc@5: 79.1667 (78.9683)  Acc@task: 58.3333 (60.3175)  time: 0.1825  data: 0.0003  max mem: 1380\n",
            "Test: [Task 8]  [21/22]  eta: 0:00:00  Loss: 2.3978 (2.4145)  Acc@1: 58.3333 (55.9687)  Acc@5: 79.1667 (79.0607)  Acc@task: 58.3333 (60.2740)  time: 0.1770  data: 0.0003  max mem: 1380\n",
            "Test: [Task 8] Total time: 0:00:04 (0.2009 s / it)\n",
            "* Acc@task 60.274 Acc@1 55.969 Acc@5 79.061 loss 2.414\n",
            "Test: [Task 9]  [ 0/21]  eta: 0:00:13  Loss: 1.3827 (1.3827)  Acc@1: 66.6667 (66.6667)  Acc@5: 87.5000 (87.5000)  Acc@task: 54.1667 (54.1667)  time: 0.6255  data: 0.4277  max mem: 1380\n",
            "Test: [Task 9]  [10/21]  eta: 0:00:02  Loss: 1.9451 (2.0044)  Acc@1: 54.1667 (56.4394)  Acc@5: 87.5000 (84.4697)  Acc@task: 54.1667 (56.8182)  time: 0.2235  data: 0.0396  max mem: 1380\n",
            "Test: [Task 9]  [20/21]  eta: 0:00:00  Loss: 1.9785 (1.9971)  Acc@1: 58.3333 (58.7174)  Acc@5: 84.2105 (83.5671)  Acc@task: 62.5000 (58.9178)  time: 0.1808  data: 0.0007  max mem: 1380\n",
            "Test: [Task 9] Total time: 0:00:04 (0.2085 s / it)\n",
            "* Acc@task 58.918 Acc@1 58.717 Acc@5 83.567 loss 1.997\n",
            "Test: [Task 10]  [ 0/21]  eta: 0:00:11  Loss: 1.8804 (1.8804)  Acc@1: 62.5000 (62.5000)  Acc@5: 91.6667 (91.6667)  Acc@task: 45.8333 (45.8333)  time: 0.5291  data: 0.3580  max mem: 1380\n",
            "Test: [Task 10]  [10/21]  eta: 0:00:02  Loss: 2.2100 (2.4028)  Acc@1: 54.1667 (51.8939)  Acc@5: 87.5000 (83.3333)  Acc@task: 45.8333 (45.4545)  time: 0.2244  data: 0.0460  max mem: 1380\n",
            "Test: [Task 10]  [20/21]  eta: 0:00:00  Loss: 2.2100 (2.4181)  Acc@1: 50.0000 (49.7942)  Acc@5: 75.0000 (82.3045)  Acc@task: 45.8333 (44.4444)  time: 0.1824  data: 0.0078  max mem: 1380\n",
            "Test: [Task 10] Total time: 0:00:04 (0.2034 s / it)\n",
            "* Acc@task 44.444 Acc@1 49.794 Acc@5 82.305 loss 2.418\n",
            "[Average accuracy till task10]\tAcc@task: 54.6736\tAcc@1: 55.8769\tAcc@5: 79.8112\tLoss: 2.3959\tForgetting: 0.0000\tBackward: 56.5527\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:17  Loss: 1.1917 (1.1917)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  Acc@task: 75.0000 (75.0000)  time: 0.4253  data: 0.2557  max mem: 1380\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 1.0415 (1.0682)  Acc@1: 70.8333 (73.8636)  Acc@5: 95.8333 (94.6970)  Acc@task: 75.0000 (74.2424)  time: 0.2073  data: 0.0298  max mem: 1380\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.9969 (1.0240)  Acc@1: 70.8333 (75.0000)  Acc@5: 95.8333 (94.4444)  Acc@task: 75.0000 (76.3889)  time: 0.1839  data: 0.0039  max mem: 1380\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.8348 (0.9358)  Acc@1: 79.1667 (76.7473)  Acc@5: 95.8333 (95.1613)  Acc@task: 79.1667 (76.3441)  time: 0.1823  data: 0.0005  max mem: 1380\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6148 (0.8758)  Acc@1: 83.3333 (77.9472)  Acc@5: 100.0000 (95.8333)  Acc@task: 79.1667 (77.6423)  time: 0.1822  data: 0.0003  max mem: 1380\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.5281 (0.8632)  Acc@1: 83.3333 (78.1000)  Acc@5: 100.0000 (95.9000)  Acc@task: 79.1667 (78.0000)  time: 0.1788  data: 0.0003  max mem: 1380\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1896 s / it)\n",
            "* Acc@task 78.000 Acc@1 78.100 Acc@5 95.900 loss 0.863\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:22  Loss: 1.0784 (1.0784)  Acc@1: 79.1667 (79.1667)  Acc@5: 87.5000 (87.5000)  Acc@task: 62.5000 (62.5000)  time: 0.5455  data: 0.3665  max mem: 1380\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.9464 (1.0477)  Acc@1: 83.3333 (81.0606)  Acc@5: 95.8333 (95.0758)  Acc@task: 70.8333 (69.6970)  time: 0.2135  data: 0.0336  max mem: 1380\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.8606 (1.1859)  Acc@1: 79.1667 (77.1825)  Acc@5: 95.8333 (94.0476)  Acc@task: 66.6667 (68.8492)  time: 0.1797  data: 0.0010  max mem: 1380\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.8574 (1.1301)  Acc@1: 79.1667 (77.4194)  Acc@5: 95.8333 (94.6237)  Acc@task: 66.6667 (70.2957)  time: 0.1797  data: 0.0013  max mem: 1380\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.7890 (1.0521)  Acc@1: 79.1667 (77.5407)  Acc@5: 95.8333 (95.2236)  Acc@task: 66.6667 (69.6138)  time: 0.1807  data: 0.0006  max mem: 1380\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.7747 (1.0443)  Acc@1: 79.1667 (77.5000)  Acc@5: 95.8333 (95.3000)  Acc@task: 66.6667 (69.7000)  time: 0.1772  data: 0.0005  max mem: 1380\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1897 s / it)\n",
            "* Acc@task 69.700 Acc@1 77.500 Acc@5 95.300 loss 1.044\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:25  Loss: 0.4470 (0.4470)  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.6092  data: 0.4416  max mem: 1380\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 1.0352 (1.0111)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (94.6970)  Acc@task: 79.1667 (79.5455)  time: 0.2175  data: 0.0404  max mem: 1380\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 1.0328 (1.0125)  Acc@1: 79.1667 (79.3651)  Acc@5: 95.8333 (94.2460)  Acc@task: 79.1667 (79.1667)  time: 0.1780  data: 0.0003  max mem: 1380\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.8694 (0.9779)  Acc@1: 79.1667 (78.4946)  Acc@5: 95.8333 (94.3548)  Acc@task: 79.1667 (78.4946)  time: 0.1783  data: 0.0003  max mem: 1380\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.8274 (0.9775)  Acc@1: 79.1667 (78.5569)  Acc@5: 95.8333 (94.2073)  Acc@task: 79.1667 (78.4553)  time: 0.1791  data: 0.0002  max mem: 1380\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.8274 (0.9792)  Acc@1: 79.1667 (78.5000)  Acc@5: 95.8333 (94.3000)  Acc@task: 79.1667 (78.3000)  time: 0.1756  data: 0.0002  max mem: 1380\n",
            "Test: [Task 3] Total time: 0:00:08 (0.1909 s / it)\n",
            "* Acc@task 78.300 Acc@1 78.500 Acc@5 94.300 loss 0.979\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:33  Loss: 1.2265 (1.2265)  Acc@1: 75.0000 (75.0000)  Acc@5: 87.5000 (87.5000)  Acc@task: 66.6667 (66.6667)  time: 0.7890  data: 0.5605  max mem: 1380\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:07  Loss: 0.9625 (1.0491)  Acc@1: 75.0000 (73.4849)  Acc@5: 95.8333 (93.1818)  Acc@task: 66.6667 (70.4545)  time: 0.2332  data: 0.0517  max mem: 1380\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.9625 (1.1263)  Acc@1: 75.0000 (73.2143)  Acc@5: 95.8333 (93.2540)  Acc@task: 70.8333 (71.8254)  time: 0.1776  data: 0.0007  max mem: 1380\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.8927 (1.0522)  Acc@1: 75.0000 (74.8656)  Acc@5: 95.8333 (93.9516)  Acc@task: 70.8333 (72.7151)  time: 0.1784  data: 0.0006  max mem: 1380\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.8927 (1.1320)  Acc@1: 75.0000 (74.5935)  Acc@5: 95.8333 (93.2927)  Acc@task: 70.8333 (72.1545)  time: 0.1792  data: 0.0003  max mem: 1380\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.8927 (1.1116)  Acc@1: 79.1667 (74.9000)  Acc@5: 95.8333 (93.4000)  Acc@task: 70.8333 (72.5000)  time: 0.1757  data: 0.0003  max mem: 1380\n",
            "Test: [Task 4] Total time: 0:00:08 (0.1940 s / it)\n",
            "* Acc@task 72.500 Acc@1 74.900 Acc@5 93.400 loss 1.112\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:23  Loss: 1.0421 (1.0421)  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  Acc@task: 75.0000 (75.0000)  time: 0.5506  data: 0.3897  max mem: 1380\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 1.0421 (0.9496)  Acc@1: 75.0000 (76.8939)  Acc@5: 100.0000 (96.5909)  Acc@task: 70.8333 (73.8636)  time: 0.2117  data: 0.0357  max mem: 1380\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.9599 (0.9959)  Acc@1: 75.0000 (77.7778)  Acc@5: 95.8333 (95.2381)  Acc@task: 70.8333 (73.4127)  time: 0.1780  data: 0.0003  max mem: 1380\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.9967 (1.0011)  Acc@1: 75.0000 (77.6882)  Acc@5: 95.8333 (94.7581)  Acc@task: 70.8333 (73.1183)  time: 0.1787  data: 0.0005  max mem: 1380\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 1.0141 (1.0666)  Acc@1: 75.0000 (76.7276)  Acc@5: 95.8333 (94.2073)  Acc@task: 66.6667 (71.5447)  time: 0.1795  data: 0.0006  max mem: 1380\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 1.0805 (1.1289)  Acc@1: 75.0000 (76.2000)  Acc@5: 91.6667 (93.9000)  Acc@task: 66.6667 (71.2000)  time: 0.1759  data: 0.0006  max mem: 1380\n",
            "Test: [Task 5] Total time: 0:00:07 (0.1884 s / it)\n",
            "* Acc@task 71.200 Acc@1 76.200 Acc@5 93.900 loss 1.129\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:21  Loss: 0.6549 (0.6549)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  Acc@task: 87.5000 (87.5000)  time: 0.5023  data: 0.3134  max mem: 1380\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:06  Loss: 0.9061 (0.9751)  Acc@1: 79.1667 (78.0303)  Acc@5: 95.8333 (95.8333)  Acc@task: 75.0000 (74.6212)  time: 0.2084  data: 0.0292  max mem: 1380\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:04  Loss: 0.9187 (1.0338)  Acc@1: 75.0000 (76.7857)  Acc@5: 95.8333 (95.8333)  Acc@task: 75.0000 (74.6032)  time: 0.1790  data: 0.0005  max mem: 1380\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 1.0798 (1.0318)  Acc@1: 75.0000 (75.8065)  Acc@5: 95.8333 (95.5645)  Acc@task: 70.8333 (72.7151)  time: 0.1791  data: 0.0003  max mem: 1380\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.0669 (1.0670)  Acc@1: 75.0000 (75.4065)  Acc@5: 95.8333 (95.6301)  Acc@task: 70.8333 (72.0528)  time: 0.1797  data: 0.0002  max mem: 1380\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.0798 (1.0685)  Acc@1: 70.8333 (75.2000)  Acc@5: 95.8333 (95.7000)  Acc@task: 70.8333 (72.2000)  time: 0.1763  data: 0.0002  max mem: 1380\n",
            "Test: [Task 6] Total time: 0:00:07 (0.1879 s / it)\n",
            "* Acc@task 72.200 Acc@1 75.200 Acc@5 95.700 loss 1.069\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:20  Loss: 1.5184 (1.5184)  Acc@1: 75.0000 (75.0000)  Acc@5: 87.5000 (87.5000)  Acc@task: 58.3333 (58.3333)  time: 0.4852  data: 0.3159  max mem: 1380\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:06  Loss: 0.8948 (1.0245)  Acc@1: 75.0000 (76.1364)  Acc@5: 95.8333 (93.5606)  Acc@task: 66.6667 (65.1515)  time: 0.2077  data: 0.0295  max mem: 1380\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:04  Loss: 0.8948 (1.0788)  Acc@1: 75.0000 (75.3968)  Acc@5: 95.8333 (93.6508)  Acc@task: 66.6667 (65.6746)  time: 0.1791  data: 0.0011  max mem: 1380\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:02  Loss: 1.0334 (1.1499)  Acc@1: 75.0000 (74.1936)  Acc@5: 91.6667 (93.1452)  Acc@task: 66.6667 (66.3979)  time: 0.1794  data: 0.0010  max mem: 1380\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 1.2174 (1.1829)  Acc@1: 70.8333 (73.1707)  Acc@5: 91.6667 (92.8862)  Acc@task: 70.8333 (66.6667)  time: 0.1810  data: 0.0005  max mem: 1380\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 1.2174 (1.1721)  Acc@1: 70.8333 (73.3000)  Acc@5: 91.6667 (92.9000)  Acc@task: 70.8333 (66.8000)  time: 0.1772  data: 0.0004  max mem: 1380\n",
            "Test: [Task 7] Total time: 0:00:07 (0.1881 s / it)\n",
            "* Acc@task 66.800 Acc@1 73.300 Acc@5 92.900 loss 1.172\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:25  Loss: 1.1403 (1.1403)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  Acc@task: 62.5000 (62.5000)  time: 0.5987  data: 0.4354  max mem: 1380\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:06  Loss: 1.0058 (1.2074)  Acc@1: 75.0000 (75.7576)  Acc@5: 91.6667 (92.0455)  Acc@task: 70.8333 (69.6970)  time: 0.2185  data: 0.0400  max mem: 1380\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:04  Loss: 0.9809 (1.0973)  Acc@1: 75.0000 (75.7937)  Acc@5: 91.6667 (93.0556)  Acc@task: 70.8333 (70.4365)  time: 0.1795  data: 0.0005  max mem: 1380\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:02  Loss: 0.9929 (1.1018)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (93.4140)  Acc@task: 70.8333 (70.6989)  time: 0.1793  data: 0.0005  max mem: 1380\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 1.0865 (1.1318)  Acc@1: 70.8333 (75.0000)  Acc@5: 91.6667 (93.0894)  Acc@task: 66.6667 (69.3089)  time: 0.1804  data: 0.0003  max mem: 1380\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 1.0865 (1.1254)  Acc@1: 70.8333 (75.1000)  Acc@5: 91.6667 (93.1000)  Acc@task: 66.6667 (69.4000)  time: 0.1767  data: 0.0003  max mem: 1380\n",
            "Test: [Task 8] Total time: 0:00:08 (0.1920 s / it)\n",
            "* Acc@task 69.400 Acc@1 75.100 Acc@5 93.100 loss 1.125\n",
            "Test: [Task 9]  [ 0/42]  eta: 0:00:37  Loss: 0.7092 (0.7092)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  Acc@task: 75.0000 (75.0000)  time: 0.8828  data: 0.7045  max mem: 1380\n",
            "Test: [Task 9]  [10/42]  eta: 0:00:07  Loss: 0.9232 (0.8954)  Acc@1: 79.1667 (76.8939)  Acc@5: 95.8333 (96.9697)  Acc@task: 62.5000 (65.5303)  time: 0.2429  data: 0.0660  max mem: 1380\n",
            "Test: [Task 9]  [20/42]  eta: 0:00:04  Loss: 0.7319 (0.8282)  Acc@1: 79.1667 (79.3651)  Acc@5: 95.8333 (97.0238)  Acc@task: 62.5000 (68.2540)  time: 0.1795  data: 0.0013  max mem: 1380\n",
            "Test: [Task 9]  [30/42]  eta: 0:00:02  Loss: 0.8164 (0.9045)  Acc@1: 79.1667 (77.8226)  Acc@5: 95.8333 (95.4301)  Acc@task: 66.6667 (67.7419)  time: 0.1805  data: 0.0005  max mem: 1380\n",
            "Test: [Task 9]  [40/42]  eta: 0:00:00  Loss: 0.6680 (0.8246)  Acc@1: 83.3333 (79.2683)  Acc@5: 95.8333 (95.8333)  Acc@task: 70.8333 (69.2073)  time: 0.1814  data: 0.0004  max mem: 1380\n",
            "Test: [Task 9]  [41/42]  eta: 0:00:00  Loss: 0.6680 (0.8062)  Acc@1: 83.3333 (79.6000)  Acc@5: 95.8333 (95.9000)  Acc@task: 70.8333 (69.4000)  time: 0.1783  data: 0.0004  max mem: 1380\n",
            "Test: [Task 9] Total time: 0:00:08 (0.1980 s / it)\n",
            "* Acc@task 69.400 Acc@1 79.600 Acc@5 95.900 loss 0.806\n",
            "Test: [Task 10]  [ 0/42]  eta: 0:00:24  Loss: 1.0166 (1.0166)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  Acc@task: 45.8333 (45.8333)  time: 0.5803  data: 0.4118  max mem: 1380\n",
            "Test: [Task 10]  [10/42]  eta: 0:00:06  Loss: 1.0937 (1.1837)  Acc@1: 70.8333 (71.9697)  Acc@5: 95.8333 (94.6970)  Acc@task: 45.8333 (45.4545)  time: 0.2179  data: 0.0378  max mem: 1380\n",
            "Test: [Task 10]  [20/42]  eta: 0:00:04  Loss: 1.0735 (1.1178)  Acc@1: 66.6667 (71.2302)  Acc@5: 95.8333 (95.0397)  Acc@task: 45.8333 (46.2302)  time: 0.1807  data: 0.0004  max mem: 1380\n",
            "Test: [Task 10]  [30/42]  eta: 0:00:02  Loss: 1.0735 (1.1227)  Acc@1: 66.6667 (70.2957)  Acc@5: 95.8333 (95.2957)  Acc@task: 50.0000 (47.1774)  time: 0.1805  data: 0.0005  max mem: 1380\n",
            "Test: [Task 10]  [40/42]  eta: 0:00:00  Loss: 1.1083 (1.1467)  Acc@1: 66.6667 (69.1057)  Acc@5: 95.8333 (95.3252)  Acc@task: 45.8333 (45.7317)  time: 0.1816  data: 0.0005  max mem: 1380\n",
            "Test: [Task 10]  [41/42]  eta: 0:00:00  Loss: 1.1323 (1.1464)  Acc@1: 66.6667 (69.2000)  Acc@5: 95.8333 (95.3000)  Acc@task: 45.8333 (45.9000)  time: 0.1779  data: 0.0005  max mem: 1380\n",
            "Test: [Task 10] Total time: 0:00:08 (0.1913 s / it)\n",
            "* Acc@task 45.900 Acc@1 69.200 Acc@5 95.300 loss 1.146\n",
            "[Average accuracy till task10]\tAcc@task: 69.3400\tAcc@1: 75.7600\tAcc@5: 94.5700\tLoss: 1.0446\tForgetting: 0.0000\tBackward: 76.4889\n",
            "Total training time: 0:43:17\n",
            "[rank0]:[W1008 15:12:52.181775449 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
          ]
        }
      ],
      "source": [
        "!torchrun --nproc_per_node=1 main.py cifar100_hideprompt_5e --original_model vit_small_patch16_224.dino --model vit_small_patch16_224.dino --batch-size 24 --epochs 10 --crct_epochs 10 --seed 20 --ca_lr 0.005 --prompt_momentum 0.1 --reg 0.1 --length 5 --larger_prompt_lr --data-path ./datasets/ --trained_original_model ./output/cifar100_full_dino_5epoch_100pct --output_dir ./output/cifar100_full_dino_5epoch_final_100pct --pct 1.0\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}