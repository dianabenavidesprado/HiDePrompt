{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1c65321c",
      "metadata": {
        "id": "1c65321c",
        "outputId": "783e5c2c-73d9-48f8-8bb9-bbd84d6014f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32512"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import os\n",
        "os.system('cmd command')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4842b918",
      "metadata": {
        "id": "4842b918",
        "outputId": "dbae3819-d912-44c9-c4e3-757b673bf4e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "31ecb10e",
      "metadata": {
        "id": "31ecb10e",
        "outputId": "eabb85f8-217c-4bed-f22c-9b780bb03abd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/HiDePrompt\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/HiDePrompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd565e2a",
      "metadata": {
        "id": "cd565e2a"
      },
      "outputs": [],
      "source": [
        "!pip install -q condacolab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a2473b5",
      "metadata": {
        "id": "6a2473b5",
        "outputId": "e435931a-a6cb-4f7a-f09c-fd79432ddd86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬ Downloading https://github.com/jaimergp/miniforge/releases/download/24.11.2-1_colab/Miniforge3-colab-24.11.2-1_colab-Linux-x86_64.sh...\n",
            "ğŸ“¦ Installing...\n",
            "ğŸ“Œ Adjusting configuration...\n",
            "ğŸ©¹ Patching environment...\n",
            "â² Done in 0:00:11\n",
            "ğŸ” Restarting kernel...\n"
          ]
        }
      ],
      "source": [
        "import condacolab\n",
        "condacolab.install()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/HiDePrompt/HiDePrompt"
      ],
      "metadata": {
        "id": "4aBTrmkLHGUx",
        "outputId": "b46ddfb0-bba3-4fd6-9a5d-4077e7a9a6f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "4aBTrmkLHGUx",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59ddd81a",
      "metadata": {
        "id": "59ddd81a",
        "outputId": "2d958915-469d-49c8-90be-f3d2d67d4cfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm (from -r requirements.txt (line 1))\n",
            "  Downloading timm-1.0.20-py3-none-any.whl.metadata (61 kB)\n",
            "Collecting pillow (from -r requirements.txt (line 2))\n",
            "  Downloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting matplotlib (from -r requirements.txt (line 3))\n",
            "  Downloading matplotlib-3.10.6-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
            "Collecting torchprofile (from -r requirements.txt (line 4))\n",
            "  Downloading torchprofile-0.0.4-py3-none-any.whl.metadata (303 bytes)\n",
            "Collecting torch (from -r requirements.txt (line 5))\n",
            "  Downloading torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
            "Collecting torchvision (from -r requirements.txt (line 6))\n",
            "  Downloading torchvision-0.23.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (2.3.0)\n",
            "Collecting scipy (from -r requirements.txt (line 8))\n",
            "  Downloading scipy-1.16.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
            "Collecting scikit-learn (from -r requirements.txt (line 9))\n",
            "  Downloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
            "Collecting numpy (from -r requirements.txt (line 10))\n",
            "  Downloading numpy-2.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "Collecting pyyaml (from timm->-r requirements.txt (line 1))\n",
            "  Downloading pyyaml-6.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)\n",
            "Collecting huggingface_hub (from timm->-r requirements.txt (line 1))\n",
            "  Downloading huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting safetensors (from timm->-r requirements.txt (line 1))\n",
            "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib->-r requirements.txt (line 3))\n",
            "  Downloading contourpy-1.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 3))\n",
            "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib->-r requirements.txt (line 3))\n",
            "  Downloading fonttools-4.60.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (111 kB)\n",
            "Collecting kiwisolver>=1.3.1 (from matplotlib->-r requirements.txt (line 3))\n",
            "  Downloading kiwisolver-1.4.9-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 3)) (24.2)\n",
            "Collecting pyparsing>=2.3.1 (from matplotlib->-r requirements.txt (line 3))\n",
            "  Downloading pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting python-dateutil>=2.7 (from matplotlib->-r requirements.txt (line 3))\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting filelock (from torch->-r requirements.txt (line 5))\n",
            "  Downloading filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting typing-extensions>=4.10.0 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting sympy>=1.13.3 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting networkx (from torch->-r requirements.txt (line 5))\n",
            "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting jinja2 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting fsspec (from torch->-r requirements.txt (line 5))\n",
            "  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.9.90 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Collecting nvidia-nccl-cu12==2.27.3 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.8.90 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.4.0 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/site-packages (from triton==3.4.0->torch->-r requirements.txt (line 5)) (65.6.3)\n",
            "Collecting joblib>=1.2.0 (from scikit-learn->-r requirements.txt (line 9))\n",
            "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn->-r requirements.txt (line 9))\n",
            "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting six>=1.5 (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 3))\n",
            "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch->-r requirements.txt (line 5))\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/site-packages (from huggingface_hub->timm->-r requirements.txt (line 1)) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/site-packages (from huggingface_hub->timm->-r requirements.txt (line 1)) (4.67.1)\n",
            "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface_hub->timm->-r requirements.txt (line 1))\n",
            "  Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch->-r requirements.txt (line 5))\n",
            "  Downloading markupsafe-3.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 1)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 1)) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 1)) (2024.12.14)\n",
            "Downloading timm-1.0.20-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m141.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.10.6-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m179.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchprofile-0.0.4-py3-none-any.whl (7.7 kB)\n",
            "Downloading torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl (888.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m888.1/888.1 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m116.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "Downloading triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m155.5/155.5 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.23.0-cp311-cp311-manylinux_2_28_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m101.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.16.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m35.9/35.9 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m107.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m112.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading contourpy-1.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (355 kB)\n",
            "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading fonttools-4.60.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (5.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m96.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
            "Downloading kiwisolver-1.4.9-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
            "Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
            "Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\n",
            "Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
            "Downloading huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyyaml-6.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (806 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m806.6/806.6 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
            "Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markupsafe-3.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\n",
            "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: nvidia-cusparselt-cu12, mpmath, typing-extensions, triton, threadpoolctl, sympy, six, safetensors, pyyaml, pyparsing, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, kiwisolver, joblib, hf-xet, fsspec, fonttools, filelock, cycler, scipy, python-dateutil, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, jinja2, huggingface_hub, contourpy, scikit-learn, nvidia-cusolver-cu12, matplotlib, torch, torchvision, torchprofile, timm\n",
            "Successfully installed MarkupSafe-3.0.3 contourpy-1.3.3 cycler-0.12.1 filelock-3.19.1 fonttools-4.60.0 fsspec-2025.9.0 hf-xet-1.1.10 huggingface_hub-0.35.3 jinja2-3.1.6 joblib-1.5.2 kiwisolver-1.4.9 matplotlib-3.10.6 mpmath-1.3.0 networkx-3.5 numpy-2.3.3 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 pillow-11.3.0 pyparsing-3.2.5 python-dateutil-2.9.0.post0 pyyaml-6.0.3 safetensors-0.6.2 scikit-learn-1.7.2 scipy-1.16.2 six-1.17.0 sympy-1.14.0 threadpoolctl-3.6.0 timm-1.0.20 torch-2.8.0 torchprofile-0.0.4 torchvision-0.23.0 triton-3.4.0 typing-extensions-4.15.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "cycler",
                  "dateutil",
                  "kiwisolver",
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy",
                  "pyparsing",
                  "six"
                ]
              },
              "id": "5e9c917c5f81486e8bc6a6c737f1745b"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/HiDePrompt/HiDePrompt/"
      ],
      "metadata": {
        "id": "nTIVenS1Ih7Q",
        "outputId": "f4ff3256-da16-42f4-c3a3-50b09556b85f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "nTIVenS1Ih7Q",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "383f6206",
      "metadata": {
        "id": "383f6206",
        "outputId": "2dfc27b4-fc2b-4de1-c59f-4d901648aa4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(subparser_name='cifar100_hideprompt_5e', pct=0.25, batch_size=24, epochs=1, original_model='vit_small_patch16_224.dino', model='vit_small_patch16_224.dino', input_size=224, pretrained=True, drop=0.0, drop_path=0.0, opt='adam', opt_eps=1e-08, opt_betas=(0.9, 0.999), clip_grad=1.0, momentum=0.9, weight_decay=0.0, reinit_optimizer=True, sched='constant', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=0, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, unscale_lr=True, color_jitter=None, aa=None, smoothing=0.1, train_interpolation='bicubic', reprob=0.0, remode='pixel', recount=1, data_path='./datasets/', dataset='Split-CIFAR100', shuffle=False, output_dir='./output/cifar100_full_dino_1epoch_25pct', device='cuda', seed=20, eval=False, num_workers=4, pin_mem=True, world_size=1, dist_url='env://', num_tasks=10, train_mask=True, task_inc=False, use_g_prompt=False, g_prompt_length=5, g_prompt_layer_idx=[], use_prefix_tune_for_g_prompt=False, use_e_prompt=True, e_prompt_layer_idx=[0, 1, 2, 3, 4], use_prefix_tune_for_e_prompt=True, larger_prompt_lr=False, prompt_pool=True, size=10, length=20, top_k=1, initializer='uniform', prompt_key=False, prompt_key_init='uniform', use_prompt_mask=True, mask_first_epoch=False, shared_prompt_pool=True, shared_prompt_key=False, batchwise_prompt=False, embedding_key='cls', predefined_key='', pull_constraint=True, pull_constraint_coeff=1.0, same_key_value=False, global_pool='token', head_type='token', freeze=['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed'], crct_epochs=30, train_inference_task_only=True, original_model_mlp_structure=[2], ca_lr=0.005, milestones=[10], trained_original_model='', prompt_momentum=0.01, reg=0.01, not_train_ca=False, ca_epochs=30, ca_storage_efficient_method='multi-centroid', n_centroids=10, print_freq=10, config='cifar100_hideprompt_5e')\n",
            "| distributed init (rank 0): env://\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "[rank0]:[W1006 10:39:32.824313923 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/helpers.py:7: FutureWarning: Importing from timm.models.helpers is deprecated, please import via timm.models\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:886: UserWarning: Overwriting vit_tiny_patch16_224 in registry with vits.hide_prompt_vision_transformer.vit_tiny_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:895: UserWarning: Overwriting vit_tiny_patch16_384 in registry with vits.hide_prompt_vision_transformer.vit_tiny_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:904: UserWarning: Overwriting vit_small_patch32_224 in registry with vits.hide_prompt_vision_transformer.vit_small_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:913: UserWarning: Overwriting vit_small_patch32_384 in registry with vits.hide_prompt_vision_transformer.vit_small_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:922: UserWarning: Overwriting vit_small_patch16_224 in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:932: UserWarning: Overwriting vit_small_patch16_384 in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:942: UserWarning: Overwriting vit_base_patch32_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:952: UserWarning: Overwriting vit_base_patch32_384 in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:962: UserWarning: Overwriting vit_base_patch16_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:972: UserWarning: Overwriting vit_base_patch16_384 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:982: UserWarning: Overwriting vit_base_patch8_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:992: UserWarning: Overwriting vit_large_patch32_224 in registry with vits.hide_prompt_vision_transformer.vit_large_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1001: UserWarning: Overwriting vit_large_patch32_384 in registry with vits.hide_prompt_vision_transformer.vit_large_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1011: UserWarning: Overwriting vit_large_patch16_224 in registry with vits.hide_prompt_vision_transformer.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1021: UserWarning: Overwriting vit_large_patch16_384 in registry with vits.hide_prompt_vision_transformer.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1031: UserWarning: Overwriting vit_large_patch14_224 in registry with vits.hide_prompt_vision_transformer.vit_large_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1040: UserWarning: Overwriting vit_huge_patch14_224 in registry with vits.hide_prompt_vision_transformer.vit_huge_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1049: UserWarning: Overwriting vit_giant_patch14_224 in registry with vits.hide_prompt_vision_transformer.vit_giant_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1058: UserWarning: Overwriting vit_gigantic_patch14_224 in registry with vits.hide_prompt_vision_transformer.vit_gigantic_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1067: UserWarning: Overwriting vit_tiny_patch16_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_tiny_patch16_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1078: UserWarning: Overwriting vit_small_patch32_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_small_patch32_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1089: UserWarning: Overwriting vit_small_patch16_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1100: UserWarning: Overwriting vit_base_patch32_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1111: UserWarning: Overwriting vit_base_patch16_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1122: UserWarning: Overwriting vit_base_patch8_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_base_patch8_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1133: UserWarning: Overwriting vit_large_patch32_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_large_patch32_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1144: UserWarning: Overwriting vit_large_patch16_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_large_patch16_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1155: UserWarning: Overwriting vit_huge_patch14_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_huge_patch14_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1166: UserWarning: Overwriting vit_base_patch16_224_sam in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_sam. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1175: UserWarning: Overwriting vit_base_patch32_224_sam in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_224_sam. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1184: UserWarning: Overwriting vit_small_patch16_224_dino in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_224_dino. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1193: UserWarning: Overwriting vit_small_patch8_224_dino in registry with vits.hide_prompt_vision_transformer.vit_small_patch8_224_dino. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1211: UserWarning: Overwriting vit_base_patch8_224_dino in registry with vits.hide_prompt_vision_transformer.vit_base_patch8_224_dino. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1220: UserWarning: Overwriting vit_base_patch16_224_miil_in21k in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_miil_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1230: UserWarning: Overwriting vit_base_patch16_224_miil in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_miil. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1242: UserWarning: Overwriting vit_base_patch32_plus_256 in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_plus_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1251: UserWarning: Overwriting vit_base_patch16_plus_240 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_plus_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1260: UserWarning: Overwriting vit_base_patch16_rpn_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_rpn_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1271: UserWarning: Overwriting vit_small_patch16_36x1_224 in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_36x1_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1282: UserWarning: Overwriting vit_small_patch16_18x2_224 in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1294: UserWarning: Overwriting vit_base_patch16_18x2_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1331: UserWarning: Overwriting vit_base_patch16_224_dino in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_dino. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "Original train size:  50000\n",
            "Sampled train size:  12500\n",
            "Original train size:  50000\n",
            "Sampled train size:  12500\n",
            "100\n",
            "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, 13, 14, 15, 16, 17, 18, 19], [20, 21, 22, 23, 24, 25, 26, 27, 28, 29], [30, 31, 32, 33, 34, 35, 36, 37, 38, 39], [40, 41, 42, 43, 44, 45, 46, 47, 48, 49], [50, 51, 52, 53, 54, 55, 56, 57, 58, 59], [60, 61, 62, 63, 64, 65, 66, 67, 68, 69], [70, 71, 72, 73, 74, 75, 76, 77, 78, 79], [80, 81, 82, 83, 84, 85, 86, 87, 88, 89], [90, 91, 92, 93, 94, 95, 96, 97, 98, 99]]\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Creating original model: vit_small_patch16_224.dino\n",
            "[Sequential(\n",
            "  (0): Linear(in_features=384, out_features=768, bias=True)\n",
            "  (1): GELU(approximate='none')\n",
            "  (2): Dropout(p=0.0, inplace=False)\n",
            "), Sequential(\n",
            "  (0): Linear(in_features=768, out_features=384, bias=True)\n",
            "  (1): Dropout(p=0.0, inplace=False)\n",
            ")]\n",
            "model.safetensors: 100% 86.7M/86.7M [00:01<00:00, 44.8MB/s]\n",
            "Namespace(subparser_name='cifar100_hideprompt_5e', pct=0.25, batch_size=24, epochs=1, original_model='vit_small_patch16_224.dino', model='vit_small_patch16_224.dino', input_size=224, pretrained=True, drop=0.0, drop_path=0.0, opt='adam', opt_eps=1e-08, opt_betas=(0.9, 0.999), clip_grad=1.0, momentum=0.9, weight_decay=0.0, reinit_optimizer=True, sched='constant', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=0, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, unscale_lr=True, color_jitter=None, aa=None, smoothing=0.1, train_interpolation='bicubic', reprob=0.0, remode='pixel', recount=1, data_path='./datasets/', dataset='Split-CIFAR100', shuffle=False, output_dir='./output/cifar100_full_dino_1epoch_25pct', device='cuda', seed=20, eval=False, num_workers=4, pin_mem=True, world_size=1, dist_url='env://', num_tasks=10, train_mask=True, task_inc=False, use_g_prompt=False, g_prompt_length=5, g_prompt_layer_idx=[], use_prefix_tune_for_g_prompt=False, use_e_prompt=True, e_prompt_layer_idx=[0, 1, 2, 3, 4], use_prefix_tune_for_e_prompt=True, larger_prompt_lr=False, prompt_pool=True, size=10, length=20, top_k=1, initializer='uniform', prompt_key=False, prompt_key_init='uniform', use_prompt_mask=True, mask_first_epoch=False, shared_prompt_pool=True, shared_prompt_key=False, batchwise_prompt=False, embedding_key='cls', predefined_key='', pull_constraint=True, pull_constraint_coeff=1.0, same_key_value=False, global_pool='token', head_type='token', freeze=['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed'], crct_epochs=30, train_inference_task_only=True, original_model_mlp_structure=[2], ca_lr=0.005, milestones=[10], trained_original_model='', prompt_momentum=0.01, reg=0.01, not_train_ca=False, ca_epochs=30, ca_storage_efficient_method='multi-centroid', n_centroids=10, print_freq=10, config='cifar100_hideprompt_5e', rank=0, gpu=0, distributed=True, dist_backend='nccl', nb_classes=100)\n",
            "number of params: 630244\n",
            "Start training for 1 epochs\n",
            "Train: Epoch[1/1]  [ 0/54]  eta: 0:03:19  Lr: 0.000047  Loss: 2.2111  Acc@1: 12.5000 (12.5000)  Acc@5: 62.5000 (62.5000)  time: 3.6929  data: 1.8664  max mem: 196\n",
            "Train: Epoch[1/1]  [10/54]  eta: 0:00:18  Lr: 0.000047  Loss: 2.3806  Acc@1: 8.3333 (9.8485)  Acc@5: 54.1667 (54.9242)  time: 0.4170  data: 0.1709  max mem: 217\n",
            "Train: Epoch[1/1]  [20/54]  eta: 0:00:08  Lr: 0.000047  Loss: 2.2170  Acc@1: 8.3333 (11.5079)  Acc@5: 54.1667 (57.1429)  time: 0.0926  data: 0.0018  max mem: 217\n",
            "Train: Epoch[1/1]  [30/54]  eta: 0:00:05  Lr: 0.000047  Loss: 2.1468  Acc@1: 20.8333 (15.5914)  Acc@5: 66.6667 (61.6936)  time: 0.0947  data: 0.0015  max mem: 217\n",
            "Train: Epoch[1/1]  [40/54]  eta: 0:00:02  Lr: 0.000047  Loss: 2.0726  Acc@1: 25.0000 (19.3089)  Acc@5: 75.0000 (64.8374)  time: 0.0895  data: 0.0010  max mem: 217\n",
            "Train: Epoch[1/1]  [50/54]  eta: 0:00:00  Lr: 0.000047  Loss: 2.0072  Acc@1: 33.3333 (22.4673)  Acc@5: 79.1667 (68.0556)  time: 0.0813  data: 0.0006  max mem: 217\n",
            "Train: Epoch[1/1]  [53/54]  eta: 0:00:00  Lr: 0.000047  Loss: 2.1266  Acc@1: 33.3333 (23.5981)  Acc@5: 79.1667 (68.3801)  time: 0.0798  data: 0.0004  max mem: 217\n",
            "Train: Epoch[1/1] Total time: 0:00:08 (0.1563 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000047  Loss: 2.1266  Acc@1: 33.3333 (23.5981)  Acc@5: 79.1667 (68.3801)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:16  Loss: 4.3314 (4.3314)  Acc@1: 12.5000 (12.5000)  Acc@5: 16.6667 (16.6667)  time: 0.3826  data: 0.3067  max mem: 217\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 4.3340 (4.3038)  Acc@1: 8.3333 (9.0909)  Acc@5: 20.8333 (25.7576)  time: 0.1023  data: 0.0289  max mem: 218\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:01  Loss: 4.3082 (4.3040)  Acc@1: 4.1667 (7.9365)  Acc@5: 20.8333 (25.3968)  time: 0.0741  data: 0.0009  max mem: 218\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 4.2850 (4.2925)  Acc@1: 4.1667 (7.9301)  Acc@5: 25.0000 (27.2849)  time: 0.0743  data: 0.0007  max mem: 218\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 4.2576 (4.2889)  Acc@1: 8.3333 (8.3333)  Acc@5: 29.1667 (27.3374)  time: 0.0740  data: 0.0005  max mem: 218\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 4.2576 (4.2888)  Acc@1: 8.3333 (8.3000)  Acc@5: 29.1667 (27.5000)  time: 0.0771  data: 0.0005  max mem: 218\n",
            "Test: [Task 1] Total time: 0:00:03 (0.0846 s / it)\n",
            "* Acc@1 8.300 Acc@5 27.500 loss 4.289\n",
            "[Average accuracy till task1]\tAcc@1: 8.3000\tAcc@5: 27.5000\tLoss: 4.2888\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:13  Loss: 4.3314 (4.3314)  Acc@1: 12.5000 (12.5000)  Acc@5: 16.6667 (16.6667)  time: 0.3170  data: 0.2479  max mem: 239\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 4.3340 (4.3038)  Acc@1: 8.3333 (9.0909)  Acc@5: 20.8333 (25.7576)  time: 0.0980  data: 0.0254  max mem: 239\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:01  Loss: 4.3082 (4.3040)  Acc@1: 4.1667 (7.9365)  Acc@5: 20.8333 (25.3968)  time: 0.0755  data: 0.0019  max mem: 239\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:00  Loss: 4.2850 (4.2925)  Acc@1: 4.1667 (7.9301)  Acc@5: 25.0000 (27.2849)  time: 0.0748  data: 0.0007  max mem: 239\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 4.2576 (4.2889)  Acc@1: 8.3333 (8.3333)  Acc@5: 29.1667 (27.3374)  time: 0.0745  data: 0.0005  max mem: 239\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 4.2576 (4.2888)  Acc@1: 8.3333 (8.3000)  Acc@5: 29.1667 (27.5000)  time: 0.0732  data: 0.0004  max mem: 239\n",
            "Test: [Task 1] Total time: 0:00:03 (0.0820 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@1 8.300 Acc@5 27.500 loss 4.289\n",
            "[Average accuracy till task1]\tAcc@1: 8.3000\tAcc@5: 27.5000\tLoss: 4.2888\n",
            "Train: Epoch[1/1]  [ 0/52]  eta: 0:00:18  Lr: 0.000047  Loss: 2.2397  Acc@1: 25.0000 (25.0000)  Acc@5: 70.8333 (70.8333)  time: 0.3502  data: 0.2625  max mem: 239\n",
            "Train: Epoch[1/1]  [10/52]  eta: 0:00:05  Lr: 0.000047  Loss: 2.5044  Acc@1: 8.3333 (11.3636)  Acc@5: 50.0000 (50.3788)  time: 0.1235  data: 0.0442  max mem: 239\n",
            "Train: Epoch[1/1]  [20/52]  eta: 0:00:03  Lr: 0.000047  Loss: 2.3589  Acc@1: 12.5000 (14.0873)  Acc@5: 54.1667 (55.9524)  time: 0.0898  data: 0.0121  max mem: 239\n",
            "Train: Epoch[1/1]  [30/52]  eta: 0:00:02  Lr: 0.000047  Loss: 2.2267  Acc@1: 20.8333 (16.1290)  Acc@5: 62.5000 (59.1398)  time: 0.0786  data: 0.0016  max mem: 239\n",
            "Train: Epoch[1/1]  [40/52]  eta: 0:00:01  Lr: 0.000047  Loss: 2.1578  Acc@1: 20.8333 (18.1911)  Acc@5: 70.8333 (63.0081)  time: 0.0786  data: 0.0014  max mem: 239\n",
            "Train: Epoch[1/1]  [50/52]  eta: 0:00:00  Lr: 0.000047  Loss: 2.1480  Acc@1: 29.1667 (20.0163)  Acc@5: 70.8333 (64.7876)  time: 0.0776  data: 0.0009  max mem: 239\n",
            "Train: Epoch[1/1]  [51/52]  eta: 0:00:00  Lr: 0.000047  Loss: 1.9304  Acc@1: 29.1667 (20.0816)  Acc@5: 70.8333 (64.8163)  time: 0.0766  data: 0.0008  max mem: 239\n",
            "Train: Epoch[1/1] Total time: 0:00:04 (0.0889 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.9304  Acc@1: 29.1667 (20.0816)  Acc@5: 70.8333 (64.8163)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:13  Loss: 4.3125 (4.3125)  Acc@1: 12.5000 (12.5000)  Acc@5: 20.8333 (20.8333)  time: 0.3288  data: 0.2548  max mem: 239\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 4.2950 (4.2780)  Acc@1: 12.5000 (11.3636)  Acc@5: 25.0000 (27.6515)  time: 0.1027  data: 0.0279  max mem: 239\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 4.2804 (4.2778)  Acc@1: 8.3333 (9.7222)  Acc@5: 25.0000 (28.7698)  time: 0.0791  data: 0.0032  max mem: 239\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 4.2654 (4.2659)  Acc@1: 8.3333 (9.8118)  Acc@5: 29.1667 (30.2419)  time: 0.0814  data: 0.0011  max mem: 239\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 4.2481 (4.2624)  Acc@1: 8.3333 (9.9593)  Acc@5: 33.3333 (30.9959)  time: 0.0871  data: 0.0038  max mem: 239\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 4.2481 (4.2621)  Acc@1: 8.3333 (9.9000)  Acc@5: 33.3333 (31.1000)  time: 0.0860  data: 0.0038  max mem: 239\n",
            "Test: [Task 1] Total time: 0:00:03 (0.0914 s / it)\n",
            "* Acc@1 9.900 Acc@5 31.100 loss 4.262\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:20  Loss: 4.3171 (4.3171)  Acc@1: 0.0000 (0.0000)  Acc@5: 12.5000 (12.5000)  time: 0.4981  data: 0.4197  max mem: 239\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:03  Loss: 4.4050 (4.3834)  Acc@1: 4.1667 (5.3030)  Acc@5: 16.6667 (15.1515)  time: 0.1218  data: 0.0441  max mem: 239\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 4.3256 (4.3597)  Acc@1: 4.1667 (6.5476)  Acc@5: 20.8333 (19.8413)  time: 0.0806  data: 0.0044  max mem: 239\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 4.3126 (4.3621)  Acc@1: 4.1667 (5.7796)  Acc@5: 25.0000 (20.0269)  time: 0.0761  data: 0.0015  max mem: 239\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 4.3672 (4.3662)  Acc@1: 4.1667 (5.2846)  Acc@5: 20.8333 (19.4106)  time: 0.0750  data: 0.0006  max mem: 239\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 4.3672 (4.3616)  Acc@1: 4.1667 (5.4000)  Acc@5: 20.8333 (19.7000)  time: 0.0737  data: 0.0005  max mem: 239\n",
            "Test: [Task 2] Total time: 0:00:03 (0.0893 s / it)\n",
            "* Acc@1 5.400 Acc@5 19.700 loss 4.362\n",
            "[Average accuracy till task2]\tAcc@1: 7.6500\tAcc@5: 25.4000\tLoss: 4.3119\tForgetting: 0.0000\tBackward: 1.6000\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([4776, 384])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 2.6740  Acc@1: 25.0000 (27.0833)  Acc@5: 66.6667 (70.0000)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000467  Loss: 2.6161  Acc@1: 29.1667 (30.0000)  Acc@5: 66.6667 (68.3333)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000464  Loss: 2.4786  Acc@1: 45.8333 (41.6667)  Acc@5: 79.1667 (79.5833)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 2.4467  Acc@1: 54.1667 (59.5833)  Acc@5: 87.5000 (88.7500)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000448  Loss: 2.2599  Acc@1: 58.3333 (58.7500)  Acc@5: 87.5000 (88.7500)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000437  Loss: 2.0376  Acc@1: 62.5000 (64.1667)  Acc@5: 91.6667 (92.9167)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 1.9820  Acc@1: 62.5000 (63.7500)  Acc@5: 91.6667 (93.3333)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000409  Loss: 2.0488  Acc@1: 70.8333 (71.2500)  Acc@5: 91.6667 (93.7500)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000391  Loss: 1.8398  Acc@1: 75.0000 (77.0833)  Acc@5: 95.8333 (96.6667)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 1.9501  Acc@1: 75.0000 (77.0833)  Acc@5: 100.0000 (98.3333)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000352  Loss: 1.8252  Acc@1: 79.1667 (81.2500)  Acc@5: 100.0000 (97.9167)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000330  Loss: 1.7160  Acc@1: 87.5000 (87.0833)  Acc@5: 95.8333 (97.5000)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 1.8349  Acc@1: 83.3333 (83.7500)  Acc@5: 100.0000 (98.3333)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000283  Loss: 1.7048  Acc@1: 87.5000 (83.7500)  Acc@5: 95.8333 (97.0833)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000259  Loss: 1.6107  Acc@1: 83.3333 (82.9167)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 1.4659  Acc@1: 87.5000 (88.7500)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000210  Loss: 1.4342  Acc@1: 91.6667 (90.8333)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000186  Loss: 1.4854  Acc@1: 83.3333 (82.5000)  Acc@5: 100.0000 (98.3333)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 1.3172  Acc@1: 83.3333 (85.8333)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000139  Loss: 1.3042  Acc@1: 83.3333 (85.0000)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000117  Loss: 1.4604  Acc@1: 87.5000 (85.0000)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 1.4300  Acc@1: 87.5000 (89.5833)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000078  Loss: 1.3639  Acc@1: 87.5000 (88.3333)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000060  Loss: 1.4611  Acc@1: 87.5000 (86.2500)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 1.2910  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000031  Loss: 1.4558  Acc@1: 87.5000 (86.6667)  Acc@5: 100.0000 (98.3333)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000020  Loss: 1.3516  Acc@1: 87.5000 (86.2500)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 1.3752  Acc@1: 83.3333 (85.4167)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000005  Loss: 1.5140  Acc@1: 87.5000 (87.9167)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([4776, 384])\n",
            "Averaged stats: Lr: 0.000001  Loss: 1.3157  Acc@1: 87.5000 (88.3333)  Acc@5: 100.0000 (99.5833)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:28  Loss: 2.5873 (2.5873)  Acc@1: 83.3333 (83.3333)  Acc@5: 91.6667 (91.6667)  time: 0.6776  data: 0.5848  max mem: 245\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 2.7358 (2.6814)  Acc@1: 83.3333 (80.3030)  Acc@5: 95.8333 (95.0758)  time: 0.1429  data: 0.0651  max mem: 245\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 2.7358 (2.6808)  Acc@1: 79.1667 (81.1508)  Acc@5: 95.8333 (94.4444)  time: 0.0895  data: 0.0128  max mem: 245\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 2.6131 (2.6500)  Acc@1: 83.3333 (83.0645)  Acc@5: 95.8333 (95.2957)  time: 0.0874  data: 0.0082  max mem: 245\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 2.5790 (2.6297)  Acc@1: 87.5000 (84.1463)  Acc@5: 95.8333 (95.5285)  time: 0.0809  data: 0.0022  max mem: 245\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 2.5678 (2.6263)  Acc@1: 87.5000 (84.2000)  Acc@5: 95.8333 (95.5000)  time: 0.0795  data: 0.0020  max mem: 245\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1005 s / it)\n",
            "* Acc@1 84.200 Acc@5 95.500 loss 2.626\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:15  Loss: 2.9070 (2.9070)  Acc@1: 83.3333 (83.3333)  Acc@5: 87.5000 (87.5000)  time: 0.3629  data: 0.2858  max mem: 245\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:03  Loss: 2.9070 (2.9395)  Acc@1: 75.0000 (76.8939)  Acc@5: 91.6667 (92.0455)  time: 0.1028  data: 0.0272  max mem: 245\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:01  Loss: 2.9031 (2.9346)  Acc@1: 75.0000 (75.7937)  Acc@5: 91.6667 (91.8651)  time: 0.0765  data: 0.0009  max mem: 245\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 2.9527 (2.9241)  Acc@1: 75.0000 (76.3441)  Acc@5: 91.6667 (91.9355)  time: 0.0763  data: 0.0004  max mem: 245\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 2.8902 (2.9219)  Acc@1: 79.1667 (76.3211)  Acc@5: 95.8333 (92.2764)  time: 0.0761  data: 0.0003  max mem: 245\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 2.8899 (2.9176)  Acc@1: 79.1667 (76.4000)  Acc@5: 95.8333 (92.3000)  time: 0.0748  data: 0.0003  max mem: 245\n",
            "Test: [Task 2] Total time: 0:00:03 (0.0843 s / it)\n",
            "* Acc@1 76.400 Acc@5 92.300 loss 2.918\n",
            "[Average accuracy till task2]\tAcc@1: 80.3000\tAcc@5: 93.9000\tLoss: 2.7719\tForgetting: 0.0000\tBackward: 75.9000\n",
            "Train: Epoch[1/1]  [ 0/54]  eta: 0:00:30  Lr: 0.000047  Loss: 2.2738  Acc@1: 12.5000 (12.5000)  Acc@5: 54.1667 (54.1667)  time: 0.5721  data: 0.4754  max mem: 245\n",
            "Train: Epoch[1/1]  [10/54]  eta: 0:00:05  Lr: 0.000047  Loss: 2.3626  Acc@1: 12.5000 (13.2576)  Acc@5: 54.1667 (54.9242)  time: 0.1257  data: 0.0440  max mem: 245\n",
            "Train: Epoch[1/1]  [20/54]  eta: 0:00:03  Lr: 0.000047  Loss: 2.2809  Acc@1: 12.5000 (14.2857)  Acc@5: 58.3333 (56.3492)  time: 0.0805  data: 0.0007  max mem: 245\n",
            "Train: Epoch[1/1]  [30/54]  eta: 0:00:02  Lr: 0.000047  Loss: 2.1570  Acc@1: 20.8333 (17.6075)  Acc@5: 58.3333 (59.6774)  time: 0.0803  data: 0.0008  max mem: 245\n",
            "Train: Epoch[1/1]  [40/54]  eta: 0:00:01  Lr: 0.000047  Loss: 2.0757  Acc@1: 25.0000 (20.1220)  Acc@5: 75.0000 (63.6179)  time: 0.0807  data: 0.0008  max mem: 245\n",
            "Train: Epoch[1/1]  [50/54]  eta: 0:00:00  Lr: 0.000047  Loss: 2.0634  Acc@1: 29.1667 (23.0392)  Acc@5: 75.0000 (66.5850)  time: 0.0804  data: 0.0006  max mem: 245\n",
            "Train: Epoch[1/1]  [53/54]  eta: 0:00:00  Lr: 0.000047  Loss: 2.0830  Acc@1: 29.1667 (23.7354)  Acc@5: 75.0000 (67.2374)  time: 0.0820  data: 0.0006  max mem: 245\n",
            "Train: Epoch[1/1] Total time: 0:00:04 (0.0918 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 2.0830  Acc@1: 29.1667 (23.7354)  Acc@5: 75.0000 (67.2374)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:15  Loss: 2.5693 (2.5693)  Acc@1: 87.5000 (87.5000)  Acc@5: 91.6667 (91.6667)  time: 0.3719  data: 0.2592  max mem: 245\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 2.7001 (2.6587)  Acc@1: 79.1667 (78.4091)  Acc@5: 95.8333 (95.0758)  time: 0.1152  data: 0.0349  max mem: 245\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 2.7001 (2.6596)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (94.0476)  time: 0.0889  data: 0.0114  max mem: 245\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 2.5945 (2.6284)  Acc@1: 83.3333 (81.5860)  Acc@5: 95.8333 (95.0269)  time: 0.0885  data: 0.0105  max mem: 245\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 2.5553 (2.6083)  Acc@1: 87.5000 (83.0285)  Acc@5: 95.8333 (95.2236)  time: 0.0855  data: 0.0075  max mem: 245\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 2.5399 (2.6049)  Acc@1: 87.5000 (83.1000)  Acc@5: 95.8333 (95.2000)  time: 0.0794  data: 0.0024  max mem: 245\n",
            "Test: [Task 1] Total time: 0:00:04 (0.0965 s / it)\n",
            "* Acc@1 83.100 Acc@5 95.200 loss 2.605\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:25  Loss: 2.8721 (2.8721)  Acc@1: 79.1667 (79.1667)  Acc@5: 87.5000 (87.5000)  time: 0.6145  data: 0.5450  max mem: 245\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 2.8721 (2.9032)  Acc@1: 75.0000 (76.5152)  Acc@5: 91.6667 (92.8030)  time: 0.1271  data: 0.0508  max mem: 245\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 2.8652 (2.8971)  Acc@1: 75.0000 (76.5873)  Acc@5: 91.6667 (92.4603)  time: 0.0785  data: 0.0012  max mem: 245\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 2.9091 (2.8874)  Acc@1: 75.0000 (77.1505)  Acc@5: 91.6667 (92.6075)  time: 0.0782  data: 0.0008  max mem: 245\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 2.8604 (2.8856)  Acc@1: 79.1667 (77.0325)  Acc@5: 95.8333 (93.0894)  time: 0.0776  data: 0.0006  max mem: 245\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 2.8473 (2.8810)  Acc@1: 83.3333 (77.2000)  Acc@5: 95.8333 (93.1000)  time: 0.0764  data: 0.0005  max mem: 245\n",
            "Test: [Task 2] Total time: 0:00:03 (0.0921 s / it)\n",
            "* Acc@1 77.200 Acc@5 93.100 loss 2.881\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:18  Loss: 4.1978 (4.1978)  Acc@1: 0.0000 (0.0000)  Acc@5: 41.6667 (41.6667)  time: 0.4324  data: 0.3448  max mem: 245\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 4.1882 (4.1942)  Acc@1: 4.1667 (5.6818)  Acc@5: 29.1667 (32.1970)  time: 0.1099  data: 0.0327  max mem: 245\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 4.1800 (4.1969)  Acc@1: 4.1667 (7.1429)  Acc@5: 29.1667 (32.1429)  time: 0.0781  data: 0.0012  max mem: 245\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 4.1391 (4.1869)  Acc@1: 4.1667 (6.5860)  Acc@5: 37.5000 (32.9301)  time: 0.0782  data: 0.0007  max mem: 245\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 4.1836 (4.1973)  Acc@1: 4.1667 (6.0976)  Acc@5: 33.3333 (32.0122)  time: 0.0777  data: 0.0003  max mem: 245\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 4.1935 (4.1980)  Acc@1: 4.1667 (6.1000)  Acc@5: 33.3333 (32.1000)  time: 0.0764  data: 0.0003  max mem: 245\n",
            "Test: [Task 3] Total time: 0:00:03 (0.0876 s / it)\n",
            "* Acc@1 6.100 Acc@5 32.100 loss 4.198\n",
            "[Average accuracy till task3]\tAcc@1: 55.4667\tAcc@5: 73.4667\tLoss: 3.2280\tForgetting: 0.0000\tBackward: 73.3000\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([7152, 384])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 2.2027  Acc@1: 58.3333 (58.1250)  Acc@5: 79.1667 (81.6667)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000467  Loss: 2.0653  Acc@1: 62.5000 (65.0000)  Acc@5: 95.8333 (93.5417)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000464  Loss: 2.1984  Acc@1: 62.5000 (66.4583)  Acc@5: 91.6667 (93.7500)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 1.8475  Acc@1: 75.0000 (76.2500)  Acc@5: 95.8333 (95.6250)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000448  Loss: 1.4191  Acc@1: 75.0000 (78.3333)  Acc@5: 95.8333 (97.5000)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000437  Loss: 1.3951  Acc@1: 79.1667 (79.7917)  Acc@5: 100.0000 (97.7083)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 1.2187  Acc@1: 83.3333 (81.8750)  Acc@5: 100.0000 (97.7083)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000409  Loss: 1.1913  Acc@1: 83.3333 (83.9583)  Acc@5: 100.0000 (97.9167)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000391  Loss: 1.1617  Acc@1: 83.3333 (83.1250)  Acc@5: 100.0000 (98.5417)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 1.1160  Acc@1: 87.5000 (87.9167)  Acc@5: 100.0000 (98.9583)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000352  Loss: 1.2358  Acc@1: 83.3333 (86.2500)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000330  Loss: 1.0393  Acc@1: 87.5000 (88.1250)  Acc@5: 100.0000 (99.3750)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 1.0284  Acc@1: 91.6667 (89.5833)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000283  Loss: 1.0881  Acc@1: 91.6667 (90.8333)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000259  Loss: 0.7944  Acc@1: 91.6667 (92.0833)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 0.9288  Acc@1: 87.5000 (90.2083)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000210  Loss: 0.9886  Acc@1: 91.6667 (90.0000)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000186  Loss: 0.8364  Acc@1: 91.6667 (91.0417)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.8298  Acc@1: 91.6667 (90.8333)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000139  Loss: 0.8893  Acc@1: 91.6667 (92.2917)  Acc@5: 100.0000 (99.3750)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000117  Loss: 0.6627  Acc@1: 95.8333 (93.7500)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 1.0840  Acc@1: 91.6667 (92.9167)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000078  Loss: 0.8441  Acc@1: 91.6667 (90.4167)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000060  Loss: 0.8232  Acc@1: 91.6667 (90.8333)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 1.0990  Acc@1: 87.5000 (89.5833)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000031  Loss: 0.7405  Acc@1: 95.8333 (93.5417)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000020  Loss: 0.6934  Acc@1: 95.8333 (92.5000)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.8989  Acc@1: 91.6667 (91.4583)  Acc@5: 100.0000 (99.3750)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000005  Loss: 0.9811  Acc@1: 91.6667 (91.4583)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([7152, 384])\n",
            "Averaged stats: Lr: 0.000001  Loss: 0.7623  Acc@1: 95.8333 (93.1250)  Acc@5: 100.0000 (100.0000)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:12  Loss: 1.2420 (1.2420)  Acc@1: 87.5000 (87.5000)  Acc@5: 95.8333 (95.8333)  time: 0.2922  data: 0.2250  max mem: 257\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 1.4082 (1.3668)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (97.7273)  time: 0.1109  data: 0.0354  max mem: 257\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 1.4082 (1.3715)  Acc@1: 79.1667 (79.9603)  Acc@5: 100.0000 (97.4206)  time: 0.0865  data: 0.0085  max mem: 257\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 1.3323 (1.3340)  Acc@1: 87.5000 (82.5269)  Acc@5: 100.0000 (97.7151)  time: 0.0806  data: 0.0007  max mem: 257\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 1.2578 (1.3077)  Acc@1: 87.5000 (83.4350)  Acc@5: 100.0000 (97.7642)  time: 0.0803  data: 0.0006  max mem: 257\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 1.2544 (1.3030)  Acc@1: 87.5000 (83.5000)  Acc@5: 100.0000 (97.7000)  time: 0.0791  data: 0.0005  max mem: 257\n",
            "Test: [Task 1] Total time: 0:00:03 (0.0910 s / it)\n",
            "* Acc@1 83.500 Acc@5 97.700 loss 1.303\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:23  Loss: 1.5039 (1.5039)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  time: 0.5690  data: 0.4440  max mem: 257\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 1.4985 (1.4790)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (98.1061)  time: 0.1294  data: 0.0444  max mem: 257\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 1.4447 (1.4908)  Acc@1: 83.3333 (83.1349)  Acc@5: 95.8333 (97.0238)  time: 0.0831  data: 0.0028  max mem: 257\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 1.4414 (1.4676)  Acc@1: 83.3333 (84.1398)  Acc@5: 95.8333 (96.9086)  time: 0.0800  data: 0.0008  max mem: 257\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 1.4048 (1.4621)  Acc@1: 83.3333 (83.5366)  Acc@5: 100.0000 (97.2561)  time: 0.0791  data: 0.0002  max mem: 257\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 1.3635 (1.4557)  Acc@1: 87.5000 (83.7000)  Acc@5: 100.0000 (97.3000)  time: 0.0778  data: 0.0002  max mem: 257\n",
            "Test: [Task 2] Total time: 0:00:03 (0.0940 s / it)\n",
            "* Acc@1 83.700 Acc@5 97.300 loss 1.456\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:14  Loss: 1.5543 (1.5543)  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  time: 0.3478  data: 0.2774  max mem: 257\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 1.6772 (1.7115)  Acc@1: 75.0000 (76.5152)  Acc@5: 95.8333 (97.3485)  time: 0.1068  data: 0.0295  max mem: 257\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 1.6675 (1.6701)  Acc@1: 75.0000 (75.7937)  Acc@5: 95.8333 (97.6190)  time: 0.0808  data: 0.0025  max mem: 257\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 1.5861 (1.6284)  Acc@1: 79.1667 (77.4194)  Acc@5: 100.0000 (97.8495)  time: 0.0796  data: 0.0004  max mem: 257\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 1.5978 (1.6488)  Acc@1: 79.1667 (77.4390)  Acc@5: 95.8333 (97.3577)  time: 0.0798  data: 0.0004  max mem: 257\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 1.6798 (1.6544)  Acc@1: 79.1667 (77.3000)  Acc@5: 95.8333 (97.4000)  time: 0.0786  data: 0.0003  max mem: 257\n",
            "Test: [Task 3] Total time: 0:00:03 (0.0878 s / it)\n",
            "* Acc@1 77.300 Acc@5 97.400 loss 1.654\n",
            "[Average accuracy till task3]\tAcc@1: 81.5000\tAcc@5: 97.4667\tLoss: 1.4710\tForgetting: 0.3500\tBackward: 41.2500\n",
            "Train: Epoch[1/1]  [ 0/51]  eta: 0:00:26  Lr: 0.000047  Loss: 2.3710  Acc@1: 12.5000 (12.5000)  Acc@5: 58.3333 (58.3333)  time: 0.5260  data: 0.4017  max mem: 257\n",
            "Train: Epoch[1/1]  [10/51]  eta: 0:00:05  Lr: 0.000047  Loss: 2.2753  Acc@1: 8.3333 (7.9545)  Acc@5: 54.1667 (50.7576)  time: 0.1233  data: 0.0372  max mem: 257\n",
            "Train: Epoch[1/1]  [20/51]  eta: 0:00:03  Lr: 0.000047  Loss: 2.1981  Acc@1: 12.5000 (10.7143)  Acc@5: 54.1667 (53.3730)  time: 0.0825  data: 0.0011  max mem: 257\n",
            "Train: Epoch[1/1]  [30/51]  eta: 0:00:02  Lr: 0.000047  Loss: 2.1473  Acc@1: 16.6667 (15.1882)  Acc@5: 62.5000 (60.0806)  time: 0.0822  data: 0.0016  max mem: 257\n",
            "Train: Epoch[1/1]  [40/51]  eta: 0:00:01  Lr: 0.000047  Loss: 2.1616  Acc@1: 20.8333 (16.1585)  Acc@5: 70.8333 (62.2968)  time: 0.0832  data: 0.0018  max mem: 257\n",
            "Train: Epoch[1/1]  [50/51]  eta: 0:00:00  Lr: 0.000047  Loss: 2.1487  Acc@1: 20.8333 (19.0008)  Acc@5: 75.0000 (65.8477)  time: 0.0825  data: 0.0012  max mem: 257\n",
            "Train: Epoch[1/1] Total time: 0:00:04 (0.0937 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 2.1487  Acc@1: 20.8333 (19.0008)  Acc@5: 75.0000 (65.8477)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:31  Loss: 1.2316 (1.2316)  Acc@1: 87.5000 (87.5000)  Acc@5: 91.6667 (91.6667)  time: 0.7537  data: 0.6350  max mem: 257\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 1.3972 (1.3634)  Acc@1: 79.1667 (79.5455)  Acc@5: 100.0000 (97.3485)  time: 0.1436  data: 0.0580  max mem: 257\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 1.3972 (1.3673)  Acc@1: 79.1667 (80.1587)  Acc@5: 100.0000 (97.4206)  time: 0.0830  data: 0.0011  max mem: 257\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 1.3289 (1.3308)  Acc@1: 87.5000 (82.7957)  Acc@5: 100.0000 (97.7151)  time: 0.0823  data: 0.0015  max mem: 257\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 1.2554 (1.3049)  Acc@1: 87.5000 (83.6382)  Acc@5: 100.0000 (97.7642)  time: 0.0807  data: 0.0007  max mem: 257\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 1.2504 (1.3000)  Acc@1: 87.5000 (83.7000)  Acc@5: 100.0000 (97.7000)  time: 0.0792  data: 0.0006  max mem: 257\n",
            "Test: [Task 1] Total time: 0:00:04 (0.0993 s / it)\n",
            "* Acc@1 83.700 Acc@5 97.700 loss 1.300\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:12  Loss: 1.5106 (1.5106)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.2882  data: 0.2193  max mem: 257\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:03  Loss: 1.5044 (1.4911)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (98.4848)  time: 0.1146  data: 0.0328  max mem: 257\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 1.4665 (1.5017)  Acc@1: 83.3333 (82.9365)  Acc@5: 95.8333 (97.0238)  time: 0.0899  data: 0.0079  max mem: 257\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 1.4545 (1.4780)  Acc@1: 83.3333 (83.8710)  Acc@5: 95.8333 (96.9086)  time: 0.0811  data: 0.0011  max mem: 257\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 1.4130 (1.4716)  Acc@1: 83.3333 (83.4350)  Acc@5: 100.0000 (97.2561)  time: 0.0800  data: 0.0004  max mem: 257\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 1.3795 (1.4651)  Acc@1: 87.5000 (83.5000)  Acc@5: 100.0000 (97.3000)  time: 0.0789  data: 0.0004  max mem: 257\n",
            "Test: [Task 2] Total time: 0:00:03 (0.0909 s / it)\n",
            "* Acc@1 83.500 Acc@5 97.300 loss 1.465\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:14  Loss: 1.5608 (1.5608)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.3381  data: 0.2577  max mem: 257\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 1.6781 (1.7158)  Acc@1: 75.0000 (76.8939)  Acc@5: 95.8333 (97.3485)  time: 0.1141  data: 0.0361  max mem: 257\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 1.6697 (1.6765)  Acc@1: 75.0000 (75.7937)  Acc@5: 95.8333 (97.6190)  time: 0.0865  data: 0.0071  max mem: 257\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 1.5938 (1.6340)  Acc@1: 75.0000 (77.4194)  Acc@5: 100.0000 (97.8495)  time: 0.0813  data: 0.0004  max mem: 257\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 1.6023 (1.6544)  Acc@1: 79.1667 (77.2358)  Acc@5: 95.8333 (97.3577)  time: 0.0810  data: 0.0004  max mem: 257\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 1.6857 (1.6597)  Acc@1: 75.0000 (77.1000)  Acc@5: 95.8333 (97.4000)  time: 0.0798  data: 0.0004  max mem: 257\n",
            "Test: [Task 3] Total time: 0:00:03 (0.0908 s / it)\n",
            "* Acc@1 77.100 Acc@5 97.400 loss 1.660\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:15  Loss: 4.5826 (4.5826)  Acc@1: 4.1667 (4.1667)  Acc@5: 8.3333 (8.3333)  time: 0.3715  data: 0.2937  max mem: 257\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:03  Loss: 4.5197 (4.5087)  Acc@1: 0.0000 (0.3788)  Acc@5: 4.1667 (6.0606)  time: 0.1091  data: 0.0282  max mem: 257\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 4.5111 (4.5052)  Acc@1: 0.0000 (0.1984)  Acc@5: 4.1667 (6.3492)  time: 0.0822  data: 0.0017  max mem: 257\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 4.4829 (4.5054)  Acc@1: 0.0000 (0.2688)  Acc@5: 8.3333 (6.8548)  time: 0.0818  data: 0.0011  max mem: 257\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 4.4814 (4.4990)  Acc@1: 0.0000 (0.3049)  Acc@5: 4.1667 (6.6057)  time: 0.0818  data: 0.0003  max mem: 257\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 4.4796 (4.4984)  Acc@1: 0.0000 (0.3000)  Acc@5: 4.1667 (6.6000)  time: 0.0806  data: 0.0003  max mem: 257\n",
            "Test: [Task 4] Total time: 0:00:03 (0.0916 s / it)\n",
            "* Acc@1 0.300 Acc@5 6.600 loss 4.498\n",
            "[Average accuracy till task4]\tAcc@1: 61.1500\tAcc@5: 74.7500\tLoss: 2.2308\tForgetting: 0.0000\tBackward: 74.8333\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([9528, 384])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 1.4283  Acc@1: 70.8333 (69.7222)  Acc@5: 79.1667 (80.0000)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000467  Loss: 1.9889  Acc@1: 70.8333 (72.6389)  Acc@5: 87.5000 (87.6389)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000464  Loss: 0.9540  Acc@1: 75.0000 (72.6389)  Acc@5: 95.8333 (93.3333)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 1.3124  Acc@1: 83.3333 (79.4444)  Acc@5: 100.0000 (97.0833)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000448  Loss: 0.9326  Acc@1: 79.1667 (80.4167)  Acc@5: 95.8333 (97.5000)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000437  Loss: 0.7672  Acc@1: 87.5000 (86.3889)  Acc@5: 100.0000 (98.8889)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 0.9697  Acc@1: 87.5000 (86.2500)  Acc@5: 100.0000 (98.8889)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000409  Loss: 0.8485  Acc@1: 87.5000 (87.3611)  Acc@5: 95.8333 (97.7778)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000391  Loss: 0.7558  Acc@1: 87.5000 (89.3056)  Acc@5: 100.0000 (98.8889)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 0.8243  Acc@1: 91.6667 (90.2778)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000352  Loss: 0.5127  Acc@1: 91.6667 (90.1389)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000330  Loss: 0.6912  Acc@1: 91.6667 (90.0000)  Acc@5: 100.0000 (99.0278)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 0.6675  Acc@1: 91.6667 (91.8056)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000283  Loss: 0.6824  Acc@1: 91.6667 (92.3611)  Acc@5: 100.0000 (99.7222)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000259  Loss: 0.5867  Acc@1: 95.8333 (92.5000)  Acc@5: 100.0000 (99.3056)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 0.6626  Acc@1: 91.6667 (91.5278)  Acc@5: 100.0000 (99.3056)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000210  Loss: 0.6590  Acc@1: 95.8333 (93.7500)  Acc@5: 100.0000 (99.7222)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000186  Loss: 0.8788  Acc@1: 95.8333 (93.7500)  Acc@5: 100.0000 (99.8611)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.5604  Acc@1: 95.8333 (94.0278)  Acc@5: 100.0000 (99.8611)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000139  Loss: 0.5472  Acc@1: 95.8333 (95.5556)  Acc@5: 100.0000 (99.8611)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000117  Loss: 0.5800  Acc@1: 95.8333 (92.9167)  Acc@5: 100.0000 (99.7222)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 0.5314  Acc@1: 91.6667 (93.0556)  Acc@5: 100.0000 (99.7222)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000078  Loss: 0.5254  Acc@1: 91.6667 (93.0556)  Acc@5: 100.0000 (99.7222)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000060  Loss: 0.7024  Acc@1: 95.8333 (93.8889)  Acc@5: 100.0000 (99.7222)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 0.7759  Acc@1: 91.6667 (92.0833)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000031  Loss: 0.4830  Acc@1: 95.8333 (94.1667)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000020  Loss: 0.7150  Acc@1: 91.6667 (93.0556)  Acc@5: 100.0000 (99.4444)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.5999  Acc@1: 91.6667 (93.8889)  Acc@5: 100.0000 (99.8611)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000005  Loss: 0.6297  Acc@1: 95.8333 (94.7222)  Acc@5: 100.0000 (99.4444)\n",
            "torch.Size([9528, 384])\n",
            "Averaged stats: Lr: 0.000001  Loss: 0.6397  Acc@1: 95.8333 (93.0556)  Acc@5: 100.0000 (99.5833)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:14  Loss: 0.8218 (0.8218)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  time: 0.3459  data: 0.2720  max mem: 257\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 0.9583 (0.9586)  Acc@1: 83.3333 (79.9242)  Acc@5: 100.0000 (97.7273)  time: 0.1149  data: 0.0368  max mem: 257\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.9737 (0.9500)  Acc@1: 83.3333 (81.3492)  Acc@5: 95.8333 (97.2222)  time: 0.0872  data: 0.0068  max mem: 257\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.8803 (0.9163)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (97.5806)  time: 0.0830  data: 0.0003  max mem: 257\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.8164 (0.8924)  Acc@1: 87.5000 (83.9431)  Acc@5: 100.0000 (97.5610)  time: 0.0830  data: 0.0003  max mem: 257\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.8147 (0.8879)  Acc@1: 87.5000 (84.0000)  Acc@5: 100.0000 (97.6000)  time: 0.0817  data: 0.0003  max mem: 257\n",
            "Test: [Task 1] Total time: 0:00:03 (0.0933 s / it)\n",
            "* Acc@1 84.000 Acc@5 97.600 loss 0.888\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:33  Loss: 1.1593 (1.1593)  Acc@1: 87.5000 (87.5000)  Acc@5: 87.5000 (87.5000)  time: 0.7860  data: 0.6889  max mem: 257\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 1.0338 (1.0526)  Acc@1: 83.3333 (81.0606)  Acc@5: 95.8333 (96.5909)  time: 0.1525  data: 0.0670  max mem: 257\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 1.0577 (1.0770)  Acc@1: 79.1667 (80.5556)  Acc@5: 95.8333 (96.0317)  time: 0.0867  data: 0.0027  max mem: 257\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 1.0276 (1.0483)  Acc@1: 79.1667 (81.4516)  Acc@5: 95.8333 (96.2366)  time: 0.0840  data: 0.0011  max mem: 257\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.9672 (1.0390)  Acc@1: 83.3333 (81.1992)  Acc@5: 100.0000 (96.6463)  time: 0.0834  data: 0.0011  max mem: 257\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.9195 (1.0328)  Acc@1: 83.3333 (81.4000)  Acc@5: 100.0000 (96.7000)  time: 0.0818  data: 0.0010  max mem: 257\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1029 s / it)\n",
            "* Acc@1 81.400 Acc@5 96.700 loss 1.033\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:12  Loss: 0.8590 (0.8590)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  time: 0.3095  data: 0.2376  max mem: 257\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 1.0171 (1.0699)  Acc@1: 79.1667 (81.8182)  Acc@5: 95.8333 (96.5909)  time: 0.1079  data: 0.0291  max mem: 257\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 1.0122 (1.0136)  Acc@1: 79.1667 (82.1429)  Acc@5: 95.8333 (97.4206)  time: 0.0858  data: 0.0043  max mem: 257\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.9046 (0.9697)  Acc@1: 83.3333 (83.7366)  Acc@5: 100.0000 (97.7151)  time: 0.0836  data: 0.0004  max mem: 257\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.9650 (0.9833)  Acc@1: 83.3333 (83.6382)  Acc@5: 95.8333 (97.1545)  time: 0.0834  data: 0.0003  max mem: 257\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.9771 (0.9901)  Acc@1: 83.3333 (83.3000)  Acc@5: 95.8333 (97.2000)  time: 0.0820  data: 0.0003  max mem: 257\n",
            "Test: [Task 3] Total time: 0:00:03 (0.0911 s / it)\n",
            "* Acc@1 83.300 Acc@5 97.200 loss 0.990\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:12  Loss: 1.6354 (1.6354)  Acc@1: 62.5000 (62.5000)  Acc@5: 95.8333 (95.8333)  time: 0.2987  data: 0.2298  max mem: 257\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:03  Loss: 1.2651 (1.3119)  Acc@1: 75.0000 (75.7576)  Acc@5: 95.8333 (96.9697)  time: 0.1103  data: 0.0317  max mem: 257\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 1.2862 (1.3289)  Acc@1: 75.0000 (73.4127)  Acc@5: 95.8333 (96.8254)  time: 0.0881  data: 0.0062  max mem: 257\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 1.3306 (1.3090)  Acc@1: 70.8333 (73.6559)  Acc@5: 95.8333 (96.9086)  time: 0.0845  data: 0.0005  max mem: 257\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 1.3306 (1.3232)  Acc@1: 75.0000 (73.9837)  Acc@5: 100.0000 (96.8496)  time: 0.0839  data: 0.0003  max mem: 257\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 1.3291 (1.3173)  Acc@1: 75.0000 (74.2000)  Acc@5: 100.0000 (96.9000)  time: 0.0824  data: 0.0002  max mem: 257\n",
            "Test: [Task 4] Total time: 0:00:03 (0.0921 s / it)\n",
            "* Acc@1 74.200 Acc@5 96.900 loss 1.317\n",
            "[Average accuracy till task4]\tAcc@1: 80.7250\tAcc@5: 97.1000\tLoss: 1.0570\tForgetting: 0.8333\tBackward: 28.9000\n",
            "Train: Epoch[1/1]  [ 0/53]  eta: 0:00:20  Lr: 0.000047  Loss: 2.4057  Acc@1: 8.3333 (8.3333)  Acc@5: 45.8333 (45.8333)  time: 0.3889  data: 0.2846  max mem: 257\n",
            "Train: Epoch[1/1]  [10/53]  eta: 0:00:05  Lr: 0.000047  Loss: 2.2108  Acc@1: 12.5000 (13.6364)  Acc@5: 62.5000 (62.5000)  time: 0.1337  data: 0.0368  max mem: 257\n",
            "Train: Epoch[1/1]  [20/53]  eta: 0:00:04  Lr: 0.000047  Loss: 2.1579  Acc@1: 16.6667 (15.6746)  Acc@5: 62.5000 (64.0873)  time: 0.1079  data: 0.0116  max mem: 257\n",
            "Train: Epoch[1/1]  [30/53]  eta: 0:00:02  Lr: 0.000047  Loss: 2.0680  Acc@1: 20.8333 (20.2957)  Acc@5: 70.8333 (68.2796)  time: 0.1046  data: 0.0068  max mem: 257\n",
            "Train: Epoch[1/1]  [40/53]  eta: 0:00:01  Lr: 0.000047  Loss: 2.0859  Acc@1: 33.3333 (22.5610)  Acc@5: 79.1667 (71.4431)  time: 0.0997  data: 0.0029  max mem: 257\n",
            "Train: Epoch[1/1]  [50/53]  eta: 0:00:00  Lr: 0.000047  Loss: 1.8651  Acc@1: 37.5000 (26.7974)  Acc@5: 83.3333 (74.8366)  time: 0.0948  data: 0.0030  max mem: 257\n",
            "Train: Epoch[1/1]  [52/53]  eta: 0:00:00  Lr: 0.000047  Loss: 1.6705  Acc@1: 37.5000 (27.4960)  Acc@5: 83.3333 (75.3566)  time: 0.0916  data: 0.0028  max mem: 257\n",
            "Train: Epoch[1/1] Total time: 0:00:05 (0.1071 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.6705  Acc@1: 37.5000 (27.4960)  Acc@5: 83.3333 (75.3566)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:13  Loss: 0.8273 (0.8273)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  time: 0.3154  data: 0.2387  max mem: 257\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 0.9523 (0.9573)  Acc@1: 83.3333 (79.9242)  Acc@5: 100.0000 (97.7273)  time: 0.1109  data: 0.0322  max mem: 257\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.9790 (0.9507)  Acc@1: 79.1667 (80.9524)  Acc@5: 95.8333 (97.2222)  time: 0.0876  data: 0.0060  max mem: 257\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.8833 (0.9179)  Acc@1: 83.3333 (82.7957)  Acc@5: 95.8333 (97.5806)  time: 0.0840  data: 0.0004  max mem: 257\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.8176 (0.8937)  Acc@1: 87.5000 (83.4350)  Acc@5: 100.0000 (97.5610)  time: 0.0832  data: 0.0003  max mem: 257\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.8164 (0.8891)  Acc@1: 87.5000 (83.5000)  Acc@5: 100.0000 (97.5000)  time: 0.0819  data: 0.0003  max mem: 257\n",
            "Test: [Task 1] Total time: 0:00:03 (0.0921 s / it)\n",
            "* Acc@1 83.500 Acc@5 97.500 loss 0.889\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:22  Loss: 1.1514 (1.1514)  Acc@1: 87.5000 (87.5000)  Acc@5: 91.6667 (91.6667)  time: 0.5445  data: 0.4682  max mem: 257\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 1.0486 (1.0461)  Acc@1: 83.3333 (82.1970)  Acc@5: 95.8333 (96.9697)  time: 0.1262  data: 0.0439  max mem: 257\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 1.0613 (1.0723)  Acc@1: 79.1667 (81.5476)  Acc@5: 95.8333 (96.2302)  time: 0.0828  data: 0.0011  max mem: 257\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 1.0208 (1.0444)  Acc@1: 79.1667 (81.9893)  Acc@5: 95.8333 (96.3710)  time: 0.0821  data: 0.0007  max mem: 257\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.9643 (1.0351)  Acc@1: 83.3333 (81.7073)  Acc@5: 100.0000 (96.7480)  time: 0.0830  data: 0.0004  max mem: 257\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.9176 (1.0289)  Acc@1: 83.3333 (81.9000)  Acc@5: 100.0000 (96.8000)  time: 0.0814  data: 0.0004  max mem: 257\n",
            "Test: [Task 2] Total time: 0:00:03 (0.0952 s / it)\n",
            "* Acc@1 81.900 Acc@5 96.800 loss 1.029\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:16  Loss: 0.8487 (0.8487)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  time: 0.4017  data: 0.2901  max mem: 257\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 1.0211 (1.0663)  Acc@1: 83.3333 (82.1970)  Acc@5: 95.8333 (96.5909)  time: 0.1130  data: 0.0274  max mem: 257\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 1.0022 (1.0086)  Acc@1: 79.1667 (82.7381)  Acc@5: 100.0000 (97.6190)  time: 0.0827  data: 0.0009  max mem: 257\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.8915 (0.9658)  Acc@1: 83.3333 (83.8710)  Acc@5: 100.0000 (97.8495)  time: 0.0832  data: 0.0015  max mem: 257\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.9752 (0.9788)  Acc@1: 83.3333 (83.9431)  Acc@5: 95.8333 (97.3577)  time: 0.0840  data: 0.0013  max mem: 257\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.9762 (0.9855)  Acc@1: 83.3333 (83.7000)  Acc@5: 95.8333 (97.4000)  time: 0.0823  data: 0.0010  max mem: 257\n",
            "Test: [Task 3] Total time: 0:00:03 (0.0935 s / it)\n",
            "* Acc@1 83.700 Acc@5 97.400 loss 0.986\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:28  Loss: 1.6782 (1.6782)  Acc@1: 62.5000 (62.5000)  Acc@5: 95.8333 (95.8333)  time: 0.6701  data: 0.5530  max mem: 257\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:04  Loss: 1.2951 (1.3345)  Acc@1: 75.0000 (74.2424)  Acc@5: 95.8333 (96.9697)  time: 0.1374  data: 0.0513  max mem: 257\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 1.3189 (1.3518)  Acc@1: 75.0000 (71.6270)  Acc@5: 95.8333 (96.8254)  time: 0.0823  data: 0.0011  max mem: 257\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 1.3488 (1.3319)  Acc@1: 70.8333 (72.1774)  Acc@5: 95.8333 (96.6398)  time: 0.0816  data: 0.0007  max mem: 257\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 1.3543 (1.3449)  Acc@1: 70.8333 (72.3577)  Acc@5: 95.8333 (96.5447)  time: 0.0823  data: 0.0002  max mem: 257\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 1.3488 (1.3392)  Acc@1: 70.8333 (72.6000)  Acc@5: 95.8333 (96.6000)  time: 0.0807  data: 0.0002  max mem: 257\n",
            "Test: [Task 4] Total time: 0:00:04 (0.0975 s / it)\n",
            "* Acc@1 72.600 Acc@5 96.600 loss 1.339\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:12  Loss: 4.7164 (4.7164)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  time: 0.2925  data: 0.2262  max mem: 257\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:03  Loss: 4.7023 (4.6653)  Acc@1: 0.0000 (0.0000)  Acc@5: 4.1667 (2.6515)  time: 0.1102  data: 0.0327  max mem: 257\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 4.6843 (4.6697)  Acc@1: 0.0000 (0.0000)  Acc@5: 4.1667 (2.9762)  time: 0.0875  data: 0.0071  max mem: 257\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 4.6876 (4.6865)  Acc@1: 0.0000 (0.0000)  Acc@5: 4.1667 (3.7634)  time: 0.0827  data: 0.0006  max mem: 257\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 4.7025 (4.6999)  Acc@1: 0.0000 (0.0000)  Acc@5: 4.1667 (3.7602)  time: 0.0822  data: 0.0003  max mem: 257\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 4.7025 (4.6998)  Acc@1: 0.0000 (0.0000)  Acc@5: 4.1667 (3.7000)  time: 0.0810  data: 0.0003  max mem: 257\n",
            "Test: [Task 5] Total time: 0:00:03 (0.0909 s / it)\n",
            "* Acc@1 0.000 Acc@5 3.700 loss 4.700\n",
            "[Average accuracy till task5]\tAcc@1: 64.3400\tAcc@5: 78.4000\tLoss: 1.7885\tForgetting: 0.4500\tBackward: 75.4000\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([11904, 384])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 1.2844  Acc@1: 79.1667 (77.0833)  Acc@5: 83.3333 (84.0625)\n",
            "torch.Size([11904, 384])\n",
            "Averaged stats: Lr: 0.000467  Loss: 1.5736  Acc@1: 75.0000 (73.3333)  Acc@5: 87.5000 (86.9792)\n",
            "torch.Size([11904, 384])\n",
            "Averaged stats: Lr: 0.000464  Loss: 1.1931  Acc@1: 75.0000 (77.3958)  Acc@5: 91.6667 (91.3542)\n",
            "torch.Size([11904, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 1.1459  Acc@1: 79.1667 (78.7500)  Acc@5: 95.8333 (95.0000)\n",
            "torch.Size([11904, 384])\n",
            "Averaged stats: Lr: 0.000448  Loss: 0.9631  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (96.5625)\n",
            "torch.Size([11904, 384])\n",
            "Averaged stats: Lr: 0.000437  Loss: 0.8591  Acc@1: 83.3333 (85.4167)  Acc@5: 100.0000 (98.2292)\n",
            "torch.Size([11904, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 0.6759  Acc@1: 87.5000 (86.1458)  Acc@5: 100.0000 (98.6458)\n",
            "torch.Size([11904, 384])\n",
            "Averaged stats: Lr: 0.000409  Loss: 0.8966  Acc@1: 87.5000 (88.9583)  Acc@5: 100.0000 (99.0625)\n",
            "torch.Size([11904, 384])\n",
            "Averaged stats: Lr: 0.000391  Loss: 0.8724  Acc@1: 91.6667 (90.5208)  Acc@5: 100.0000 (99.6875)\n",
            "torch.Size([11904, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 0.5722  Acc@1: 91.6667 (91.3542)  Acc@5: 100.0000 (99.6875)\n",
            "torch.Size([11904, 384])\n",
            "Averaged stats: Lr: 0.000352  Loss: 0.4460  Acc@1: 95.8333 (92.5000)  Acc@5: 100.0000 (99.8958)\n",
            "torch.Size([11904, 384])\n",
            "Averaged stats: Lr: 0.000330  Loss: 0.8522  Acc@1: 95.8333 (92.7083)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([11904, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 0.5562  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (99.8958)\n",
            "torch.Size([11904, 384])\n",
            "Averaged stats: Lr: 0.000283  Loss: 0.6154  Acc@1: 91.6667 (92.9167)  Acc@5: 100.0000 (99.8958)\n",
            "torch.Size([11904, 384])\n",
            "Averaged stats: Lr: 0.000259  Loss: 0.3642  Acc@1: 91.6667 (93.8542)  Acc@5: 100.0000 (99.8958)\n",
            "torch.Size([11904, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 0.3872  Acc@1: 95.8333 (94.1667)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([11904, 384])\n",
            "Averaged stats: Lr: 0.000210  Loss: 0.5755  Acc@1: 95.8333 (94.7917)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([11904, 384])\n",
            "Averaged stats: Lr: 0.000186  Loss: 0.5516  Acc@1: 95.8333 (94.2708)  Acc@5: 100.0000 (99.8958)\n",
            "torch.Size([11904, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.6198  Acc@1: 95.8333 (95.7292)  Acc@5: 100.0000 (99.8958)\n",
            "torch.Size([11904, 384])\n",
            "Averaged stats: Lr: 0.000139  Loss: 0.4464  Acc@1: 95.8333 (94.2708)  Acc@5: 100.0000 (99.6875)\n",
            "torch.Size([11904, 384])\n",
            "Averaged stats: Lr: 0.000117  Loss: 0.3936  Acc@1: 95.8333 (94.0625)  Acc@5: 100.0000 (99.3750)\n",
            "torch.Size([11904, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 0.7710  Acc@1: 95.8333 (93.7500)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([11904, 384])\n",
            "Averaged stats: Lr: 0.000078  Loss: 0.6488  Acc@1: 95.8333 (95.6250)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([11904, 384])\n",
            "Averaged stats: Lr: 0.000060  Loss: 0.3896  Acc@1: 95.8333 (94.6875)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([11904, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 0.4985  Acc@1: 95.8333 (95.6250)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([11904, 384])\n",
            "Averaged stats: Lr: 0.000031  Loss: 0.4561  Acc@1: 95.8333 (95.5208)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([11904, 384])\n",
            "Averaged stats: Lr: 0.000020  Loss: 0.4975  Acc@1: 95.8333 (95.4167)  Acc@5: 100.0000 (99.8958)\n",
            "torch.Size([11904, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.4551  Acc@1: 95.8333 (96.2500)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([11904, 384])\n",
            "Averaged stats: Lr: 0.000005  Loss: 0.7228  Acc@1: 95.8333 (94.7917)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([11904, 384])\n",
            "Averaged stats: Lr: 0.000001  Loss: 0.5788  Acc@1: 95.8333 (95.7292)  Acc@5: 100.0000 (100.0000)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:12  Loss: 0.8102 (0.8102)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  time: 0.3031  data: 0.2336  max mem: 257\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 0.8352 (0.8481)  Acc@1: 79.1667 (79.9242)  Acc@5: 100.0000 (96.9697)  time: 0.1153  data: 0.0372  max mem: 257\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.8370 (0.8387)  Acc@1: 83.3333 (81.1508)  Acc@5: 95.8333 (96.8254)  time: 0.0900  data: 0.0095  max mem: 257\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.7457 (0.8083)  Acc@1: 83.3333 (82.3925)  Acc@5: 95.8333 (97.1774)  time: 0.0836  data: 0.0015  max mem: 257\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.7175 (0.7834)  Acc@1: 87.5000 (83.0285)  Acc@5: 95.8333 (97.3577)  time: 0.0827  data: 0.0013  max mem: 257\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6836 (0.7785)  Acc@1: 87.5000 (83.1000)  Acc@5: 100.0000 (97.4000)  time: 0.0813  data: 0.0012  max mem: 257\n",
            "Test: [Task 1] Total time: 0:00:03 (0.0941 s / it)\n",
            "* Acc@1 83.100 Acc@5 97.400 loss 0.779\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:26  Loss: 1.0538 (1.0538)  Acc@1: 83.3333 (83.3333)  Acc@5: 91.6667 (91.6667)  time: 0.6377  data: 0.5386  max mem: 257\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 0.8791 (0.9236)  Acc@1: 83.3333 (81.4394)  Acc@5: 95.8333 (96.9697)  time: 0.1334  data: 0.0500  max mem: 257\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 0.9434 (0.9626)  Acc@1: 79.1667 (79.3651)  Acc@5: 95.8333 (95.8333)  time: 0.0816  data: 0.0009  max mem: 257\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.9159 (0.9314)  Acc@1: 79.1667 (80.1075)  Acc@5: 95.8333 (96.1022)  time: 0.0816  data: 0.0005  max mem: 257\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.8521 (0.9188)  Acc@1: 83.3333 (80.1829)  Acc@5: 95.8333 (96.4431)  time: 0.0831  data: 0.0003  max mem: 257\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.8306 (0.9130)  Acc@1: 83.3333 (80.3000)  Acc@5: 100.0000 (96.5000)  time: 0.0818  data: 0.0002  max mem: 257\n",
            "Test: [Task 2] Total time: 0:00:04 (0.0967 s / it)\n",
            "* Acc@1 80.300 Acc@5 96.500 loss 0.913\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:18  Loss: 0.6512 (0.6512)  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)  time: 0.4391  data: 0.3692  max mem: 257\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 0.8733 (0.9162)  Acc@1: 79.1667 (79.9242)  Acc@5: 95.8333 (96.5909)  time: 0.1145  data: 0.0341  max mem: 257\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 0.8731 (0.8672)  Acc@1: 79.1667 (81.1508)  Acc@5: 100.0000 (97.2222)  time: 0.0814  data: 0.0004  max mem: 257\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.7828 (0.8272)  Acc@1: 83.3333 (81.8548)  Acc@5: 100.0000 (97.3118)  time: 0.0819  data: 0.0003  max mem: 257\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.8170 (0.8348)  Acc@1: 83.3333 (82.3171)  Acc@5: 95.8333 (96.8496)  time: 0.0831  data: 0.0002  max mem: 257\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.8192 (0.8398)  Acc@1: 83.3333 (82.2000)  Acc@5: 95.8333 (96.9000)  time: 0.0817  data: 0.0002  max mem: 257\n",
            "Test: [Task 3] Total time: 0:00:03 (0.0918 s / it)\n",
            "* Acc@1 82.200 Acc@5 96.900 loss 0.840\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:13  Loss: 1.3209 (1.3209)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  time: 0.3168  data: 0.2440  max mem: 257\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:03  Loss: 0.9086 (0.9587)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (96.9697)  time: 0.1100  data: 0.0296  max mem: 257\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 0.9183 (0.9583)  Acc@1: 79.1667 (78.7698)  Acc@5: 95.8333 (96.8254)  time: 0.0858  data: 0.0042  max mem: 257\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 0.9210 (0.9304)  Acc@1: 75.0000 (78.3602)  Acc@5: 95.8333 (97.0430)  time: 0.0828  data: 0.0003  max mem: 257\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.9210 (0.9509)  Acc@1: 75.0000 (78.6585)  Acc@5: 95.8333 (96.8496)  time: 0.0833  data: 0.0004  max mem: 257\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.9067 (0.9440)  Acc@1: 75.0000 (78.8000)  Acc@5: 100.0000 (96.9000)  time: 0.0818  data: 0.0003  max mem: 257\n",
            "Test: [Task 4] Total time: 0:00:03 (0.0925 s / it)\n",
            "* Acc@1 78.800 Acc@5 96.900 loss 0.944\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:29  Loss: 1.0753 (1.0753)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  time: 0.6928  data: 0.5843  max mem: 257\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:04  Loss: 1.1495 (1.1412)  Acc@1: 75.0000 (73.4848)  Acc@5: 100.0000 (97.7273)  time: 0.1410  data: 0.0550  max mem: 257\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 1.1804 (1.1720)  Acc@1: 75.0000 (72.2222)  Acc@5: 100.0000 (97.4206)  time: 0.0842  data: 0.0017  max mem: 257\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 1.2201 (1.1955)  Acc@1: 75.0000 (72.5806)  Acc@5: 95.8333 (97.0430)  time: 0.0834  data: 0.0013  max mem: 257\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 1.2891 (1.2376)  Acc@1: 70.8333 (70.9350)  Acc@5: 95.8333 (96.3415)  time: 0.0839  data: 0.0008  max mem: 257\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 1.3278 (1.2509)  Acc@1: 70.8333 (70.6000)  Acc@5: 95.8333 (96.2000)  time: 0.0825  data: 0.0008  max mem: 257\n",
            "Test: [Task 5] Total time: 0:00:04 (0.1004 s / it)\n",
            "* Acc@1 70.600 Acc@5 96.200 loss 1.251\n",
            "[Average accuracy till task5]\tAcc@1: 79.0000\tAcc@5: 96.7800\tLoss: 0.9453\tForgetting: 1.4000\tBackward: 22.0500\n",
            "Train: Epoch[1/1]  [ 0/53]  eta: 0:00:24  Lr: 0.000047  Loss: 2.4133  Acc@1: 12.5000 (12.5000)  Acc@5: 50.0000 (50.0000)  time: 0.4655  data: 0.3597  max mem: 257\n",
            "Train: Epoch[1/1]  [10/53]  eta: 0:00:05  Lr: 0.000047  Loss: 2.3085  Acc@1: 12.5000 (11.3636)  Acc@5: 50.0000 (52.6515)  time: 0.1334  data: 0.0335  max mem: 257\n",
            "Train: Epoch[1/1]  [20/53]  eta: 0:00:03  Lr: 0.000047  Loss: 2.1592  Acc@1: 12.5000 (13.6905)  Acc@5: 58.3333 (61.9048)  time: 0.0935  data: 0.0011  max mem: 257\n",
            "Train: Epoch[1/1]  [30/53]  eta: 0:00:02  Lr: 0.000047  Loss: 2.0830  Acc@1: 20.8333 (17.2043)  Acc@5: 75.0000 (66.3979)  time: 0.0863  data: 0.0012  max mem: 257\n",
            "Train: Epoch[1/1]  [40/53]  eta: 0:00:01  Lr: 0.000047  Loss: 1.9174  Acc@1: 29.1667 (21.0366)  Acc@5: 79.1667 (70.8333)  time: 0.0860  data: 0.0009  max mem: 257\n",
            "Train: Epoch[1/1]  [50/53]  eta: 0:00:00  Lr: 0.000047  Loss: 1.8846  Acc@1: 41.6667 (24.5915)  Acc@5: 87.5000 (74.9183)  time: 0.0855  data: 0.0007  max mem: 257\n",
            "Train: Epoch[1/1]  [52/53]  eta: 0:00:00  Lr: 0.000047  Loss: 1.8172  Acc@1: 41.6667 (24.9801)  Acc@5: 87.5000 (75.2586)  time: 0.0831  data: 0.0005  max mem: 257\n",
            "Train: Epoch[1/1] Total time: 0:00:05 (0.0967 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.8172  Acc@1: 41.6667 (24.9801)  Acc@5: 87.5000 (75.2586)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:15  Loss: 0.7900 (0.7900)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  time: 0.3704  data: 0.2975  max mem: 257\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 0.8021 (0.8279)  Acc@1: 79.1667 (79.9242)  Acc@5: 100.0000 (97.7273)  time: 0.1115  data: 0.0290  max mem: 257\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.8147 (0.8218)  Acc@1: 83.3333 (81.7460)  Acc@5: 95.8333 (97.2222)  time: 0.0838  data: 0.0012  max mem: 257\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.7282 (0.7930)  Acc@1: 83.3333 (82.7957)  Acc@5: 95.8333 (97.3118)  time: 0.0834  data: 0.0003  max mem: 257\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.7082 (0.7692)  Acc@1: 87.5000 (83.4350)  Acc@5: 95.8333 (97.4594)  time: 0.0848  data: 0.0002  max mem: 257\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6764 (0.7643)  Acc@1: 87.5000 (83.5000)  Acc@5: 95.8333 (97.5000)  time: 0.0830  data: 0.0002  max mem: 257\n",
            "Test: [Task 1] Total time: 0:00:03 (0.0938 s / it)\n",
            "* Acc@1 83.500 Acc@5 97.500 loss 0.764\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:15  Loss: 1.0584 (1.0584)  Acc@1: 83.3333 (83.3333)  Acc@5: 91.6667 (91.6667)  time: 0.3798  data: 0.3042  max mem: 257\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 0.8932 (0.9305)  Acc@1: 83.3333 (81.8182)  Acc@5: 95.8333 (96.9697)  time: 0.1503  data: 0.0636  max mem: 257\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 0.9421 (0.9683)  Acc@1: 79.1667 (79.3651)  Acc@5: 95.8333 (95.8333)  time: 0.1140  data: 0.0237  max mem: 257\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.9167 (0.9375)  Acc@1: 79.1667 (79.8387)  Acc@5: 95.8333 (96.1022)  time: 0.0943  data: 0.0046  max mem: 257\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.8557 (0.9259)  Acc@1: 83.3333 (79.9797)  Acc@5: 95.8333 (96.4431)  time: 0.0862  data: 0.0009  max mem: 257\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.8423 (0.9202)  Acc@1: 83.3333 (80.1000)  Acc@5: 100.0000 (96.5000)  time: 0.0842  data: 0.0007  max mem: 257\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1076 s / it)\n",
            "* Acc@1 80.100 Acc@5 96.500 loss 0.920\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:19  Loss: 0.6696 (0.6696)  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)  time: 0.4737  data: 0.3991  max mem: 257\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 0.8992 (0.9335)  Acc@1: 79.1667 (79.5455)  Acc@5: 95.8333 (96.2121)  time: 0.1165  data: 0.0373  max mem: 257\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 0.8902 (0.8834)  Acc@1: 79.1667 (81.1508)  Acc@5: 95.8333 (96.8254)  time: 0.0819  data: 0.0007  max mem: 257\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.8098 (0.8436)  Acc@1: 83.3333 (81.5860)  Acc@5: 100.0000 (97.0430)  time: 0.0835  data: 0.0003  max mem: 257\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.8388 (0.8524)  Acc@1: 83.3333 (81.8089)  Acc@5: 95.8333 (96.7480)  time: 0.0838  data: 0.0002  max mem: 257\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.8450 (0.8579)  Acc@1: 83.3333 (81.7000)  Acc@5: 95.8333 (96.8000)  time: 0.0825  data: 0.0002  max mem: 257\n",
            "Test: [Task 3] Total time: 0:00:03 (0.0933 s / it)\n",
            "* Acc@1 81.700 Acc@5 96.800 loss 0.858\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:14  Loss: 1.3289 (1.3289)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  time: 0.3488  data: 0.2759  max mem: 257\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:03  Loss: 0.9170 (0.9747)  Acc@1: 79.1667 (79.5455)  Acc@5: 95.8333 (97.3485)  time: 0.1141  data: 0.0355  max mem: 257\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 0.9419 (0.9735)  Acc@1: 79.1667 (78.9683)  Acc@5: 95.8333 (96.6270)  time: 0.0875  data: 0.0059  max mem: 257\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 0.9366 (0.9450)  Acc@1: 79.1667 (78.8979)  Acc@5: 95.8333 (96.9086)  time: 0.0837  data: 0.0003  max mem: 257\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.9318 (0.9662)  Acc@1: 79.1667 (79.0650)  Acc@5: 95.8333 (96.5447)  time: 0.0831  data: 0.0002  max mem: 257\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.9251 (0.9597)  Acc@1: 79.1667 (79.2000)  Acc@5: 95.8333 (96.6000)  time: 0.0818  data: 0.0002  max mem: 257\n",
            "Test: [Task 4] Total time: 0:00:03 (0.0926 s / it)\n",
            "* Acc@1 79.200 Acc@5 96.600 loss 0.960\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:19  Loss: 1.0728 (1.0728)  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  time: 0.4605  data: 0.3819  max mem: 257\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:03  Loss: 1.1697 (1.1460)  Acc@1: 75.0000 (73.1061)  Acc@5: 100.0000 (97.7273)  time: 0.1167  data: 0.0357  max mem: 257\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 1.1915 (1.1752)  Acc@1: 75.0000 (72.2222)  Acc@5: 95.8333 (97.4206)  time: 0.0822  data: 0.0010  max mem: 257\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 1.2337 (1.1989)  Acc@1: 75.0000 (72.7151)  Acc@5: 95.8333 (96.9086)  time: 0.0838  data: 0.0008  max mem: 257\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 1.2980 (1.2407)  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (96.1382)  time: 0.0842  data: 0.0006  max mem: 257\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 1.3329 (1.2543)  Acc@1: 66.6667 (70.4000)  Acc@5: 95.8333 (95.9000)  time: 0.0825  data: 0.0006  max mem: 257\n",
            "Test: [Task 5] Total time: 0:00:03 (0.0945 s / it)\n",
            "* Acc@1 70.400 Acc@5 95.900 loss 1.254\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:29  Loss: 5.3410 (5.3410)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  time: 0.7065  data: 0.6091  max mem: 257\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:04  Loss: 5.4100 (5.4379)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  time: 0.1384  data: 0.0564  max mem: 257\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 5.4097 (5.3876)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.1984)  time: 0.0814  data: 0.0009  max mem: 257\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 5.3876 (5.3962)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.2688)  time: 0.0815  data: 0.0005  max mem: 257\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 5.3910 (5.4005)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.2033)  time: 0.0812  data: 0.0003  max mem: 257\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 5.3910 (5.4008)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.2000)  time: 0.0798  data: 0.0003  max mem: 257\n",
            "Test: [Task 6] Total time: 0:00:04 (0.0974 s / it)\n",
            "* Acc@1 0.000 Acc@5 0.200 loss 5.401\n",
            "[Average accuracy till task6]\tAcc@1: 65.8167\tAcc@5: 80.5833\tLoss: 1.6929\tForgetting: 1.1200\tBackward: 74.9600\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([14280, 384])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 1.6096  Acc@1: 79.1667 (81.6667)  Acc@5: 83.3333 (84.8333)\n",
            "torch.Size([14280, 384])\n",
            "Averaged stats: Lr: 0.000467  Loss: 1.1202  Acc@1: 83.3333 (81.0000)  Acc@5: 87.5000 (86.8333)\n",
            "torch.Size([14280, 384])\n",
            "Averaged stats: Lr: 0.000464  Loss: 1.0794  Acc@1: 83.3333 (80.2500)  Acc@5: 95.8333 (93.3333)\n",
            "torch.Size([14280, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 1.0081  Acc@1: 79.1667 (81.5000)  Acc@5: 91.6667 (94.1667)\n",
            "torch.Size([14280, 384])\n",
            "Averaged stats: Lr: 0.000448  Loss: 0.7973  Acc@1: 83.3333 (84.0833)  Acc@5: 100.0000 (97.9167)\n",
            "torch.Size([14280, 384])\n",
            "Averaged stats: Lr: 0.000437  Loss: 0.5523  Acc@1: 87.5000 (87.3333)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([14280, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 0.9216  Acc@1: 87.5000 (86.7500)  Acc@5: 100.0000 (99.0833)\n",
            "torch.Size([14280, 384])\n",
            "Averaged stats: Lr: 0.000409  Loss: 0.6676  Acc@1: 91.6667 (90.9167)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([14280, 384])\n",
            "Averaged stats: Lr: 0.000391  Loss: 0.7014  Acc@1: 91.6667 (89.2500)  Acc@5: 100.0000 (99.9167)\n",
            "torch.Size([14280, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 0.6584  Acc@1: 95.8333 (92.0000)  Acc@5: 100.0000 (99.8333)\n",
            "torch.Size([14280, 384])\n",
            "Averaged stats: Lr: 0.000352  Loss: 0.6169  Acc@1: 91.6667 (92.5000)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([14280, 384])\n",
            "Averaged stats: Lr: 0.000330  Loss: 0.5561  Acc@1: 91.6667 (92.0833)  Acc@5: 100.0000 (99.9167)\n",
            "torch.Size([14280, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 0.4239  Acc@1: 95.8333 (94.1667)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([14280, 384])\n",
            "Averaged stats: Lr: 0.000283  Loss: 0.7339  Acc@1: 91.6667 (93.0833)  Acc@5: 100.0000 (99.6667)\n",
            "torch.Size([14280, 384])\n",
            "Averaged stats: Lr: 0.000259  Loss: 0.4657  Acc@1: 91.6667 (92.5833)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([14280, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 0.4786  Acc@1: 91.6667 (94.0833)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([14280, 384])\n",
            "Averaged stats: Lr: 0.000210  Loss: 0.5356  Acc@1: 95.8333 (95.1667)  Acc@5: 100.0000 (99.9167)\n",
            "torch.Size([14280, 384])\n",
            "Averaged stats: Lr: 0.000186  Loss: 0.5162  Acc@1: 95.8333 (94.0833)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([14280, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.6016  Acc@1: 91.6667 (93.6667)  Acc@5: 100.0000 (99.9167)\n",
            "torch.Size([14280, 384])\n",
            "Averaged stats: Lr: 0.000139  Loss: 0.4622  Acc@1: 95.8333 (95.1667)  Acc@5: 100.0000 (99.7500)\n",
            "torch.Size([14280, 384])\n",
            "Averaged stats: Lr: 0.000117  Loss: 0.3121  Acc@1: 95.8333 (95.3333)  Acc@5: 100.0000 (99.8333)\n",
            "torch.Size([14280, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 0.4193  Acc@1: 95.8333 (94.1667)  Acc@5: 100.0000 (99.9167)\n",
            "torch.Size([14280, 384])\n",
            "Averaged stats: Lr: 0.000078  Loss: 0.6663  Acc@1: 95.8333 (94.0000)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([14280, 384])\n",
            "Averaged stats: Lr: 0.000060  Loss: 0.4688  Acc@1: 95.8333 (94.4167)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([14280, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 0.5276  Acc@1: 95.8333 (94.3333)  Acc@5: 100.0000 (99.7500)\n",
            "torch.Size([14280, 384])\n",
            "Averaged stats: Lr: 0.000031  Loss: 0.4899  Acc@1: 95.8333 (95.0833)  Acc@5: 100.0000 (99.8333)\n",
            "torch.Size([14280, 384])\n",
            "Averaged stats: Lr: 0.000020  Loss: 0.5063  Acc@1: 95.8333 (95.6667)  Acc@5: 100.0000 (99.8333)\n",
            "torch.Size([14280, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.5493  Acc@1: 95.8333 (93.3333)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([14280, 384])\n",
            "Averaged stats: Lr: 0.000005  Loss: 0.5791  Acc@1: 95.8333 (95.0000)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([14280, 384])\n",
            "Averaged stats: Lr: 0.000001  Loss: 0.4894  Acc@1: 95.8333 (95.0000)  Acc@5: 100.0000 (100.0000)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:23  Loss: 0.9069 (0.9069)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  time: 0.5639  data: 0.4674  max mem: 257\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 0.8228 (0.8297)  Acc@1: 79.1667 (79.5455)  Acc@5: 95.8333 (95.8333)  time: 0.1247  data: 0.0435  max mem: 257\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.7987 (0.8258)  Acc@1: 79.1667 (79.3651)  Acc@5: 95.8333 (96.2302)  time: 0.0809  data: 0.0008  max mem: 257\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.7464 (0.7922)  Acc@1: 79.1667 (80.5108)  Acc@5: 95.8333 (96.7742)  time: 0.0813  data: 0.0004  max mem: 257\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6999 (0.7696)  Acc@1: 83.3333 (81.0976)  Acc@5: 95.8333 (96.8496)  time: 0.0818  data: 0.0002  max mem: 257\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6752 (0.7644)  Acc@1: 83.3333 (81.2000)  Acc@5: 95.8333 (96.9000)  time: 0.0804  data: 0.0002  max mem: 257\n",
            "Test: [Task 1] Total time: 0:00:03 (0.0942 s / it)\n",
            "* Acc@1 81.200 Acc@5 96.900 loss 0.764\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:16  Loss: 1.0280 (1.0280)  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  time: 0.3991  data: 0.3239  max mem: 257\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:03  Loss: 0.8666 (0.8937)  Acc@1: 83.3333 (80.6818)  Acc@5: 95.8333 (96.2121)  time: 0.1115  data: 0.0312  max mem: 257\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 0.9098 (0.9472)  Acc@1: 79.1667 (78.1746)  Acc@5: 95.8333 (94.6429)  time: 0.0819  data: 0.0011  max mem: 257\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.8787 (0.9141)  Acc@1: 75.0000 (78.2258)  Acc@5: 95.8333 (95.0269)  time: 0.0820  data: 0.0003  max mem: 257\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.7858 (0.8942)  Acc@1: 79.1667 (78.8618)  Acc@5: 95.8333 (95.3252)  time: 0.0827  data: 0.0004  max mem: 257\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.7848 (0.8896)  Acc@1: 79.1667 (79.0000)  Acc@5: 95.8333 (95.4000)  time: 0.0812  data: 0.0004  max mem: 257\n",
            "Test: [Task 2] Total time: 0:00:03 (0.0910 s / it)\n",
            "* Acc@1 79.000 Acc@5 95.400 loss 0.890\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:14  Loss: 0.5544 (0.5544)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.3534  data: 0.2808  max mem: 257\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 0.8013 (0.8390)  Acc@1: 79.1667 (78.7879)  Acc@5: 95.8333 (96.5909)  time: 0.1117  data: 0.0312  max mem: 257\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 0.8004 (0.7909)  Acc@1: 79.1667 (80.9524)  Acc@5: 95.8333 (96.6270)  time: 0.0849  data: 0.0033  max mem: 257\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.6967 (0.7539)  Acc@1: 83.3333 (81.3172)  Acc@5: 95.8333 (97.1774)  time: 0.0830  data: 0.0004  max mem: 257\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.7196 (0.7559)  Acc@1: 83.3333 (81.9106)  Acc@5: 95.8333 (96.8496)  time: 0.0835  data: 0.0006  max mem: 257\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.7196 (0.7600)  Acc@1: 83.3333 (81.8000)  Acc@5: 95.8333 (96.9000)  time: 0.0817  data: 0.0006  max mem: 257\n",
            "Test: [Task 3] Total time: 0:00:03 (0.0933 s / it)\n",
            "* Acc@1 81.800 Acc@5 96.900 loss 0.760\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:31  Loss: 1.2371 (1.2371)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  time: 0.7501  data: 0.6564  max mem: 257\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:04  Loss: 0.7945 (0.8511)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (96.9697)  time: 0.1457  data: 0.0614  max mem: 257\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 0.8145 (0.8389)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (96.0317)  time: 0.0846  data: 0.0012  max mem: 257\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 0.7914 (0.8113)  Acc@1: 79.1667 (79.7043)  Acc@5: 95.8333 (96.5054)  time: 0.0837  data: 0.0011  max mem: 257\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7914 (0.8370)  Acc@1: 79.1667 (79.6748)  Acc@5: 95.8333 (96.1382)  time: 0.0835  data: 0.0009  max mem: 257\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.7776 (0.8301)  Acc@1: 79.1667 (79.9000)  Acc@5: 95.8333 (96.2000)  time: 0.0817  data: 0.0007  max mem: 257\n",
            "Test: [Task 4] Total time: 0:00:04 (0.1012 s / it)\n",
            "* Acc@1 79.900 Acc@5 96.200 loss 0.830\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:20  Loss: 0.8628 (0.8628)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  time: 0.4768  data: 0.4008  max mem: 257\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:03  Loss: 0.8727 (0.8737)  Acc@1: 79.1667 (81.0606)  Acc@5: 100.0000 (98.1061)  time: 0.1204  data: 0.0375  max mem: 257\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 0.9301 (0.9017)  Acc@1: 79.1667 (77.5794)  Acc@5: 100.0000 (97.6190)  time: 0.0830  data: 0.0008  max mem: 257\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 0.9383 (0.9228)  Acc@1: 75.0000 (77.2849)  Acc@5: 95.8333 (97.0430)  time: 0.0830  data: 0.0004  max mem: 257\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 1.0027 (0.9571)  Acc@1: 75.0000 (76.6260)  Acc@5: 95.8333 (96.6463)  time: 0.0843  data: 0.0003  max mem: 257\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 1.0144 (0.9720)  Acc@1: 75.0000 (76.3000)  Acc@5: 95.8333 (96.5000)  time: 0.0824  data: 0.0003  max mem: 257\n",
            "Test: [Task 5] Total time: 0:00:03 (0.0944 s / it)\n",
            "* Acc@1 76.300 Acc@5 96.500 loss 0.972\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:14  Loss: 0.9470 (0.9470)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.3482  data: 0.2752  max mem: 257\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:03  Loss: 1.1156 (1.1672)  Acc@1: 66.6667 (66.6667)  Acc@5: 95.8333 (96.9697)  time: 0.1129  data: 0.0306  max mem: 257\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 1.1928 (1.2136)  Acc@1: 62.5000 (64.8810)  Acc@5: 95.8333 (96.4286)  time: 0.0855  data: 0.0033  max mem: 257\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 1.3237 (1.2503)  Acc@1: 62.5000 (63.3065)  Acc@5: 95.8333 (96.9086)  time: 0.0830  data: 0.0005  max mem: 257\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.3237 (1.2554)  Acc@1: 62.5000 (63.9228)  Acc@5: 95.8333 (97.0528)  time: 0.0848  data: 0.0003  max mem: 257\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.3115 (1.2560)  Acc@1: 62.5000 (64.0000)  Acc@5: 100.0000 (97.1000)  time: 0.0829  data: 0.0003  max mem: 257\n",
            "Test: [Task 6] Total time: 0:00:03 (0.0925 s / it)\n",
            "* Acc@1 64.000 Acc@5 97.100 loss 1.256\n",
            "[Average accuracy till task6]\tAcc@1: 77.0333\tAcc@5: 96.5000\tLoss: 0.9120\tForgetting: 1.8400\tBackward: 18.2800\n",
            "Train: Epoch[1/1]  [ 0/53]  eta: 0:00:35  Lr: 0.000047  Loss: 2.4361  Acc@1: 8.3333 (8.3333)  Acc@5: 50.0000 (50.0000)  time: 0.6635  data: 0.5518  max mem: 257\n",
            "Train: Epoch[1/1]  [10/53]  eta: 0:00:06  Lr: 0.000047  Loss: 2.3199  Acc@1: 8.3333 (7.5758)  Acc@5: 45.8333 (45.0758)  time: 0.1566  data: 0.0619  max mem: 257\n",
            "Train: Epoch[1/1]  [20/53]  eta: 0:00:04  Lr: 0.000047  Loss: 2.0697  Acc@1: 8.3333 (10.7143)  Acc@5: 54.1667 (52.5794)  time: 0.1113  data: 0.0162  max mem: 257\n",
            "Train: Epoch[1/1]  [30/53]  eta: 0:00:02  Lr: 0.000047  Loss: 2.0296  Acc@1: 16.6667 (14.7849)  Acc@5: 62.5000 (58.8710)  time: 0.1108  data: 0.0104  max mem: 257\n",
            "Train: Epoch[1/1]  [40/53]  eta: 0:00:01  Lr: 0.000047  Loss: 1.9315  Acc@1: 25.0000 (18.1911)  Acc@5: 70.8333 (62.2968)  time: 0.1003  data: 0.0015  max mem: 257\n",
            "Train: Epoch[1/1]  [50/53]  eta: 0:00:00  Lr: 0.000047  Loss: 2.0296  Acc@1: 29.1667 (21.6503)  Acc@5: 79.1667 (66.0948)  time: 0.0906  data: 0.0010  max mem: 257\n",
            "Train: Epoch[1/1]  [52/53]  eta: 0:00:00  Lr: 0.000047  Loss: 1.7641  Acc@1: 29.1667 (22.3798)  Acc@5: 79.1667 (66.6667)  time: 0.0887  data: 0.0008  max mem: 257\n",
            "Train: Epoch[1/1] Total time: 0:00:06 (0.1132 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.7641  Acc@1: 29.1667 (22.3798)  Acc@5: 79.1667 (66.6667)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:13  Loss: 0.9114 (0.9114)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  time: 0.3289  data: 0.2594  max mem: 257\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 0.8185 (0.8251)  Acc@1: 79.1667 (79.5455)  Acc@5: 95.8333 (95.8333)  time: 0.1147  data: 0.0346  max mem: 257\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.7841 (0.8241)  Acc@1: 79.1667 (79.5635)  Acc@5: 95.8333 (96.2302)  time: 0.0892  data: 0.0063  max mem: 257\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.7610 (0.7913)  Acc@1: 79.1667 (80.6452)  Acc@5: 95.8333 (96.6398)  time: 0.0849  data: 0.0005  max mem: 257\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6984 (0.7689)  Acc@1: 83.3333 (81.1992)  Acc@5: 95.8333 (96.9512)  time: 0.0848  data: 0.0003  max mem: 257\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6681 (0.7636)  Acc@1: 83.3333 (81.3000)  Acc@5: 95.8333 (97.0000)  time: 0.0831  data: 0.0003  max mem: 257\n",
            "Test: [Task 1] Total time: 0:00:03 (0.0941 s / it)\n",
            "* Acc@1 81.300 Acc@5 97.000 loss 0.764\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:23  Loss: 1.0272 (1.0272)  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  time: 0.5516  data: 0.4517  max mem: 257\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 0.8699 (0.9032)  Acc@1: 83.3333 (80.6818)  Acc@5: 95.8333 (95.8333)  time: 0.1255  data: 0.0424  max mem: 257\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 0.9049 (0.9543)  Acc@1: 79.1667 (78.7698)  Acc@5: 95.8333 (94.4444)  time: 0.0824  data: 0.0009  max mem: 257\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.8852 (0.9222)  Acc@1: 75.0000 (78.6290)  Acc@5: 95.8333 (95.0269)  time: 0.0831  data: 0.0003  max mem: 257\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.8022 (0.9031)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.3252)  time: 0.0839  data: 0.0002  max mem: 257\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.7992 (0.8979)  Acc@1: 79.1667 (79.3000)  Acc@5: 95.8333 (95.4000)  time: 0.0824  data: 0.0002  max mem: 257\n",
            "Test: [Task 2] Total time: 0:00:04 (0.0956 s / it)\n",
            "* Acc@1 79.300 Acc@5 95.400 loss 0.898\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:18  Loss: 0.5720 (0.5720)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.4362  data: 0.3518  max mem: 257\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 0.8159 (0.8545)  Acc@1: 79.1667 (78.7879)  Acc@5: 95.8333 (96.5909)  time: 0.1153  data: 0.0338  max mem: 257\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 0.8129 (0.8074)  Acc@1: 79.1667 (80.5556)  Acc@5: 95.8333 (96.4286)  time: 0.0847  data: 0.0021  max mem: 257\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.7180 (0.7704)  Acc@1: 83.3333 (81.0484)  Acc@5: 95.8333 (97.0430)  time: 0.0855  data: 0.0017  max mem: 257\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.7340 (0.7722)  Acc@1: 83.3333 (81.5041)  Acc@5: 95.8333 (96.5447)  time: 0.0849  data: 0.0027  max mem: 257\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.7340 (0.7762)  Acc@1: 83.3333 (81.4000)  Acc@5: 95.8333 (96.6000)  time: 0.0835  data: 0.0026  max mem: 257\n",
            "Test: [Task 3] Total time: 0:00:04 (0.0953 s / it)\n",
            "* Acc@1 81.400 Acc@5 96.600 loss 0.776\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:22  Loss: 1.2486 (1.2486)  Acc@1: 70.8333 (70.8333)  Acc@5: 87.5000 (87.5000)  time: 0.5450  data: 0.4701  max mem: 257\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:03  Loss: 0.7986 (0.8543)  Acc@1: 79.1667 (78.7879)  Acc@5: 100.0000 (97.3485)  time: 0.1247  data: 0.0444  max mem: 257\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 0.8091 (0.8418)  Acc@1: 79.1667 (78.7698)  Acc@5: 95.8333 (96.2302)  time: 0.0819  data: 0.0010  max mem: 257\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 0.7914 (0.8137)  Acc@1: 79.1667 (79.5699)  Acc@5: 95.8333 (96.6398)  time: 0.0819  data: 0.0003  max mem: 257\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7914 (0.8405)  Acc@1: 79.1667 (79.5732)  Acc@5: 95.8333 (96.1382)  time: 0.0828  data: 0.0003  max mem: 257\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.7777 (0.8338)  Acc@1: 79.1667 (79.8000)  Acc@5: 95.8333 (96.2000)  time: 0.0814  data: 0.0003  max mem: 257\n",
            "Test: [Task 4] Total time: 0:00:03 (0.0945 s / it)\n",
            "* Acc@1 79.800 Acc@5 96.200 loss 0.834\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:18  Loss: 0.8637 (0.8637)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  time: 0.4420  data: 0.3720  max mem: 257\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:03  Loss: 0.8748 (0.8746)  Acc@1: 79.1667 (80.6818)  Acc@5: 100.0000 (98.1061)  time: 0.1166  data: 0.0344  max mem: 257\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 0.9250 (0.9031)  Acc@1: 79.1667 (77.7778)  Acc@5: 100.0000 (97.6190)  time: 0.0820  data: 0.0007  max mem: 257\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 0.9496 (0.9246)  Acc@1: 75.0000 (77.5538)  Acc@5: 95.8333 (97.0430)  time: 0.0816  data: 0.0007  max mem: 257\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.9954 (0.9574)  Acc@1: 75.0000 (76.9309)  Acc@5: 95.8333 (96.8496)  time: 0.0827  data: 0.0004  max mem: 257\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 1.0104 (0.9719)  Acc@1: 75.0000 (76.7000)  Acc@5: 95.8333 (96.7000)  time: 0.0812  data: 0.0004  max mem: 257\n",
            "Test: [Task 5] Total time: 0:00:03 (0.0921 s / it)\n",
            "* Acc@1 76.700 Acc@5 96.700 loss 0.972\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:15  Loss: 0.9403 (0.9403)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.3782  data: 0.3096  max mem: 257\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:03  Loss: 1.1176 (1.1746)  Acc@1: 62.5000 (65.5303)  Acc@5: 95.8333 (96.5909)  time: 0.1097  data: 0.0296  max mem: 257\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 1.2033 (1.2211)  Acc@1: 62.5000 (64.0873)  Acc@5: 95.8333 (96.4286)  time: 0.0817  data: 0.0010  max mem: 257\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 1.3269 (1.2578)  Acc@1: 62.5000 (62.7688)  Acc@5: 95.8333 (96.6398)  time: 0.0818  data: 0.0003  max mem: 257\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.3269 (1.2620)  Acc@1: 62.5000 (63.4146)  Acc@5: 95.8333 (96.7480)  time: 0.0821  data: 0.0003  max mem: 257\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.3139 (1.2623)  Acc@1: 62.5000 (63.5000)  Acc@5: 95.8333 (96.8000)  time: 0.0805  data: 0.0003  max mem: 257\n",
            "Test: [Task 6] Total time: 0:00:03 (0.0921 s / it)\n",
            "* Acc@1 63.500 Acc@5 96.800 loss 1.262\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:30  Loss: 5.5129 (5.5129)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  time: 0.7353  data: 0.6360  max mem: 257\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:04  Loss: 5.4491 (5.4560)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  time: 0.1423  data: 0.0595  max mem: 257\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:02  Loss: 5.4434 (5.4482)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.1984)  time: 0.0826  data: 0.0016  max mem: 257\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:01  Loss: 5.4123 (5.4417)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.1344)  time: 0.0821  data: 0.0011  max mem: 257\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 5.4323 (5.4492)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.1016)  time: 0.0815  data: 0.0007  max mem: 257\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 5.4323 (5.4405)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.1000)  time: 0.0801  data: 0.0007  max mem: 257\n",
            "Test: [Task 7] Total time: 0:00:04 (0.0992 s / it)\n",
            "* Acc@1 0.000 Acc@5 0.100 loss 5.441\n",
            "[Average accuracy till task7]\tAcc@1: 66.0000\tAcc@5: 82.6857\tLoss: 1.5638\tForgetting: 1.4833\tBackward: 73.6500\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([16608, 384])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 1.2985  Acc@1: 83.3333 (81.4583)  Acc@5: 87.5000 (87.0139)\n",
            "torch.Size([16608, 384])\n",
            "Averaged stats: Lr: 0.000467  Loss: 0.7850  Acc@1: 83.3333 (81.9444)  Acc@5: 91.6667 (88.6111)\n",
            "torch.Size([16608, 384])\n",
            "Averaged stats: Lr: 0.000464  Loss: 0.3863  Acc@1: 79.1667 (81.2500)  Acc@5: 91.6667 (91.0417)\n",
            "torch.Size([16608, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 1.0031  Acc@1: 83.3333 (82.4306)  Acc@5: 95.8333 (94.5833)\n",
            "torch.Size([16608, 384])\n",
            "Averaged stats: Lr: 0.000448  Loss: 0.8332  Acc@1: 83.3333 (84.5139)  Acc@5: 95.8333 (96.5278)\n",
            "torch.Size([16608, 384])\n",
            "Averaged stats: Lr: 0.000437  Loss: 0.6660  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (98.4028)\n",
            "torch.Size([16608, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 0.9882  Acc@1: 87.5000 (88.7500)  Acc@5: 100.0000 (98.8194)\n",
            "torch.Size([16608, 384])\n",
            "Averaged stats: Lr: 0.000409  Loss: 0.6865  Acc@1: 91.6667 (90.3472)  Acc@5: 100.0000 (99.3750)\n",
            "torch.Size([16608, 384])\n",
            "Averaged stats: Lr: 0.000391  Loss: 0.4915  Acc@1: 91.6667 (92.0833)  Acc@5: 100.0000 (99.5139)\n",
            "torch.Size([16608, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 0.5584  Acc@1: 91.6667 (91.5972)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([16608, 384])\n",
            "Averaged stats: Lr: 0.000352  Loss: 0.6581  Acc@1: 91.6667 (92.9167)  Acc@5: 100.0000 (99.5139)\n",
            "torch.Size([16608, 384])\n",
            "Averaged stats: Lr: 0.000330  Loss: 0.5895  Acc@1: 91.6667 (92.0833)  Acc@5: 100.0000 (99.7222)\n",
            "torch.Size([16608, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 0.6159  Acc@1: 91.6667 (92.9167)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([16608, 384])\n",
            "Averaged stats: Lr: 0.000283  Loss: 0.4627  Acc@1: 91.6667 (93.8194)  Acc@5: 100.0000 (99.9306)\n",
            "torch.Size([16608, 384])\n",
            "Averaged stats: Lr: 0.000259  Loss: 0.4237  Acc@1: 91.6667 (93.8889)  Acc@5: 100.0000 (99.9306)\n",
            "torch.Size([16608, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 0.3524  Acc@1: 95.8333 (93.6111)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([16608, 384])\n",
            "Averaged stats: Lr: 0.000210  Loss: 0.4355  Acc@1: 95.8333 (94.3750)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([16608, 384])\n",
            "Averaged stats: Lr: 0.000186  Loss: 0.5760  Acc@1: 95.8333 (94.2361)  Acc@5: 100.0000 (99.9306)\n",
            "torch.Size([16608, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.3970  Acc@1: 95.8333 (94.3056)  Acc@5: 100.0000 (99.7222)\n",
            "torch.Size([16608, 384])\n",
            "Averaged stats: Lr: 0.000139  Loss: 0.3633  Acc@1: 95.8333 (94.0972)  Acc@5: 100.0000 (99.8611)\n",
            "torch.Size([16608, 384])\n",
            "Averaged stats: Lr: 0.000117  Loss: 0.4582  Acc@1: 95.8333 (94.9306)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([16608, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 0.3797  Acc@1: 95.8333 (95.5556)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([16608, 384])\n",
            "Averaged stats: Lr: 0.000078  Loss: 0.4648  Acc@1: 95.8333 (94.9306)  Acc@5: 100.0000 (99.9306)\n",
            "torch.Size([16608, 384])\n",
            "Averaged stats: Lr: 0.000060  Loss: 0.5045  Acc@1: 91.6667 (94.4444)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([16608, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 0.6849  Acc@1: 95.8333 (95.0000)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([16608, 384])\n",
            "Averaged stats: Lr: 0.000031  Loss: 0.3203  Acc@1: 95.8333 (95.0000)  Acc@5: 100.0000 (99.9306)\n",
            "torch.Size([16608, 384])\n",
            "Averaged stats: Lr: 0.000020  Loss: 0.4796  Acc@1: 95.8333 (95.2083)  Acc@5: 100.0000 (99.9306)\n",
            "torch.Size([16608, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.5796  Acc@1: 95.8333 (95.3472)  Acc@5: 100.0000 (99.9306)\n",
            "torch.Size([16608, 384])\n",
            "Averaged stats: Lr: 0.000005  Loss: 0.3037  Acc@1: 91.6667 (94.4444)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([16608, 384])\n",
            "Averaged stats: Lr: 0.000001  Loss: 0.5170  Acc@1: 91.6667 (94.5139)  Acc@5: 100.0000 (99.8611)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:20  Loss: 0.8880 (0.8880)  Acc@1: 70.8333 (70.8333)  Acc@5: 87.5000 (87.5000)  time: 0.4903  data: 0.4222  max mem: 259\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 0.7950 (0.8172)  Acc@1: 75.0000 (77.6515)  Acc@5: 95.8333 (95.0758)  time: 0.1187  data: 0.0395  max mem: 259\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.7884 (0.8155)  Acc@1: 79.1667 (77.7778)  Acc@5: 95.8333 (95.8333)  time: 0.0815  data: 0.0008  max mem: 259\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.7405 (0.7812)  Acc@1: 79.1667 (78.7634)  Acc@5: 95.8333 (96.3710)  time: 0.0822  data: 0.0003  max mem: 259\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6928 (0.7563)  Acc@1: 79.1667 (79.5732)  Acc@5: 95.8333 (96.7480)  time: 0.0826  data: 0.0003  max mem: 259\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6863 (0.7507)  Acc@1: 79.1667 (79.7000)  Acc@5: 95.8333 (96.8000)  time: 0.0811  data: 0.0003  max mem: 259\n",
            "Test: [Task 1] Total time: 0:00:03 (0.0929 s / it)\n",
            "* Acc@1 79.700 Acc@5 96.800 loss 0.751\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:12  Loss: 0.9866 (0.9866)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  time: 0.3074  data: 0.2335  max mem: 259\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:03  Loss: 0.8627 (0.8776)  Acc@1: 79.1667 (78.4091)  Acc@5: 95.8333 (95.8333)  time: 0.1135  data: 0.0345  max mem: 259\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 0.8757 (0.9296)  Acc@1: 75.0000 (77.1825)  Acc@5: 95.8333 (94.4444)  time: 0.0888  data: 0.0075  max mem: 259\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.8461 (0.8981)  Acc@1: 75.0000 (77.8226)  Acc@5: 95.8333 (95.1613)  time: 0.0832  data: 0.0008  max mem: 259\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.8001 (0.8772)  Acc@1: 79.1667 (78.1504)  Acc@5: 95.8333 (95.2236)  time: 0.0828  data: 0.0009  max mem: 259\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.7931 (0.8721)  Acc@1: 79.1667 (78.2000)  Acc@5: 95.8333 (95.3000)  time: 0.0815  data: 0.0009  max mem: 259\n",
            "Test: [Task 2] Total time: 0:00:03 (0.0938 s / it)\n",
            "* Acc@1 78.200 Acc@5 95.300 loss 0.872\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:28  Loss: 0.5463 (0.5463)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.6724  data: 0.5648  max mem: 259\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:04  Loss: 0.8245 (0.8277)  Acc@1: 79.1667 (77.6515)  Acc@5: 95.8333 (95.8333)  time: 0.1399  data: 0.0542  max mem: 259\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 0.7672 (0.7738)  Acc@1: 79.1667 (79.9603)  Acc@5: 95.8333 (95.8333)  time: 0.0847  data: 0.0020  max mem: 259\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.6362 (0.7400)  Acc@1: 83.3333 (79.9731)  Acc@5: 95.8333 (96.3710)  time: 0.0829  data: 0.0006  max mem: 259\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.6780 (0.7409)  Acc@1: 83.3333 (80.8943)  Acc@5: 95.8333 (95.9350)  time: 0.0834  data: 0.0003  max mem: 259\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.6780 (0.7437)  Acc@1: 83.3333 (80.8000)  Acc@5: 95.8333 (96.0000)  time: 0.0816  data: 0.0003  max mem: 259\n",
            "Test: [Task 3] Total time: 0:00:04 (0.0993 s / it)\n",
            "* Acc@1 80.800 Acc@5 96.000 loss 0.744\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:18  Loss: 1.2514 (1.2514)  Acc@1: 66.6667 (66.6667)  Acc@5: 83.3333 (83.3333)  time: 0.4479  data: 0.3694  max mem: 259\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:03  Loss: 0.7707 (0.8375)  Acc@1: 79.1667 (77.2727)  Acc@5: 95.8333 (95.4545)  time: 0.1187  data: 0.0348  max mem: 259\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 0.8097 (0.8245)  Acc@1: 79.1667 (78.5714)  Acc@5: 95.8333 (95.4365)  time: 0.0832  data: 0.0008  max mem: 259\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 0.7891 (0.7918)  Acc@1: 79.1667 (78.7634)  Acc@5: 95.8333 (95.8333)  time: 0.0827  data: 0.0004  max mem: 259\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7891 (0.8184)  Acc@1: 79.1667 (78.4553)  Acc@5: 95.8333 (95.3252)  time: 0.0850  data: 0.0004  max mem: 259\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.7216 (0.8113)  Acc@1: 79.1667 (78.6000)  Acc@5: 95.8333 (95.4000)  time: 0.0830  data: 0.0004  max mem: 259\n",
            "Test: [Task 4] Total time: 0:00:03 (0.0939 s / it)\n",
            "* Acc@1 78.600 Acc@5 95.400 loss 0.811\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:12  Loss: 0.7701 (0.7701)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  time: 0.2993  data: 0.2313  max mem: 259\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:03  Loss: 0.7701 (0.7759)  Acc@1: 83.3333 (81.8182)  Acc@5: 100.0000 (98.1061)  time: 0.1118  data: 0.0332  max mem: 259\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 0.8252 (0.8031)  Acc@1: 79.1667 (80.3571)  Acc@5: 100.0000 (97.4206)  time: 0.0893  data: 0.0069  max mem: 259\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 0.8318 (0.8228)  Acc@1: 79.1667 (80.1075)  Acc@5: 95.8333 (96.5054)  time: 0.0852  data: 0.0004  max mem: 259\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.8932 (0.8573)  Acc@1: 79.1667 (79.0650)  Acc@5: 95.8333 (96.3415)  time: 0.0850  data: 0.0003  max mem: 259\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.9024 (0.8731)  Acc@1: 75.0000 (78.7000)  Acc@5: 95.8333 (96.2000)  time: 0.0836  data: 0.0003  max mem: 259\n",
            "Test: [Task 5] Total time: 0:00:03 (0.0942 s / it)\n",
            "* Acc@1 78.700 Acc@5 96.200 loss 0.873\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:33  Loss: 0.7704 (0.7704)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  time: 0.8075  data: 0.7146  max mem: 259\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:04  Loss: 0.8988 (0.9329)  Acc@1: 75.0000 (74.6212)  Acc@5: 95.8333 (96.9697)  time: 0.1524  data: 0.0664  max mem: 259\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 0.9529 (0.9812)  Acc@1: 70.8333 (72.2222)  Acc@5: 95.8333 (96.2302)  time: 0.0904  data: 0.0076  max mem: 259\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 1.0549 (1.0190)  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (96.5054)  time: 0.0900  data: 0.0076  max mem: 259\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.0584 (1.0247)  Acc@1: 70.8333 (71.3415)  Acc@5: 95.8333 (96.5447)  time: 0.0849  data: 0.0011  max mem: 259\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.0584 (1.0272)  Acc@1: 70.8333 (71.2000)  Acc@5: 95.8333 (96.6000)  time: 0.0832  data: 0.0011  max mem: 259\n",
            "Test: [Task 6] Total time: 0:00:04 (0.1061 s / it)\n",
            "* Acc@1 71.200 Acc@5 96.600 loss 1.027\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:15  Loss: 1.2368 (1.2368)  Acc@1: 75.0000 (75.0000)  Acc@5: 87.5000 (87.5000)  time: 0.3721  data: 0.3029  max mem: 259\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:03  Loss: 1.2368 (1.3037)  Acc@1: 62.5000 (63.6364)  Acc@5: 95.8333 (93.5606)  time: 0.1153  data: 0.0331  max mem: 259\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:02  Loss: 1.2753 (1.3112)  Acc@1: 62.5000 (63.2937)  Acc@5: 95.8333 (94.2460)  time: 0.0868  data: 0.0032  max mem: 259\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:01  Loss: 1.2896 (1.3318)  Acc@1: 62.5000 (63.4409)  Acc@5: 91.6667 (93.0108)  time: 0.0842  data: 0.0003  max mem: 259\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 1.3000 (1.3293)  Acc@1: 62.5000 (64.4309)  Acc@5: 91.6667 (92.6829)  time: 0.0847  data: 0.0002  max mem: 259\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 1.2747 (1.3274)  Acc@1: 62.5000 (64.5000)  Acc@5: 91.6667 (92.8000)  time: 0.0828  data: 0.0002  max mem: 259\n",
            "Test: [Task 7] Total time: 0:00:03 (0.0938 s / it)\n",
            "* Acc@1 64.500 Acc@5 92.800 loss 1.327\n",
            "[Average accuracy till task7]\tAcc@1: 75.9571\tAcc@5: 95.5857\tLoss: 0.9151\tForgetting: 2.3000\tBackward: 16.0667\n",
            "Train: Epoch[1/1]  [ 0/52]  eta: 0:00:29  Lr: 0.000047  Loss: 2.3904  Acc@1: 4.1667 (4.1667)  Acc@5: 45.8333 (45.8333)  time: 0.5657  data: 0.4737  max mem: 259\n",
            "Train: Epoch[1/1]  [10/52]  eta: 0:00:05  Lr: 0.000047  Loss: 2.3546  Acc@1: 16.6667 (17.4242)  Acc@5: 66.6667 (62.8788)  time: 0.1315  data: 0.0443  max mem: 259\n",
            "Train: Epoch[1/1]  [20/52]  eta: 0:00:03  Lr: 0.000047  Loss: 2.1720  Acc@1: 20.8333 (20.6349)  Acc@5: 58.3333 (62.8968)  time: 0.0860  data: 0.0010  max mem: 259\n",
            "Train: Epoch[1/1]  [30/52]  eta: 0:00:02  Lr: 0.000047  Loss: 2.1019  Acc@1: 25.0000 (22.8495)  Acc@5: 62.5000 (64.1129)  time: 0.0844  data: 0.0006  max mem: 259\n",
            "Train: Epoch[1/1]  [40/52]  eta: 0:00:01  Lr: 0.000047  Loss: 1.9049  Acc@1: 33.3333 (27.6423)  Acc@5: 75.0000 (69.1057)  time: 0.0859  data: 0.0007  max mem: 259\n",
            "Train: Epoch[1/1]  [50/52]  eta: 0:00:00  Lr: 0.000047  Loss: 1.6871  Acc@1: 41.6667 (31.0458)  Acc@5: 87.5000 (72.7124)  time: 0.0859  data: 0.0004  max mem: 259\n",
            "Train: Epoch[1/1]  [51/52]  eta: 0:00:00  Lr: 0.000047  Loss: 1.7108  Acc@1: 41.6667 (31.2298)  Acc@5: 87.5000 (72.8964)  time: 0.0837  data: 0.0004  max mem: 259\n",
            "Train: Epoch[1/1] Total time: 0:00:04 (0.0958 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.7108  Acc@1: 41.6667 (31.2298)  Acc@5: 87.5000 (72.8964)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:22  Loss: 0.8665 (0.8665)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  time: 0.5438  data: 0.4244  max mem: 259\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 0.8046 (0.8007)  Acc@1: 79.1667 (78.0303)  Acc@5: 95.8333 (95.4545)  time: 0.1367  data: 0.0533  max mem: 259\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.7802 (0.7954)  Acc@1: 79.1667 (78.5714)  Acc@5: 95.8333 (96.0317)  time: 0.0905  data: 0.0086  max mem: 259\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.7315 (0.7627)  Acc@1: 79.1667 (79.3011)  Acc@5: 95.8333 (96.5054)  time: 0.0864  data: 0.0051  max mem: 259\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6817 (0.7381)  Acc@1: 83.3333 (80.1829)  Acc@5: 95.8333 (96.8496)  time: 0.0867  data: 0.0047  max mem: 259\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6501 (0.7329)  Acc@1: 83.3333 (80.3000)  Acc@5: 95.8333 (96.9000)  time: 0.0855  data: 0.0046  max mem: 259\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1012 s / it)\n",
            "* Acc@1 80.300 Acc@5 96.900 loss 0.733\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:12  Loss: 0.9714 (0.9714)  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  time: 0.2958  data: 0.2274  max mem: 259\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:03  Loss: 0.8769 (0.8869)  Acc@1: 79.1667 (78.0303)  Acc@5: 95.8333 (96.2121)  time: 0.1115  data: 0.0328  max mem: 259\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 0.8769 (0.9384)  Acc@1: 75.0000 (76.9841)  Acc@5: 95.8333 (94.4444)  time: 0.0884  data: 0.0073  max mem: 259\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.8550 (0.9067)  Acc@1: 75.0000 (77.5538)  Acc@5: 95.8333 (95.0269)  time: 0.0836  data: 0.0008  max mem: 259\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.8060 (0.8859)  Acc@1: 79.1667 (77.9472)  Acc@5: 95.8333 (95.1220)  time: 0.0831  data: 0.0003  max mem: 259\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.7966 (0.8807)  Acc@1: 79.1667 (78.0000)  Acc@5: 95.8333 (95.2000)  time: 0.0818  data: 0.0003  max mem: 259\n",
            "Test: [Task 2] Total time: 0:00:03 (0.0920 s / it)\n",
            "* Acc@1 78.000 Acc@5 95.200 loss 0.881\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:12  Loss: 0.5202 (0.5202)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.2892  data: 0.2229  max mem: 259\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 0.7983 (0.8051)  Acc@1: 79.1667 (78.7879)  Acc@5: 95.8333 (95.8333)  time: 0.1115  data: 0.0340  max mem: 259\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 0.7440 (0.7525)  Acc@1: 79.1667 (80.3571)  Acc@5: 95.8333 (95.8333)  time: 0.0889  data: 0.0077  max mem: 259\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.6193 (0.7202)  Acc@1: 83.3333 (80.7796)  Acc@5: 95.8333 (96.5054)  time: 0.0833  data: 0.0003  max mem: 259\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.6685 (0.7237)  Acc@1: 87.5000 (81.6057)  Acc@5: 95.8333 (96.1382)  time: 0.0826  data: 0.0002  max mem: 259\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.6685 (0.7266)  Acc@1: 87.5000 (81.5000)  Acc@5: 95.8333 (96.2000)  time: 0.0814  data: 0.0002  max mem: 259\n",
            "Test: [Task 3] Total time: 0:00:03 (0.0917 s / it)\n",
            "* Acc@1 81.500 Acc@5 96.200 loss 0.727\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:17  Loss: 1.2235 (1.2235)  Acc@1: 66.6667 (66.6667)  Acc@5: 83.3333 (83.3333)  time: 0.4101  data: 0.3395  max mem: 259\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:03  Loss: 0.7767 (0.8386)  Acc@1: 79.1667 (76.8939)  Acc@5: 95.8333 (95.4545)  time: 0.1125  data: 0.0323  max mem: 259\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 0.8105 (0.8288)  Acc@1: 79.1667 (77.7778)  Acc@5: 95.8333 (95.4365)  time: 0.0817  data: 0.0009  max mem: 259\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 0.8085 (0.7968)  Acc@1: 79.1667 (77.9570)  Acc@5: 95.8333 (95.9677)  time: 0.0819  data: 0.0005  max mem: 259\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.8085 (0.8235)  Acc@1: 79.1667 (77.9472)  Acc@5: 95.8333 (95.5285)  time: 0.0822  data: 0.0006  max mem: 259\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.7406 (0.8166)  Acc@1: 79.1667 (78.1000)  Acc@5: 95.8333 (95.6000)  time: 0.0807  data: 0.0005  max mem: 259\n",
            "Test: [Task 4] Total time: 0:00:03 (0.0921 s / it)\n",
            "* Acc@1 78.100 Acc@5 95.600 loss 0.817\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:31  Loss: 0.7781 (0.7781)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  time: 0.7615  data: 0.6665  max mem: 259\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:04  Loss: 0.7781 (0.7838)  Acc@1: 79.1667 (80.3030)  Acc@5: 100.0000 (98.1061)  time: 0.1461  data: 0.0616  max mem: 259\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 0.8377 (0.8122)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (97.4206)  time: 0.0841  data: 0.0010  max mem: 259\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 0.8509 (0.8313)  Acc@1: 75.0000 (78.7634)  Acc@5: 95.8333 (96.5054)  time: 0.0824  data: 0.0011  max mem: 259\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.8937 (0.8664)  Acc@1: 75.0000 (77.8455)  Acc@5: 95.8333 (96.4431)  time: 0.0812  data: 0.0007  max mem: 259\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.9078 (0.8822)  Acc@1: 75.0000 (77.5000)  Acc@5: 95.8333 (96.3000)  time: 0.0797  data: 0.0006  max mem: 259\n",
            "Test: [Task 5] Total time: 0:00:04 (0.1002 s / it)\n",
            "* Acc@1 77.500 Acc@5 96.300 loss 0.882\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:14  Loss: 0.7792 (0.7792)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  time: 0.3380  data: 0.2575  max mem: 259\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:03  Loss: 0.9008 (0.9385)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (96.5909)  time: 0.1106  data: 0.0324  max mem: 259\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 0.9536 (0.9919)  Acc@1: 70.8333 (72.8175)  Acc@5: 95.8333 (96.0317)  time: 0.0849  data: 0.0052  max mem: 259\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 1.0764 (1.0303)  Acc@1: 70.8333 (71.1022)  Acc@5: 95.8333 (96.3710)  time: 0.0817  data: 0.0004  max mem: 259\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.0840 (1.0363)  Acc@1: 70.8333 (71.5447)  Acc@5: 95.8333 (96.4431)  time: 0.0813  data: 0.0003  max mem: 259\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.0840 (1.0389)  Acc@1: 70.8333 (71.3000)  Acc@5: 95.8333 (96.5000)  time: 0.0799  data: 0.0003  max mem: 259\n",
            "Test: [Task 6] Total time: 0:00:03 (0.0905 s / it)\n",
            "* Acc@1 71.300 Acc@5 96.500 loss 1.039\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:20  Loss: 1.2921 (1.2921)  Acc@1: 75.0000 (75.0000)  Acc@5: 87.5000 (87.5000)  time: 0.4917  data: 0.4180  max mem: 259\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:03  Loss: 1.2921 (1.3496)  Acc@1: 62.5000 (62.8788)  Acc@5: 91.6667 (93.5606)  time: 0.1178  data: 0.0385  max mem: 259\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:02  Loss: 1.3240 (1.3568)  Acc@1: 62.5000 (62.3016)  Acc@5: 95.8333 (94.0476)  time: 0.0806  data: 0.0004  max mem: 259\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:01  Loss: 1.3249 (1.3765)  Acc@1: 62.5000 (61.8280)  Acc@5: 91.6667 (92.8763)  time: 0.0811  data: 0.0003  max mem: 259\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 1.3406 (1.3748)  Acc@1: 62.5000 (62.6016)  Acc@5: 91.6667 (92.5813)  time: 0.0814  data: 0.0002  max mem: 259\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 1.3249 (1.3729)  Acc@1: 62.5000 (62.7000)  Acc@5: 91.6667 (92.7000)  time: 0.0801  data: 0.0002  max mem: 259\n",
            "Test: [Task 7] Total time: 0:00:03 (0.0920 s / it)\n",
            "* Acc@1 62.700 Acc@5 92.700 loss 1.373\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:12  Loss: 5.8416 (5.8416)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  time: 0.2998  data: 0.2281  max mem: 259\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:03  Loss: 5.7063 (5.7262)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  time: 0.1124  data: 0.0322  max mem: 259\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:02  Loss: 5.7063 (5.7238)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  time: 0.0886  data: 0.0072  max mem: 259\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:01  Loss: 5.7251 (5.7337)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  time: 0.0896  data: 0.0086  max mem: 259\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 5.7446 (5.7327)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  time: 0.0895  data: 0.0088  max mem: 259\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 5.7446 (5.7324)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  time: 0.0884  data: 0.0087  max mem: 259\n",
            "Test: [Task 8] Total time: 0:00:04 (0.0967 s / it)\n",
            "* Acc@1 0.000 Acc@5 0.000 loss 5.732\n",
            "[Average accuracy till task8]\tAcc@1: 66.1750\tAcc@5: 83.6750\tLoss: 1.5229\tForgetting: 1.8286\tBackward: 72.7571\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([18936, 384])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 1.1927  Acc@1: 79.1667 (82.6191)  Acc@5: 87.5000 (88.0952)\n",
            "torch.Size([18936, 384])\n",
            "Averaged stats: Lr: 0.000467  Loss: 0.9081  Acc@1: 83.3333 (83.0357)  Acc@5: 87.5000 (88.2143)\n",
            "torch.Size([18936, 384])\n",
            "Averaged stats: Lr: 0.000464  Loss: 0.4669  Acc@1: 83.3333 (83.1548)  Acc@5: 95.8333 (92.3214)\n",
            "torch.Size([18936, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 1.0495  Acc@1: 83.3333 (86.0119)  Acc@5: 95.8333 (95.2976)\n",
            "torch.Size([18936, 384])\n",
            "Averaged stats: Lr: 0.000448  Loss: 0.5322  Acc@1: 87.5000 (86.9048)  Acc@5: 100.0000 (98.2738)\n",
            "torch.Size([18936, 384])\n",
            "Averaged stats: Lr: 0.000437  Loss: 0.5472  Acc@1: 87.5000 (88.1548)  Acc@5: 100.0000 (98.6310)\n",
            "torch.Size([18936, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 0.4608  Acc@1: 91.6667 (89.5238)  Acc@5: 100.0000 (99.0476)\n",
            "torch.Size([18936, 384])\n",
            "Averaged stats: Lr: 0.000409  Loss: 0.6003  Acc@1: 91.6667 (90.4762)  Acc@5: 100.0000 (99.4643)\n",
            "torch.Size([18936, 384])\n",
            "Averaged stats: Lr: 0.000391  Loss: 0.5950  Acc@1: 91.6667 (91.0119)  Acc@5: 100.0000 (99.7619)\n",
            "torch.Size([18936, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 0.4154  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([18936, 384])\n",
            "Averaged stats: Lr: 0.000352  Loss: 0.5797  Acc@1: 91.6667 (92.3214)  Acc@5: 100.0000 (99.9405)\n",
            "torch.Size([18936, 384])\n",
            "Averaged stats: Lr: 0.000330  Loss: 0.6145  Acc@1: 91.6667 (91.9643)  Acc@5: 100.0000 (99.4643)\n",
            "torch.Size([18936, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 0.6262  Acc@1: 91.6667 (92.6786)  Acc@5: 100.0000 (99.8214)\n",
            "torch.Size([18936, 384])\n",
            "Averaged stats: Lr: 0.000283  Loss: 0.2927  Acc@1: 95.8333 (93.8691)  Acc@5: 100.0000 (99.8810)\n",
            "torch.Size([18936, 384])\n",
            "Averaged stats: Lr: 0.000259  Loss: 0.3619  Acc@1: 95.8333 (93.5119)  Acc@5: 100.0000 (99.8810)\n",
            "torch.Size([18936, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 0.5939  Acc@1: 95.8333 (94.0476)  Acc@5: 100.0000 (99.9405)\n",
            "torch.Size([18936, 384])\n",
            "Averaged stats: Lr: 0.000210  Loss: 0.4441  Acc@1: 95.8333 (94.4643)  Acc@5: 100.0000 (99.7619)\n",
            "torch.Size([18936, 384])\n",
            "Averaged stats: Lr: 0.000186  Loss: 0.4227  Acc@1: 95.8333 (94.5833)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([18936, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.4378  Acc@1: 95.8333 (94.5833)  Acc@5: 100.0000 (99.7619)\n",
            "torch.Size([18936, 384])\n",
            "Averaged stats: Lr: 0.000139  Loss: 0.4088  Acc@1: 95.8333 (96.1905)  Acc@5: 100.0000 (99.7024)\n",
            "torch.Size([18936, 384])\n",
            "Averaged stats: Lr: 0.000117  Loss: 0.3631  Acc@1: 95.8333 (94.5238)  Acc@5: 100.0000 (99.7619)\n",
            "torch.Size([18936, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 0.3508  Acc@1: 95.8333 (94.2857)  Acc@5: 100.0000 (99.7619)\n",
            "torch.Size([18936, 384])\n",
            "Averaged stats: Lr: 0.000078  Loss: 0.3012  Acc@1: 95.8333 (95.0000)  Acc@5: 100.0000 (99.7619)\n",
            "torch.Size([18936, 384])\n",
            "Averaged stats: Lr: 0.000060  Loss: 0.3613  Acc@1: 95.8333 (94.7024)  Acc@5: 100.0000 (99.8214)\n",
            "torch.Size([18936, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 0.4578  Acc@1: 95.8333 (94.9405)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([18936, 384])\n",
            "Averaged stats: Lr: 0.000031  Loss: 0.4293  Acc@1: 95.8333 (94.5833)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([18936, 384])\n",
            "Averaged stats: Lr: 0.000020  Loss: 0.3782  Acc@1: 95.8333 (93.9286)  Acc@5: 100.0000 (99.7024)\n",
            "torch.Size([18936, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.3721  Acc@1: 95.8333 (93.6905)  Acc@5: 100.0000 (99.8810)\n",
            "torch.Size([18936, 384])\n",
            "Averaged stats: Lr: 0.000005  Loss: 0.3502  Acc@1: 95.8333 (94.6429)  Acc@5: 100.0000 (99.8214)\n",
            "torch.Size([18936, 384])\n",
            "Averaged stats: Lr: 0.000001  Loss: 0.3843  Acc@1: 95.8333 (94.8214)  Acc@5: 100.0000 (99.8810)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:27  Loss: 0.8610 (0.8610)  Acc@1: 70.8333 (70.8333)  Acc@5: 87.5000 (87.5000)  time: 0.6453  data: 0.5484  max mem: 259\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 0.8466 (0.8121)  Acc@1: 75.0000 (76.5152)  Acc@5: 95.8333 (95.4545)  time: 0.1407  data: 0.0569  max mem: 259\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.7533 (0.8092)  Acc@1: 79.1667 (77.1825)  Acc@5: 95.8333 (95.4365)  time: 0.0922  data: 0.0113  max mem: 259\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.7358 (0.7742)  Acc@1: 79.1667 (78.3602)  Acc@5: 95.8333 (95.9677)  time: 0.0926  data: 0.0120  max mem: 259\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6754 (0.7479)  Acc@1: 79.1667 (79.2683)  Acc@5: 95.8333 (96.3415)  time: 0.0874  data: 0.0048  max mem: 259\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6728 (0.7421)  Acc@1: 79.1667 (79.4000)  Acc@5: 95.8333 (96.4000)  time: 0.0861  data: 0.0047  max mem: 259\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1047 s / it)\n",
            "* Acc@1 79.400 Acc@5 96.400 loss 0.742\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:14  Loss: 1.0117 (1.0117)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  time: 0.3504  data: 0.2796  max mem: 259\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:03  Loss: 0.8462 (0.8784)  Acc@1: 75.0000 (75.3788)  Acc@5: 95.8333 (96.2121)  time: 0.1141  data: 0.0360  max mem: 259\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 0.8693 (0.9281)  Acc@1: 75.0000 (75.1984)  Acc@5: 95.8333 (94.2460)  time: 0.0875  data: 0.0059  max mem: 259\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.8676 (0.8950)  Acc@1: 75.0000 (76.0753)  Acc@5: 95.8333 (95.0269)  time: 0.0841  data: 0.0003  max mem: 259\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.7979 (0.8744)  Acc@1: 79.1667 (76.8293)  Acc@5: 95.8333 (95.0203)  time: 0.0841  data: 0.0002  max mem: 259\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.7877 (0.8704)  Acc@1: 79.1667 (76.8000)  Acc@5: 95.8333 (95.1000)  time: 0.0828  data: 0.0002  max mem: 259\n",
            "Test: [Task 2] Total time: 0:00:03 (0.0932 s / it)\n",
            "* Acc@1 76.800 Acc@5 95.100 loss 0.870\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:20  Loss: 0.5187 (0.5187)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.4838  data: 0.4101  max mem: 259\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 0.8087 (0.8148)  Acc@1: 79.1667 (76.8939)  Acc@5: 95.8333 (95.4545)  time: 0.1222  data: 0.0379  max mem: 259\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 0.7608 (0.7664)  Acc@1: 79.1667 (78.5714)  Acc@5: 95.8333 (95.8333)  time: 0.0833  data: 0.0005  max mem: 259\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.6371 (0.7344)  Acc@1: 83.3333 (79.1667)  Acc@5: 95.8333 (96.2366)  time: 0.0828  data: 0.0003  max mem: 259\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.6661 (0.7375)  Acc@1: 83.3333 (80.2846)  Acc@5: 95.8333 (95.7317)  time: 0.0852  data: 0.0002  max mem: 259\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.6661 (0.7406)  Acc@1: 83.3333 (80.1000)  Acc@5: 95.8333 (95.8000)  time: 0.0829  data: 0.0002  max mem: 259\n",
            "Test: [Task 3] Total time: 0:00:03 (0.0949 s / it)\n",
            "* Acc@1 80.100 Acc@5 95.800 loss 0.741\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:19  Loss: 1.2552 (1.2552)  Acc@1: 66.6667 (66.6667)  Acc@5: 83.3333 (83.3333)  time: 0.4744  data: 0.3795  max mem: 259\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:03  Loss: 0.8290 (0.8485)  Acc@1: 75.0000 (74.2424)  Acc@5: 95.8333 (95.0758)  time: 0.1179  data: 0.0355  max mem: 259\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 0.8093 (0.8327)  Acc@1: 75.0000 (75.3968)  Acc@5: 95.8333 (95.0397)  time: 0.0841  data: 0.0007  max mem: 259\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 0.7700 (0.7969)  Acc@1: 75.0000 (75.9409)  Acc@5: 95.8333 (95.5645)  time: 0.0859  data: 0.0011  max mem: 259\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7700 (0.8235)  Acc@1: 75.0000 (76.0163)  Acc@5: 95.8333 (95.1220)  time: 0.0851  data: 0.0010  max mem: 259\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.7241 (0.8156)  Acc@1: 75.0000 (76.2000)  Acc@5: 95.8333 (95.2000)  time: 0.0835  data: 0.0010  max mem: 259\n",
            "Test: [Task 4] Total time: 0:00:04 (0.0968 s / it)\n",
            "* Acc@1 76.200 Acc@5 95.200 loss 0.816\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:29  Loss: 0.7176 (0.7176)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  time: 0.6988  data: 0.6223  max mem: 259\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:04  Loss: 0.7176 (0.7271)  Acc@1: 79.1667 (81.0606)  Acc@5: 95.8333 (97.3485)  time: 0.1433  data: 0.0581  max mem: 259\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 0.7629 (0.7545)  Acc@1: 79.1667 (80.3571)  Acc@5: 95.8333 (97.2222)  time: 0.0845  data: 0.0012  max mem: 259\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 0.7964 (0.7769)  Acc@1: 75.0000 (79.5699)  Acc@5: 95.8333 (96.3710)  time: 0.0830  data: 0.0006  max mem: 259\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.8252 (0.8136)  Acc@1: 75.0000 (78.5569)  Acc@5: 95.8333 (96.2398)  time: 0.0849  data: 0.0003  max mem: 259\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.8369 (0.8306)  Acc@1: 75.0000 (78.3000)  Acc@5: 95.8333 (96.1000)  time: 0.0831  data: 0.0003  max mem: 259\n",
            "Test: [Task 5] Total time: 0:00:04 (0.1005 s / it)\n",
            "* Acc@1 78.300 Acc@5 96.100 loss 0.831\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:17  Loss: 0.7381 (0.7381)  Acc@1: 83.3333 (83.3333)  Acc@5: 91.6667 (91.6667)  time: 0.4084  data: 0.3276  max mem: 259\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:03  Loss: 0.8515 (0.8755)  Acc@1: 83.3333 (76.5152)  Acc@5: 95.8333 (95.8333)  time: 0.1127  data: 0.0305  max mem: 259\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 0.9175 (0.9360)  Acc@1: 70.8333 (74.4048)  Acc@5: 95.8333 (95.6349)  time: 0.0835  data: 0.0007  max mem: 259\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 0.9964 (0.9752)  Acc@1: 66.6667 (71.6398)  Acc@5: 95.8333 (95.8333)  time: 0.0844  data: 0.0004  max mem: 259\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.0032 (0.9866)  Acc@1: 70.8333 (71.3415)  Acc@5: 95.8333 (95.8333)  time: 0.0844  data: 0.0002  max mem: 259\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.0032 (0.9890)  Acc@1: 70.8333 (71.2000)  Acc@5: 95.8333 (95.8000)  time: 0.0829  data: 0.0002  max mem: 259\n",
            "Test: [Task 6] Total time: 0:00:03 (0.0929 s / it)\n",
            "* Acc@1 71.200 Acc@5 95.800 loss 0.989\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:15  Loss: 1.0456 (1.0456)  Acc@1: 79.1667 (79.1667)  Acc@5: 87.5000 (87.5000)  time: 0.3606  data: 0.2834  max mem: 259\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:03  Loss: 1.0235 (1.0714)  Acc@1: 75.0000 (70.4545)  Acc@5: 91.6667 (93.9394)  time: 0.1131  data: 0.0324  max mem: 259\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:02  Loss: 1.0135 (1.0950)  Acc@1: 70.8333 (70.0397)  Acc@5: 91.6667 (94.2460)  time: 0.0857  data: 0.0038  max mem: 259\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:01  Loss: 1.1239 (1.1258)  Acc@1: 66.6667 (69.0860)  Acc@5: 91.6667 (92.7419)  time: 0.0837  data: 0.0003  max mem: 259\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 1.1044 (1.1305)  Acc@1: 66.6667 (69.2073)  Acc@5: 91.6667 (92.8862)  time: 0.0839  data: 0.0002  max mem: 259\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 1.1010 (1.1281)  Acc@1: 66.6667 (69.2000)  Acc@5: 91.6667 (92.9000)  time: 0.0822  data: 0.0002  max mem: 259\n",
            "Test: [Task 7] Total time: 0:00:03 (0.0930 s / it)\n",
            "* Acc@1 69.200 Acc@5 92.900 loss 1.128\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:32  Loss: 1.2079 (1.2079)  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.8333)  time: 0.7725  data: 0.6819  max mem: 259\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:04  Loss: 1.3711 (1.4043)  Acc@1: 54.1667 (57.9545)  Acc@5: 91.6667 (92.0455)  time: 0.1527  data: 0.0705  max mem: 259\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:02  Loss: 1.3711 (1.3947)  Acc@1: 54.1667 (57.3413)  Acc@5: 91.6667 (92.2619)  time: 0.0872  data: 0.0053  max mem: 259\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:01  Loss: 1.3969 (1.3826)  Acc@1: 58.3333 (57.9301)  Acc@5: 95.8333 (92.6075)  time: 0.0835  data: 0.0018  max mem: 259\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 1.4024 (1.3914)  Acc@1: 54.1667 (57.6220)  Acc@5: 91.6667 (92.2764)  time: 0.0830  data: 0.0013  max mem: 259\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 1.4024 (1.3878)  Acc@1: 54.1667 (57.9000)  Acc@5: 91.6667 (92.3000)  time: 0.0817  data: 0.0013  max mem: 259\n",
            "Test: [Task 8] Total time: 0:00:04 (0.1028 s / it)\n",
            "* Acc@1 57.900 Acc@5 92.300 loss 1.388\n",
            "[Average accuracy till task8]\tAcc@1: 73.6375\tAcc@5: 94.9500\tLoss: 0.9380\tForgetting: 2.7143\tBackward: 13.7000\n",
            "Train: Epoch[1/1]  [ 0/52]  eta: 0:00:31  Lr: 0.000047  Loss: 2.4007  Acc@1: 12.5000 (12.5000)  Acc@5: 37.5000 (37.5000)  time: 0.6037  data: 0.5195  max mem: 259\n",
            "Train: Epoch[1/1]  [10/52]  eta: 0:00:05  Lr: 0.000047  Loss: 2.3672  Acc@1: 8.3333 (11.7424)  Acc@5: 54.1667 (48.4849)  time: 0.1328  data: 0.0481  max mem: 259\n",
            "Train: Epoch[1/1]  [20/52]  eta: 0:00:03  Lr: 0.000047  Loss: 2.2635  Acc@1: 16.6667 (14.8810)  Acc@5: 58.3333 (53.3730)  time: 0.0849  data: 0.0008  max mem: 259\n",
            "Train: Epoch[1/1]  [30/52]  eta: 0:00:02  Lr: 0.000047  Loss: 1.9966  Acc@1: 20.8333 (20.0269)  Acc@5: 66.6667 (61.2903)  time: 0.0846  data: 0.0005  max mem: 259\n",
            "Train: Epoch[1/1]  [40/52]  eta: 0:00:01  Lr: 0.000047  Loss: 1.9655  Acc@1: 29.1667 (21.6463)  Acc@5: 75.0000 (64.2276)  time: 0.0852  data: 0.0004  max mem: 259\n",
            "Train: Epoch[1/1]  [50/52]  eta: 0:00:00  Lr: 0.000047  Loss: 1.9324  Acc@1: 33.3333 (24.7549)  Acc@5: 79.1667 (69.0359)  time: 0.0842  data: 0.0004  max mem: 259\n",
            "Train: Epoch[1/1]  [51/52]  eta: 0:00:00  Lr: 0.000047  Loss: 1.7459  Acc@1: 33.3333 (24.9594)  Acc@5: 83.3333 (69.1870)  time: 0.0812  data: 0.0004  max mem: 259\n",
            "Train: Epoch[1/1] Total time: 0:00:04 (0.0950 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.7459  Acc@1: 33.3333 (24.9594)  Acc@5: 83.3333 (69.1870)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:19  Loss: 0.8796 (0.8796)  Acc@1: 70.8333 (70.8333)  Acc@5: 87.5000 (87.5000)  time: 0.4633  data: 0.3661  max mem: 259\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 0.8408 (0.8208)  Acc@1: 75.0000 (76.1364)  Acc@5: 95.8333 (95.4545)  time: 0.1170  data: 0.0342  max mem: 259\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.7680 (0.8172)  Acc@1: 79.1667 (76.9841)  Acc@5: 95.8333 (95.4365)  time: 0.0814  data: 0.0006  max mem: 259\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.7419 (0.7817)  Acc@1: 79.1667 (78.0914)  Acc@5: 95.8333 (95.9677)  time: 0.0816  data: 0.0004  max mem: 259\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6764 (0.7553)  Acc@1: 79.1667 (78.7602)  Acc@5: 95.8333 (96.2398)  time: 0.0826  data: 0.0003  max mem: 259\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6701 (0.7494)  Acc@1: 79.1667 (78.9000)  Acc@5: 95.8333 (96.3000)  time: 0.0811  data: 0.0002  max mem: 259\n",
            "Test: [Task 1] Total time: 0:00:03 (0.0922 s / it)\n",
            "* Acc@1 78.900 Acc@5 96.300 loss 0.749\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:21  Loss: 1.0335 (1.0335)  Acc@1: 66.6667 (66.6667)  Acc@5: 91.6667 (91.6667)  time: 0.5029  data: 0.4006  max mem: 259\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 0.8704 (0.8995)  Acc@1: 75.0000 (74.6212)  Acc@5: 95.8333 (96.9697)  time: 0.1272  data: 0.0435  max mem: 259\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 0.8924 (0.9494)  Acc@1: 75.0000 (74.2064)  Acc@5: 95.8333 (94.8413)  time: 0.0953  data: 0.0121  max mem: 259\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.8794 (0.9181)  Acc@1: 75.0000 (74.8656)  Acc@5: 95.8333 (95.4301)  time: 0.1082  data: 0.0229  max mem: 259\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.8346 (0.8986)  Acc@1: 75.0000 (75.2033)  Acc@5: 95.8333 (95.1220)  time: 0.0998  data: 0.0149  max mem: 259\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.8296 (0.8950)  Acc@1: 75.0000 (75.2000)  Acc@5: 95.8333 (95.2000)  time: 0.0939  data: 0.0104  max mem: 259\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1086 s / it)\n",
            "* Acc@1 75.200 Acc@5 95.200 loss 0.895\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:20  Loss: 0.5398 (0.5398)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.4927  data: 0.4181  max mem: 259\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 0.8165 (0.8361)  Acc@1: 79.1667 (77.6515)  Acc@5: 95.8333 (95.4545)  time: 0.1186  data: 0.0387  max mem: 259\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 0.7771 (0.7861)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.6349)  time: 0.0811  data: 0.0006  max mem: 259\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.6549 (0.7534)  Acc@1: 83.3333 (79.5699)  Acc@5: 95.8333 (96.1022)  time: 0.0812  data: 0.0004  max mem: 259\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.6749 (0.7564)  Acc@1: 83.3333 (80.4878)  Acc@5: 95.8333 (95.6301)  time: 0.0814  data: 0.0003  max mem: 259\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.6749 (0.7602)  Acc@1: 83.3333 (80.3000)  Acc@5: 95.8333 (95.7000)  time: 0.0800  data: 0.0003  max mem: 259\n",
            "Test: [Task 3] Total time: 0:00:03 (0.0920 s / it)\n",
            "* Acc@1 80.300 Acc@5 95.700 loss 0.760\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:13  Loss: 1.2115 (1.2115)  Acc@1: 66.6667 (66.6667)  Acc@5: 87.5000 (87.5000)  time: 0.3265  data: 0.2529  max mem: 259\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:03  Loss: 0.8001 (0.8307)  Acc@1: 75.0000 (74.6212)  Acc@5: 95.8333 (96.2121)  time: 0.1121  data: 0.0338  max mem: 259\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 0.8001 (0.8216)  Acc@1: 75.0000 (75.7937)  Acc@5: 95.8333 (95.6349)  time: 0.0866  data: 0.0062  max mem: 259\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 0.7711 (0.7858)  Acc@1: 75.0000 (76.2097)  Acc@5: 95.8333 (95.9677)  time: 0.0819  data: 0.0005  max mem: 259\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7711 (0.8134)  Acc@1: 75.0000 (76.2195)  Acc@5: 95.8333 (95.5285)  time: 0.0814  data: 0.0003  max mem: 259\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.7137 (0.8053)  Acc@1: 75.0000 (76.4000)  Acc@5: 95.8333 (95.6000)  time: 0.0801  data: 0.0003  max mem: 259\n",
            "Test: [Task 4] Total time: 0:00:03 (0.0909 s / it)\n",
            "* Acc@1 76.400 Acc@5 95.600 loss 0.805\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:19  Loss: 0.7088 (0.7088)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  time: 0.4528  data: 0.3837  max mem: 259\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:03  Loss: 0.7088 (0.7227)  Acc@1: 83.3333 (81.4394)  Acc@5: 95.8333 (97.3485)  time: 0.1151  data: 0.0356  max mem: 259\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 0.7625 (0.7517)  Acc@1: 79.1667 (80.5556)  Acc@5: 95.8333 (97.2222)  time: 0.0808  data: 0.0006  max mem: 259\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 0.7972 (0.7745)  Acc@1: 79.1667 (79.9731)  Acc@5: 95.8333 (96.3710)  time: 0.0808  data: 0.0005  max mem: 259\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.8251 (0.8116)  Acc@1: 75.0000 (79.0650)  Acc@5: 95.8333 (96.1382)  time: 0.0809  data: 0.0005  max mem: 259\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.8400 (0.8288)  Acc@1: 75.0000 (78.8000)  Acc@5: 95.8333 (96.0000)  time: 0.0796  data: 0.0005  max mem: 259\n",
            "Test: [Task 5] Total time: 0:00:03 (0.0924 s / it)\n",
            "* Acc@1 78.800 Acc@5 96.000 loss 0.829\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:28  Loss: 0.7269 (0.7269)  Acc@1: 87.5000 (87.5000)  Acc@5: 95.8333 (95.8333)  time: 0.6864  data: 0.5735  max mem: 259\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:04  Loss: 0.8473 (0.8682)  Acc@1: 79.1667 (77.6515)  Acc@5: 95.8333 (96.2121)  time: 0.1399  data: 0.0561  max mem: 259\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 0.9054 (0.9258)  Acc@1: 75.0000 (75.5952)  Acc@5: 95.8333 (95.6349)  time: 0.0839  data: 0.0027  max mem: 259\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 0.9900 (0.9658)  Acc@1: 70.8333 (72.7151)  Acc@5: 95.8333 (95.8333)  time: 0.0813  data: 0.0008  max mem: 259\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.0167 (0.9760)  Acc@1: 70.8333 (72.1545)  Acc@5: 95.8333 (95.9350)  time: 0.0803  data: 0.0003  max mem: 259\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.0167 (0.9778)  Acc@1: 70.8333 (72.0000)  Acc@5: 95.8333 (95.9000)  time: 0.0790  data: 0.0002  max mem: 259\n",
            "Test: [Task 6] Total time: 0:00:04 (0.0979 s / it)\n",
            "* Acc@1 72.000 Acc@5 95.900 loss 0.978\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:14  Loss: 1.0416 (1.0416)  Acc@1: 79.1667 (79.1667)  Acc@5: 87.5000 (87.5000)  time: 0.3415  data: 0.2706  max mem: 259\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:03  Loss: 1.0151 (1.0673)  Acc@1: 75.0000 (71.2121)  Acc@5: 91.6667 (92.8030)  time: 0.1069  data: 0.0275  max mem: 259\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:02  Loss: 1.0092 (1.0901)  Acc@1: 70.8333 (70.2381)  Acc@5: 91.6667 (93.8492)  time: 0.0819  data: 0.0018  max mem: 259\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:01  Loss: 1.1209 (1.1221)  Acc@1: 70.8333 (69.3548)  Acc@5: 91.6667 (92.4731)  time: 0.0802  data: 0.0004  max mem: 259\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 1.1105 (1.1284)  Acc@1: 66.6667 (69.2073)  Acc@5: 91.6667 (92.4797)  time: 0.0805  data: 0.0004  max mem: 259\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 1.0992 (1.1260)  Acc@1: 66.6667 (69.1000)  Acc@5: 91.6667 (92.5000)  time: 0.0792  data: 0.0003  max mem: 259\n",
            "Test: [Task 7] Total time: 0:00:03 (0.0886 s / it)\n",
            "* Acc@1 69.100 Acc@5 92.500 loss 1.126\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:16  Loss: 1.2228 (1.2228)  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.8333)  time: 0.3971  data: 0.3305  max mem: 259\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:03  Loss: 1.3839 (1.4147)  Acc@1: 58.3333 (57.1970)  Acc@5: 91.6667 (91.6667)  time: 0.1101  data: 0.0313  max mem: 259\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:02  Loss: 1.3839 (1.4059)  Acc@1: 58.3333 (57.1429)  Acc@5: 95.8333 (92.2619)  time: 0.0807  data: 0.0008  max mem: 259\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:01  Loss: 1.4084 (1.3951)  Acc@1: 58.3333 (57.1237)  Acc@5: 95.8333 (92.4731)  time: 0.0803  data: 0.0004  max mem: 259\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 1.4157 (1.4024)  Acc@1: 54.1667 (57.1138)  Acc@5: 91.6667 (92.1748)  time: 0.0808  data: 0.0003  max mem: 259\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 1.4157 (1.3987)  Acc@1: 54.1667 (57.3000)  Acc@5: 91.6667 (92.2000)  time: 0.0795  data: 0.0003  max mem: 259\n",
            "Test: [Task 8] Total time: 0:00:03 (0.0895 s / it)\n",
            "* Acc@1 57.300 Acc@5 92.200 loss 1.399\n",
            "Test: [Task 9]  [ 0/42]  eta: 0:00:14  Loss: 6.8479 (6.8479)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  time: 0.3488  data: 0.2760  max mem: 259\n",
            "Test: [Task 9]  [10/42]  eta: 0:00:03  Loss: 6.5734 (6.5607)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  time: 0.1127  data: 0.0331  max mem: 259\n",
            "Test: [Task 9]  [20/42]  eta: 0:00:02  Loss: 6.4746 (6.5317)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  time: 0.0854  data: 0.0045  max mem: 259\n",
            "Test: [Task 9]  [30/42]  eta: 0:00:01  Loss: 6.5136 (6.5269)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  time: 0.0827  data: 0.0009  max mem: 259\n",
            "Test: [Task 9]  [40/42]  eta: 0:00:00  Loss: 6.5136 (6.5132)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  time: 0.0842  data: 0.0038  max mem: 259\n",
            "Test: [Task 9]  [41/42]  eta: 0:00:00  Loss: 6.5136 (6.5114)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  time: 0.0826  data: 0.0038  max mem: 259\n",
            "Test: [Task 9] Total time: 0:00:03 (0.0937 s / it)\n",
            "* Acc@1 0.000 Acc@5 0.000 loss 6.511\n",
            "[Average accuracy till task9]\tAcc@1: 65.3333\tAcc@5: 84.3778\tLoss: 1.5614\tForgetting: 2.4875\tBackward: 70.9875\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([21288, 384])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 0.2996  Acc@1: 83.3333 (83.3854)  Acc@5: 87.5000 (88.1250)\n",
            "torch.Size([21288, 384])\n",
            "Averaged stats: Lr: 0.000467  Loss: 0.8308  Acc@1: 83.3333 (83.5938)  Acc@5: 91.6667 (88.6979)\n",
            "torch.Size([21288, 384])\n",
            "Averaged stats: Lr: 0.000464  Loss: 1.0331  Acc@1: 83.3333 (84.4792)  Acc@5: 95.8333 (93.5417)\n",
            "torch.Size([21288, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 0.5322  Acc@1: 87.5000 (86.3542)  Acc@5: 95.8333 (96.4063)\n",
            "torch.Size([21288, 384])\n",
            "Averaged stats: Lr: 0.000448  Loss: 0.5454  Acc@1: 91.6667 (87.8646)  Acc@5: 100.0000 (98.4896)\n",
            "torch.Size([21288, 384])\n",
            "Averaged stats: Lr: 0.000437  Loss: 0.7327  Acc@1: 91.6667 (88.2813)  Acc@5: 100.0000 (98.9583)\n",
            "torch.Size([21288, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 0.5734  Acc@1: 91.6667 (91.3542)  Acc@5: 100.0000 (99.4271)\n",
            "torch.Size([21288, 384])\n",
            "Averaged stats: Lr: 0.000409  Loss: 0.5185  Acc@1: 91.6667 (91.0938)  Acc@5: 100.0000 (99.6354)\n",
            "torch.Size([21288, 384])\n",
            "Averaged stats: Lr: 0.000391  Loss: 0.2864  Acc@1: 91.6667 (92.1354)  Acc@5: 100.0000 (99.5313)\n",
            "torch.Size([21288, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 0.5523  Acc@1: 95.8333 (93.7500)  Acc@5: 100.0000 (99.9479)\n",
            "torch.Size([21288, 384])\n",
            "Averaged stats: Lr: 0.000352  Loss: 0.3817  Acc@1: 95.8333 (92.9167)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([21288, 384])\n",
            "Averaged stats: Lr: 0.000330  Loss: 0.4439  Acc@1: 95.8333 (94.3750)  Acc@5: 100.0000 (99.7396)\n",
            "torch.Size([21288, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 0.3445  Acc@1: 95.8333 (93.4896)  Acc@5: 100.0000 (99.6875)\n",
            "torch.Size([21288, 384])\n",
            "Averaged stats: Lr: 0.000283  Loss: 0.2599  Acc@1: 95.8333 (94.6354)  Acc@5: 100.0000 (99.6875)\n",
            "torch.Size([21288, 384])\n",
            "Averaged stats: Lr: 0.000259  Loss: 0.4383  Acc@1: 95.8333 (95.2083)  Acc@5: 100.0000 (99.8438)\n",
            "torch.Size([21288, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 0.4373  Acc@1: 95.8333 (95.1042)  Acc@5: 100.0000 (99.7396)\n",
            "torch.Size([21288, 384])\n",
            "Averaged stats: Lr: 0.000210  Loss: 0.3415  Acc@1: 95.8333 (93.8542)  Acc@5: 100.0000 (99.8958)\n",
            "torch.Size([21288, 384])\n",
            "Averaged stats: Lr: 0.000186  Loss: 0.3456  Acc@1: 95.8333 (95.4688)  Acc@5: 100.0000 (99.9479)\n",
            "torch.Size([21288, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.4717  Acc@1: 95.8333 (95.4167)  Acc@5: 100.0000 (99.8958)\n",
            "torch.Size([21288, 384])\n",
            "Averaged stats: Lr: 0.000139  Loss: 0.3095  Acc@1: 95.8333 (95.4167)  Acc@5: 100.0000 (99.6875)\n",
            "torch.Size([21288, 384])\n",
            "Averaged stats: Lr: 0.000117  Loss: 0.4273  Acc@1: 95.8333 (94.0625)  Acc@5: 100.0000 (99.6354)\n",
            "torch.Size([21288, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 0.5073  Acc@1: 95.8333 (95.1563)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([21288, 384])\n",
            "Averaged stats: Lr: 0.000078  Loss: 0.4572  Acc@1: 95.8333 (94.8438)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([21288, 384])\n",
            "Averaged stats: Lr: 0.000060  Loss: 0.2771  Acc@1: 95.8333 (95.7813)  Acc@5: 100.0000 (99.9479)\n",
            "torch.Size([21288, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 0.2825  Acc@1: 95.8333 (95.0521)  Acc@5: 100.0000 (99.7396)\n",
            "torch.Size([21288, 384])\n",
            "Averaged stats: Lr: 0.000031  Loss: 0.2621  Acc@1: 95.8333 (95.2604)  Acc@5: 100.0000 (99.8958)\n",
            "torch.Size([21288, 384])\n",
            "Averaged stats: Lr: 0.000020  Loss: 0.3590  Acc@1: 95.8333 (95.3646)  Acc@5: 100.0000 (99.9479)\n",
            "torch.Size([21288, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.2964  Acc@1: 95.8333 (96.0938)  Acc@5: 100.0000 (99.9479)\n",
            "torch.Size([21288, 384])\n",
            "Averaged stats: Lr: 0.000005  Loss: 0.4994  Acc@1: 95.8333 (95.4167)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([21288, 384])\n",
            "Averaged stats: Lr: 0.000001  Loss: 0.3124  Acc@1: 95.8333 (95.5208)  Acc@5: 100.0000 (99.8958)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:23  Loss: 0.9078 (0.9078)  Acc@1: 75.0000 (75.0000)  Acc@5: 87.5000 (87.5000)  time: 0.5503  data: 0.4679  max mem: 259\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 0.8389 (0.8301)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (94.6970)  time: 0.1255  data: 0.0432  max mem: 259\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.7642 (0.8269)  Acc@1: 79.1667 (76.5873)  Acc@5: 95.8333 (95.0397)  time: 0.0822  data: 0.0007  max mem: 259\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.7581 (0.7923)  Acc@1: 79.1667 (77.9570)  Acc@5: 95.8333 (95.6989)  time: 0.0825  data: 0.0004  max mem: 259\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.7035 (0.7658)  Acc@1: 79.1667 (78.8618)  Acc@5: 95.8333 (96.0366)  time: 0.0839  data: 0.0002  max mem: 259\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6911 (0.7591)  Acc@1: 79.1667 (79.0000)  Acc@5: 95.8333 (96.1000)  time: 0.0822  data: 0.0002  max mem: 259\n",
            "Test: [Task 1] Total time: 0:00:04 (0.0955 s / it)\n",
            "* Acc@1 79.000 Acc@5 96.100 loss 0.759\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:19  Loss: 1.0431 (1.0431)  Acc@1: 66.6667 (66.6667)  Acc@5: 91.6667 (91.6667)  time: 0.4557  data: 0.3691  max mem: 259\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:03  Loss: 0.8537 (0.9005)  Acc@1: 79.1667 (76.1364)  Acc@5: 95.8333 (96.9697)  time: 0.1173  data: 0.0354  max mem: 259\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 0.8950 (0.9788)  Acc@1: 70.8333 (73.4127)  Acc@5: 95.8333 (93.8492)  time: 0.0829  data: 0.0013  max mem: 259\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.8934 (0.9415)  Acc@1: 70.8333 (74.0591)  Acc@5: 91.6667 (94.4892)  time: 0.0836  data: 0.0006  max mem: 259\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.8176 (0.9178)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (94.5122)  time: 0.0840  data: 0.0008  max mem: 259\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.8176 (0.9155)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (94.6000)  time: 0.0823  data: 0.0008  max mem: 259\n",
            "Test: [Task 2] Total time: 0:00:03 (0.0948 s / it)\n",
            "* Acc@1 75.000 Acc@5 94.600 loss 0.915\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:30  Loss: 0.5552 (0.5552)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.7281  data: 0.6364  max mem: 259\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:04  Loss: 0.8369 (0.8439)  Acc@1: 79.1667 (77.2727)  Acc@5: 95.8333 (95.4545)  time: 0.1487  data: 0.0638  max mem: 259\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 0.8015 (0.7900)  Acc@1: 79.1667 (78.7698)  Acc@5: 95.8333 (95.6349)  time: 0.0886  data: 0.0050  max mem: 259\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.6760 (0.7592)  Acc@1: 79.1667 (78.7634)  Acc@5: 95.8333 (95.5645)  time: 0.0850  data: 0.0021  max mem: 259\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.6782 (0.7626)  Acc@1: 83.3333 (79.6748)  Acc@5: 95.8333 (95.1220)  time: 0.0839  data: 0.0005  max mem: 259\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.6782 (0.7644)  Acc@1: 83.3333 (79.5000)  Acc@5: 95.8333 (95.2000)  time: 0.0826  data: 0.0004  max mem: 259\n",
            "Test: [Task 3] Total time: 0:00:04 (0.1032 s / it)\n",
            "* Acc@1 79.500 Acc@5 95.200 loss 0.764\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:19  Loss: 1.3022 (1.3022)  Acc@1: 62.5000 (62.5000)  Acc@5: 79.1667 (79.1667)  time: 0.4655  data: 0.3892  max mem: 259\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:03  Loss: 0.8238 (0.8610)  Acc@1: 75.0000 (72.7273)  Acc@5: 95.8333 (95.0758)  time: 0.1174  data: 0.0362  max mem: 259\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 0.8129 (0.8437)  Acc@1: 75.0000 (74.2064)  Acc@5: 95.8333 (94.2460)  time: 0.0849  data: 0.0018  max mem: 259\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 0.7614 (0.8059)  Acc@1: 75.0000 (75.2688)  Acc@5: 95.8333 (94.7581)  time: 0.0866  data: 0.0023  max mem: 259\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7384 (0.8340)  Acc@1: 75.0000 (75.8130)  Acc@5: 95.8333 (94.5122)  time: 0.0846  data: 0.0012  max mem: 259\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.7157 (0.8252)  Acc@1: 75.0000 (76.0000)  Acc@5: 95.8333 (94.6000)  time: 0.0833  data: 0.0012  max mem: 259\n",
            "Test: [Task 4] Total time: 0:00:04 (0.0971 s / it)\n",
            "* Acc@1 76.000 Acc@5 94.600 loss 0.825\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:29  Loss: 0.7287 (0.7287)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  time: 0.6914  data: 0.5921  max mem: 259\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:04  Loss: 0.7287 (0.7348)  Acc@1: 79.1667 (80.3030)  Acc@5: 95.8333 (97.3485)  time: 0.1388  data: 0.0544  max mem: 259\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 0.7788 (0.7723)  Acc@1: 79.1667 (79.3651)  Acc@5: 95.8333 (96.8254)  time: 0.0826  data: 0.0005  max mem: 259\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 0.8180 (0.8011)  Acc@1: 79.1667 (78.8979)  Acc@5: 95.8333 (95.8333)  time: 0.0823  data: 0.0004  max mem: 259\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.8514 (0.8416)  Acc@1: 75.0000 (77.9472)  Acc@5: 95.8333 (95.6301)  time: 0.0830  data: 0.0002  max mem: 259\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.8677 (0.8605)  Acc@1: 75.0000 (77.7000)  Acc@5: 95.8333 (95.4000)  time: 0.0812  data: 0.0002  max mem: 259\n",
            "Test: [Task 5] Total time: 0:00:04 (0.0998 s / it)\n",
            "* Acc@1 77.700 Acc@5 95.400 loss 0.861\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:30  Loss: 0.7421 (0.7421)  Acc@1: 87.5000 (87.5000)  Acc@5: 91.6667 (91.6667)  time: 0.7172  data: 0.6010  max mem: 259\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:04  Loss: 0.8086 (0.8359)  Acc@1: 79.1667 (78.4091)  Acc@5: 95.8333 (96.2121)  time: 0.1438  data: 0.0563  max mem: 259\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 0.8719 (0.9105)  Acc@1: 70.8333 (74.6032)  Acc@5: 95.8333 (95.6349)  time: 0.0924  data: 0.0098  max mem: 259\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 0.9446 (0.9455)  Acc@1: 70.8333 (72.3118)  Acc@5: 95.8333 (95.6989)  time: 0.0934  data: 0.0122  max mem: 259\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 0.9571 (0.9642)  Acc@1: 66.6667 (71.6463)  Acc@5: 95.8333 (95.7317)  time: 0.0861  data: 0.0039  max mem: 259\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 0.9571 (0.9678)  Acc@1: 66.6667 (71.5000)  Acc@5: 95.8333 (95.7000)  time: 0.0846  data: 0.0038  max mem: 259\n",
            "Test: [Task 6] Total time: 0:00:04 (0.1056 s / it)\n",
            "* Acc@1 71.500 Acc@5 95.700 loss 0.968\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:20  Loss: 1.0231 (1.0231)  Acc@1: 79.1667 (79.1667)  Acc@5: 87.5000 (87.5000)  time: 0.4913  data: 0.4234  max mem: 259\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:03  Loss: 0.9931 (1.0120)  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (93.1818)  time: 0.1198  data: 0.0395  max mem: 259\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:02  Loss: 0.9754 (1.0376)  Acc@1: 70.8333 (70.6349)  Acc@5: 91.6667 (93.4524)  time: 0.0816  data: 0.0007  max mem: 259\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:01  Loss: 1.0490 (1.0688)  Acc@1: 70.8333 (70.1613)  Acc@5: 91.6667 (92.2043)  time: 0.0818  data: 0.0004  max mem: 259\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 1.0490 (1.0748)  Acc@1: 70.8333 (69.9187)  Acc@5: 91.6667 (92.2764)  time: 0.0829  data: 0.0004  max mem: 259\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 1.0490 (1.0701)  Acc@1: 70.8333 (70.0000)  Acc@5: 91.6667 (92.4000)  time: 0.0815  data: 0.0004  max mem: 259\n",
            "Test: [Task 7] Total time: 0:00:03 (0.0932 s / it)\n",
            "* Acc@1 70.000 Acc@5 92.400 loss 1.070\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:14  Loss: 0.9590 (0.9590)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  time: 0.3483  data: 0.2686  max mem: 259\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:03  Loss: 1.0946 (1.1124)  Acc@1: 66.6667 (65.5303)  Acc@5: 95.8333 (93.1818)  time: 0.1117  data: 0.0322  max mem: 259\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:02  Loss: 1.0946 (1.0991)  Acc@1: 66.6667 (65.6746)  Acc@5: 95.8333 (93.6508)  time: 0.0854  data: 0.0045  max mem: 259\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:01  Loss: 1.0608 (1.0802)  Acc@1: 66.6667 (67.2043)  Acc@5: 95.8333 (93.9516)  time: 0.0828  data: 0.0004  max mem: 259\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 1.1073 (1.0952)  Acc@1: 66.6667 (66.4634)  Acc@5: 91.6667 (93.6992)  time: 0.0826  data: 0.0002  max mem: 259\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 1.1073 (1.0919)  Acc@1: 66.6667 (66.7000)  Acc@5: 91.6667 (93.8000)  time: 0.0814  data: 0.0002  max mem: 259\n",
            "Test: [Task 8] Total time: 0:00:03 (0.0917 s / it)\n",
            "* Acc@1 66.700 Acc@5 93.800 loss 1.092\n",
            "Test: [Task 9]  [ 0/42]  eta: 0:00:19  Loss: 1.1219 (1.1219)  Acc@1: 62.5000 (62.5000)  Acc@5: 100.0000 (100.0000)  time: 0.4706  data: 0.4013  max mem: 259\n",
            "Test: [Task 9]  [10/42]  eta: 0:00:03  Loss: 1.1229 (1.1647)  Acc@1: 66.6667 (68.9394)  Acc@5: 95.8333 (93.5606)  time: 0.1164  data: 0.0373  max mem: 259\n",
            "Test: [Task 9]  [20/42]  eta: 0:00:02  Loss: 1.1885 (1.1666)  Acc@1: 66.6667 (66.6667)  Acc@5: 95.8333 (94.4444)  time: 0.0820  data: 0.0011  max mem: 259\n",
            "Test: [Task 9]  [30/42]  eta: 0:00:01  Loss: 1.2220 (1.2079)  Acc@1: 62.5000 (65.4570)  Acc@5: 95.8333 (93.8172)  time: 0.0832  data: 0.0010  max mem: 259\n",
            "Test: [Task 9]  [40/42]  eta: 0:00:00  Loss: 1.0804 (1.1672)  Acc@1: 66.6667 (66.2602)  Acc@5: 95.8333 (94.3089)  time: 0.0828  data: 0.0005  max mem: 259\n",
            "Test: [Task 9]  [41/42]  eta: 0:00:00  Loss: 1.0804 (1.1565)  Acc@1: 66.6667 (66.7000)  Acc@5: 95.8333 (94.4000)  time: 0.0816  data: 0.0003  max mem: 259\n",
            "Test: [Task 9] Total time: 0:00:03 (0.0947 s / it)\n",
            "* Acc@1 66.700 Acc@5 94.400 loss 1.157\n",
            "[Average accuracy till task9]\tAcc@1: 73.5667\tAcc@5: 94.6889\tLoss: 0.9346\tForgetting: 2.8250\tBackward: 12.7750\n",
            "Train: Epoch[1/1]  [ 0/52]  eta: 0:00:45  Lr: 0.000047  Loss: 2.3834  Acc@1: 8.3333 (8.3333)  Acc@5: 37.5000 (37.5000)  time: 0.8751  data: 0.7239  max mem: 259\n",
            "Train: Epoch[1/1]  [10/52]  eta: 0:00:06  Lr: 0.000047  Loss: 2.2964  Acc@1: 4.1667 (5.6818)  Acc@5: 54.1667 (54.1667)  time: 0.1619  data: 0.0675  max mem: 259\n",
            "Train: Epoch[1/1]  [20/52]  eta: 0:00:03  Lr: 0.000047  Loss: 2.1464  Acc@1: 8.3333 (12.1032)  Acc@5: 58.3333 (58.5317)  time: 0.0868  data: 0.0018  max mem: 259\n",
            "Train: Epoch[1/1]  [30/52]  eta: 0:00:02  Lr: 0.000047  Loss: 1.9005  Acc@1: 25.0000 (17.6075)  Acc@5: 70.8333 (65.5914)  time: 0.0839  data: 0.0014  max mem: 259\n",
            "Train: Epoch[1/1]  [40/52]  eta: 0:00:01  Lr: 0.000047  Loss: 1.8989  Acc@1: 29.1667 (21.7480)  Acc@5: 83.3333 (69.4106)  time: 0.0839  data: 0.0010  max mem: 259\n",
            "Train: Epoch[1/1]  [50/52]  eta: 0:00:00  Lr: 0.000047  Loss: 1.7540  Acc@1: 45.8333 (26.7974)  Acc@5: 87.5000 (73.4477)  time: 0.0826  data: 0.0005  max mem: 259\n",
            "Train: Epoch[1/1]  [51/52]  eta: 0:00:00  Lr: 0.000047  Loss: 1.7112  Acc@1: 45.8333 (27.0512)  Acc@5: 87.5000 (73.4362)  time: 0.0799  data: 0.0005  max mem: 259\n",
            "Train: Epoch[1/1] Total time: 0:00:05 (0.1005 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.7112  Acc@1: 45.8333 (27.0512)  Acc@5: 87.5000 (73.4362)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:13  Loss: 0.9318 (0.9318)  Acc@1: 70.8333 (70.8333)  Acc@5: 87.5000 (87.5000)  time: 0.3162  data: 0.2456  max mem: 259\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 0.8356 (0.8315)  Acc@1: 79.1667 (75.0000)  Acc@5: 95.8333 (94.6970)  time: 0.1172  data: 0.0405  max mem: 259\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.7670 (0.8307)  Acc@1: 79.1667 (76.5873)  Acc@5: 95.8333 (94.2460)  time: 0.0898  data: 0.0103  max mem: 259\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.7665 (0.7962)  Acc@1: 83.3333 (77.6882)  Acc@5: 95.8333 (95.0269)  time: 0.0819  data: 0.0004  max mem: 259\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6957 (0.7697)  Acc@1: 79.1667 (78.3537)  Acc@5: 95.8333 (95.5285)  time: 0.0812  data: 0.0004  max mem: 259\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6923 (0.7630)  Acc@1: 79.1667 (78.5000)  Acc@5: 95.8333 (95.6000)  time: 0.0799  data: 0.0003  max mem: 259\n",
            "Test: [Task 1] Total time: 0:00:03 (0.0921 s / it)\n",
            "* Acc@1 78.500 Acc@5 95.600 loss 0.763\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:21  Loss: 1.0336 (1.0336)  Acc@1: 66.6667 (66.6667)  Acc@5: 91.6667 (91.6667)  time: 0.5062  data: 0.4288  max mem: 259\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:03  Loss: 0.8322 (0.8985)  Acc@1: 75.0000 (74.2424)  Acc@5: 95.8333 (96.5909)  time: 0.1204  data: 0.0399  max mem: 259\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 0.8926 (0.9745)  Acc@1: 75.0000 (72.8175)  Acc@5: 91.6667 (93.6508)  time: 0.0818  data: 0.0010  max mem: 259\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.8978 (0.9406)  Acc@1: 75.0000 (73.9247)  Acc@5: 91.6667 (94.3548)  time: 0.0840  data: 0.0012  max mem: 259\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.8151 (0.9180)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (94.3089)  time: 0.0843  data: 0.0009  max mem: 259\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.8151 (0.9162)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (94.4000)  time: 0.0831  data: 0.0008  max mem: 259\n",
            "Test: [Task 2] Total time: 0:00:04 (0.0962 s / it)\n",
            "* Acc@1 75.000 Acc@5 94.400 loss 0.916\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:29  Loss: 0.5675 (0.5675)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.7056  data: 0.6208  max mem: 259\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:04  Loss: 0.8263 (0.8481)  Acc@1: 79.1667 (77.6515)  Acc@5: 95.8333 (95.0758)  time: 0.1378  data: 0.0573  max mem: 259\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 0.8054 (0.8003)  Acc@1: 79.1667 (78.9683)  Acc@5: 95.8333 (95.4365)  time: 0.0816  data: 0.0011  max mem: 259\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.6873 (0.7690)  Acc@1: 79.1667 (78.4946)  Acc@5: 95.8333 (95.4301)  time: 0.0815  data: 0.0009  max mem: 259\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.6777 (0.7715)  Acc@1: 79.1667 (79.5732)  Acc@5: 95.8333 (95.0203)  time: 0.0808  data: 0.0004  max mem: 259\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.6777 (0.7735)  Acc@1: 79.1667 (79.4000)  Acc@5: 95.8333 (95.1000)  time: 0.0793  data: 0.0004  max mem: 259\n",
            "Test: [Task 3] Total time: 0:00:04 (0.0983 s / it)\n",
            "* Acc@1 79.400 Acc@5 95.100 loss 0.774\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:15  Loss: 1.3210 (1.3210)  Acc@1: 62.5000 (62.5000)  Acc@5: 75.0000 (75.0000)  time: 0.3721  data: 0.2923  max mem: 259\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:03  Loss: 0.8187 (0.8593)  Acc@1: 75.0000 (74.2424)  Acc@5: 95.8333 (94.3182)  time: 0.1111  data: 0.0324  max mem: 259\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 0.8187 (0.8410)  Acc@1: 75.0000 (75.3968)  Acc@5: 95.8333 (94.4444)  time: 0.0832  data: 0.0035  max mem: 259\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 0.7467 (0.8055)  Acc@1: 75.0000 (75.9409)  Acc@5: 95.8333 (94.7581)  time: 0.0809  data: 0.0004  max mem: 259\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7448 (0.8325)  Acc@1: 75.0000 (76.1179)  Acc@5: 95.8333 (94.5122)  time: 0.0807  data: 0.0003  max mem: 259\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.7251 (0.8237)  Acc@1: 75.0000 (76.3000)  Acc@5: 95.8333 (94.6000)  time: 0.0795  data: 0.0002  max mem: 259\n",
            "Test: [Task 4] Total time: 0:00:03 (0.0907 s / it)\n",
            "* Acc@1 76.300 Acc@5 94.600 loss 0.824\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:16  Loss: 0.7295 (0.7295)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  time: 0.3913  data: 0.3236  max mem: 259\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:03  Loss: 0.7295 (0.7351)  Acc@1: 79.1667 (79.9242)  Acc@5: 95.8333 (96.9697)  time: 0.1094  data: 0.0299  max mem: 259\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 0.7748 (0.7727)  Acc@1: 79.1667 (78.7698)  Acc@5: 95.8333 (96.8254)  time: 0.0809  data: 0.0004  max mem: 259\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 0.8208 (0.8011)  Acc@1: 79.1667 (78.3602)  Acc@5: 95.8333 (95.8333)  time: 0.0809  data: 0.0005  max mem: 259\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.8395 (0.8404)  Acc@1: 75.0000 (77.2358)  Acc@5: 95.8333 (95.5285)  time: 0.0815  data: 0.0004  max mem: 259\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.8645 (0.8591)  Acc@1: 75.0000 (77.0000)  Acc@5: 95.8333 (95.4000)  time: 0.0801  data: 0.0004  max mem: 259\n",
            "Test: [Task 5] Total time: 0:00:03 (0.0899 s / it)\n",
            "* Acc@1 77.000 Acc@5 95.400 loss 0.859\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:20  Loss: 0.7531 (0.7531)  Acc@1: 87.5000 (87.5000)  Acc@5: 91.6667 (91.6667)  time: 0.4992  data: 0.3840  max mem: 259\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:03  Loss: 0.8044 (0.8467)  Acc@1: 79.1667 (76.1364)  Acc@5: 95.8333 (95.8333)  time: 0.1228  data: 0.0368  max mem: 259\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 0.8857 (0.9219)  Acc@1: 70.8333 (73.2143)  Acc@5: 95.8333 (95.4365)  time: 0.0870  data: 0.0055  max mem: 259\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 0.9641 (0.9597)  Acc@1: 66.6667 (71.5054)  Acc@5: 95.8333 (95.5645)  time: 0.0884  data: 0.0073  max mem: 259\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 0.9774 (0.9783)  Acc@1: 66.6667 (71.1382)  Acc@5: 95.8333 (95.7317)  time: 0.0851  data: 0.0030  max mem: 259\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 0.9774 (0.9817)  Acc@1: 66.6667 (71.0000)  Acc@5: 95.8333 (95.7000)  time: 0.0834  data: 0.0030  max mem: 259\n",
            "Test: [Task 6] Total time: 0:00:04 (0.0984 s / it)\n",
            "* Acc@1 71.000 Acc@5 95.700 loss 0.982\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:18  Loss: 1.0237 (1.0237)  Acc@1: 79.1667 (79.1667)  Acc@5: 87.5000 (87.5000)  time: 0.4433  data: 0.3716  max mem: 259\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:03  Loss: 0.9943 (1.0098)  Acc@1: 75.0000 (71.2121)  Acc@5: 95.8333 (93.9394)  time: 0.1150  data: 0.0348  max mem: 259\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:02  Loss: 0.9628 (1.0346)  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (93.8492)  time: 0.0814  data: 0.0009  max mem: 259\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:01  Loss: 1.0444 (1.0660)  Acc@1: 70.8333 (70.6989)  Acc@5: 91.6667 (92.4731)  time: 0.0811  data: 0.0005  max mem: 259\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 1.0500 (1.0728)  Acc@1: 70.8333 (70.4268)  Acc@5: 91.6667 (92.4797)  time: 0.0814  data: 0.0003  max mem: 259\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 1.0444 (1.0683)  Acc@1: 70.8333 (70.6000)  Acc@5: 91.6667 (92.5000)  time: 0.0801  data: 0.0003  max mem: 259\n",
            "Test: [Task 7] Total time: 0:00:03 (0.0915 s / it)\n",
            "* Acc@1 70.600 Acc@5 92.500 loss 1.068\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:21  Loss: 0.9736 (0.9736)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  time: 0.5217  data: 0.4469  max mem: 259\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:03  Loss: 1.1227 (1.1319)  Acc@1: 62.5000 (64.7727)  Acc@5: 95.8333 (93.1818)  time: 0.1217  data: 0.0413  max mem: 259\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:02  Loss: 1.1227 (1.1162)  Acc@1: 66.6667 (65.2778)  Acc@5: 95.8333 (93.8492)  time: 0.0814  data: 0.0005  max mem: 259\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:01  Loss: 1.0631 (1.0979)  Acc@1: 66.6667 (66.3979)  Acc@5: 95.8333 (93.9516)  time: 0.0812  data: 0.0003  max mem: 259\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 1.1226 (1.1140)  Acc@1: 66.6667 (65.8537)  Acc@5: 91.6667 (93.8008)  time: 0.0815  data: 0.0003  max mem: 259\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 1.1226 (1.1116)  Acc@1: 66.6667 (66.1000)  Acc@5: 91.6667 (93.8000)  time: 0.0801  data: 0.0003  max mem: 259\n",
            "Test: [Task 8] Total time: 0:00:03 (0.0932 s / it)\n",
            "* Acc@1 66.100 Acc@5 93.800 loss 1.112\n",
            "Test: [Task 9]  [ 0/42]  eta: 0:00:13  Loss: 1.1113 (1.1113)  Acc@1: 58.3333 (58.3333)  Acc@5: 100.0000 (100.0000)  time: 0.3140  data: 0.2381  max mem: 259\n",
            "Test: [Task 9]  [10/42]  eta: 0:00:03  Loss: 1.1438 (1.1725)  Acc@1: 70.8333 (68.5606)  Acc@5: 95.8333 (93.9394)  time: 0.1119  data: 0.0342  max mem: 259\n",
            "Test: [Task 9]  [20/42]  eta: 0:00:02  Loss: 1.1981 (1.1769)  Acc@1: 66.6667 (66.4683)  Acc@5: 95.8333 (94.4444)  time: 0.0871  data: 0.0071  max mem: 259\n",
            "Test: [Task 9]  [30/42]  eta: 0:00:01  Loss: 1.2338 (1.2197)  Acc@1: 62.5000 (65.1882)  Acc@5: 95.8333 (93.8172)  time: 0.0826  data: 0.0005  max mem: 259\n",
            "Test: [Task 9]  [40/42]  eta: 0:00:00  Loss: 1.1117 (1.1784)  Acc@1: 66.6667 (65.8537)  Acc@5: 95.8333 (94.3089)  time: 0.0824  data: 0.0007  max mem: 259\n",
            "Test: [Task 9]  [41/42]  eta: 0:00:00  Loss: 1.1117 (1.1678)  Acc@1: 66.6667 (66.2000)  Acc@5: 95.8333 (94.4000)  time: 0.0810  data: 0.0007  max mem: 259\n",
            "Test: [Task 9] Total time: 0:00:03 (0.0924 s / it)\n",
            "* Acc@1 66.200 Acc@5 94.400 loss 1.168\n",
            "Test: [Task 10]  [ 0/42]  eta: 0:00:23  Loss: 6.6182 (6.6182)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  time: 0.5609  data: 0.4529  max mem: 259\n",
            "Test: [Task 10]  [10/42]  eta: 0:00:04  Loss: 6.6620 (6.6527)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  time: 0.1386  data: 0.0538  max mem: 259\n",
            "Test: [Task 10]  [20/42]  eta: 0:00:02  Loss: 6.6596 (6.6514)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  time: 0.0911  data: 0.0071  max mem: 259\n",
            "Test: [Task 10]  [30/42]  eta: 0:00:01  Loss: 6.7245 (6.6757)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  time: 0.0846  data: 0.0009  max mem: 259\n",
            "Test: [Task 10]  [40/42]  eta: 0:00:00  Loss: 6.7258 (6.6767)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  time: 0.0822  data: 0.0008  max mem: 259\n",
            "Test: [Task 10]  [41/42]  eta: 0:00:00  Loss: 6.7245 (6.6694)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  time: 0.0810  data: 0.0006  max mem: 259\n",
            "Test: [Task 10] Total time: 0:00:04 (0.0994 s / it)\n",
            "* Acc@1 0.000 Acc@5 0.000 loss 6.669\n",
            "[Average accuracy till task10]\tAcc@1: 66.0100\tAcc@5: 85.1500\tLoss: 1.5134\tForgetting: 2.7000\tBackward: 71.1111\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([23568, 384])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 0.9932  Acc@1: 83.3333 (84.2130)  Acc@5: 87.5000 (88.1019)\n",
            "torch.Size([23568, 384])\n",
            "Averaged stats: Lr: 0.000467  Loss: 0.5040  Acc@1: 87.5000 (86.7130)  Acc@5: 91.6667 (91.3426)\n",
            "torch.Size([23568, 384])\n",
            "Averaged stats: Lr: 0.000464  Loss: 1.1690  Acc@1: 83.3333 (86.8519)  Acc@5: 95.8333 (93.2870)\n",
            "torch.Size([23568, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 0.5866  Acc@1: 87.5000 (88.0556)  Acc@5: 95.8333 (95.9259)\n",
            "torch.Size([23568, 384])\n",
            "Averaged stats: Lr: 0.000448  Loss: 0.6984  Acc@1: 91.6667 (88.2407)  Acc@5: 100.0000 (97.6852)\n",
            "torch.Size([23568, 384])\n",
            "Averaged stats: Lr: 0.000437  Loss: 0.5567  Acc@1: 91.6667 (89.1667)  Acc@5: 100.0000 (99.3519)\n",
            "torch.Size([23568, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 0.4550  Acc@1: 91.6667 (90.6482)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([23568, 384])\n",
            "Averaged stats: Lr: 0.000409  Loss: 0.3181  Acc@1: 91.6667 (90.8796)  Acc@5: 100.0000 (99.8611)\n",
            "torch.Size([23568, 384])\n",
            "Averaged stats: Lr: 0.000391  Loss: 0.5505  Acc@1: 91.6667 (91.9907)  Acc@5: 100.0000 (99.8148)\n",
            "torch.Size([23568, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 0.4599  Acc@1: 95.8333 (92.5000)  Acc@5: 100.0000 (99.8611)\n",
            "torch.Size([23568, 384])\n",
            "Averaged stats: Lr: 0.000352  Loss: 0.3693  Acc@1: 95.8333 (93.6574)  Acc@5: 100.0000 (99.8148)\n",
            "torch.Size([23568, 384])\n",
            "Averaged stats: Lr: 0.000330  Loss: 0.3308  Acc@1: 95.8333 (93.7963)  Acc@5: 100.0000 (99.7222)\n",
            "torch.Size([23568, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 0.2769  Acc@1: 95.8333 (93.9815)  Acc@5: 100.0000 (99.9074)\n",
            "torch.Size([23568, 384])\n",
            "Averaged stats: Lr: 0.000283  Loss: 0.3269  Acc@1: 95.8333 (94.3982)  Acc@5: 100.0000 (99.8148)\n",
            "torch.Size([23568, 384])\n",
            "Averaged stats: Lr: 0.000259  Loss: 0.3454  Acc@1: 91.6667 (94.0741)  Acc@5: 100.0000 (99.8611)\n",
            "torch.Size([23568, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 0.3507  Acc@1: 95.8333 (94.5370)  Acc@5: 100.0000 (99.9074)\n",
            "torch.Size([23568, 384])\n",
            "Averaged stats: Lr: 0.000210  Loss: 0.3296  Acc@1: 95.8333 (94.8611)  Acc@5: 100.0000 (99.9537)\n",
            "torch.Size([23568, 384])\n",
            "Averaged stats: Lr: 0.000186  Loss: 0.3463  Acc@1: 95.8333 (94.5833)  Acc@5: 100.0000 (99.9074)\n",
            "torch.Size([23568, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.1761  Acc@1: 95.8333 (95.0926)  Acc@5: 100.0000 (99.9074)\n",
            "torch.Size([23568, 384])\n",
            "Averaged stats: Lr: 0.000139  Loss: 0.3141  Acc@1: 95.8333 (93.6111)  Acc@5: 100.0000 (99.9537)\n",
            "torch.Size([23568, 384])\n",
            "Averaged stats: Lr: 0.000117  Loss: 0.5730  Acc@1: 95.8333 (94.9074)  Acc@5: 100.0000 (99.9074)\n",
            "torch.Size([23568, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 0.4890  Acc@1: 95.8333 (94.7222)  Acc@5: 100.0000 (99.8148)\n",
            "torch.Size([23568, 384])\n",
            "Averaged stats: Lr: 0.000078  Loss: 0.3707  Acc@1: 95.8333 (95.0463)  Acc@5: 100.0000 (99.9537)\n",
            "torch.Size([23568, 384])\n",
            "Averaged stats: Lr: 0.000060  Loss: 0.4293  Acc@1: 91.6667 (94.8611)  Acc@5: 100.0000 (99.8148)\n",
            "torch.Size([23568, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 0.4530  Acc@1: 95.8333 (94.7685)  Acc@5: 100.0000 (99.8148)\n",
            "torch.Size([23568, 384])\n",
            "Averaged stats: Lr: 0.000031  Loss: 0.3625  Acc@1: 91.6667 (93.7963)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([23568, 384])\n",
            "Averaged stats: Lr: 0.000020  Loss: 0.4409  Acc@1: 95.8333 (94.6759)  Acc@5: 100.0000 (99.9537)\n",
            "torch.Size([23568, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.4109  Acc@1: 95.8333 (94.7222)  Acc@5: 100.0000 (99.7685)\n",
            "torch.Size([23568, 384])\n",
            "Averaged stats: Lr: 0.000005  Loss: 0.2827  Acc@1: 91.6667 (94.1667)  Acc@5: 100.0000 (99.8611)\n",
            "torch.Size([23568, 384])\n",
            "Averaged stats: Lr: 0.000001  Loss: 0.3335  Acc@1: 95.8333 (94.9074)  Acc@5: 100.0000 (99.9074)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:30  Loss: 0.9381 (0.9381)  Acc@1: 79.1667 (79.1667)  Acc@5: 87.5000 (87.5000)  time: 0.7290  data: 0.6243  max mem: 259\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 0.8302 (0.8444)  Acc@1: 79.1667 (75.3788)  Acc@5: 95.8333 (93.5606)  time: 0.1425  data: 0.0576  max mem: 259\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.7686 (0.8316)  Acc@1: 79.1667 (76.9841)  Acc@5: 95.8333 (94.4444)  time: 0.0821  data: 0.0007  max mem: 259\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.7686 (0.7966)  Acc@1: 83.3333 (78.0914)  Acc@5: 95.8333 (95.1613)  time: 0.0816  data: 0.0004  max mem: 259\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6934 (0.7721)  Acc@1: 79.1667 (78.6585)  Acc@5: 95.8333 (95.5285)  time: 0.0830  data: 0.0003  max mem: 259\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6912 (0.7654)  Acc@1: 83.3333 (78.8000)  Acc@5: 95.8333 (95.6000)  time: 0.0815  data: 0.0003  max mem: 259\n",
            "Test: [Task 1] Total time: 0:00:04 (0.0994 s / it)\n",
            "* Acc@1 78.800 Acc@5 95.600 loss 0.765\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:18  Loss: 1.0813 (1.0813)  Acc@1: 66.6667 (66.6667)  Acc@5: 87.5000 (87.5000)  time: 0.4290  data: 0.3489  max mem: 259\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:03  Loss: 0.8860 (0.9375)  Acc@1: 75.0000 (74.6212)  Acc@5: 95.8333 (95.8333)  time: 0.1152  data: 0.0325  max mem: 259\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 0.9183 (1.0197)  Acc@1: 70.8333 (71.4286)  Acc@5: 91.6667 (93.2540)  time: 0.0819  data: 0.0006  max mem: 259\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.9202 (0.9791)  Acc@1: 70.8333 (72.8495)  Acc@5: 91.6667 (94.0860)  time: 0.0815  data: 0.0004  max mem: 259\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.8283 (0.9550)  Acc@1: 75.0000 (73.8821)  Acc@5: 95.8333 (94.0041)  time: 0.0831  data: 0.0004  max mem: 259\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.8283 (0.9538)  Acc@1: 75.0000 (73.9000)  Acc@5: 95.8333 (94.1000)  time: 0.0816  data: 0.0004  max mem: 259\n",
            "Test: [Task 2] Total time: 0:00:03 (0.0922 s / it)\n",
            "* Acc@1 73.900 Acc@5 94.100 loss 0.954\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:19  Loss: 0.5108 (0.5108)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.4721  data: 0.3787  max mem: 259\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 0.8394 (0.8357)  Acc@1: 79.1667 (78.0303)  Acc@5: 95.8333 (94.6970)  time: 0.1182  data: 0.0351  max mem: 259\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 0.7943 (0.7837)  Acc@1: 79.1667 (79.3651)  Acc@5: 95.8333 (95.4365)  time: 0.0822  data: 0.0006  max mem: 259\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.6933 (0.7559)  Acc@1: 79.1667 (78.7634)  Acc@5: 95.8333 (95.4301)  time: 0.0826  data: 0.0005  max mem: 259\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.6933 (0.7593)  Acc@1: 79.1667 (79.3699)  Acc@5: 95.8333 (95.0203)  time: 0.0833  data: 0.0006  max mem: 259\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.6933 (0.7610)  Acc@1: 79.1667 (79.2000)  Acc@5: 95.8333 (95.1000)  time: 0.0815  data: 0.0006  max mem: 259\n",
            "Test: [Task 3] Total time: 0:00:03 (0.0945 s / it)\n",
            "* Acc@1 79.200 Acc@5 95.100 loss 0.761\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:25  Loss: 1.3773 (1.3773)  Acc@1: 62.5000 (62.5000)  Acc@5: 79.1667 (79.1667)  time: 0.6095  data: 0.5218  max mem: 259\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:04  Loss: 0.8708 (0.9478)  Acc@1: 70.8333 (69.3182)  Acc@5: 95.8333 (93.5606)  time: 0.1380  data: 0.0548  max mem: 259\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 0.8499 (0.9207)  Acc@1: 70.8333 (72.0238)  Acc@5: 95.8333 (93.6508)  time: 0.0894  data: 0.0061  max mem: 259\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 0.8400 (0.8840)  Acc@1: 70.8333 (72.9839)  Acc@5: 95.8333 (94.0860)  time: 0.0854  data: 0.0028  max mem: 259\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.9150 (0.9182)  Acc@1: 70.8333 (73.3740)  Acc@5: 95.8333 (93.3943)  time: 0.0834  data: 0.0009  max mem: 259\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.8080 (0.9093)  Acc@1: 70.8333 (73.6000)  Acc@5: 93.7500 (93.4000)  time: 0.0822  data: 0.0009  max mem: 259\n",
            "Test: [Task 4] Total time: 0:00:04 (0.1000 s / it)\n",
            "* Acc@1 73.600 Acc@5 93.400 loss 0.909\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:19  Loss: 0.7515 (0.7515)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.4704  data: 0.3860  max mem: 259\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:03  Loss: 0.7515 (0.7337)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (97.7273)  time: 0.1192  data: 0.0357  max mem: 259\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 0.7805 (0.7827)  Acc@1: 79.1667 (77.7778)  Acc@5: 95.8333 (96.8254)  time: 0.0827  data: 0.0007  max mem: 259\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 0.8275 (0.8109)  Acc@1: 79.1667 (77.8226)  Acc@5: 95.8333 (95.8333)  time: 0.0827  data: 0.0006  max mem: 259\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.8438 (0.8499)  Acc@1: 75.0000 (77.1341)  Acc@5: 95.8333 (95.5285)  time: 0.0839  data: 0.0004  max mem: 259\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.8572 (0.8703)  Acc@1: 75.0000 (76.7000)  Acc@5: 95.8333 (95.3000)  time: 0.0821  data: 0.0004  max mem: 259\n",
            "Test: [Task 5] Total time: 0:00:03 (0.0938 s / it)\n",
            "* Acc@1 76.700 Acc@5 95.300 loss 0.870\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:19  Loss: 0.7154 (0.7154)  Acc@1: 87.5000 (87.5000)  Acc@5: 91.6667 (91.6667)  time: 0.4638  data: 0.3887  max mem: 259\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:03  Loss: 0.7886 (0.8414)  Acc@1: 75.0000 (75.3788)  Acc@5: 95.8333 (95.8333)  time: 0.1192  data: 0.0360  max mem: 259\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 0.9021 (0.9306)  Acc@1: 70.8333 (73.4127)  Acc@5: 95.8333 (95.4365)  time: 0.0826  data: 0.0006  max mem: 259\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 0.9691 (0.9721)  Acc@1: 66.6667 (70.2957)  Acc@5: 95.8333 (95.2957)  time: 0.0820  data: 0.0005  max mem: 259\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 0.9709 (0.9882)  Acc@1: 66.6667 (70.1220)  Acc@5: 95.8333 (95.1220)  time: 0.0833  data: 0.0003  max mem: 259\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 0.9709 (0.9919)  Acc@1: 66.6667 (69.9000)  Acc@5: 95.8333 (95.1000)  time: 0.0814  data: 0.0003  max mem: 259\n",
            "Test: [Task 6] Total time: 0:00:03 (0.0932 s / it)\n",
            "* Acc@1 69.900 Acc@5 95.100 loss 0.992\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:20  Loss: 1.0570 (1.0570)  Acc@1: 79.1667 (79.1667)  Acc@5: 87.5000 (87.5000)  time: 0.4934  data: 0.4284  max mem: 259\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:03  Loss: 0.9886 (0.9877)  Acc@1: 75.0000 (70.0758)  Acc@5: 95.8333 (93.1818)  time: 0.1210  data: 0.0413  max mem: 259\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:02  Loss: 0.9374 (1.0134)  Acc@1: 70.8333 (70.4365)  Acc@5: 95.8333 (93.4524)  time: 0.0828  data: 0.0020  max mem: 259\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:01  Loss: 1.0332 (1.0451)  Acc@1: 70.8333 (70.2957)  Acc@5: 91.6667 (92.3387)  time: 0.0830  data: 0.0008  max mem: 259\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 1.0312 (1.0462)  Acc@1: 70.8333 (70.3252)  Acc@5: 91.6667 (92.5813)  time: 0.0837  data: 0.0005  max mem: 259\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 1.0209 (1.0399)  Acc@1: 70.8333 (70.5000)  Acc@5: 91.6667 (92.7000)  time: 0.0821  data: 0.0005  max mem: 259\n",
            "Test: [Task 7] Total time: 0:00:03 (0.0951 s / it)\n",
            "* Acc@1 70.500 Acc@5 92.700 loss 1.040\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:24  Loss: 0.9568 (0.9568)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  time: 0.5805  data: 0.4984  max mem: 259\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:04  Loss: 1.0956 (1.0948)  Acc@1: 70.8333 (68.9394)  Acc@5: 91.6667 (91.6667)  time: 0.1324  data: 0.0503  max mem: 259\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:02  Loss: 1.0795 (1.0764)  Acc@1: 66.6667 (69.4444)  Acc@5: 95.8333 (92.8571)  time: 0.0841  data: 0.0031  max mem: 259\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:01  Loss: 1.0665 (1.0577)  Acc@1: 66.6667 (69.6237)  Acc@5: 95.8333 (93.1452)  time: 0.0817  data: 0.0005  max mem: 259\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 1.0665 (1.0714)  Acc@1: 66.6667 (68.4959)  Acc@5: 91.6667 (92.9878)  time: 0.0826  data: 0.0002  max mem: 259\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 1.0665 (1.0695)  Acc@1: 66.6667 (68.6000)  Acc@5: 91.6667 (93.1000)  time: 0.0812  data: 0.0002  max mem: 259\n",
            "Test: [Task 8] Total time: 0:00:04 (0.0965 s / it)\n",
            "* Acc@1 68.600 Acc@5 93.100 loss 1.069\n",
            "Test: [Task 9]  [ 0/42]  eta: 0:00:18  Loss: 0.8658 (0.8658)  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  time: 0.4495  data: 0.3808  max mem: 259\n",
            "Test: [Task 9]  [10/42]  eta: 0:00:03  Loss: 0.8981 (0.9260)  Acc@1: 70.8333 (74.2424)  Acc@5: 95.8333 (95.4545)  time: 0.1170  data: 0.0357  max mem: 259\n",
            "Test: [Task 9]  [20/42]  eta: 0:00:02  Loss: 0.9056 (0.9197)  Acc@1: 70.8333 (75.0000)  Acc@5: 95.8333 (95.4365)  time: 0.0820  data: 0.0011  max mem: 259\n",
            "Test: [Task 9]  [30/42]  eta: 0:00:01  Loss: 0.9705 (0.9656)  Acc@1: 75.0000 (73.9247)  Acc@5: 95.8333 (94.8925)  time: 0.0815  data: 0.0007  max mem: 259\n",
            "Test: [Task 9]  [40/42]  eta: 0:00:00  Loss: 0.8663 (0.9137)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.3252)  time: 0.0822  data: 0.0002  max mem: 259\n",
            "Test: [Task 9]  [41/42]  eta: 0:00:00  Loss: 0.8663 (0.9018)  Acc@1: 75.0000 (75.3000)  Acc@5: 95.8333 (95.4000)  time: 0.0808  data: 0.0002  max mem: 259\n",
            "Test: [Task 9] Total time: 0:00:03 (0.0921 s / it)\n",
            "* Acc@1 75.300 Acc@5 95.400 loss 0.902\n",
            "Test: [Task 10]  [ 0/42]  eta: 0:00:20  Loss: 1.2933 (1.2933)  Acc@1: 62.5000 (62.5000)  Acc@5: 87.5000 (87.5000)  time: 0.4942  data: 0.4092  max mem: 259\n",
            "Test: [Task 10]  [10/42]  eta: 0:00:03  Loss: 1.4850 (1.4305)  Acc@1: 58.3333 (56.8182)  Acc@5: 91.6667 (90.5303)  time: 0.1192  data: 0.0385  max mem: 259\n",
            "Test: [Task 10]  [20/42]  eta: 0:00:02  Loss: 1.4410 (1.3988)  Acc@1: 58.3333 (58.7302)  Acc@5: 91.6667 (90.4762)  time: 0.0812  data: 0.0009  max mem: 259\n",
            "Test: [Task 10]  [30/42]  eta: 0:00:01  Loss: 1.4382 (1.3973)  Acc@1: 58.3333 (58.6022)  Acc@5: 91.6667 (90.7258)  time: 0.0818  data: 0.0004  max mem: 259\n",
            "Test: [Task 10]  [40/42]  eta: 0:00:00  Loss: 1.4504 (1.4007)  Acc@1: 54.1667 (57.4187)  Acc@5: 91.6667 (91.3618)  time: 0.0821  data: 0.0004  max mem: 259\n",
            "Test: [Task 10]  [41/42]  eta: 0:00:00  Loss: 1.4382 (1.3927)  Acc@1: 54.1667 (57.7000)  Acc@5: 91.6667 (91.3000)  time: 0.0808  data: 0.0004  max mem: 259\n",
            "Test: [Task 10] Total time: 0:00:03 (0.0948 s / it)\n",
            "* Acc@1 57.700 Acc@5 91.300 loss 1.393\n",
            "[Average accuracy till task10]\tAcc@1: 72.4200\tAcc@5: 94.1100\tLoss: 0.9656\tForgetting: 3.2444\tBackward: 11.8444\n",
            "Total training time: 0:15:44\n",
            "[rank0]:[W1006 10:55:42.072985490 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
          ]
        }
      ],
      "source": [
        "!torchrun --nproc_per_node=1 main.py cifar100_hideprompt_5e --original_model vit_small_patch16_224.dino --model vit_small_patch16_224.dino --batch-size 24 --data-path ./datasets/ --output_dir ./output/cifar100_full_dino_1epoch_10pct --epochs 1 --sched constant --seed 20 --train_inference_task_only --lr 0.0005 --pct 0.10\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "902dd0fc",
      "metadata": {
        "id": "902dd0fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7972b70-a7ad-471c-d9dd-a1e5717a2d10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(subparser_name='cifar100_hideprompt_5e', pct=0.25, batch_size=24, epochs=1, original_model='vit_small_patch16_224.dino', model='vit_small_patch16_224.dino', input_size=224, pretrained=True, drop=0.0, drop_path=0.0, opt='adam', opt_eps=1e-08, opt_betas=(0.9, 0.999), clip_grad=1.0, momentum=0.9, weight_decay=0.0, reinit_optimizer=True, sched='step', lr=0.03, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=0, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, unscale_lr=True, color_jitter=None, aa=None, smoothing=0.1, train_interpolation='bicubic', reprob=0.0, remode='pixel', recount=1, data_path='./datasets/', dataset='Split-CIFAR100', shuffle=False, output_dir='./output/cifar100_full_dino_1epoch_final_25pct', device='cuda', seed=20, eval=False, num_workers=4, pin_mem=True, world_size=1, dist_url='env://', num_tasks=10, train_mask=True, task_inc=False, use_g_prompt=False, g_prompt_length=5, g_prompt_layer_idx=[], use_prefix_tune_for_g_prompt=False, use_e_prompt=True, e_prompt_layer_idx=[0, 1, 2, 3, 4], use_prefix_tune_for_e_prompt=True, larger_prompt_lr=True, prompt_pool=True, size=10, length=5, top_k=1, initializer='uniform', prompt_key=False, prompt_key_init='uniform', use_prompt_mask=True, mask_first_epoch=False, shared_prompt_pool=True, shared_prompt_key=False, batchwise_prompt=False, embedding_key='cls', predefined_key='', pull_constraint=True, pull_constraint_coeff=1.0, same_key_value=False, global_pool='token', head_type='token', freeze=['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed'], crct_epochs=1, train_inference_task_only=False, original_model_mlp_structure=[2], ca_lr=0.005, milestones=[10], trained_original_model='./output/cifar100_full_dino_1epoch_25pct', prompt_momentum=0.1, reg=0.1, not_train_ca=False, ca_epochs=30, ca_storage_efficient_method='multi-centroid', n_centroids=10, print_freq=10, config='cifar100_hideprompt_5e')\n",
            "| distributed init (rank 0): env://\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "[rank0]:[W1006 10:56:51.837160810 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/helpers.py:7: FutureWarning: Importing from timm.models.helpers is deprecated, please import via timm.models\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:886: UserWarning: Overwriting vit_tiny_patch16_224 in registry with vits.hide_prompt_vision_transformer.vit_tiny_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:895: UserWarning: Overwriting vit_tiny_patch16_384 in registry with vits.hide_prompt_vision_transformer.vit_tiny_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:904: UserWarning: Overwriting vit_small_patch32_224 in registry with vits.hide_prompt_vision_transformer.vit_small_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:913: UserWarning: Overwriting vit_small_patch32_384 in registry with vits.hide_prompt_vision_transformer.vit_small_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:922: UserWarning: Overwriting vit_small_patch16_224 in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:932: UserWarning: Overwriting vit_small_patch16_384 in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:942: UserWarning: Overwriting vit_base_patch32_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:952: UserWarning: Overwriting vit_base_patch32_384 in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:962: UserWarning: Overwriting vit_base_patch16_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:972: UserWarning: Overwriting vit_base_patch16_384 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:982: UserWarning: Overwriting vit_base_patch8_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:992: UserWarning: Overwriting vit_large_patch32_224 in registry with vits.hide_prompt_vision_transformer.vit_large_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1001: UserWarning: Overwriting vit_large_patch32_384 in registry with vits.hide_prompt_vision_transformer.vit_large_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1011: UserWarning: Overwriting vit_large_patch16_224 in registry with vits.hide_prompt_vision_transformer.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1021: UserWarning: Overwriting vit_large_patch16_384 in registry with vits.hide_prompt_vision_transformer.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1031: UserWarning: Overwriting vit_large_patch14_224 in registry with vits.hide_prompt_vision_transformer.vit_large_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1040: UserWarning: Overwriting vit_huge_patch14_224 in registry with vits.hide_prompt_vision_transformer.vit_huge_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1049: UserWarning: Overwriting vit_giant_patch14_224 in registry with vits.hide_prompt_vision_transformer.vit_giant_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1058: UserWarning: Overwriting vit_gigantic_patch14_224 in registry with vits.hide_prompt_vision_transformer.vit_gigantic_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1067: UserWarning: Overwriting vit_tiny_patch16_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_tiny_patch16_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1078: UserWarning: Overwriting vit_small_patch32_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_small_patch32_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1089: UserWarning: Overwriting vit_small_patch16_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1100: UserWarning: Overwriting vit_base_patch32_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1111: UserWarning: Overwriting vit_base_patch16_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1122: UserWarning: Overwriting vit_base_patch8_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_base_patch8_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1133: UserWarning: Overwriting vit_large_patch32_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_large_patch32_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1144: UserWarning: Overwriting vit_large_patch16_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_large_patch16_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1155: UserWarning: Overwriting vit_huge_patch14_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_huge_patch14_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1166: UserWarning: Overwriting vit_base_patch16_224_sam in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_sam. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1175: UserWarning: Overwriting vit_base_patch32_224_sam in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_224_sam. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1184: UserWarning: Overwriting vit_small_patch16_224_dino in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_224_dino. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1193: UserWarning: Overwriting vit_small_patch8_224_dino in registry with vits.hide_prompt_vision_transformer.vit_small_patch8_224_dino. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1211: UserWarning: Overwriting vit_base_patch8_224_dino in registry with vits.hide_prompt_vision_transformer.vit_base_patch8_224_dino. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1220: UserWarning: Overwriting vit_base_patch16_224_miil_in21k in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_miil_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1230: UserWarning: Overwriting vit_base_patch16_224_miil in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_miil. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1242: UserWarning: Overwriting vit_base_patch32_plus_256 in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_plus_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1251: UserWarning: Overwriting vit_base_patch16_plus_240 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_plus_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1260: UserWarning: Overwriting vit_base_patch16_rpn_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_rpn_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1271: UserWarning: Overwriting vit_small_patch16_36x1_224 in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_36x1_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1282: UserWarning: Overwriting vit_small_patch16_18x2_224 in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1294: UserWarning: Overwriting vit_base_patch16_18x2_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1331: UserWarning: Overwriting vit_base_patch16_224_dino in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_dino. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "Original train size:  50000\n",
            "Sampled train size:  12500\n",
            "Original train size:  50000\n",
            "Sampled train size:  12500\n",
            "100\n",
            "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, 13, 14, 15, 16, 17, 18, 19], [20, 21, 22, 23, 24, 25, 26, 27, 28, 29], [30, 31, 32, 33, 34, 35, 36, 37, 38, 39], [40, 41, 42, 43, 44, 45, 46, 47, 48, 49], [50, 51, 52, 53, 54, 55, 56, 57, 58, 59], [60, 61, 62, 63, 64, 65, 66, 67, 68, 69], [70, 71, 72, 73, 74, 75, 76, 77, 78, 79], [80, 81, 82, 83, 84, 85, 86, 87, 88, 89], [90, 91, 92, 93, 94, 95, 96, 97, 98, 99]]\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Creating original model: vit_small_patch16_224.dino\n",
            "[Sequential(\n",
            "  (0): Linear(in_features=384, out_features=768, bias=True)\n",
            "  (1): GELU(approximate='none')\n",
            "  (2): Dropout(p=0.0, inplace=False)\n",
            "), Sequential(\n",
            "  (0): Linear(in_features=768, out_features=384, bias=True)\n",
            "  (1): Dropout(p=0.0, inplace=False)\n",
            ")]\n",
            "Creating model: vit_small_patch16_224.dino\n",
            "Namespace(subparser_name='cifar100_hideprompt_5e', pct=0.25, batch_size=24, epochs=1, original_model='vit_small_patch16_224.dino', model='vit_small_patch16_224.dino', input_size=224, pretrained=True, drop=0.0, drop_path=0.0, opt='adam', opt_eps=1e-08, opt_betas=(0.9, 0.999), clip_grad=1.0, momentum=0.9, weight_decay=0.0, reinit_optimizer=True, sched='step', lr=0.03, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=0, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, unscale_lr=True, color_jitter=None, aa=None, smoothing=0.1, train_interpolation='bicubic', reprob=0.0, remode='pixel', recount=1, data_path='./datasets/', dataset='Split-CIFAR100', shuffle=False, output_dir='./output/cifar100_full_dino_1epoch_final_25pct', device='cuda', seed=20, eval=False, num_workers=4, pin_mem=True, world_size=1, dist_url='env://', num_tasks=10, train_mask=True, task_inc=False, use_g_prompt=False, g_prompt_length=5, g_prompt_layer_idx=[], use_prefix_tune_for_g_prompt=False, use_e_prompt=True, e_prompt_layer_idx=[0, 1, 2, 3, 4], use_prefix_tune_for_e_prompt=True, larger_prompt_lr=True, prompt_pool=True, size=10, length=5, top_k=1, initializer='uniform', prompt_key=False, prompt_key_init='uniform', use_prompt_mask=True, mask_first_epoch=False, shared_prompt_pool=True, shared_prompt_key=False, batchwise_prompt=False, embedding_key='cls', predefined_key='', pull_constraint=True, pull_constraint_coeff=1.0, same_key_value=False, global_pool='token', head_type='token', freeze=['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed'], crct_epochs=1, train_inference_task_only=False, original_model_mlp_structure=[2], ca_lr=0.005, milestones=[10], trained_original_model='./output/cifar100_full_dino_1epoch_25pct', prompt_momentum=0.1, reg=0.1, not_train_ca=False, ca_epochs=30, ca_storage_efficient_method='multi-centroid', n_centroids=10, print_freq=10, config='cifar100_hideprompt_5e', rank=0, gpu=0, distributed=True, dist_backend='nccl', nb_classes=100)\n",
            "number of params: 230500\n",
            "Start training for 1 epochs\n",
            "Loading checkpoint from: ./output/cifar100_full_dino_1epoch_25pct/checkpoint/task1_checkpoint.pth\n",
            "[rank0]:[W1006 10:57:02.655868532 reducer.cpp:1457] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/engines/hide_promtp_wtp_and_tap_engine.py:545: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  loss = torch.nn.functional.cross_entropy(sim, torch.range(0, sim.shape[0] - 1).long().to(device))\n",
            "Train: Epoch[1/1]  [ 0/54]  eta: 0:02:14  Lr: 0.002812  Loss: 3.3383  Acc@1: 16.6667 (16.6667)  Acc@5: 58.3333 (58.3333)  time: 2.4911  data: 1.2502  max mem: 1368\n",
            "Train: Epoch[1/1]  [10/54]  eta: 0:00:19  Lr: 0.002812  Loss: 2.2149  Acc@1: 16.6667 (16.2879)  Acc@5: 50.0000 (53.4091)  time: 0.4417  data: 0.1139  max mem: 1370\n",
            "Train: Epoch[1/1]  [20/54]  eta: 0:00:11  Lr: 0.002812  Loss: 2.3892  Acc@1: 16.6667 (18.6508)  Acc@5: 58.3333 (59.7222)  time: 0.2380  data: 0.0004  max mem: 1370\n",
            "Train: Epoch[1/1]  [30/54]  eta: 0:00:07  Lr: 0.002812  Loss: 1.9842  Acc@1: 20.8333 (20.0269)  Acc@5: 75.0000 (64.9194)  time: 0.2412  data: 0.0009  max mem: 1370\n",
            "Train: Epoch[1/1]  [40/54]  eta: 0:00:04  Lr: 0.002812  Loss: 1.6835  Acc@1: 29.1667 (25.7114)  Acc@5: 79.1667 (69.6138)  time: 0.2419  data: 0.0009  max mem: 1370\n",
            "Train: Epoch[1/1]  [50/54]  eta: 0:00:01  Lr: 0.002812  Loss: 1.0219  Acc@1: 45.8333 (30.3922)  Acc@5: 87.5000 (73.5294)  time: 0.2407  data: 0.0003  max mem: 1370\n",
            "Train: Epoch[1/1]  [53/54]  eta: 0:00:00  Lr: 0.002812  Loss: 1.3430  Acc@1: 45.8333 (31.0748)  Acc@5: 87.5000 (74.2991)  time: 0.2397  data: 0.0003  max mem: 1370\n",
            "Train: Epoch[1/1] Total time: 0:00:15 (0.2826 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.3430  Acc@1: 45.8333 (31.0748)  Acc@5: 87.5000 (74.2991)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:18  Loss: 2.9719 (2.9719)  Acc@1: 41.6667 (41.6667)  Acc@5: 54.1667 (54.1667)  Acc@task: 100.0000 (100.0000)  time: 0.4335  data: 0.2702  max mem: 1370\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:05  Loss: 3.2749 (3.2952)  Acc@1: 20.8333 (23.8636)  Acc@5: 50.0000 (48.4848)  Acc@task: 100.0000 (100.0000)  time: 0.1816  data: 0.0251  max mem: 1370\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:03  Loss: 3.2661 (3.2614)  Acc@1: 20.8333 (24.0079)  Acc@5: 45.8333 (48.6111)  Acc@task: 100.0000 (100.0000)  time: 0.1572  data: 0.0004  max mem: 1370\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 3.1822 (3.2148)  Acc@1: 20.8333 (24.1935)  Acc@5: 50.0000 (50.1344)  Acc@task: 100.0000 (100.0000)  time: 0.1578  data: 0.0003  max mem: 1370\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 3.0914 (3.1983)  Acc@1: 25.0000 (24.5935)  Acc@5: 50.0000 (50.3049)  Acc@task: 100.0000 (100.0000)  time: 0.1577  data: 0.0003  max mem: 1370\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 3.0657 (3.1857)  Acc@1: 25.0000 (24.7000)  Acc@5: 50.0000 (50.5000)  Acc@task: 100.0000 (100.0000)  time: 0.1600  data: 0.0002  max mem: 1370\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1670 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@task 100.000 Acc@1 24.700 Acc@5 50.500 loss 3.186\n",
            "[Average accuracy till task1]\tAcc@task: 100.0000\tAcc@1: 24.7000\tAcc@5: 50.5000\tLoss: 3.1857\n",
            "Loading checkpoint from: ./output/cifar100_full_dino_1epoch_25pct/checkpoint/task2_checkpoint.pth\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/engines/hide_promtp_wtp_and_tap_engine.py:540: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  loss = torch.nn.functional.cross_entropy(sim, torch.range(0, sim.shape[0] - 1).long().to(device))\n",
            "Train: Epoch[1/1]  [ 0/52]  eta: 0:00:42  Lr: 0.002812  Loss: 4.3146  Acc@1: 16.6667 (16.6667)  Acc@5: 50.0000 (50.0000)  time: 0.8123  data: 0.3593  max mem: 1371\n",
            "Train: Epoch[1/1]  [10/52]  eta: 0:00:12  Lr: 0.002812  Loss: 3.0070  Acc@1: 12.5000 (11.7424)  Acc@5: 50.0000 (49.6212)  time: 0.3018  data: 0.0350  max mem: 1373\n",
            "Train: Epoch[1/1]  [20/52]  eta: 0:00:08  Lr: 0.002812  Loss: 2.6126  Acc@1: 12.5000 (13.2937)  Acc@5: 54.1667 (53.7698)  time: 0.2520  data: 0.0019  max mem: 1373\n",
            "Train: Epoch[1/1]  [30/52]  eta: 0:00:05  Lr: 0.002812  Loss: 2.6176  Acc@1: 16.6667 (15.4570)  Acc@5: 62.5000 (57.9301)  time: 0.2541  data: 0.0011  max mem: 1373\n",
            "Train: Epoch[1/1]  [40/52]  eta: 0:00:03  Lr: 0.002812  Loss: 2.2188  Acc@1: 20.8333 (20.0203)  Acc@5: 75.0000 (63.9228)  time: 0.2518  data: 0.0007  max mem: 1373\n",
            "Train: Epoch[1/1]  [50/52]  eta: 0:00:00  Lr: 0.002812  Loss: 2.1738  Acc@1: 37.5000 (24.1830)  Acc@5: 83.3333 (67.8105)  time: 0.2497  data: 0.0004  max mem: 1373\n",
            "Train: Epoch[1/1]  [51/52]  eta: 0:00:00  Lr: 0.002812  Loss: 0.5881  Acc@1: 37.5000 (24.2449)  Acc@5: 83.3333 (67.8367)  time: 0.2421  data: 0.0004  max mem: 1373\n",
            "Train: Epoch[1/1] Total time: 0:00:13 (0.2611 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.5881  Acc@1: 37.5000 (24.2449)  Acc@5: 83.3333 (67.8367)\n",
            "torch.Size([5, 2, 5, 6, 64])\n",
            "torch.Size([5, 2, 1, 5, 6, 64])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:23  Loss: 2.9851 (2.9851)  Acc@1: 41.6667 (41.6667)  Acc@5: 54.1667 (54.1667)  Acc@task: 95.8333 (95.8333)  time: 0.5574  data: 0.4006  max mem: 1373\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 3.2782 (3.2882)  Acc@1: 20.8333 (22.3485)  Acc@5: 50.0000 (48.1061)  Acc@task: 91.6667 (90.1515)  time: 0.1988  data: 0.0369  max mem: 1373\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 3.2369 (3.2498)  Acc@1: 20.8333 (22.0238)  Acc@5: 45.8333 (47.2222)  Acc@task: 91.6667 (90.8730)  time: 0.1645  data: 0.0005  max mem: 1373\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 3.1499 (3.2087)  Acc@1: 20.8333 (21.7742)  Acc@5: 50.0000 (49.3280)  Acc@task: 95.8333 (92.2043)  time: 0.1655  data: 0.0004  max mem: 1373\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 3.1012 (3.1911)  Acc@1: 20.8333 (22.7642)  Acc@5: 50.0000 (49.8984)  Acc@task: 95.8333 (92.8862)  time: 0.1653  data: 0.0002  max mem: 1373\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 3.0675 (3.1781)  Acc@1: 20.8333 (23.0000)  Acc@5: 50.0000 (50.1000)  Acc@task: 95.8333 (92.9000)  time: 0.1628  data: 0.0002  max mem: 1373\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1748 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@task 92.900 Acc@1 23.000 Acc@5 50.100 loss 3.178\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:20  Loss: 4.0661 (4.0661)  Acc@1: 4.1667 (4.1667)  Acc@5: 33.3333 (33.3333)  Acc@task: 91.6667 (91.6667)  time: 0.4984  data: 0.3196  max mem: 1373\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 3.9053 (4.0019)  Acc@1: 4.1667 (5.6818)  Acc@5: 29.1667 (31.4394)  Acc@task: 87.5000 (85.2273)  time: 0.1966  data: 0.0306  max mem: 1373\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 3.8179 (3.8824)  Acc@1: 8.3333 (8.1349)  Acc@5: 33.3333 (33.9286)  Acc@task: 87.5000 (85.9127)  time: 0.1673  data: 0.0014  max mem: 1373\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 3.7974 (3.8834)  Acc@1: 8.3333 (8.8710)  Acc@5: 29.1667 (32.2581)  Acc@task: 87.5000 (86.1559)  time: 0.1677  data: 0.0010  max mem: 1373\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 3.8572 (3.8602)  Acc@1: 8.3333 (8.2317)  Acc@5: 29.1667 (33.4350)  Acc@task: 83.3333 (85.6707)  time: 0.1668  data: 0.0006  max mem: 1373\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 3.7602 (3.8452)  Acc@1: 8.3333 (8.4000)  Acc@5: 33.3333 (33.7000)  Acc@task: 83.3333 (85.7000)  time: 0.1642  data: 0.0005  max mem: 1373\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1760 s / it)\n",
            "* Acc@task 85.700 Acc@1 8.400 Acc@5 33.700 loss 3.845\n",
            "[Average accuracy till task2]\tAcc@task: 89.3000\tAcc@1: 15.7000\tAcc@5: 41.9000\tLoss: 3.5116\tForgetting: 0.0000\tBackward: 23.0000\n",
            "torch.Size([23400, 384])\n",
            "Averaged stats: Lr: 0.005000  Loss: 0.4578  Acc@1: 61.6667 (64.7500)  Acc@5: 88.3333 (91.5000)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:17  Loss: 0.8258 (0.8258)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  Acc@task: 95.8333 (95.8333)  time: 0.4097  data: 0.2445  max mem: 1373\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 1.0924 (1.2308)  Acc@1: 66.6667 (66.2879)  Acc@5: 91.6667 (89.3939)  Acc@task: 91.6667 (90.1515)  time: 0.1920  data: 0.0246  max mem: 1373\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:03  Loss: 1.0924 (1.1690)  Acc@1: 66.6667 (68.6508)  Acc@5: 87.5000 (89.6825)  Acc@task: 91.6667 (90.8730)  time: 0.1693  data: 0.0014  max mem: 1373\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.9988 (1.0920)  Acc@1: 75.0000 (70.4301)  Acc@5: 91.6667 (90.9946)  Acc@task: 95.8333 (92.2043)  time: 0.1686  data: 0.0003  max mem: 1373\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.9230 (1.0584)  Acc@1: 75.0000 (71.7480)  Acc@5: 91.6667 (91.2602)  Acc@task: 95.8333 (92.8862)  time: 0.1685  data: 0.0002  max mem: 1373\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.8189 (1.0515)  Acc@1: 75.0000 (72.0000)  Acc@5: 95.8333 (91.4000)  Acc@task: 95.8333 (92.9000)  time: 0.1658  data: 0.0002  max mem: 1373\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1761 s / it)\n",
            "* Acc@task 92.900 Acc@1 72.000 Acc@5 91.400 loss 1.052\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:33  Loss: 0.9910 (0.9910)  Acc@1: 70.8333 (70.8333)  Acc@5: 100.0000 (100.0000)  Acc@task: 91.6667 (91.6667)  time: 0.7950  data: 0.6096  max mem: 1373\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:07  Loss: 0.7770 (0.8610)  Acc@1: 79.1667 (76.1364)  Acc@5: 95.8333 (95.4545)  Acc@task: 87.5000 (85.2273)  time: 0.2262  data: 0.0561  max mem: 1373\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.9005 (0.9159)  Acc@1: 79.1667 (75.9921)  Acc@5: 91.6667 (93.0556)  Acc@task: 87.5000 (85.9127)  time: 0.1687  data: 0.0008  max mem: 1373\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.8855 (0.9031)  Acc@1: 75.0000 (76.8817)  Acc@5: 95.8333 (93.5484)  Acc@task: 87.5000 (86.1559)  time: 0.1679  data: 0.0006  max mem: 1373\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.7929 (0.8734)  Acc@1: 79.1667 (78.1504)  Acc@5: 95.8333 (93.5976)  Acc@task: 83.3333 (85.6707)  time: 0.1670  data: 0.0002  max mem: 1373\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.7785 (0.8688)  Acc@1: 79.1667 (78.3000)  Acc@5: 95.8333 (93.7000)  Acc@task: 83.3333 (85.7000)  time: 0.1643  data: 0.0002  max mem: 1373\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1837 s / it)\n",
            "* Acc@task 85.700 Acc@1 78.300 Acc@5 93.700 loss 0.869\n",
            "[Average accuracy till task2]\tAcc@task: 89.3000\tAcc@1: 75.1500\tAcc@5: 92.5500\tLoss: 0.9602\tForgetting: 0.0000\tBackward: 47.3000\n",
            "Loading checkpoint from: ./output/cifar100_full_dino_1epoch_25pct/checkpoint/task3_checkpoint.pth\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/engines/hide_promtp_wtp_and_tap_engine.py:540: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  loss = torch.nn.functional.cross_entropy(sim, torch.range(0, sim.shape[0] - 1).long().to(device))\n",
            "Train: Epoch[1/1]  [ 0/54]  eta: 0:00:36  Lr: 0.002812  Loss: 3.7012  Acc@1: 20.8333 (20.8333)  Acc@5: 66.6667 (66.6667)  time: 0.6739  data: 0.3532  max mem: 1375\n",
            "Train: Epoch[1/1]  [10/54]  eta: 0:00:12  Lr: 0.002812  Loss: 2.3385  Acc@1: 8.3333 (11.3636)  Acc@5: 58.3333 (59.0909)  time: 0.2948  data: 0.0324  max mem: 1377\n",
            "Train: Epoch[1/1]  [20/54]  eta: 0:00:09  Lr: 0.002812  Loss: 3.3626  Acc@1: 12.5000 (14.4841)  Acc@5: 58.3333 (59.1270)  time: 0.2578  data: 0.0004  max mem: 1377\n",
            "Train: Epoch[1/1]  [30/54]  eta: 0:00:06  Lr: 0.002812  Loss: 2.6021  Acc@1: 20.8333 (18.6828)  Acc@5: 66.6667 (62.9032)  time: 0.2580  data: 0.0007  max mem: 1377\n",
            "Train: Epoch[1/1]  [40/54]  eta: 0:00:03  Lr: 0.002812  Loss: 1.8634  Acc@1: 33.3333 (22.6626)  Acc@5: 75.0000 (66.8699)  time: 0.2563  data: 0.0006  max mem: 1377\n",
            "Train: Epoch[1/1]  [50/54]  eta: 0:00:01  Lr: 0.002812  Loss: 1.6330  Acc@1: 37.5000 (27.4510)  Acc@5: 83.3333 (70.6699)  time: 0.2543  data: 0.0003  max mem: 1377\n",
            "Train: Epoch[1/1]  [53/54]  eta: 0:00:00  Lr: 0.002812  Loss: 0.9978  Acc@1: 45.8333 (29.1051)  Acc@5: 87.5000 (71.5175)  time: 0.2543  data: 0.0003  max mem: 1377\n",
            "Train: Epoch[1/1] Total time: 0:00:14 (0.2655 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.9978  Acc@1: 45.8333 (29.1051)  Acc@5: 87.5000 (71.5175)\n",
            "torch.Size([5, 2, 5, 6, 64])\n",
            "torch.Size([5, 2, 1, 5, 6, 64])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:18  Loss: 0.8272 (0.8272)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  Acc@task: 95.8333 (95.8333)  time: 0.4493  data: 0.2845  max mem: 1377\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:05  Loss: 1.0982 (1.2249)  Acc@1: 66.6667 (67.0455)  Acc@5: 91.6667 (89.0152)  Acc@task: 87.5000 (84.4697)  time: 0.1874  data: 0.0264  max mem: 1377\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:03  Loss: 1.0982 (1.1652)  Acc@1: 66.6667 (69.0476)  Acc@5: 87.5000 (89.4841)  Acc@task: 87.5000 (84.5238)  time: 0.1608  data: 0.0004  max mem: 1377\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.9965 (1.0910)  Acc@1: 75.0000 (70.6989)  Acc@5: 91.6667 (90.9946)  Acc@task: 87.5000 (86.0215)  time: 0.1611  data: 0.0003  max mem: 1377\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.9159 (1.0566)  Acc@1: 75.0000 (71.9512)  Acc@5: 91.6667 (91.2602)  Acc@task: 91.6667 (87.1951)  time: 0.1613  data: 0.0002  max mem: 1377\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.8145 (1.0497)  Acc@1: 75.0000 (72.1000)  Acc@5: 95.8333 (91.4000)  Acc@task: 91.6667 (87.3000)  time: 0.1586  data: 0.0002  max mem: 1377\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1688 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@task 87.300 Acc@1 72.100 Acc@5 91.400 loss 1.050\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:21  Loss: 0.9896 (0.9896)  Acc@1: 70.8333 (70.8333)  Acc@5: 100.0000 (100.0000)  Acc@task: 87.5000 (87.5000)  time: 0.5168  data: 0.3589  max mem: 1377\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.7648 (0.8458)  Acc@1: 79.1667 (76.5152)  Acc@5: 95.8333 (95.4545)  Acc@task: 87.5000 (88.2576)  time: 0.1935  data: 0.0335  max mem: 1377\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:03  Loss: 0.8561 (0.8983)  Acc@1: 79.1667 (76.1905)  Acc@5: 91.6667 (93.0556)  Acc@task: 91.6667 (88.8889)  time: 0.1617  data: 0.0009  max mem: 1377\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.8561 (0.8866)  Acc@1: 75.0000 (77.0161)  Acc@5: 95.8333 (93.5484)  Acc@task: 91.6667 (89.6505)  time: 0.1617  data: 0.0009  max mem: 1377\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.7829 (0.8576)  Acc@1: 79.1667 (78.4553)  Acc@5: 95.8333 (93.8008)  Acc@task: 87.5000 (88.6179)  time: 0.1613  data: 0.0008  max mem: 1377\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.7657 (0.8530)  Acc@1: 79.1667 (78.6000)  Acc@5: 95.8333 (93.9000)  Acc@task: 87.5000 (88.8000)  time: 0.1588  data: 0.0008  max mem: 1377\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1707 s / it)\n",
            "* Acc@task 88.800 Acc@1 78.600 Acc@5 93.900 loss 0.853\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:23  Loss: 4.6593 (4.6593)  Acc@1: 0.0000 (0.0000)  Acc@5: 37.5000 (37.5000)  Acc@task: 79.1667 (79.1667)  time: 0.5558  data: 0.3913  max mem: 1377\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 4.6637 (4.6994)  Acc@1: 4.1667 (3.0303)  Acc@5: 33.3333 (33.3333)  Acc@task: 79.1667 (81.0606)  time: 0.1969  data: 0.0358  max mem: 1377\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:03  Loss: 4.5933 (4.5127)  Acc@1: 4.1667 (3.7698)  Acc@5: 37.5000 (37.8968)  Acc@task: 79.1667 (80.3571)  time: 0.1618  data: 0.0003  max mem: 1377\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 4.4426 (4.5120)  Acc@1: 4.1667 (3.8978)  Acc@5: 41.6667 (39.3817)  Acc@task: 79.1667 (81.5860)  time: 0.1622  data: 0.0003  max mem: 1377\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 4.4426 (4.4885)  Acc@1: 4.1667 (3.9634)  Acc@5: 41.6667 (39.3293)  Acc@task: 79.1667 (81.4024)  time: 0.1615  data: 0.0003  max mem: 1377\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 4.4220 (4.4793)  Acc@1: 4.1667 (4.1000)  Acc@5: 41.6667 (39.5000)  Acc@task: 79.1667 (81.4000)  time: 0.1590  data: 0.0003  max mem: 1377\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1718 s / it)\n",
            "* Acc@task 81.400 Acc@1 4.100 Acc@5 39.500 loss 4.479\n",
            "[Average accuracy till task3]\tAcc@task: 85.8333\tAcc@1: 51.6000\tAcc@5: 74.9333\tLoss: 2.1274\tForgetting: 0.0000\tBackward: 71.1500\n",
            "torch.Size([35160, 384])\n",
            "Averaged stats: Lr: 0.005000  Loss: 0.1613  Acc@1: 84.1667 (80.6667)  Acc@5: 96.6667 (95.7500)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:21  Loss: 0.4780 (0.4780)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 95.8333 (95.8333)  time: 0.5098  data: 0.3343  max mem: 1377\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.7845 (0.7139)  Acc@1: 79.1667 (78.7879)  Acc@5: 100.0000 (96.9697)  Acc@task: 87.5000 (84.4697)  time: 0.1941  data: 0.0314  max mem: 1377\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:03  Loss: 0.6607 (0.6897)  Acc@1: 79.1667 (79.3651)  Acc@5: 95.8333 (96.4286)  Acc@task: 87.5000 (84.5238)  time: 0.1624  data: 0.0014  max mem: 1377\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.6193 (0.6774)  Acc@1: 87.5000 (81.1828)  Acc@5: 95.8333 (96.9086)  Acc@task: 87.5000 (86.0215)  time: 0.1620  data: 0.0012  max mem: 1377\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6193 (0.6734)  Acc@1: 87.5000 (81.6057)  Acc@5: 100.0000 (96.9512)  Acc@task: 91.6667 (87.1951)  time: 0.1616  data: 0.0005  max mem: 1377\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6052 (0.6649)  Acc@1: 87.5000 (81.7000)  Acc@5: 100.0000 (97.0000)  Acc@task: 91.6667 (87.3000)  time: 0.1590  data: 0.0003  max mem: 1377\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1713 s / it)\n",
            "* Acc@task 87.300 Acc@1 81.700 Acc@5 97.000 loss 0.665\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:20  Loss: 0.8926 (0.8926)  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.8333)  Acc@task: 87.5000 (87.5000)  time: 0.4882  data: 0.3204  max mem: 1377\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.9785 (0.9879)  Acc@1: 70.8333 (72.7273)  Acc@5: 95.8333 (94.3182)  Acc@task: 87.5000 (88.2576)  time: 0.1935  data: 0.0297  max mem: 1377\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:03  Loss: 0.9925 (1.0781)  Acc@1: 70.8333 (71.0317)  Acc@5: 91.6667 (92.8571)  Acc@task: 91.6667 (88.8889)  time: 0.1649  data: 0.0005  max mem: 1377\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 1.0078 (1.0627)  Acc@1: 70.8333 (71.5054)  Acc@5: 91.6667 (93.1452)  Acc@task: 91.6667 (89.6505)  time: 0.1650  data: 0.0005  max mem: 1377\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.9633 (1.0423)  Acc@1: 70.8333 (72.0528)  Acc@5: 91.6667 (93.0894)  Acc@task: 87.5000 (88.6179)  time: 0.1637  data: 0.0004  max mem: 1377\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.8647 (1.0292)  Acc@1: 75.0000 (72.3000)  Acc@5: 91.6667 (93.2000)  Acc@task: 87.5000 (88.8000)  time: 0.1611  data: 0.0003  max mem: 1377\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1729 s / it)\n",
            "* Acc@task 88.800 Acc@1 72.300 Acc@5 93.200 loss 1.029\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:35  Loss: 0.8811 (0.8811)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  Acc@task: 79.1667 (79.1667)  time: 0.8470  data: 0.6655  max mem: 1377\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:07  Loss: 0.8811 (0.7848)  Acc@1: 83.3333 (82.1970)  Acc@5: 95.8333 (93.5606)  Acc@task: 79.1667 (81.0606)  time: 0.2273  data: 0.0617  max mem: 1377\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.7571 (0.7519)  Acc@1: 83.3333 (80.9524)  Acc@5: 91.6667 (93.4524)  Acc@task: 79.1667 (80.3571)  time: 0.1650  data: 0.0012  max mem: 1377\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.6777 (0.7217)  Acc@1: 83.3333 (82.7957)  Acc@5: 95.8333 (94.0860)  Acc@task: 79.1667 (81.5860)  time: 0.1649  data: 0.0007  max mem: 1377\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.6707 (0.7303)  Acc@1: 83.3333 (83.1301)  Acc@5: 95.8333 (94.2073)  Acc@task: 79.1667 (81.4024)  time: 0.1651  data: 0.0002  max mem: 1377\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.6434 (0.7210)  Acc@1: 83.3333 (83.1000)  Acc@5: 95.8333 (94.3000)  Acc@task: 79.1667 (81.4000)  time: 0.1622  data: 0.0002  max mem: 1377\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1822 s / it)\n",
            "* Acc@task 81.400 Acc@1 83.100 Acc@5 94.300 loss 0.721\n",
            "[Average accuracy till task3]\tAcc@task: 85.8333\tAcc@1: 79.0333\tAcc@5: 94.8333\tLoss: 0.8051\tForgetting: 3.0000\tBackward: 25.5000\n",
            "Loading checkpoint from: ./output/cifar100_full_dino_1epoch_25pct/checkpoint/task4_checkpoint.pth\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/engines/hide_promtp_wtp_and_tap_engine.py:540: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  loss = torch.nn.functional.cross_entropy(sim, torch.range(0, sim.shape[0] - 1).long().to(device))\n",
            "Train: Epoch[1/1]  [ 0/51]  eta: 0:00:33  Lr: 0.002812  Loss: 3.1986  Acc@1: 20.8333 (20.8333)  Acc@5: 62.5000 (62.5000)  time: 0.6643  data: 0.3610  max mem: 1377\n",
            "Train: Epoch[1/1]  [10/51]  eta: 0:00:12  Lr: 0.002812  Loss: 3.0092  Acc@1: 12.5000 (15.1515)  Acc@5: 66.6667 (64.0152)  time: 0.2935  data: 0.0331  max mem: 1377\n",
            "Train: Epoch[1/1]  [20/51]  eta: 0:00:08  Lr: 0.002812  Loss: 1.9958  Acc@1: 20.8333 (21.0317)  Acc@5: 66.6667 (68.2540)  time: 0.2569  data: 0.0003  max mem: 1377\n",
            "Train: Epoch[1/1]  [30/51]  eta: 0:00:05  Lr: 0.002812  Loss: 1.6392  Acc@1: 29.1667 (25.1344)  Acc@5: 79.1667 (73.1183)  time: 0.2582  data: 0.0006  max mem: 1377\n",
            "Train: Epoch[1/1]  [40/51]  eta: 0:00:02  Lr: 0.002812  Loss: 1.6339  Acc@1: 41.6667 (30.5894)  Acc@5: 83.3333 (76.5244)  time: 0.2590  data: 0.0011  max mem: 1377\n",
            "Train: Epoch[1/1]  [50/51]  eta: 0:00:00  Lr: 0.002812  Loss: 1.3381  Acc@1: 50.0000 (35.6265)  Acc@5: 91.6667 (79.3612)  time: 0.2571  data: 0.0007  max mem: 1377\n",
            "Train: Epoch[1/1] Total time: 0:00:13 (0.2672 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.3381  Acc@1: 50.0000 (35.6265)  Acc@5: 91.6667 (79.3612)\n",
            "torch.Size([5, 2, 5, 6, 64])\n",
            "torch.Size([5, 2, 1, 5, 6, 64])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:18  Loss: 0.4928 (0.4928)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  Acc@task: 95.8333 (95.8333)  time: 0.4353  data: 0.2668  max mem: 1377\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.7840 (0.7214)  Acc@1: 79.1667 (78.4091)  Acc@5: 100.0000 (96.5909)  Acc@task: 87.5000 (85.2273)  time: 0.1879  data: 0.0245  max mem: 1377\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:03  Loss: 0.6623 (0.6930)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (96.4286)  Acc@task: 83.3333 (85.3175)  time: 0.1642  data: 0.0003  max mem: 1377\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.6123 (0.6815)  Acc@1: 83.3333 (80.7796)  Acc@5: 100.0000 (96.9086)  Acc@task: 87.5000 (86.2903)  time: 0.1649  data: 0.0003  max mem: 1377\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6123 (0.6832)  Acc@1: 83.3333 (81.3008)  Acc@5: 100.0000 (96.8496)  Acc@task: 91.6667 (86.8902)  time: 0.1630  data: 0.0002  max mem: 1377\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6121 (0.6745)  Acc@1: 87.5000 (81.4000)  Acc@5: 100.0000 (96.9000)  Acc@task: 91.6667 (87.0000)  time: 0.1603  data: 0.0002  max mem: 1377\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1709 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@task 87.000 Acc@1 81.400 Acc@5 96.900 loss 0.674\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:21  Loss: 0.9358 (0.9358)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  Acc@task: 91.6667 (91.6667)  time: 0.5112  data: 0.3485  max mem: 1377\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.9710 (0.9936)  Acc@1: 70.8333 (72.3485)  Acc@5: 91.6667 (93.5606)  Acc@task: 83.3333 (84.8485)  time: 0.1939  data: 0.0320  max mem: 1377\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:03  Loss: 0.9884 (1.0815)  Acc@1: 75.0000 (71.0317)  Acc@5: 91.6667 (92.0635)  Acc@task: 83.3333 (84.1270)  time: 0.1639  data: 0.0004  max mem: 1377\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.9936 (1.0624)  Acc@1: 75.0000 (71.7742)  Acc@5: 91.6667 (92.7419)  Acc@task: 83.3333 (84.9462)  time: 0.1645  data: 0.0006  max mem: 1377\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.9784 (1.0422)  Acc@1: 75.0000 (72.1545)  Acc@5: 91.6667 (92.7846)  Acc@task: 83.3333 (84.5528)  time: 0.1631  data: 0.0008  max mem: 1377\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.8377 (1.0290)  Acc@1: 75.0000 (72.4000)  Acc@5: 91.6667 (92.9000)  Acc@task: 83.3333 (84.8000)  time: 0.1604  data: 0.0007  max mem: 1377\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1727 s / it)\n",
            "* Acc@task 84.800 Acc@1 72.400 Acc@5 92.900 loss 1.029\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:22  Loss: 0.7918 (0.7918)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  Acc@task: 100.0000 (100.0000)  time: 0.5343  data: 0.3770  max mem: 1377\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 0.7918 (0.7427)  Acc@1: 83.3333 (82.9545)  Acc@5: 95.8333 (94.3182)  Acc@task: 83.3333 (85.6061)  time: 0.1959  data: 0.0345  max mem: 1377\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:03  Loss: 0.6919 (0.7138)  Acc@1: 83.3333 (81.5476)  Acc@5: 95.8333 (93.8492)  Acc@task: 83.3333 (85.3175)  time: 0.1635  data: 0.0003  max mem: 1377\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.6237 (0.6870)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (94.4892)  Acc@task: 87.5000 (86.2903)  time: 0.1643  data: 0.0003  max mem: 1377\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.6237 (0.6922)  Acc@1: 83.3333 (83.6382)  Acc@5: 95.8333 (94.6138)  Acc@task: 87.5000 (86.3821)  time: 0.1633  data: 0.0002  max mem: 1377\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.6131 (0.6838)  Acc@1: 83.3333 (83.6000)  Acc@5: 95.8333 (94.7000)  Acc@task: 87.5000 (86.2000)  time: 0.1607  data: 0.0002  max mem: 1377\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1731 s / it)\n",
            "* Acc@task 86.200 Acc@1 83.600 Acc@5 94.700 loss 0.684\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:21  Loss: 5.6060 (5.6060)  Acc@1: 12.5000 (12.5000)  Acc@5: 33.3333 (33.3333)  Acc@task: 66.6667 (66.6667)  time: 0.5169  data: 0.3547  max mem: 1377\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:06  Loss: 5.5248 (5.5624)  Acc@1: 4.1667 (5.3030)  Acc@5: 25.0000 (26.8939)  Acc@task: 79.1667 (79.1667)  time: 0.1947  data: 0.0325  max mem: 1377\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:03  Loss: 5.4710 (5.5738)  Acc@1: 4.1667 (3.9683)  Acc@5: 25.0000 (26.1905)  Acc@task: 79.1667 (76.7857)  time: 0.1634  data: 0.0004  max mem: 1377\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 5.3377 (5.4845)  Acc@1: 4.1667 (3.8978)  Acc@5: 29.1667 (28.0914)  Acc@task: 75.0000 (76.6129)  time: 0.1638  data: 0.0007  max mem: 1377\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 5.3911 (5.4940)  Acc@1: 4.1667 (3.6585)  Acc@5: 29.1667 (27.2358)  Acc@task: 79.1667 (77.0325)  time: 0.1631  data: 0.0006  max mem: 1377\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 5.4067 (5.4920)  Acc@1: 4.1667 (3.6000)  Acc@5: 25.0000 (27.2000)  Acc@task: 79.1667 (77.2000)  time: 0.1607  data: 0.0006  max mem: 1377\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1725 s / it)\n",
            "* Acc@task 77.200 Acc@1 3.600 Acc@5 27.200 loss 5.492\n",
            "[Average accuracy till task4]\tAcc@task: 83.8000\tAcc@1: 60.2500\tAcc@5: 77.9250\tLoss: 1.9698\tForgetting: 2.0667\tBackward: 74.9667\n",
            "torch.Size([47040, 384])\n",
            "Averaged stats: Lr: 0.005000  Loss: 0.1558  Acc@1: 93.3333 (87.8611)  Acc@5: 100.0000 (97.4722)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:21  Loss: 0.4276 (0.4276)  Acc@1: 91.6667 (91.6667)  Acc@5: 95.8333 (95.8333)  Acc@task: 95.8333 (95.8333)  time: 0.5076  data: 0.3371  max mem: 1377\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.6390 (0.7062)  Acc@1: 83.3333 (79.9242)  Acc@5: 100.0000 (97.3485)  Acc@task: 87.5000 (85.2273)  time: 0.1940  data: 0.0312  max mem: 1377\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:03  Loss: 0.6390 (0.6658)  Acc@1: 79.1667 (81.9444)  Acc@5: 95.8333 (97.0238)  Acc@task: 83.3333 (85.3175)  time: 0.1641  data: 0.0005  max mem: 1377\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.5395 (0.6425)  Acc@1: 87.5000 (82.5269)  Acc@5: 100.0000 (97.4462)  Acc@task: 87.5000 (86.2903)  time: 0.1642  data: 0.0003  max mem: 1377\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.5840 (0.6424)  Acc@1: 83.3333 (83.0285)  Acc@5: 100.0000 (97.5610)  Acc@task: 91.6667 (86.8902)  time: 0.1629  data: 0.0003  max mem: 1377\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.5820 (0.6366)  Acc@1: 87.5000 (83.2000)  Acc@5: 100.0000 (97.6000)  Acc@task: 91.6667 (87.0000)  time: 0.1603  data: 0.0003  max mem: 1377\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1726 s / it)\n",
            "* Acc@task 87.000 Acc@1 83.200 Acc@5 97.600 loss 0.637\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:18  Loss: 1.0521 (1.0521)  Acc@1: 66.6667 (66.6667)  Acc@5: 95.8333 (95.8333)  Acc@task: 91.6667 (91.6667)  time: 0.4394  data: 0.2627  max mem: 1377\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.8915 (0.9303)  Acc@1: 79.1667 (75.7576)  Acc@5: 95.8333 (96.2121)  Acc@task: 83.3333 (84.8485)  time: 0.1922  data: 0.0284  max mem: 1377\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:03  Loss: 0.8408 (0.9648)  Acc@1: 75.0000 (73.6111)  Acc@5: 95.8333 (95.4365)  Acc@task: 83.3333 (84.1270)  time: 0.1658  data: 0.0034  max mem: 1377\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.7992 (0.9575)  Acc@1: 75.0000 (73.5215)  Acc@5: 95.8333 (95.1613)  Acc@task: 83.3333 (84.9462)  time: 0.1629  data: 0.0011  max mem: 1377\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.8351 (0.9318)  Acc@1: 75.0000 (73.6789)  Acc@5: 95.8333 (95.3252)  Acc@task: 83.3333 (84.5528)  time: 0.1617  data: 0.0002  max mem: 1377\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.8199 (0.9238)  Acc@1: 75.0000 (73.9000)  Acc@5: 95.8333 (95.4000)  Acc@task: 83.3333 (84.8000)  time: 0.1590  data: 0.0002  max mem: 1377\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1712 s / it)\n",
            "* Acc@task 84.800 Acc@1 73.900 Acc@5 95.400 loss 0.924\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:17  Loss: 0.5273 (0.5273)  Acc@1: 87.5000 (87.5000)  Acc@5: 95.8333 (95.8333)  Acc@task: 100.0000 (100.0000)  time: 0.4165  data: 0.2588  max mem: 1377\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 0.7903 (0.7947)  Acc@1: 79.1667 (80.6818)  Acc@5: 95.8333 (94.6970)  Acc@task: 83.3333 (85.6061)  time: 0.1891  data: 0.0243  max mem: 1377\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:03  Loss: 0.7621 (0.7461)  Acc@1: 79.1667 (80.5556)  Acc@5: 95.8333 (95.4365)  Acc@task: 83.3333 (85.3175)  time: 0.1655  data: 0.0006  max mem: 1377\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.5778 (0.6886)  Acc@1: 83.3333 (81.3172)  Acc@5: 95.8333 (95.8333)  Acc@task: 87.5000 (86.2903)  time: 0.1638  data: 0.0003  max mem: 1377\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.5833 (0.6664)  Acc@1: 83.3333 (82.4187)  Acc@5: 95.8333 (95.9350)  Acc@task: 87.5000 (86.3821)  time: 0.1630  data: 0.0002  max mem: 1377\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.5833 (0.6778)  Acc@1: 83.3333 (82.1000)  Acc@5: 95.8333 (96.0000)  Acc@task: 87.5000 (86.2000)  time: 0.1604  data: 0.0002  max mem: 1377\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1720 s / it)\n",
            "* Acc@task 86.200 Acc@1 82.100 Acc@5 96.000 loss 0.678\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:34  Loss: 1.1782 (1.1782)  Acc@1: 66.6667 (66.6667)  Acc@5: 87.5000 (87.5000)  Acc@task: 66.6667 (66.6667)  time: 0.8296  data: 0.6180  max mem: 1377\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:07  Loss: 0.4967 (0.6335)  Acc@1: 83.3333 (80.3030)  Acc@5: 95.8333 (95.4545)  Acc@task: 79.1667 (79.1667)  time: 0.2240  data: 0.0573  max mem: 1377\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.6568 (0.7155)  Acc@1: 79.1667 (79.3651)  Acc@5: 95.8333 (95.4365)  Acc@task: 79.1667 (76.7857)  time: 0.1637  data: 0.0010  max mem: 1377\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.6828 (0.7072)  Acc@1: 79.1667 (79.8387)  Acc@5: 95.8333 (95.4301)  Acc@task: 75.0000 (76.6129)  time: 0.1637  data: 0.0008  max mem: 1377\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7137 (0.7263)  Acc@1: 79.1667 (79.6748)  Acc@5: 95.8333 (94.8171)  Acc@task: 79.1667 (77.0325)  time: 0.1632  data: 0.0005  max mem: 1377\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.7104 (0.7171)  Acc@1: 79.1667 (79.9000)  Acc@5: 95.8333 (94.8000)  Acc@task: 79.1667 (77.2000)  time: 0.1607  data: 0.0004  max mem: 1377\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1802 s / it)\n",
            "* Acc@task 77.200 Acc@1 79.900 Acc@5 94.800 loss 0.717\n",
            "[Average accuracy till task4]\tAcc@task: 83.8000\tAcc@1: 79.7750\tAcc@5: 95.9500\tLoss: 0.7388\tForgetting: 1.8000\tBackward: 17.7000\n",
            "Loading checkpoint from: ./output/cifar100_full_dino_1epoch_25pct/checkpoint/task5_checkpoint.pth\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/engines/hide_promtp_wtp_and_tap_engine.py:540: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  loss = torch.nn.functional.cross_entropy(sim, torch.range(0, sim.shape[0] - 1).long().to(device))\n",
            "Train: Epoch[1/1]  [ 0/53]  eta: 0:00:43  Lr: 0.002812  Loss: 4.2997  Acc@1: 4.1667 (4.1667)  Acc@5: 25.0000 (25.0000)  time: 0.8241  data: 0.5660  max mem: 1377\n",
            "Train: Epoch[1/1]  [10/53]  eta: 0:00:13  Lr: 0.002812  Loss: 3.0137  Acc@1: 16.6667 (17.8030)  Acc@5: 50.0000 (49.6212)  time: 0.3057  data: 0.0517  max mem: 1378\n",
            "Train: Epoch[1/1]  [20/53]  eta: 0:00:09  Lr: 0.002812  Loss: 2.2980  Acc@1: 20.8333 (22.2222)  Acc@5: 62.5000 (58.7302)  time: 0.2548  data: 0.0002  max mem: 1378\n",
            "Train: Epoch[1/1]  [30/53]  eta: 0:00:06  Lr: 0.002812  Loss: 1.7888  Acc@1: 33.3333 (26.7473)  Acc@5: 70.8333 (64.5161)  time: 0.2558  data: 0.0007  max mem: 1378\n",
            "Train: Epoch[1/1]  [40/53]  eta: 0:00:03  Lr: 0.002812  Loss: 1.3446  Acc@1: 41.6667 (31.0976)  Acc@5: 83.3333 (70.3252)  time: 0.2557  data: 0.0010  max mem: 1378\n",
            "Train: Epoch[1/1]  [50/53]  eta: 0:00:00  Lr: 0.002812  Loss: 1.2802  Acc@1: 45.8333 (34.7222)  Acc@5: 87.5000 (73.6111)  time: 0.2552  data: 0.0005  max mem: 1378\n",
            "Train: Epoch[1/1]  [52/53]  eta: 0:00:00  Lr: 0.002812  Loss: 1.2615  Acc@1: 50.0000 (35.4200)  Acc@5: 87.5000 (74.0888)  time: 0.2499  data: 0.0003  max mem: 1378\n",
            "Train: Epoch[1/1] Total time: 0:00:14 (0.2657 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.2615  Acc@1: 50.0000 (35.4200)  Acc@5: 87.5000 (74.0888)\n",
            "torch.Size([5, 2, 5, 6, 64])\n",
            "torch.Size([5, 2, 1, 5, 6, 64])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:21  Loss: 0.4294 (0.4294)  Acc@1: 91.6667 (91.6667)  Acc@5: 95.8333 (95.8333)  Acc@task: 87.5000 (87.5000)  time: 0.5187  data: 0.3513  max mem: 1378\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.7074 (0.7161)  Acc@1: 79.1667 (79.5455)  Acc@5: 100.0000 (97.3485)  Acc@task: 87.5000 (84.0909)  time: 0.1950  data: 0.0322  max mem: 1378\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:03  Loss: 0.6902 (0.6785)  Acc@1: 79.1667 (81.5476)  Acc@5: 95.8333 (97.2222)  Acc@task: 83.3333 (84.3254)  time: 0.1627  data: 0.0004  max mem: 1378\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.5396 (0.6637)  Acc@1: 87.5000 (82.2581)  Acc@5: 100.0000 (97.4462)  Acc@task: 83.3333 (84.9462)  time: 0.1622  data: 0.0004  max mem: 1378\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.5830 (0.6589)  Acc@1: 83.3333 (82.7236)  Acc@5: 100.0000 (97.5610)  Acc@task: 87.5000 (85.6707)  time: 0.1616  data: 0.0002  max mem: 1378\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.5821 (0.6527)  Acc@1: 87.5000 (82.9000)  Acc@5: 100.0000 (97.6000)  Acc@task: 87.5000 (85.8000)  time: 0.1592  data: 0.0002  max mem: 1378\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1716 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@task 85.800 Acc@1 82.900 Acc@5 97.600 loss 0.653\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:22  Loss: 1.1382 (1.1382)  Acc@1: 66.6667 (66.6667)  Acc@5: 95.8333 (95.8333)  Acc@task: 87.5000 (87.5000)  time: 0.5264  data: 0.3566  max mem: 1378\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.8878 (0.9484)  Acc@1: 79.1667 (75.3788)  Acc@5: 95.8333 (96.2121)  Acc@task: 83.3333 (84.8485)  time: 0.1958  data: 0.0328  max mem: 1378\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:03  Loss: 0.8396 (0.9833)  Acc@1: 75.0000 (73.2143)  Acc@5: 95.8333 (95.2381)  Acc@task: 83.3333 (82.3413)  time: 0.1635  data: 0.0007  max mem: 1378\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.7992 (0.9705)  Acc@1: 75.0000 (73.3871)  Acc@5: 95.8333 (95.0269)  Acc@task: 83.3333 (83.4677)  time: 0.1630  data: 0.0010  max mem: 1378\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.8354 (0.9413)  Acc@1: 75.0000 (73.4756)  Acc@5: 95.8333 (95.2236)  Acc@task: 83.3333 (83.4350)  time: 0.1623  data: 0.0006  max mem: 1378\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.8190 (0.9359)  Acc@1: 75.0000 (73.6000)  Acc@5: 95.8333 (95.2000)  Acc@task: 87.5000 (83.5000)  time: 0.1598  data: 0.0005  max mem: 1378\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1724 s / it)\n",
            "* Acc@task 83.500 Acc@1 73.600 Acc@5 95.200 loss 0.936\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:17  Loss: 0.5258 (0.5258)  Acc@1: 87.5000 (87.5000)  Acc@5: 95.8333 (95.8333)  Acc@task: 95.8333 (95.8333)  time: 0.4123  data: 0.2565  max mem: 1378\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 0.7647 (0.7836)  Acc@1: 79.1667 (80.3030)  Acc@5: 95.8333 (95.0758)  Acc@task: 83.3333 (83.7121)  time: 0.1924  data: 0.0310  max mem: 1378\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:03  Loss: 0.7287 (0.7361)  Acc@1: 79.1667 (80.3571)  Acc@5: 95.8333 (95.6349)  Acc@task: 83.3333 (83.9286)  time: 0.1681  data: 0.0044  max mem: 1378\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.5657 (0.6782)  Acc@1: 83.3333 (81.5860)  Acc@5: 95.8333 (95.9677)  Acc@task: 83.3333 (84.0054)  time: 0.1650  data: 0.0003  max mem: 1378\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.5679 (0.6579)  Acc@1: 83.3333 (82.8252)  Acc@5: 95.8333 (96.0366)  Acc@task: 83.3333 (84.2480)  time: 0.1643  data: 0.0002  max mem: 1378\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.5679 (0.6691)  Acc@1: 83.3333 (82.5000)  Acc@5: 95.8333 (96.1000)  Acc@task: 83.3333 (84.2000)  time: 0.1618  data: 0.0002  max mem: 1378\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1728 s / it)\n",
            "* Acc@task 84.200 Acc@1 82.500 Acc@5 96.100 loss 0.669\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:22  Loss: 1.1874 (1.1874)  Acc@1: 62.5000 (62.5000)  Acc@5: 87.5000 (87.5000)  Acc@task: 75.0000 (75.0000)  time: 0.5305  data: 0.3498  max mem: 1378\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:06  Loss: 0.4816 (0.6248)  Acc@1: 83.3333 (79.5455)  Acc@5: 95.8333 (96.2121)  Acc@task: 83.3333 (81.0606)  time: 0.1973  data: 0.0330  max mem: 1378\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:03  Loss: 0.6167 (0.6457)  Acc@1: 79.1667 (80.1587)  Acc@5: 95.8333 (96.4286)  Acc@task: 83.3333 (81.1508)  time: 0.1641  data: 0.0016  max mem: 1378\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.6512 (0.6388)  Acc@1: 79.1667 (81.0484)  Acc@5: 95.8333 (96.3710)  Acc@task: 79.1667 (80.6452)  time: 0.1633  data: 0.0014  max mem: 1378\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.6499 (0.6624)  Acc@1: 79.1667 (80.9959)  Acc@5: 95.8333 (95.7317)  Acc@task: 79.1667 (81.0976)  time: 0.1628  data: 0.0006  max mem: 1378\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.5874 (0.6548)  Acc@1: 79.1667 (81.2000)  Acc@5: 95.8333 (95.7000)  Acc@task: 79.1667 (81.2000)  time: 0.1601  data: 0.0006  max mem: 1378\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1729 s / it)\n",
            "* Acc@task 81.200 Acc@1 81.200 Acc@5 95.700 loss 0.655\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:25  Loss: 6.0782 (6.0782)  Acc@1: 0.0000 (0.0000)  Acc@5: 29.1667 (29.1667)  Acc@task: 83.3333 (83.3333)  time: 0.6179  data: 0.4658  max mem: 1378\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 7.0692 (6.8695)  Acc@1: 0.0000 (0.3788)  Acc@5: 20.8333 (20.0758)  Acc@task: 75.0000 (77.2727)  time: 0.2051  data: 0.0428  max mem: 1378\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 7.1110 (6.9929)  Acc@1: 0.0000 (0.1984)  Acc@5: 20.8333 (20.0397)  Acc@task: 75.0000 (75.1984)  time: 0.1645  data: 0.0004  max mem: 1378\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 6.9756 (6.9732)  Acc@1: 0.0000 (0.2688)  Acc@5: 20.8333 (20.0269)  Acc@task: 75.0000 (75.6720)  time: 0.1649  data: 0.0004  max mem: 1378\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 6.9707 (6.9989)  Acc@1: 0.0000 (0.3049)  Acc@5: 16.6667 (18.6992)  Acc@task: 75.0000 (74.1870)  time: 0.1644  data: 0.0003  max mem: 1378\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 6.9707 (7.0042)  Acc@1: 0.0000 (0.3000)  Acc@5: 16.6667 (18.7000)  Acc@task: 75.0000 (74.0000)  time: 0.1620  data: 0.0003  max mem: 1378\n",
            "Test: [Task 5] Total time: 0:00:07 (0.1760 s / it)\n",
            "* Acc@task 74.000 Acc@1 0.300 Acc@5 18.700 loss 7.004\n",
            "[Average accuracy till task5]\tAcc@task: 81.7400\tAcc@1: 64.1000\tAcc@5: 80.6600\tLoss: 1.9833\tForgetting: 1.5250\tBackward: 76.0250\n",
            "torch.Size([58680, 384])\n",
            "Averaged stats: Lr: 0.005000  Loss: 0.0693  Acc@1: 95.8333 (90.6250)  Acc@5: 100.0000 (97.8750)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:35  Loss: 0.7462 (0.7462)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 87.5000 (87.5000)  time: 0.8404  data: 0.6463  max mem: 1378\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:07  Loss: 0.8767 (0.8819)  Acc@1: 79.1667 (77.6515)  Acc@5: 95.8333 (95.4545)  Acc@task: 87.5000 (84.0909)  time: 0.2259  data: 0.0604  max mem: 1378\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.8767 (0.9045)  Acc@1: 79.1667 (78.1746)  Acc@5: 95.8333 (95.6349)  Acc@task: 83.3333 (84.3254)  time: 0.1631  data: 0.0011  max mem: 1378\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.7761 (0.8715)  Acc@1: 79.1667 (77.8226)  Acc@5: 95.8333 (96.1022)  Acc@task: 83.3333 (84.9462)  time: 0.1619  data: 0.0004  max mem: 1378\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.7106 (0.8464)  Acc@1: 79.1667 (78.6585)  Acc@5: 100.0000 (96.4431)  Acc@task: 87.5000 (85.6707)  time: 0.1618  data: 0.0003  max mem: 1378\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6337 (0.8370)  Acc@1: 79.1667 (78.9000)  Acc@5: 100.0000 (96.5000)  Acc@task: 87.5000 (85.8000)  time: 0.1592  data: 0.0003  max mem: 1378\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1794 s / it)\n",
            "* Acc@task 85.800 Acc@1 78.900 Acc@5 96.500 loss 0.837\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:22  Loss: 0.8172 (0.8172)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 87.5000 (87.5000)  time: 0.5407  data: 0.3841  max mem: 1378\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.8172 (0.8356)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (97.7273)  Acc@task: 83.3333 (84.8485)  time: 0.1977  data: 0.0354  max mem: 1378\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.9219 (0.9167)  Acc@1: 75.0000 (76.9841)  Acc@5: 95.8333 (96.4286)  Acc@task: 83.3333 (82.3413)  time: 0.1644  data: 0.0004  max mem: 1378\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.9219 (0.8960)  Acc@1: 75.0000 (77.5538)  Acc@5: 95.8333 (96.1022)  Acc@task: 83.3333 (83.4677)  time: 0.1642  data: 0.0004  max mem: 1378\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.7344 (0.8580)  Acc@1: 79.1667 (78.0488)  Acc@5: 95.8333 (96.2398)  Acc@task: 83.3333 (83.4350)  time: 0.1627  data: 0.0003  max mem: 1378\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.7344 (0.8585)  Acc@1: 79.1667 (78.0000)  Acc@5: 95.8333 (96.1000)  Acc@task: 87.5000 (83.5000)  time: 0.1602  data: 0.0003  max mem: 1378\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1742 s / it)\n",
            "* Acc@task 83.500 Acc@1 78.000 Acc@5 96.100 loss 0.858\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:35  Loss: 0.6091 (0.6091)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  Acc@task: 95.8333 (95.8333)  time: 0.8548  data: 0.6400  max mem: 1378\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:07  Loss: 0.8802 (0.9078)  Acc@1: 79.1667 (79.9242)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (83.7121)  time: 0.2250  data: 0.0585  max mem: 1378\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.8175 (0.8235)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (83.9286)  time: 0.1631  data: 0.0003  max mem: 1378\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.6411 (0.7493)  Acc@1: 79.1667 (79.9731)  Acc@5: 95.8333 (95.9677)  Acc@task: 83.3333 (84.0054)  time: 0.1640  data: 0.0003  max mem: 1378\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.6411 (0.7489)  Acc@1: 79.1667 (80.4878)  Acc@5: 95.8333 (95.9350)  Acc@task: 83.3333 (84.2480)  time: 0.1631  data: 0.0002  max mem: 1378\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.6411 (0.7550)  Acc@1: 79.1667 (80.2000)  Acc@5: 95.8333 (95.9000)  Acc@task: 83.3333 (84.2000)  time: 0.1606  data: 0.0002  max mem: 1378\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1803 s / it)\n",
            "* Acc@task 84.200 Acc@1 80.200 Acc@5 95.900 loss 0.755\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:22  Loss: 0.8509 (0.8509)  Acc@1: 66.6667 (66.6667)  Acc@5: 91.6667 (91.6667)  Acc@task: 75.0000 (75.0000)  time: 0.5380  data: 0.3815  max mem: 1378\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:06  Loss: 0.6600 (0.6138)  Acc@1: 83.3333 (80.6818)  Acc@5: 95.8333 (96.9697)  Acc@task: 83.3333 (81.0606)  time: 0.1971  data: 0.0351  max mem: 1378\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.6600 (0.6792)  Acc@1: 83.3333 (80.5556)  Acc@5: 95.8333 (97.2222)  Acc@task: 83.3333 (81.1508)  time: 0.1642  data: 0.0004  max mem: 1378\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.7238 (0.6903)  Acc@1: 79.1667 (80.6452)  Acc@5: 100.0000 (97.0430)  Acc@task: 79.1667 (80.6452)  time: 0.1642  data: 0.0008  max mem: 1378\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7954 (0.7460)  Acc@1: 79.1667 (79.8781)  Acc@5: 95.8333 (96.1382)  Acc@task: 79.1667 (81.0976)  time: 0.1622  data: 0.0010  max mem: 1378\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.7669 (0.7334)  Acc@1: 79.1667 (80.1000)  Acc@5: 95.8333 (96.2000)  Acc@task: 79.1667 (81.2000)  time: 0.1596  data: 0.0010  max mem: 1378\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1731 s / it)\n",
            "* Acc@task 81.200 Acc@1 80.100 Acc@5 96.200 loss 0.733\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:33  Loss: 0.1280 (0.1280)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.8046  data: 0.6186  max mem: 1378\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:07  Loss: 0.7640 (0.6590)  Acc@1: 79.1667 (80.3030)  Acc@5: 100.0000 (98.1061)  Acc@task: 75.0000 (77.2727)  time: 0.2215  data: 0.0577  max mem: 1378\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.7795 (0.6918)  Acc@1: 79.1667 (79.7619)  Acc@5: 95.8333 (96.8254)  Acc@task: 75.0000 (75.1984)  time: 0.1627  data: 0.0017  max mem: 1378\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.7479 (0.6939)  Acc@1: 79.1667 (80.7796)  Acc@5: 95.8333 (96.5054)  Acc@task: 75.0000 (75.6720)  time: 0.1619  data: 0.0011  max mem: 1378\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.6959 (0.7313)  Acc@1: 79.1667 (80.2846)  Acc@5: 95.8333 (96.3415)  Acc@task: 75.0000 (74.1870)  time: 0.1618  data: 0.0002  max mem: 1378\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.7033 (0.7637)  Acc@1: 79.1667 (80.0000)  Acc@5: 95.8333 (96.2000)  Acc@task: 75.0000 (74.0000)  time: 0.1592  data: 0.0002  max mem: 1378\n",
            "Test: [Task 5] Total time: 0:00:07 (0.1785 s / it)\n",
            "* Acc@task 74.000 Acc@1 80.000 Acc@5 96.200 loss 0.764\n",
            "[Average accuracy till task5]\tAcc@task: 81.7400\tAcc@1: 79.4400\tAcc@5: 96.1800\tLoss: 0.7895\tForgetting: 1.8750\tBackward: 12.8000\n",
            "Loading checkpoint from: ./output/cifar100_full_dino_1epoch_25pct/checkpoint/task6_checkpoint.pth\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/engines/hide_promtp_wtp_and_tap_engine.py:540: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  loss = torch.nn.functional.cross_entropy(sim, torch.range(0, sim.shape[0] - 1).long().to(device))\n",
            "Train: Epoch[1/1]  [ 0/53]  eta: 0:00:37  Lr: 0.002812  Loss: 3.7415  Acc@1: 0.0000 (0.0000)  Acc@5: 66.6667 (66.6667)  time: 0.7059  data: 0.4317  max mem: 1378\n",
            "Train: Epoch[1/1]  [10/53]  eta: 0:00:12  Lr: 0.002812  Loss: 3.3423  Acc@1: 8.3333 (9.0909)  Acc@5: 54.1667 (56.4394)  time: 0.2978  data: 0.0397  max mem: 1378\n",
            "Train: Epoch[1/1]  [20/53]  eta: 0:00:09  Lr: 0.002812  Loss: 2.2053  Acc@1: 12.5000 (13.6905)  Acc@5: 54.1667 (61.7064)  time: 0.2574  data: 0.0009  max mem: 1378\n",
            "Train: Epoch[1/1]  [30/53]  eta: 0:00:06  Lr: 0.002812  Loss: 1.8214  Acc@1: 25.0000 (19.4892)  Acc@5: 70.8333 (67.2043)  time: 0.2572  data: 0.0009  max mem: 1378\n",
            "Train: Epoch[1/1]  [40/53]  eta: 0:00:03  Lr: 0.002812  Loss: 1.6542  Acc@1: 33.3333 (24.6951)  Acc@5: 83.3333 (72.0528)  time: 0.2563  data: 0.0004  max mem: 1378\n",
            "Train: Epoch[1/1]  [50/53]  eta: 0:00:00  Lr: 0.002812  Loss: 1.8920  Acc@1: 45.8333 (29.3301)  Acc@5: 87.5000 (75.2451)  time: 0.2562  data: 0.0003  max mem: 1378\n",
            "Train: Epoch[1/1]  [52/53]  eta: 0:00:00  Lr: 0.002812  Loss: 1.2782  Acc@1: 45.8333 (29.9920)  Acc@5: 91.6667 (75.7359)  time: 0.2490  data: 0.0002  max mem: 1378\n",
            "Train: Epoch[1/1] Total time: 0:00:13 (0.2641 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.2782  Acc@1: 45.8333 (29.9920)  Acc@5: 91.6667 (75.7359)\n",
            "torch.Size([5, 2, 5, 6, 64])\n",
            "torch.Size([5, 2, 1, 5, 6, 64])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:19  Loss: 0.7443 (0.7443)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.4700  data: 0.3086  max mem: 1378\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.8754 (0.8848)  Acc@1: 79.1667 (77.6515)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (83.3333)  time: 0.1901  data: 0.0289  max mem: 1378\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:03  Loss: 0.8794 (0.9481)  Acc@1: 79.1667 (77.7778)  Acc@5: 95.8333 (95.0397)  Acc@task: 83.3333 (81.9444)  time: 0.1631  data: 0.0007  max mem: 1378\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.8092 (0.9020)  Acc@1: 75.0000 (77.4194)  Acc@5: 95.8333 (95.6989)  Acc@task: 83.3333 (82.6613)  time: 0.1630  data: 0.0004  max mem: 1378\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.7718 (0.8769)  Acc@1: 75.0000 (78.1504)  Acc@5: 100.0000 (96.0366)  Acc@task: 87.5000 (83.2317)  time: 0.1616  data: 0.0003  max mem: 1378\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6320 (0.8668)  Acc@1: 79.1667 (78.4000)  Acc@5: 100.0000 (96.1000)  Acc@task: 87.5000 (83.4000)  time: 0.1591  data: 0.0003  max mem: 1378\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1714 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@task 83.400 Acc@1 78.400 Acc@5 96.100 loss 0.867\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:34  Loss: 0.7939 (0.7939)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (83.3333)  time: 0.8262  data: 0.6261  max mem: 1378\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:07  Loss: 0.7939 (0.8331)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (98.1061)  Acc@task: 83.3333 (83.3333)  time: 0.2243  data: 0.0581  max mem: 1378\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.9262 (0.9283)  Acc@1: 75.0000 (76.7857)  Acc@5: 95.8333 (97.0238)  Acc@task: 83.3333 (80.7540)  time: 0.1641  data: 0.0009  max mem: 1378\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.9262 (0.9052)  Acc@1: 75.0000 (77.4194)  Acc@5: 95.8333 (96.5054)  Acc@task: 79.1667 (81.1828)  time: 0.1639  data: 0.0005  max mem: 1378\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.7376 (0.8675)  Acc@1: 79.1667 (77.9472)  Acc@5: 95.8333 (96.5447)  Acc@task: 79.1667 (81.5041)  time: 0.1632  data: 0.0003  max mem: 1378\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.7376 (0.8677)  Acc@1: 79.1667 (77.9000)  Acc@5: 95.8333 (96.4000)  Acc@task: 83.3333 (81.6000)  time: 0.1608  data: 0.0003  max mem: 1378\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1803 s / it)\n",
            "* Acc@task 81.600 Acc@1 77.900 Acc@5 96.400 loss 0.868\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:26  Loss: 0.6123 (0.6123)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  Acc@task: 91.6667 (91.6667)  time: 0.6196  data: 0.4693  max mem: 1378\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 0.8777 (0.9021)  Acc@1: 79.1667 (79.9242)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (83.3333)  time: 0.2046  data: 0.0429  max mem: 1378\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.8338 (0.8253)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.6349)  Acc@task: 83.3333 (83.9286)  time: 0.1639  data: 0.0004  max mem: 1378\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.6373 (0.7503)  Acc@1: 79.1667 (80.1075)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (83.6022)  time: 0.1649  data: 0.0004  max mem: 1378\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.6373 (0.7486)  Acc@1: 79.1667 (80.4878)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (84.0447)  time: 0.1638  data: 0.0003  max mem: 1378\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.6373 (0.7547)  Acc@1: 79.1667 (80.2000)  Acc@5: 95.8333 (95.8000)  Acc@task: 83.3333 (84.0000)  time: 0.1613  data: 0.0003  max mem: 1378\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1768 s / it)\n",
            "* Acc@task 84.000 Acc@1 80.200 Acc@5 95.800 loss 0.755\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:33  Loss: 0.7510 (0.7510)  Acc@1: 66.6667 (66.6667)  Acc@5: 95.8333 (95.8333)  Acc@task: 75.0000 (75.0000)  time: 0.7962  data: 0.6281  max mem: 1378\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:07  Loss: 0.6529 (0.5842)  Acc@1: 83.3333 (81.0606)  Acc@5: 95.8333 (97.3485)  Acc@task: 79.1667 (81.0606)  time: 0.2215  data: 0.0581  max mem: 1378\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.6529 (0.6543)  Acc@1: 83.3333 (80.7540)  Acc@5: 95.8333 (97.4206)  Acc@task: 79.1667 (81.3492)  time: 0.1647  data: 0.0008  max mem: 1378\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.6942 (0.6533)  Acc@1: 83.3333 (81.3172)  Acc@5: 100.0000 (97.3118)  Acc@task: 83.3333 (81.7204)  time: 0.1651  data: 0.0006  max mem: 1378\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7225 (0.7109)  Acc@1: 79.1667 (80.7927)  Acc@5: 95.8333 (96.5447)  Acc@task: 83.3333 (81.6057)  time: 0.1641  data: 0.0004  max mem: 1378\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.7025 (0.6991)  Acc@1: 79.1667 (81.0000)  Acc@5: 95.8333 (96.6000)  Acc@task: 83.3333 (81.8000)  time: 0.1616  data: 0.0004  max mem: 1378\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1806 s / it)\n",
            "* Acc@task 81.800 Acc@1 81.000 Acc@5 96.600 loss 0.699\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:23  Loss: 0.1247 (0.1247)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.5562  data: 0.3898  max mem: 1378\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 0.5846 (0.5959)  Acc@1: 83.3333 (81.4394)  Acc@5: 100.0000 (98.4848)  Acc@task: 83.3333 (83.3333)  time: 0.1991  data: 0.0361  max mem: 1378\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.7194 (0.6454)  Acc@1: 79.1667 (80.7540)  Acc@5: 95.8333 (97.6190)  Acc@task: 79.1667 (79.7619)  time: 0.1645  data: 0.0005  max mem: 1378\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.7058 (0.6507)  Acc@1: 79.1667 (81.4516)  Acc@5: 95.8333 (97.0430)  Acc@task: 75.0000 (79.4355)  time: 0.1653  data: 0.0004  max mem: 1378\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.5990 (0.6802)  Acc@1: 83.3333 (81.3008)  Acc@5: 95.8333 (96.7480)  Acc@task: 75.0000 (78.6585)  time: 0.1639  data: 0.0004  max mem: 1378\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.6413 (0.7134)  Acc@1: 83.3333 (81.0000)  Acc@5: 95.8333 (96.6000)  Acc@task: 75.0000 (78.5000)  time: 0.1613  data: 0.0004  max mem: 1378\n",
            "Test: [Task 5] Total time: 0:00:07 (0.1753 s / it)\n",
            "* Acc@task 78.500 Acc@1 81.000 Acc@5 96.600 loss 0.713\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:24  Loss: 8.5618 (8.5618)  Acc@1: 0.0000 (0.0000)  Acc@5: 8.3333 (8.3333)  Acc@task: 83.3333 (83.3333)  time: 0.5844  data: 0.4197  max mem: 1378\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:06  Loss: 8.3658 (8.4941)  Acc@1: 0.0000 (0.0000)  Acc@5: 8.3333 (9.0909)  Acc@task: 75.0000 (73.8636)  time: 0.2018  data: 0.0385  max mem: 1378\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:04  Loss: 8.2608 (8.3064)  Acc@1: 0.0000 (0.0000)  Acc@5: 8.3333 (9.9206)  Acc@task: 70.8333 (73.0159)  time: 0.1647  data: 0.0003  max mem: 1378\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 8.1219 (8.3489)  Acc@1: 0.0000 (0.0000)  Acc@5: 8.3333 (9.1398)  Acc@task: 70.8333 (70.9677)  time: 0.1654  data: 0.0003  max mem: 1378\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 8.1138 (8.2743)  Acc@1: 0.0000 (0.0000)  Acc@5: 8.3333 (10.0610)  Acc@task: 70.8333 (70.8333)  time: 0.1640  data: 0.0003  max mem: 1378\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 8.1138 (8.2928)  Acc@1: 0.0000 (0.0000)  Acc@5: 8.3333 (10.1000)  Acc@task: 70.8333 (71.0000)  time: 0.1615  data: 0.0002  max mem: 1378\n",
            "Test: [Task 6] Total time: 0:00:07 (0.1757 s / it)\n",
            "* Acc@task 71.000 Acc@1 0.000 Acc@5 10.100 loss 8.293\n",
            "[Average accuracy till task6]\tAcc@task: 80.0500\tAcc@1: 66.4167\tAcc@5: 81.9333\tLoss: 2.0324\tForgetting: 1.7600\tBackward: 76.4200\n",
            "torch.Size([70560, 384])\n",
            "Averaged stats: Lr: 0.005000  Loss: 0.1474  Acc@1: 96.6667 (91.8000)  Acc@5: 100.0000 (98.4333)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:23  Loss: 0.8307 (0.8307)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (83.3333)  time: 0.5583  data: 0.4065  max mem: 1378\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.8209 (0.8379)  Acc@1: 79.1667 (78.0303)  Acc@5: 95.8333 (96.5909)  Acc@task: 83.3333 (83.3333)  time: 0.1989  data: 0.0373  max mem: 1378\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.8209 (0.8920)  Acc@1: 79.1667 (77.5794)  Acc@5: 95.8333 (95.2381)  Acc@task: 83.3333 (81.9444)  time: 0.1638  data: 0.0011  max mem: 1378\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.8045 (0.8418)  Acc@1: 79.1667 (78.7634)  Acc@5: 95.8333 (95.5645)  Acc@task: 83.3333 (82.6613)  time: 0.1629  data: 0.0015  max mem: 1378\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6455 (0.8096)  Acc@1: 79.1667 (79.0650)  Acc@5: 100.0000 (95.9350)  Acc@task: 87.5000 (83.2317)  time: 0.1614  data: 0.0007  max mem: 1378\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6427 (0.8039)  Acc@1: 79.1667 (79.2000)  Acc@5: 100.0000 (96.0000)  Acc@task: 87.5000 (83.4000)  time: 0.1588  data: 0.0007  max mem: 1378\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1730 s / it)\n",
            "* Acc@task 83.400 Acc@1 79.200 Acc@5 96.000 loss 0.804\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:25  Loss: 0.9258 (0.9258)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (83.3333)  time: 0.6075  data: 0.4516  max mem: 1378\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.9199 (0.9351)  Acc@1: 79.1667 (77.2727)  Acc@5: 95.8333 (96.2121)  Acc@task: 83.3333 (83.3333)  time: 0.2026  data: 0.0413  max mem: 1378\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.9199 (1.0060)  Acc@1: 75.0000 (75.5952)  Acc@5: 95.8333 (95.0397)  Acc@task: 83.3333 (80.7540)  time: 0.1633  data: 0.0003  max mem: 1378\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.9492 (0.9847)  Acc@1: 70.8333 (75.8065)  Acc@5: 95.8333 (95.2957)  Acc@task: 79.1667 (81.1828)  time: 0.1639  data: 0.0004  max mem: 1378\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.8667 (0.9476)  Acc@1: 79.1667 (76.4228)  Acc@5: 95.8333 (95.1220)  Acc@task: 79.1667 (81.5041)  time: 0.1624  data: 0.0003  max mem: 1378\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.8667 (0.9475)  Acc@1: 79.1667 (76.5000)  Acc@5: 95.8333 (95.0000)  Acc@task: 83.3333 (81.6000)  time: 0.1599  data: 0.0003  max mem: 1378\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1742 s / it)\n",
            "* Acc@task 81.600 Acc@1 76.500 Acc@5 95.000 loss 0.948\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:19  Loss: 0.5822 (0.5822)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  Acc@task: 91.6667 (91.6667)  time: 0.4651  data: 0.3020  max mem: 1378\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 0.7644 (0.8245)  Acc@1: 79.1667 (80.6818)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (83.3333)  time: 0.1908  data: 0.0290  max mem: 1378\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:03  Loss: 0.7644 (0.7796)  Acc@1: 79.1667 (80.3571)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (83.9286)  time: 0.1627  data: 0.0017  max mem: 1378\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.6090 (0.7172)  Acc@1: 83.3333 (81.3172)  Acc@5: 95.8333 (96.3710)  Acc@task: 83.3333 (83.6022)  time: 0.1619  data: 0.0011  max mem: 1378\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.5917 (0.7113)  Acc@1: 83.3333 (81.4024)  Acc@5: 95.8333 (96.4431)  Acc@task: 83.3333 (84.0447)  time: 0.1615  data: 0.0004  max mem: 1378\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.5917 (0.7115)  Acc@1: 83.3333 (81.3000)  Acc@5: 95.8333 (96.5000)  Acc@task: 83.3333 (84.0000)  time: 0.1590  data: 0.0004  max mem: 1378\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1702 s / it)\n",
            "* Acc@task 84.000 Acc@1 81.300 Acc@5 96.500 loss 0.711\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:26  Loss: 0.5869 (0.5869)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 75.0000 (75.0000)  time: 0.6423  data: 0.4845  max mem: 1378\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:06  Loss: 0.6229 (0.6226)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (97.7273)  Acc@task: 79.1667 (81.0606)  time: 0.2057  data: 0.0443  max mem: 1378\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.7004 (0.6740)  Acc@1: 83.3333 (81.3492)  Acc@5: 95.8333 (97.0238)  Acc@task: 79.1667 (81.3492)  time: 0.1635  data: 0.0003  max mem: 1378\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.7458 (0.6593)  Acc@1: 79.1667 (81.8548)  Acc@5: 95.8333 (96.9086)  Acc@task: 83.3333 (81.7204)  time: 0.1641  data: 0.0003  max mem: 1378\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7554 (0.7303)  Acc@1: 83.3333 (81.1992)  Acc@5: 95.8333 (96.1382)  Acc@task: 83.3333 (81.6057)  time: 0.1627  data: 0.0002  max mem: 1378\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.7554 (0.7187)  Acc@1: 83.3333 (81.4000)  Acc@5: 95.8333 (96.2000)  Acc@task: 83.3333 (81.8000)  time: 0.1601  data: 0.0002  max mem: 1378\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1752 s / it)\n",
            "* Acc@task 81.800 Acc@1 81.400 Acc@5 96.200 loss 0.719\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:19  Loss: 0.2656 (0.2656)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.4651  data: 0.2912  max mem: 1378\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 0.7127 (0.7099)  Acc@1: 79.1667 (78.7879)  Acc@5: 95.8333 (96.9697)  Acc@task: 83.3333 (83.3333)  time: 0.2027  data: 0.0376  max mem: 1378\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.7139 (0.7233)  Acc@1: 75.0000 (78.9683)  Acc@5: 95.8333 (96.2302)  Acc@task: 79.1667 (79.7619)  time: 0.1693  data: 0.0068  max mem: 1378\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.7040 (0.7213)  Acc@1: 79.1667 (79.3011)  Acc@5: 95.8333 (96.1022)  Acc@task: 75.0000 (79.4355)  time: 0.1617  data: 0.0008  max mem: 1378\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.7947 (0.7834)  Acc@1: 75.0000 (78.2520)  Acc@5: 95.8333 (95.9350)  Acc@task: 75.0000 (78.6585)  time: 0.1615  data: 0.0002  max mem: 1378\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.7955 (0.8217)  Acc@1: 75.0000 (77.8000)  Acc@5: 95.8333 (95.7000)  Acc@task: 75.0000 (78.5000)  time: 0.1590  data: 0.0002  max mem: 1378\n",
            "Test: [Task 5] Total time: 0:00:07 (0.1735 s / it)\n",
            "* Acc@task 78.500 Acc@1 77.800 Acc@5 95.700 loss 0.822\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:27  Loss: 0.4657 (0.4657)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (83.3333)  time: 0.6470  data: 0.4810  max mem: 1378\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:06  Loss: 0.9612 (0.9429)  Acc@1: 79.1667 (75.7576)  Acc@5: 95.8333 (95.0758)  Acc@task: 75.0000 (73.8636)  time: 0.2065  data: 0.0440  max mem: 1378\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:04  Loss: 0.9556 (0.9152)  Acc@1: 75.0000 (74.4048)  Acc@5: 95.8333 (96.2302)  Acc@task: 70.8333 (73.0159)  time: 0.1635  data: 0.0003  max mem: 1378\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 0.8430 (0.9244)  Acc@1: 70.8333 (74.1936)  Acc@5: 95.8333 (96.2366)  Acc@task: 70.8333 (70.9677)  time: 0.1633  data: 0.0004  max mem: 1378\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 0.8214 (0.9343)  Acc@1: 75.0000 (74.1870)  Acc@5: 95.8333 (95.8333)  Acc@task: 70.8333 (70.8333)  time: 0.1616  data: 0.0003  max mem: 1378\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 0.8214 (0.9352)  Acc@1: 75.0000 (74.0000)  Acc@5: 95.8333 (95.9000)  Acc@task: 70.8333 (71.0000)  time: 0.1590  data: 0.0003  max mem: 1378\n",
            "Test: [Task 6] Total time: 0:00:07 (0.1757 s / it)\n",
            "* Acc@task 71.000 Acc@1 74.000 Acc@5 95.900 loss 0.935\n",
            "[Average accuracy till task6]\tAcc@task: 80.0500\tAcc@1: 78.3667\tAcc@5: 95.8833\tLoss: 0.8231\tForgetting: 1.9600\tBackward: 10.0400\n",
            "Loading checkpoint from: ./output/cifar100_full_dino_1epoch_25pct/checkpoint/task7_checkpoint.pth\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/engines/hide_promtp_wtp_and_tap_engine.py:540: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  loss = torch.nn.functional.cross_entropy(sim, torch.range(0, sim.shape[0] - 1).long().to(device))\n",
            "Train: Epoch[1/1]  [ 0/53]  eta: 0:00:43  Lr: 0.002812  Loss: 3.6289  Acc@1: 8.3333 (8.3333)  Acc@5: 54.1667 (54.1667)  time: 0.8136  data: 0.4588  max mem: 1378\n",
            "Train: Epoch[1/1]  [10/53]  eta: 0:00:13  Lr: 0.002812  Loss: 2.8104  Acc@1: 12.5000 (13.6364)  Acc@5: 54.1667 (53.4091)  time: 0.3122  data: 0.0428  max mem: 1380\n",
            "Train: Epoch[1/1]  [20/53]  eta: 0:00:09  Lr: 0.002812  Loss: 2.1245  Acc@1: 20.8333 (19.2460)  Acc@5: 58.3333 (61.9048)  time: 0.2586  data: 0.0007  max mem: 1380\n",
            "Train: Epoch[1/1]  [30/53]  eta: 0:00:06  Lr: 0.002812  Loss: 1.9976  Acc@1: 29.1667 (23.5215)  Acc@5: 75.0000 (67.6075)  time: 0.2559  data: 0.0003  max mem: 1380\n",
            "Train: Epoch[1/1]  [40/53]  eta: 0:00:03  Lr: 0.002812  Loss: 1.6522  Acc@1: 37.5000 (28.7602)  Acc@5: 83.3333 (71.5447)  time: 0.2568  data: 0.0003  max mem: 1380\n",
            "Train: Epoch[1/1]  [50/53]  eta: 0:00:00  Lr: 0.002812  Loss: 1.2556  Acc@1: 50.0000 (33.9052)  Acc@5: 87.5000 (75.0817)  time: 0.2568  data: 0.0003  max mem: 1380\n",
            "Train: Epoch[1/1]  [52/53]  eta: 0:00:00  Lr: 0.002812  Loss: 1.7579  Acc@1: 50.0000 (34.5942)  Acc@5: 87.5000 (75.7289)  time: 0.2553  data: 0.0003  max mem: 1380\n",
            "Train: Epoch[1/1] Total time: 0:00:14 (0.2703 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.7579  Acc@1: 50.0000 (34.5942)  Acc@5: 87.5000 (75.7289)\n",
            "torch.Size([5, 2, 5, 6, 64])\n",
            "torch.Size([5, 2, 1, 5, 6, 64])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:22  Loss: 0.8196 (0.8196)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  Acc@task: 79.1667 (79.1667)  time: 0.5425  data: 0.3817  max mem: 1380\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.8196 (0.8590)  Acc@1: 79.1667 (78.7879)  Acc@5: 95.8333 (95.4545)  Acc@task: 79.1667 (81.8182)  time: 0.1971  data: 0.0353  max mem: 1380\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:03  Loss: 0.7481 (0.8993)  Acc@1: 79.1667 (77.7778)  Acc@5: 95.8333 (95.0397)  Acc@task: 79.1667 (80.5556)  time: 0.1635  data: 0.0005  max mem: 1380\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.7481 (0.8505)  Acc@1: 79.1667 (78.6290)  Acc@5: 95.8333 (95.4301)  Acc@task: 79.1667 (81.0484)  time: 0.1642  data: 0.0011  max mem: 1380\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6981 (0.8172)  Acc@1: 79.1667 (78.8618)  Acc@5: 100.0000 (95.8333)  Acc@task: 83.3333 (81.9106)  time: 0.1632  data: 0.0011  max mem: 1380\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6469 (0.8114)  Acc@1: 79.1667 (79.0000)  Acc@5: 100.0000 (95.9000)  Acc@task: 83.3333 (82.1000)  time: 0.1608  data: 0.0011  max mem: 1380\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1737 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@task 82.100 Acc@1 79.000 Acc@5 95.900 loss 0.811\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:24  Loss: 1.0718 (1.0718)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 79.1667 (79.1667)  time: 0.5940  data: 0.4370  max mem: 1380\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.9180 (0.9595)  Acc@1: 79.1667 (77.2727)  Acc@5: 95.8333 (95.8333)  Acc@task: 79.1667 (79.9242)  time: 0.2017  data: 0.0400  max mem: 1380\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.9180 (1.0234)  Acc@1: 75.0000 (75.7937)  Acc@5: 95.8333 (95.0397)  Acc@task: 79.1667 (79.1667)  time: 0.1634  data: 0.0003  max mem: 1380\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 1.0037 (0.9992)  Acc@1: 70.8333 (75.6720)  Acc@5: 95.8333 (95.2957)  Acc@task: 79.1667 (80.1075)  time: 0.1635  data: 0.0003  max mem: 1380\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.8622 (0.9610)  Acc@1: 79.1667 (76.2195)  Acc@5: 95.8333 (95.1220)  Acc@task: 79.1667 (80.3862)  time: 0.1631  data: 0.0002  max mem: 1380\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.8622 (0.9607)  Acc@1: 79.1667 (76.3000)  Acc@5: 95.8333 (95.0000)  Acc@task: 79.1667 (80.4000)  time: 0.1604  data: 0.0002  max mem: 1380\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1744 s / it)\n",
            "* Acc@task 80.400 Acc@1 76.300 Acc@5 95.000 loss 0.961\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:17  Loss: 0.5926 (0.5926)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 87.5000 (87.5000)  time: 0.4188  data: 0.2635  max mem: 1380\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 0.7687 (0.8311)  Acc@1: 79.1667 (80.3030)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (81.8182)  time: 0.1883  data: 0.0244  max mem: 1380\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:03  Loss: 0.7764 (0.7875)  Acc@1: 79.1667 (79.9603)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (82.7381)  time: 0.1653  data: 0.0008  max mem: 1380\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.6080 (0.7238)  Acc@1: 83.3333 (81.0484)  Acc@5: 95.8333 (96.3710)  Acc@task: 83.3333 (82.1237)  time: 0.1639  data: 0.0011  max mem: 1380\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.5922 (0.7179)  Acc@1: 83.3333 (81.3008)  Acc@5: 95.8333 (96.4431)  Acc@task: 83.3333 (82.9268)  time: 0.1636  data: 0.0008  max mem: 1380\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.5922 (0.7178)  Acc@1: 83.3333 (81.2000)  Acc@5: 95.8333 (96.5000)  Acc@task: 83.3333 (82.9000)  time: 0.1611  data: 0.0007  max mem: 1380\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1714 s / it)\n",
            "* Acc@task 82.900 Acc@1 81.200 Acc@5 96.500 loss 0.718\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:22  Loss: 0.5915 (0.5915)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 66.6667 (66.6667)  time: 0.5386  data: 0.3679  max mem: 1380\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:06  Loss: 0.5915 (0.6040)  Acc@1: 83.3333 (83.7121)  Acc@5: 100.0000 (97.7273)  Acc@task: 79.1667 (79.1667)  time: 0.1976  data: 0.0337  max mem: 1380\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.6898 (0.6727)  Acc@1: 83.3333 (81.5476)  Acc@5: 95.8333 (97.0238)  Acc@task: 79.1667 (80.5556)  time: 0.1646  data: 0.0003  max mem: 1380\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.7048 (0.6578)  Acc@1: 79.1667 (82.1237)  Acc@5: 95.8333 (96.9086)  Acc@task: 83.3333 (80.7796)  time: 0.1654  data: 0.0003  max mem: 1380\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7048 (0.7285)  Acc@1: 83.3333 (81.4024)  Acc@5: 95.8333 (96.2398)  Acc@task: 79.1667 (80.3862)  time: 0.1646  data: 0.0002  max mem: 1380\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.6936 (0.7169)  Acc@1: 83.3333 (81.6000)  Acc@5: 95.8333 (96.3000)  Acc@task: 79.1667 (80.5000)  time: 0.1621  data: 0.0002  max mem: 1380\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1743 s / it)\n",
            "* Acc@task 80.500 Acc@1 81.600 Acc@5 96.300 loss 0.717\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:25  Loss: 0.2620 (0.2620)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.6188  data: 0.4549  max mem: 1380\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 0.7012 (0.7088)  Acc@1: 79.1667 (78.7879)  Acc@5: 95.8333 (96.9697)  Acc@task: 83.3333 (84.0909)  time: 0.2058  data: 0.0418  max mem: 1380\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.7136 (0.7184)  Acc@1: 75.0000 (79.1667)  Acc@5: 95.8333 (96.4286)  Acc@task: 83.3333 (82.1429)  time: 0.1650  data: 0.0013  max mem: 1380\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.7032 (0.7107)  Acc@1: 79.1667 (79.4355)  Acc@5: 95.8333 (96.2366)  Acc@task: 79.1667 (81.8548)  time: 0.1653  data: 0.0015  max mem: 1380\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.7572 (0.7719)  Acc@1: 75.0000 (78.5569)  Acc@5: 95.8333 (96.1382)  Acc@task: 79.1667 (80.6911)  time: 0.1650  data: 0.0006  max mem: 1380\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.7930 (0.8104)  Acc@1: 75.0000 (78.1000)  Acc@5: 95.8333 (95.9000)  Acc@task: 79.1667 (80.5000)  time: 0.1623  data: 0.0005  max mem: 1380\n",
            "Test: [Task 5] Total time: 0:00:07 (0.1765 s / it)\n",
            "* Acc@task 80.500 Acc@1 78.100 Acc@5 95.900 loss 0.810\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:19  Loss: 0.4654 (0.4654)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (83.3333)  time: 0.4682  data: 0.3035  max mem: 1380\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:06  Loss: 0.8701 (0.8771)  Acc@1: 79.1667 (77.2727)  Acc@5: 95.8333 (95.4545)  Acc@task: 79.1667 (79.5455)  time: 0.1925  data: 0.0300  max mem: 1380\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:03  Loss: 0.9120 (0.8497)  Acc@1: 75.0000 (76.3889)  Acc@5: 95.8333 (96.4286)  Acc@task: 79.1667 (78.9683)  time: 0.1649  data: 0.0015  max mem: 1380\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 0.8036 (0.8601)  Acc@1: 75.0000 (75.9409)  Acc@5: 95.8333 (96.5054)  Acc@task: 75.0000 (77.5538)  time: 0.1641  data: 0.0003  max mem: 1380\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 0.7836 (0.8695)  Acc@1: 79.1667 (76.1179)  Acc@5: 95.8333 (96.2398)  Acc@task: 75.0000 (77.1341)  time: 0.1638  data: 0.0002  max mem: 1380\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 0.7836 (0.8712)  Acc@1: 79.1667 (76.0000)  Acc@5: 95.8333 (96.3000)  Acc@task: 75.0000 (77.3000)  time: 0.1611  data: 0.0002  max mem: 1380\n",
            "Test: [Task 6] Total time: 0:00:07 (0.1724 s / it)\n",
            "* Acc@task 77.300 Acc@1 76.000 Acc@5 96.300 loss 0.871\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:30  Loss: 8.0949 (8.0949)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  Acc@task: 75.0000 (75.0000)  time: 0.7162  data: 0.4928  max mem: 1380\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:06  Loss: 8.0415 (7.8061)  Acc@1: 0.0000 (0.3788)  Acc@5: 8.3333 (8.7121)  Acc@task: 62.5000 (65.1515)  time: 0.2137  data: 0.0470  max mem: 1380\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:04  Loss: 7.7912 (7.7260)  Acc@1: 0.0000 (0.1984)  Acc@5: 8.3333 (9.1270)  Acc@task: 62.5000 (65.6746)  time: 0.1628  data: 0.0019  max mem: 1380\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:02  Loss: 7.9249 (7.8119)  Acc@1: 0.0000 (0.1344)  Acc@5: 8.3333 (9.2742)  Acc@task: 66.6667 (66.3979)  time: 0.1620  data: 0.0009  max mem: 1380\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 7.9268 (7.8213)  Acc@1: 0.0000 (0.1016)  Acc@5: 8.3333 (9.2480)  Acc@task: 66.6667 (67.3781)  time: 0.1617  data: 0.0002  max mem: 1380\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 7.9249 (7.7992)  Acc@1: 0.0000 (0.2000)  Acc@5: 8.3333 (9.4000)  Acc@task: 68.7500 (67.4000)  time: 0.1591  data: 0.0002  max mem: 1380\n",
            "Test: [Task 7] Total time: 0:00:07 (0.1763 s / it)\n",
            "* Acc@task 67.400 Acc@1 0.200 Acc@5 9.400 loss 7.799\n",
            "[Average accuracy till task7]\tAcc@task: 78.7286\tAcc@1: 67.4857\tAcc@5: 83.6143\tLoss: 1.8125\tForgetting: 1.9167\tBackward: 75.9667\n",
            "torch.Size([82440, 384])\n",
            "Averaged stats: Lr: 0.005000  Loss: 0.0510  Acc@1: 97.5000 (93.3750)  Acc@5: 100.0000 (98.6250)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:24  Loss: 0.7024 (0.7024)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 79.1667 (79.1667)  time: 0.5808  data: 0.4212  max mem: 1380\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.7350 (0.8023)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (98.1061)  Acc@task: 79.1667 (81.8182)  time: 0.2005  data: 0.0388  max mem: 1380\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.7714 (0.8432)  Acc@1: 79.1667 (78.5714)  Acc@5: 95.8333 (96.6270)  Acc@task: 79.1667 (80.5556)  time: 0.1632  data: 0.0004  max mem: 1380\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.6083 (0.8025)  Acc@1: 75.0000 (78.7634)  Acc@5: 95.8333 (96.9086)  Acc@task: 79.1667 (81.0484)  time: 0.1632  data: 0.0003  max mem: 1380\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6083 (0.7664)  Acc@1: 83.3333 (79.8781)  Acc@5: 100.0000 (96.9512)  Acc@task: 83.3333 (81.9106)  time: 0.1619  data: 0.0005  max mem: 1380\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.5695 (0.7593)  Acc@1: 83.3333 (80.1000)  Acc@5: 100.0000 (97.0000)  Acc@task: 83.3333 (82.1000)  time: 0.1593  data: 0.0005  max mem: 1380\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1751 s / it)\n",
            "* Acc@task 82.100 Acc@1 80.100 Acc@5 97.000 loss 0.759\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:34  Loss: 1.0864 (1.0864)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  Acc@task: 79.1667 (79.1667)  time: 0.8138  data: 0.6107  max mem: 1380\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:07  Loss: 0.8889 (0.9136)  Acc@1: 79.1667 (78.0303)  Acc@5: 95.8333 (96.5909)  Acc@task: 79.1667 (79.9242)  time: 0.2222  data: 0.0563  max mem: 1380\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.8889 (0.9896)  Acc@1: 75.0000 (75.7937)  Acc@5: 95.8333 (95.4365)  Acc@task: 79.1667 (79.1667)  time: 0.1627  data: 0.0006  max mem: 1380\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.9389 (0.9682)  Acc@1: 75.0000 (76.2097)  Acc@5: 95.8333 (95.5645)  Acc@task: 79.1667 (80.1075)  time: 0.1622  data: 0.0003  max mem: 1380\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.8091 (0.9444)  Acc@1: 79.1667 (76.4228)  Acc@5: 95.8333 (95.4268)  Acc@task: 79.1667 (80.3862)  time: 0.1616  data: 0.0002  max mem: 1380\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.8091 (0.9415)  Acc@1: 79.1667 (76.4000)  Acc@5: 95.8333 (95.3000)  Acc@task: 79.1667 (80.4000)  time: 0.1591  data: 0.0002  max mem: 1380\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1788 s / it)\n",
            "* Acc@task 80.400 Acc@1 76.400 Acc@5 95.300 loss 0.942\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:21  Loss: 0.6327 (0.6327)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  Acc@task: 87.5000 (87.5000)  time: 0.5221  data: 0.3589  max mem: 1380\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 0.8440 (0.9068)  Acc@1: 79.1667 (77.2727)  Acc@5: 95.8333 (96.2121)  Acc@task: 83.3333 (81.8182)  time: 0.1949  data: 0.0329  max mem: 1380\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:03  Loss: 0.8440 (0.8542)  Acc@1: 79.1667 (77.7778)  Acc@5: 95.8333 (95.4365)  Acc@task: 83.3333 (82.7381)  time: 0.1632  data: 0.0003  max mem: 1380\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.6675 (0.7830)  Acc@1: 79.1667 (78.8979)  Acc@5: 95.8333 (95.6989)  Acc@task: 83.3333 (82.1237)  time: 0.1632  data: 0.0004  max mem: 1380\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.7077 (0.7724)  Acc@1: 79.1667 (79.6748)  Acc@5: 95.8333 (95.6301)  Acc@task: 83.3333 (82.9268)  time: 0.1615  data: 0.0005  max mem: 1380\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.6840 (0.7703)  Acc@1: 79.1667 (79.6000)  Acc@5: 95.8333 (95.7000)  Acc@task: 83.3333 (82.9000)  time: 0.1589  data: 0.0005  max mem: 1380\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1730 s / it)\n",
            "* Acc@task 82.900 Acc@1 79.600 Acc@5 95.700 loss 0.770\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:25  Loss: 1.0062 (1.0062)  Acc@1: 62.5000 (62.5000)  Acc@5: 87.5000 (87.5000)  Acc@task: 66.6667 (66.6667)  time: 0.6133  data: 0.4360  max mem: 1380\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:06  Loss: 0.8558 (0.7580)  Acc@1: 79.1667 (80.3030)  Acc@5: 95.8333 (94.6970)  Acc@task: 79.1667 (79.1667)  time: 0.2041  data: 0.0399  max mem: 1380\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.8997 (0.8338)  Acc@1: 79.1667 (78.3730)  Acc@5: 95.8333 (94.8413)  Acc@task: 79.1667 (80.5556)  time: 0.1637  data: 0.0004  max mem: 1380\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.8997 (0.8221)  Acc@1: 75.0000 (78.4946)  Acc@5: 95.8333 (94.8925)  Acc@task: 83.3333 (80.7796)  time: 0.1641  data: 0.0004  max mem: 1380\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.8968 (0.8774)  Acc@1: 79.1667 (78.5569)  Acc@5: 95.8333 (94.4106)  Acc@task: 79.1667 (80.3862)  time: 0.1633  data: 0.0002  max mem: 1380\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.8968 (0.8645)  Acc@1: 79.1667 (78.7000)  Acc@5: 95.8333 (94.5000)  Acc@task: 79.1667 (80.5000)  time: 0.1609  data: 0.0002  max mem: 1380\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1750 s / it)\n",
            "* Acc@task 80.500 Acc@1 78.700 Acc@5 94.500 loss 0.864\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:24  Loss: 0.2633 (0.2633)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.5830  data: 0.4172  max mem: 1380\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 0.6793 (0.6729)  Acc@1: 83.3333 (81.4394)  Acc@5: 95.8333 (96.9697)  Acc@task: 83.3333 (84.0909)  time: 0.2002  data: 0.0382  max mem: 1380\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.7476 (0.7082)  Acc@1: 79.1667 (80.3571)  Acc@5: 95.8333 (96.6270)  Acc@task: 83.3333 (82.1429)  time: 0.1637  data: 0.0004  max mem: 1380\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.6723 (0.7002)  Acc@1: 79.1667 (81.0484)  Acc@5: 95.8333 (95.9677)  Acc@task: 79.1667 (81.8548)  time: 0.1642  data: 0.0010  max mem: 1380\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.6752 (0.7531)  Acc@1: 79.1667 (80.3862)  Acc@5: 95.8333 (95.4268)  Acc@task: 79.1667 (80.6911)  time: 0.1622  data: 0.0009  max mem: 1380\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.6980 (0.7919)  Acc@1: 79.1667 (79.8000)  Acc@5: 95.8333 (95.2000)  Acc@task: 79.1667 (80.5000)  time: 0.1596  data: 0.0009  max mem: 1380\n",
            "Test: [Task 5] Total time: 0:00:07 (0.1737 s / it)\n",
            "* Acc@task 80.500 Acc@1 79.800 Acc@5 95.200 loss 0.792\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:22  Loss: 0.6149 (0.6149)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (83.3333)  time: 0.5433  data: 0.3728  max mem: 1380\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:06  Loss: 1.0164 (1.0716)  Acc@1: 70.8333 (71.9697)  Acc@5: 95.8333 (94.3182)  Acc@task: 79.1667 (79.5455)  time: 0.1974  data: 0.0343  max mem: 1380\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:04  Loss: 1.0164 (1.0751)  Acc@1: 70.8333 (70.4365)  Acc@5: 95.8333 (94.6429)  Acc@task: 79.1667 (78.9683)  time: 0.1640  data: 0.0004  max mem: 1380\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 1.1588 (1.1099)  Acc@1: 70.8333 (70.4301)  Acc@5: 95.8333 (94.8925)  Acc@task: 75.0000 (77.5538)  time: 0.1645  data: 0.0004  max mem: 1380\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.1616 (1.1086)  Acc@1: 70.8333 (70.5285)  Acc@5: 95.8333 (94.5122)  Acc@task: 75.0000 (77.1341)  time: 0.1632  data: 0.0004  max mem: 1380\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.1616 (1.1073)  Acc@1: 70.8333 (70.5000)  Acc@5: 95.8333 (94.5000)  Acc@task: 75.0000 (77.3000)  time: 0.1606  data: 0.0004  max mem: 1380\n",
            "Test: [Task 6] Total time: 0:00:07 (0.1735 s / it)\n",
            "* Acc@task 77.300 Acc@1 70.500 Acc@5 94.500 loss 1.107\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:21  Loss: 1.0503 (1.0503)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  Acc@task: 75.0000 (75.0000)  time: 0.5164  data: 0.3516  max mem: 1380\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:06  Loss: 0.7744 (0.7655)  Acc@1: 75.0000 (76.5152)  Acc@5: 95.8333 (94.6970)  Acc@task: 62.5000 (65.1515)  time: 0.1945  data: 0.0330  max mem: 1380\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:03  Loss: 0.8505 (0.9020)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (94.4444)  Acc@task: 62.5000 (65.6746)  time: 0.1640  data: 0.0011  max mem: 1380\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:02  Loss: 0.9312 (0.9609)  Acc@1: 70.8333 (73.9247)  Acc@5: 95.8333 (93.0108)  Acc@task: 66.6667 (66.3979)  time: 0.1652  data: 0.0012  max mem: 1380\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 0.9270 (0.9833)  Acc@1: 70.8333 (73.6789)  Acc@5: 91.6667 (92.7846)  Acc@task: 66.6667 (67.3781)  time: 0.1639  data: 0.0009  max mem: 1380\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 0.9235 (0.9706)  Acc@1: 70.8333 (74.0000)  Acc@5: 91.6667 (92.9000)  Acc@task: 68.7500 (67.4000)  time: 0.1613  data: 0.0008  max mem: 1380\n",
            "Test: [Task 7] Total time: 0:00:07 (0.1736 s / it)\n",
            "* Acc@task 67.400 Acc@1 74.000 Acc@5 92.900 loss 0.971\n",
            "[Average accuracy till task7]\tAcc@task: 78.7286\tAcc@1: 77.0143\tAcc@5: 95.0143\tLoss: 0.8865\tForgetting: 2.4833\tBackward: 7.5167\n",
            "Loading checkpoint from: ./output/cifar100_full_dino_1epoch_25pct/checkpoint/task8_checkpoint.pth\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/engines/hide_promtp_wtp_and_tap_engine.py:540: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  loss = torch.nn.functional.cross_entropy(sim, torch.range(0, sim.shape[0] - 1).long().to(device))\n",
            "Train: Epoch[1/1]  [ 0/52]  eta: 0:00:39  Lr: 0.002812  Loss: 4.3561  Acc@1: 12.5000 (12.5000)  Acc@5: 41.6667 (41.6667)  time: 0.7523  data: 0.4804  max mem: 1380\n",
            "Train: Epoch[1/1]  [10/52]  eta: 0:00:12  Lr: 0.002812  Loss: 3.1365  Acc@1: 12.5000 (12.5000)  Acc@5: 50.0000 (48.1061)  time: 0.3056  data: 0.0442  max mem: 1380\n",
            "Train: Epoch[1/1]  [20/52]  eta: 0:00:09  Lr: 0.002812  Loss: 2.7898  Acc@1: 12.5000 (16.4683)  Acc@5: 54.1667 (55.5556)  time: 0.2609  data: 0.0005  max mem: 1380\n",
            "Train: Epoch[1/1]  [30/52]  eta: 0:00:06  Lr: 0.002812  Loss: 1.9587  Acc@1: 25.0000 (20.2957)  Acc@5: 66.6667 (60.2151)  time: 0.2611  data: 0.0003  max mem: 1380\n",
            "Train: Epoch[1/1]  [40/52]  eta: 0:00:03  Lr: 0.002812  Loss: 2.2126  Acc@1: 33.3333 (26.2195)  Acc@5: 79.1667 (66.4634)  time: 0.2635  data: 0.0006  max mem: 1380\n",
            "Train: Epoch[1/1]  [50/52]  eta: 0:00:00  Lr: 0.002812  Loss: 1.5635  Acc@1: 45.8333 (31.4542)  Acc@5: 83.3333 (70.7516)  time: 0.2628  data: 0.0007  max mem: 1380\n",
            "Train: Epoch[1/1]  [51/52]  eta: 0:00:00  Lr: 0.002812  Loss: 1.8123  Acc@1: 45.8333 (31.5534)  Acc@5: 83.3333 (70.8738)  time: 0.2566  data: 0.0007  max mem: 1380\n",
            "Train: Epoch[1/1] Total time: 0:00:14 (0.2707 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.8123  Acc@1: 45.8333 (31.5534)  Acc@5: 83.3333 (70.8738)\n",
            "torch.Size([5, 2, 5, 6, 64])\n",
            "torch.Size([5, 2, 1, 5, 6, 64])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:35  Loss: 0.7023 (0.7023)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 79.1667 (79.1667)  time: 0.8508  data: 0.6180  max mem: 1380\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:07  Loss: 0.7348 (0.8166)  Acc@1: 79.1667 (78.7879)  Acc@5: 100.0000 (97.7273)  Acc@task: 79.1667 (80.3030)  time: 0.2265  data: 0.0568  max mem: 1380\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.7920 (0.8641)  Acc@1: 75.0000 (78.3730)  Acc@5: 95.8333 (96.2302)  Acc@task: 79.1667 (79.5635)  time: 0.1630  data: 0.0012  max mem: 1380\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.6283 (0.8182)  Acc@1: 75.0000 (78.7634)  Acc@5: 95.8333 (96.5054)  Acc@task: 83.3333 (80.3763)  time: 0.1618  data: 0.0011  max mem: 1380\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6079 (0.7794)  Acc@1: 83.3333 (79.8781)  Acc@5: 95.8333 (96.6463)  Acc@task: 83.3333 (81.4024)  time: 0.1617  data: 0.0003  max mem: 1380\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.5695 (0.7720)  Acc@1: 83.3333 (80.1000)  Acc@5: 100.0000 (96.7000)  Acc@task: 87.5000 (81.6000)  time: 0.1590  data: 0.0003  max mem: 1380\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1800 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@task 81.600 Acc@1 80.100 Acc@5 96.700 loss 0.772\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:23  Loss: 1.3161 (1.3161)  Acc@1: 83.3333 (83.3333)  Acc@5: 91.6667 (91.6667)  Acc@task: 75.0000 (75.0000)  time: 0.5634  data: 0.4102  max mem: 1380\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.8943 (0.9484)  Acc@1: 79.1667 (78.0303)  Acc@5: 95.8333 (95.8333)  Acc@task: 75.0000 (76.5152)  time: 0.1986  data: 0.0377  max mem: 1380\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.8943 (1.0178)  Acc@1: 75.0000 (75.5952)  Acc@5: 95.8333 (95.0397)  Acc@task: 75.0000 (76.9841)  time: 0.1634  data: 0.0004  max mem: 1380\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.9387 (0.9858)  Acc@1: 75.0000 (75.9409)  Acc@5: 95.8333 (95.2957)  Acc@task: 79.1667 (78.0914)  time: 0.1638  data: 0.0003  max mem: 1380\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.8062 (0.9577)  Acc@1: 75.0000 (76.0163)  Acc@5: 95.8333 (95.3252)  Acc@task: 79.1667 (78.8618)  time: 0.1620  data: 0.0003  max mem: 1380\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.8062 (0.9545)  Acc@1: 75.0000 (76.0000)  Acc@5: 95.8333 (95.2000)  Acc@task: 79.1667 (78.8000)  time: 0.1593  data: 0.0002  max mem: 1380\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1741 s / it)\n",
            "* Acc@task 78.800 Acc@1 76.000 Acc@5 95.200 loss 0.954\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:36  Loss: 0.6328 (0.6328)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  Acc@task: 87.5000 (87.5000)  time: 0.8638  data: 0.6610  max mem: 1380\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:07  Loss: 0.8435 (0.9082)  Acc@1: 79.1667 (76.8939)  Acc@5: 95.8333 (96.2121)  Acc@task: 83.3333 (80.6818)  time: 0.2271  data: 0.0608  max mem: 1380\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.8435 (0.8559)  Acc@1: 79.1667 (77.5794)  Acc@5: 95.8333 (95.6349)  Acc@task: 79.1667 (81.1508)  time: 0.1628  data: 0.0006  max mem: 1380\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.6812 (0.7835)  Acc@1: 79.1667 (78.8979)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (81.0484)  time: 0.1620  data: 0.0009  max mem: 1380\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.7075 (0.7717)  Acc@1: 79.1667 (79.6748)  Acc@5: 95.8333 (95.7317)  Acc@task: 83.3333 (82.1138)  time: 0.1616  data: 0.0009  max mem: 1380\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.6837 (0.7696)  Acc@1: 79.1667 (79.6000)  Acc@5: 95.8333 (95.8000)  Acc@task: 83.3333 (82.0000)  time: 0.1590  data: 0.0009  max mem: 1380\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1798 s / it)\n",
            "* Acc@task 82.000 Acc@1 79.600 Acc@5 95.800 loss 0.770\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:27  Loss: 1.0312 (1.0312)  Acc@1: 62.5000 (62.5000)  Acc@5: 87.5000 (87.5000)  Acc@task: 66.6667 (66.6667)  time: 0.6484  data: 0.4934  max mem: 1380\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:06  Loss: 0.8002 (0.7555)  Acc@1: 79.1667 (80.3030)  Acc@5: 95.8333 (94.6970)  Acc@task: 75.0000 (76.1364)  time: 0.2069  data: 0.0451  max mem: 1380\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.8927 (0.8328)  Acc@1: 79.1667 (78.3730)  Acc@5: 95.8333 (94.8413)  Acc@task: 79.1667 (77.1825)  time: 0.1637  data: 0.0003  max mem: 1380\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.8992 (0.8212)  Acc@1: 75.0000 (78.4946)  Acc@5: 95.8333 (94.8925)  Acc@task: 79.1667 (77.6882)  time: 0.1642  data: 0.0004  max mem: 1380\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.9001 (0.8761)  Acc@1: 79.1667 (78.5569)  Acc@5: 95.8333 (94.4106)  Acc@task: 79.1667 (77.8455)  time: 0.1631  data: 0.0004  max mem: 1380\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.9001 (0.8631)  Acc@1: 79.1667 (78.7000)  Acc@5: 95.8333 (94.5000)  Acc@task: 79.1667 (78.0000)  time: 0.1606  data: 0.0004  max mem: 1380\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1771 s / it)\n",
            "* Acc@task 78.000 Acc@1 78.700 Acc@5 94.500 loss 0.863\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:31  Loss: 0.2631 (0.2631)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.7492  data: 0.5605  max mem: 1380\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 0.6775 (0.6675)  Acc@1: 83.3333 (81.4394)  Acc@5: 95.8333 (96.9697)  Acc@task: 79.1667 (82.1970)  time: 0.2165  data: 0.0513  max mem: 1380\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.7471 (0.6999)  Acc@1: 79.1667 (80.5556)  Acc@5: 95.8333 (96.6270)  Acc@task: 79.1667 (81.3492)  time: 0.1644  data: 0.0004  max mem: 1380\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.7096 (0.6986)  Acc@1: 79.1667 (81.1828)  Acc@5: 95.8333 (95.9677)  Acc@task: 79.1667 (80.7796)  time: 0.1641  data: 0.0003  max mem: 1380\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.7096 (0.7530)  Acc@1: 79.1667 (80.4878)  Acc@5: 95.8333 (95.4268)  Acc@task: 75.0000 (79.7764)  time: 0.1636  data: 0.0002  max mem: 1380\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.7120 (0.7910)  Acc@1: 79.1667 (80.0000)  Acc@5: 95.8333 (95.2000)  Acc@task: 75.0000 (79.6000)  time: 0.1611  data: 0.0002  max mem: 1380\n",
            "Test: [Task 5] Total time: 0:00:07 (0.1792 s / it)\n",
            "* Acc@task 79.600 Acc@1 80.000 Acc@5 95.200 loss 0.791\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:24  Loss: 0.6129 (0.6129)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 87.5000 (87.5000)  time: 0.5872  data: 0.4280  max mem: 1380\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:06  Loss: 1.0163 (1.0076)  Acc@1: 75.0000 (73.4849)  Acc@5: 95.8333 (95.4545)  Acc@task: 83.3333 (81.0606)  time: 0.2018  data: 0.0393  max mem: 1380\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:04  Loss: 1.0163 (1.0364)  Acc@1: 70.8333 (71.4286)  Acc@5: 95.8333 (95.2381)  Acc@task: 79.1667 (80.3571)  time: 0.1646  data: 0.0004  max mem: 1380\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 1.1319 (1.0895)  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.1613)  Acc@task: 75.0000 (77.6882)  time: 0.1651  data: 0.0008  max mem: 1380\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.1619 (1.0902)  Acc@1: 70.8333 (70.7317)  Acc@5: 95.8333 (94.7154)  Acc@task: 70.8333 (76.5244)  time: 0.1635  data: 0.0010  max mem: 1380\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.1619 (1.0891)  Acc@1: 70.8333 (70.7000)  Acc@5: 95.8333 (94.7000)  Acc@task: 70.8333 (76.6000)  time: 0.1610  data: 0.0010  max mem: 1380\n",
            "Test: [Task 6] Total time: 0:00:07 (0.1750 s / it)\n",
            "* Acc@task 76.600 Acc@1 70.700 Acc@5 94.700 loss 1.089\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:23  Loss: 1.0219 (1.0219)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  Acc@task: 79.1667 (79.1667)  time: 0.5614  data: 0.4047  max mem: 1380\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:06  Loss: 0.6579 (0.7360)  Acc@1: 79.1667 (77.6515)  Acc@5: 95.8333 (95.0758)  Acc@task: 75.0000 (71.2121)  time: 0.1998  data: 0.0374  max mem: 1380\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:04  Loss: 0.8023 (0.8675)  Acc@1: 75.0000 (75.9921)  Acc@5: 95.8333 (95.0397)  Acc@task: 70.8333 (71.6270)  time: 0.1647  data: 0.0006  max mem: 1380\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:02  Loss: 0.9305 (0.9270)  Acc@1: 70.8333 (74.5968)  Acc@5: 95.8333 (93.6828)  Acc@task: 70.8333 (70.9677)  time: 0.1652  data: 0.0005  max mem: 1380\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 0.9252 (0.9538)  Acc@1: 70.8333 (74.1870)  Acc@5: 91.6667 (93.2927)  Acc@task: 70.8333 (71.2398)  time: 0.1638  data: 0.0003  max mem: 1380\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 0.8888 (0.9418)  Acc@1: 70.8333 (74.5000)  Acc@5: 91.6667 (93.4000)  Acc@task: 70.8333 (71.2000)  time: 0.1613  data: 0.0003  max mem: 1380\n",
            "Test: [Task 7] Total time: 0:00:07 (0.1746 s / it)\n",
            "* Acc@task 71.200 Acc@1 74.500 Acc@5 93.400 loss 0.942\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:22  Loss: 9.0950 (9.0950)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  Acc@task: 70.8333 (70.8333)  time: 0.5272  data: 0.3596  max mem: 1380\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:06  Loss: 9.0458 (8.9358)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.3788)  Acc@task: 58.3333 (59.8485)  time: 0.1951  data: 0.0330  max mem: 1380\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:03  Loss: 8.6134 (8.8611)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (1.5873)  Acc@task: 58.3333 (59.3254)  time: 0.1637  data: 0.0004  max mem: 1380\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:02  Loss: 8.7613 (8.9338)  Acc@1: 0.0000 (0.0000)  Acc@5: 4.1667 (2.0161)  Acc@task: 58.3333 (59.6774)  time: 0.1642  data: 0.0008  max mem: 1380\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 8.9225 (8.9140)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (1.9309)  Acc@task: 58.3333 (59.1463)  time: 0.1630  data: 0.0008  max mem: 1380\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 8.9472 (8.9177)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (1.9000)  Acc@task: 58.3333 (59.5000)  time: 0.1604  data: 0.0007  max mem: 1380\n",
            "Test: [Task 8] Total time: 0:00:07 (0.1729 s / it)\n",
            "* Acc@task 59.500 Acc@1 0.000 Acc@5 1.900 loss 8.918\n",
            "[Average accuracy till task8]\tAcc@task: 75.9125\tAcc@1: 67.4500\tAcc@5: 83.4250\tLoss: 1.8874\tForgetting: 2.6571\tBackward: 74.7143\n",
            "torch.Size([94320, 384])\n",
            "Averaged stats: Lr: 0.005000  Loss: 0.0732  Acc@1: 96.6667 (92.9881)  Acc@5: 100.0000 (98.6310)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:18  Loss: 0.6061 (0.6061)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  Acc@task: 79.1667 (79.1667)  time: 0.4312  data: 0.2653  max mem: 1380\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.6498 (0.8067)  Acc@1: 79.1667 (78.0303)  Acc@5: 100.0000 (97.7273)  Acc@task: 79.1667 (80.3030)  time: 0.1926  data: 0.0288  max mem: 1380\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:03  Loss: 0.7083 (0.8696)  Acc@1: 79.1667 (78.5714)  Acc@5: 95.8333 (96.0317)  Acc@task: 79.1667 (79.5635)  time: 0.1665  data: 0.0028  max mem: 1380\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.7144 (0.8260)  Acc@1: 75.0000 (78.8979)  Acc@5: 95.8333 (96.2366)  Acc@task: 83.3333 (80.3763)  time: 0.1629  data: 0.0004  max mem: 1380\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.7144 (0.7907)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (96.4431)  Acc@task: 83.3333 (81.4024)  time: 0.1617  data: 0.0003  max mem: 1380\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6345 (0.7821)  Acc@1: 79.1667 (79.4000)  Acc@5: 95.8333 (96.5000)  Acc@task: 87.5000 (81.6000)  time: 0.1592  data: 0.0003  max mem: 1380\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1727 s / it)\n",
            "* Acc@task 81.600 Acc@1 79.400 Acc@5 96.500 loss 0.782\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:34  Loss: 1.4198 (1.4198)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  Acc@task: 75.0000 (75.0000)  time: 0.8251  data: 0.6426  max mem: 1380\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:07  Loss: 1.1234 (1.0532)  Acc@1: 79.1667 (76.5152)  Acc@5: 95.8333 (95.0758)  Acc@task: 75.0000 (76.5152)  time: 0.2240  data: 0.0602  max mem: 1380\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 1.0296 (1.1209)  Acc@1: 79.1667 (74.8016)  Acc@5: 95.8333 (94.4444)  Acc@task: 75.0000 (76.9841)  time: 0.1632  data: 0.0014  max mem: 1380\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 1.0176 (1.0631)  Acc@1: 75.0000 (75.4032)  Acc@5: 95.8333 (95.0269)  Acc@task: 79.1667 (78.0914)  time: 0.1621  data: 0.0006  max mem: 1380\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.8919 (1.0189)  Acc@1: 75.0000 (75.3049)  Acc@5: 95.8333 (95.1220)  Acc@task: 79.1667 (78.8618)  time: 0.1623  data: 0.0002  max mem: 1380\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.8919 (1.0166)  Acc@1: 75.0000 (75.3000)  Acc@5: 95.8333 (95.0000)  Acc@task: 79.1667 (78.8000)  time: 0.1598  data: 0.0002  max mem: 1380\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1793 s / it)\n",
            "* Acc@task 78.800 Acc@1 75.300 Acc@5 95.000 loss 1.017\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:22  Loss: 0.6768 (0.6768)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  Acc@task: 87.5000 (87.5000)  time: 0.5387  data: 0.3763  max mem: 1380\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 0.7375 (0.8223)  Acc@1: 79.1667 (80.6818)  Acc@5: 95.8333 (96.2121)  Acc@task: 83.3333 (80.6818)  time: 0.1968  data: 0.0349  max mem: 1380\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:03  Loss: 0.7206 (0.7632)  Acc@1: 79.1667 (80.7540)  Acc@5: 95.8333 (95.6349)  Acc@task: 79.1667 (81.1508)  time: 0.1639  data: 0.0006  max mem: 1380\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.5715 (0.7068)  Acc@1: 79.1667 (81.7204)  Acc@5: 95.8333 (96.3710)  Acc@task: 83.3333 (81.0484)  time: 0.1643  data: 0.0005  max mem: 1380\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.5715 (0.7158)  Acc@1: 83.3333 (81.8089)  Acc@5: 95.8333 (96.3415)  Acc@task: 83.3333 (82.1138)  time: 0.1630  data: 0.0004  max mem: 1380\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.5509 (0.7075)  Acc@1: 83.3333 (81.9000)  Acc@5: 95.8333 (96.4000)  Acc@task: 83.3333 (82.0000)  time: 0.1603  data: 0.0003  max mem: 1380\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1740 s / it)\n",
            "* Acc@task 82.000 Acc@1 81.900 Acc@5 96.400 loss 0.707\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:33  Loss: 1.1044 (1.1044)  Acc@1: 58.3333 (58.3333)  Acc@5: 87.5000 (87.5000)  Acc@task: 66.6667 (66.6667)  time: 0.7911  data: 0.6367  max mem: 1380\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:07  Loss: 0.8674 (0.8554)  Acc@1: 83.3333 (78.4091)  Acc@5: 95.8333 (94.3182)  Acc@task: 75.0000 (76.1364)  time: 0.2206  data: 0.0589  max mem: 1380\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.9441 (0.9699)  Acc@1: 75.0000 (75.7937)  Acc@5: 95.8333 (95.2381)  Acc@task: 79.1667 (77.1825)  time: 0.1636  data: 0.0008  max mem: 1380\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.9486 (0.9416)  Acc@1: 75.0000 (76.2097)  Acc@5: 95.8333 (95.4301)  Acc@task: 79.1667 (77.6882)  time: 0.1628  data: 0.0004  max mem: 1380\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.9604 (1.0072)  Acc@1: 75.0000 (76.4228)  Acc@5: 91.6667 (94.1057)  Acc@task: 79.1667 (77.8455)  time: 0.1616  data: 0.0003  max mem: 1380\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.9604 (0.9916)  Acc@1: 75.0000 (76.7000)  Acc@5: 91.6667 (94.1000)  Acc@task: 79.1667 (78.0000)  time: 0.1591  data: 0.0003  max mem: 1380\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1784 s / it)\n",
            "* Acc@task 78.000 Acc@1 76.700 Acc@5 94.100 loss 0.992\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:22  Loss: 0.4869 (0.4869)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.5249  data: 0.3458  max mem: 1380\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 0.7821 (0.7420)  Acc@1: 75.0000 (77.2727)  Acc@5: 95.8333 (96.9697)  Acc@task: 79.1667 (82.1970)  time: 0.1960  data: 0.0317  max mem: 1380\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:03  Loss: 0.8075 (0.8096)  Acc@1: 75.0000 (77.5794)  Acc@5: 95.8333 (96.2302)  Acc@task: 79.1667 (81.3492)  time: 0.1637  data: 0.0004  max mem: 1380\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.8722 (0.8270)  Acc@1: 79.1667 (77.5538)  Acc@5: 95.8333 (95.4301)  Acc@task: 79.1667 (80.7796)  time: 0.1632  data: 0.0004  max mem: 1380\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.8922 (0.8962)  Acc@1: 75.0000 (76.5244)  Acc@5: 95.8333 (95.6301)  Acc@task: 75.0000 (79.7764)  time: 0.1625  data: 0.0002  max mem: 1380\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.9297 (0.9345)  Acc@1: 70.8333 (76.1000)  Acc@5: 95.8333 (95.4000)  Acc@task: 75.0000 (79.6000)  time: 0.1600  data: 0.0002  max mem: 1380\n",
            "Test: [Task 5] Total time: 0:00:07 (0.1735 s / it)\n",
            "* Acc@task 79.600 Acc@1 76.100 Acc@5 95.400 loss 0.934\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:32  Loss: 0.7327 (0.7327)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  Acc@task: 87.5000 (87.5000)  time: 0.7853  data: 0.5996  max mem: 1380\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:07  Loss: 1.2257 (1.0952)  Acc@1: 75.0000 (74.2424)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (81.0606)  time: 0.2189  data: 0.0550  max mem: 1380\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:04  Loss: 1.0989 (1.0848)  Acc@1: 70.8333 (72.8175)  Acc@5: 95.8333 (95.4365)  Acc@task: 79.1667 (80.3571)  time: 0.1632  data: 0.0004  max mem: 1380\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 1.1235 (1.1302)  Acc@1: 70.8333 (71.6398)  Acc@5: 95.8333 (95.0269)  Acc@task: 75.0000 (77.6882)  time: 0.1639  data: 0.0003  max mem: 1380\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.1550 (1.1458)  Acc@1: 70.8333 (72.1545)  Acc@5: 95.8333 (94.5122)  Acc@task: 70.8333 (76.5244)  time: 0.1624  data: 0.0002  max mem: 1380\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.1550 (1.1461)  Acc@1: 70.8333 (72.1000)  Acc@5: 95.8333 (94.4000)  Acc@task: 70.8333 (76.6000)  time: 0.1598  data: 0.0002  max mem: 1380\n",
            "Test: [Task 6] Total time: 0:00:07 (0.1786 s / it)\n",
            "* Acc@task 76.600 Acc@1 72.100 Acc@5 94.400 loss 1.146\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:22  Loss: 0.9649 (0.9649)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  Acc@task: 79.1667 (79.1667)  time: 0.5449  data: 0.3575  max mem: 1380\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:06  Loss: 0.7828 (0.7591)  Acc@1: 79.1667 (78.4091)  Acc@5: 95.8333 (93.9394)  Acc@task: 75.0000 (71.2121)  time: 0.1979  data: 0.0329  max mem: 1380\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:04  Loss: 0.8363 (0.9080)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (93.6508)  Acc@task: 70.8333 (71.6270)  time: 0.1637  data: 0.0004  max mem: 1380\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:02  Loss: 1.1583 (0.9900)  Acc@1: 70.8333 (73.6559)  Acc@5: 91.6667 (93.1452)  Acc@task: 70.8333 (70.9677)  time: 0.1631  data: 0.0009  max mem: 1380\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 1.1364 (0.9985)  Acc@1: 70.8333 (73.4756)  Acc@5: 95.8333 (93.2927)  Acc@task: 70.8333 (71.2398)  time: 0.1616  data: 0.0010  max mem: 1380\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 1.1299 (0.9881)  Acc@1: 70.8333 (73.7000)  Acc@5: 95.8333 (93.3000)  Acc@task: 70.8333 (71.2000)  time: 0.1591  data: 0.0010  max mem: 1380\n",
            "Test: [Task 7] Total time: 0:00:07 (0.1727 s / it)\n",
            "* Acc@task 71.200 Acc@1 73.700 Acc@5 93.300 loss 0.988\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:25  Loss: 0.9355 (0.9355)  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  Acc@task: 70.8333 (70.8333)  time: 0.6123  data: 0.4563  max mem: 1380\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:06  Loss: 0.9866 (1.1660)  Acc@1: 75.0000 (70.8333)  Acc@5: 91.6667 (91.2879)  Acc@task: 58.3333 (59.8485)  time: 0.2030  data: 0.0421  max mem: 1380\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:04  Loss: 0.9273 (1.1091)  Acc@1: 75.0000 (71.4286)  Acc@5: 91.6667 (93.2540)  Acc@task: 58.3333 (59.3254)  time: 0.1634  data: 0.0005  max mem: 1380\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:02  Loss: 0.9500 (1.0910)  Acc@1: 70.8333 (71.2366)  Acc@5: 95.8333 (93.8172)  Acc@task: 58.3333 (59.6774)  time: 0.1634  data: 0.0003  max mem: 1380\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 1.1581 (1.1347)  Acc@1: 66.6667 (70.2236)  Acc@5: 95.8333 (93.1911)  Acc@task: 58.3333 (59.1463)  time: 0.1616  data: 0.0003  max mem: 1380\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 1.2357 (1.1401)  Acc@1: 66.6667 (70.1000)  Acc@5: 95.8333 (93.3000)  Acc@task: 58.3333 (59.5000)  time: 0.1591  data: 0.0002  max mem: 1380\n",
            "Test: [Task 8] Total time: 0:00:07 (0.1740 s / it)\n",
            "* Acc@task 59.500 Acc@1 70.100 Acc@5 93.300 loss 1.140\n",
            "[Average accuracy till task8]\tAcc@task: 75.9125\tAcc@1: 75.6625\tAcc@5: 94.8000\tLoss: 0.9633\tForgetting: 2.6857\tBackward: 5.8857\n",
            "Loading checkpoint from: ./output/cifar100_full_dino_1epoch_25pct/checkpoint/task9_checkpoint.pth\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/engines/hide_promtp_wtp_and_tap_engine.py:540: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  loss = torch.nn.functional.cross_entropy(sim, torch.range(0, sim.shape[0] - 1).long().to(device))\n",
            "Train: Epoch[1/1]  [ 0/52]  eta: 0:00:33  Lr: 0.002812  Loss: 3.6073  Acc@1: 12.5000 (12.5000)  Acc@5: 41.6667 (41.6667)  time: 0.6412  data: 0.3528  max mem: 1380\n",
            "Train: Epoch[1/1]  [10/52]  eta: 0:00:12  Lr: 0.002812  Loss: 2.5306  Acc@1: 12.5000 (14.3939)  Acc@5: 58.3333 (59.8485)  time: 0.2976  data: 0.0325  max mem: 1380\n",
            "Train: Epoch[1/1]  [20/52]  eta: 0:00:09  Lr: 0.002812  Loss: 2.3649  Acc@1: 20.8333 (21.4286)  Acc@5: 70.8333 (67.8571)  time: 0.2633  data: 0.0010  max mem: 1380\n",
            "Train: Epoch[1/1]  [30/52]  eta: 0:00:06  Lr: 0.002812  Loss: 1.8143  Acc@1: 37.5000 (28.4946)  Acc@5: 79.1667 (73.9247)  time: 0.2629  data: 0.0009  max mem: 1380\n",
            "Train: Epoch[1/1]  [40/52]  eta: 0:00:03  Lr: 0.002812  Loss: 1.6581  Acc@1: 45.8333 (34.4512)  Acc@5: 87.5000 (77.3374)  time: 0.2630  data: 0.0003  max mem: 1380\n",
            "Train: Epoch[1/1]  [50/52]  eta: 0:00:00  Lr: 0.002812  Loss: 1.3256  Acc@1: 54.1667 (38.1536)  Acc@5: 91.6667 (80.2288)  time: 0.2634  data: 0.0003  max mem: 1380\n",
            "Train: Epoch[1/1]  [51/52]  eta: 0:00:00  Lr: 0.002812  Loss: 1.3597  Acc@1: 54.1667 (38.2114)  Acc@5: 91.6667 (80.3252)  time: 0.2542  data: 0.0003  max mem: 1380\n",
            "Train: Epoch[1/1] Total time: 0:00:13 (0.2687 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.3597  Acc@1: 54.1667 (38.2114)  Acc@5: 91.6667 (80.3252)\n",
            "torch.Size([5, 2, 5, 6, 64])\n",
            "torch.Size([5, 2, 1, 5, 6, 64])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:22  Loss: 0.6638 (0.6638)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 79.1667 (79.1667)  time: 0.5357  data: 0.3721  max mem: 1380\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.7107 (0.8186)  Acc@1: 79.1667 (77.6515)  Acc@5: 100.0000 (97.7273)  Acc@task: 79.1667 (78.0303)  time: 0.1954  data: 0.0341  max mem: 1380\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:03  Loss: 0.7474 (0.8813)  Acc@1: 79.1667 (77.9762)  Acc@5: 95.8333 (96.2302)  Acc@task: 79.1667 (78.5714)  time: 0.1625  data: 0.0003  max mem: 1380\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.7003 (0.8353)  Acc@1: 75.0000 (78.4946)  Acc@5: 95.8333 (96.3710)  Acc@task: 79.1667 (79.5699)  time: 0.1628  data: 0.0003  max mem: 1380\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.7003 (0.8012)  Acc@1: 79.1667 (78.7602)  Acc@5: 95.8333 (96.5447)  Acc@task: 83.3333 (80.6911)  time: 0.1619  data: 0.0003  max mem: 1380\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6345 (0.7923)  Acc@1: 79.1667 (79.0000)  Acc@5: 95.8333 (96.6000)  Acc@task: 83.3333 (80.9000)  time: 0.1593  data: 0.0003  max mem: 1380\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1740 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@task 80.900 Acc@1 79.000 Acc@5 96.600 loss 0.792\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:35  Loss: 1.4215 (1.4215)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  Acc@task: 66.6667 (66.6667)  time: 0.8384  data: 0.6631  max mem: 1380\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:07  Loss: 1.0398 (1.0561)  Acc@1: 79.1667 (76.5152)  Acc@5: 95.8333 (95.0758)  Acc@task: 79.1667 (76.8939)  time: 0.2254  data: 0.0626  max mem: 1380\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 1.0296 (1.1359)  Acc@1: 75.0000 (74.6032)  Acc@5: 95.8333 (94.4444)  Acc@task: 75.0000 (75.1984)  time: 0.1635  data: 0.0015  max mem: 1380\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 1.0183 (1.0819)  Acc@1: 75.0000 (75.2688)  Acc@5: 95.8333 (95.0269)  Acc@task: 75.0000 (75.8065)  time: 0.1630  data: 0.0004  max mem: 1380\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.9101 (1.0386)  Acc@1: 75.0000 (75.3049)  Acc@5: 95.8333 (95.1220)  Acc@task: 75.0000 (76.8293)  time: 0.1622  data: 0.0003  max mem: 1380\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.9101 (1.0359)  Acc@1: 75.0000 (75.3000)  Acc@5: 95.8333 (95.0000)  Acc@task: 75.0000 (76.8000)  time: 0.1596  data: 0.0002  max mem: 1380\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1797 s / it)\n",
            "* Acc@task 76.800 Acc@1 75.300 Acc@5 95.000 loss 1.036\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:18  Loss: 0.6770 (0.6770)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  Acc@task: 87.5000 (87.5000)  time: 0.4325  data: 0.2721  max mem: 1380\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:05  Loss: 0.7397 (0.8247)  Acc@1: 79.1667 (80.3030)  Acc@5: 95.8333 (96.2121)  Acc@task: 83.3333 (80.6818)  time: 0.1870  data: 0.0250  max mem: 1380\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:03  Loss: 0.7180 (0.7722)  Acc@1: 79.1667 (80.1587)  Acc@5: 95.8333 (95.6349)  Acc@task: 79.1667 (80.9524)  time: 0.1637  data: 0.0004  max mem: 1380\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.5808 (0.7123)  Acc@1: 79.1667 (81.3172)  Acc@5: 95.8333 (96.2366)  Acc@task: 79.1667 (80.5108)  time: 0.1641  data: 0.0003  max mem: 1380\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.5528 (0.7205)  Acc@1: 83.3333 (81.5041)  Acc@5: 95.8333 (96.2398)  Acc@task: 83.3333 (81.5041)  time: 0.1631  data: 0.0003  max mem: 1380\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.5259 (0.7121)  Acc@1: 83.3333 (81.6000)  Acc@5: 95.8333 (96.3000)  Acc@task: 83.3333 (81.4000)  time: 0.1605  data: 0.0003  max mem: 1380\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1718 s / it)\n",
            "* Acc@task 81.400 Acc@1 81.600 Acc@5 96.300 loss 0.712\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:29  Loss: 1.0583 (1.0583)  Acc@1: 62.5000 (62.5000)  Acc@5: 91.6667 (91.6667)  Acc@task: 62.5000 (62.5000)  time: 0.6949  data: 0.4926  max mem: 1380\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:06  Loss: 0.7852 (0.8462)  Acc@1: 83.3333 (78.7879)  Acc@5: 95.8333 (94.6970)  Acc@task: 75.0000 (74.2424)  time: 0.2132  data: 0.0454  max mem: 1380\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.9448 (0.9695)  Acc@1: 75.0000 (75.7937)  Acc@5: 95.8333 (95.4365)  Acc@task: 75.0000 (75.5952)  time: 0.1650  data: 0.0005  max mem: 1380\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.9706 (0.9396)  Acc@1: 75.0000 (76.4785)  Acc@5: 95.8333 (95.4301)  Acc@task: 79.1667 (76.6129)  time: 0.1650  data: 0.0004  max mem: 1380\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.9706 (1.0015)  Acc@1: 75.0000 (76.6260)  Acc@5: 91.6667 (94.1057)  Acc@task: 79.1667 (77.2358)  time: 0.1648  data: 0.0003  max mem: 1380\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.9706 (0.9861)  Acc@1: 75.0000 (76.9000)  Acc@5: 91.6667 (94.1000)  Acc@task: 79.1667 (77.4000)  time: 0.1622  data: 0.0003  max mem: 1380\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1786 s / it)\n",
            "* Acc@task 77.400 Acc@1 76.900 Acc@5 94.100 loss 0.986\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:23  Loss: 0.5200 (0.5200)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  Acc@task: 79.1667 (79.1667)  time: 0.5558  data: 0.3937  max mem: 1380\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 0.7822 (0.7351)  Acc@1: 79.1667 (77.6515)  Acc@5: 95.8333 (96.9697)  Acc@task: 83.3333 (81.4394)  time: 0.1998  data: 0.0368  max mem: 1380\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.8112 (0.8062)  Acc@1: 75.0000 (77.7778)  Acc@5: 95.8333 (96.2302)  Acc@task: 83.3333 (80.1587)  time: 0.1650  data: 0.0008  max mem: 1380\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.8684 (0.8194)  Acc@1: 79.1667 (77.6882)  Acc@5: 95.8333 (95.4301)  Acc@task: 79.1667 (79.5699)  time: 0.1652  data: 0.0007  max mem: 1380\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.8684 (0.8870)  Acc@1: 75.0000 (76.5244)  Acc@5: 95.8333 (95.6301)  Acc@task: 79.1667 (78.7602)  time: 0.1641  data: 0.0007  max mem: 1380\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.8722 (0.9255)  Acc@1: 70.8333 (76.1000)  Acc@5: 95.8333 (95.4000)  Acc@task: 75.0000 (78.6000)  time: 0.1616  data: 0.0007  max mem: 1380\n",
            "Test: [Task 5] Total time: 0:00:07 (0.1753 s / it)\n",
            "* Acc@task 78.600 Acc@1 76.100 Acc@5 95.400 loss 0.925\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:38  Loss: 0.7324 (0.7324)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  Acc@task: 91.6667 (91.6667)  time: 0.9101  data: 0.7359  max mem: 1380\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:07  Loss: 1.1242 (1.0761)  Acc@1: 75.0000 (74.6212)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (82.5758)  time: 0.2317  data: 0.0672  max mem: 1380\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:04  Loss: 1.0635 (1.0727)  Acc@1: 75.0000 (73.2143)  Acc@5: 95.8333 (95.4365)  Acc@task: 79.1667 (80.5556)  time: 0.1644  data: 0.0003  max mem: 1380\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 1.1232 (1.1188)  Acc@1: 70.8333 (71.7742)  Acc@5: 95.8333 (95.0269)  Acc@task: 79.1667 (78.0914)  time: 0.1640  data: 0.0003  max mem: 1380\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.1554 (1.1311)  Acc@1: 70.8333 (72.3577)  Acc@5: 95.8333 (94.7154)  Acc@task: 70.8333 (76.6260)  time: 0.1630  data: 0.0002  max mem: 1380\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.1554 (1.1318)  Acc@1: 70.8333 (72.3000)  Acc@5: 95.8333 (94.6000)  Acc@task: 70.8333 (76.7000)  time: 0.1604  data: 0.0002  max mem: 1380\n",
            "Test: [Task 6] Total time: 0:00:07 (0.1823 s / it)\n",
            "* Acc@task 76.700 Acc@1 72.300 Acc@5 94.600 loss 1.132\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:26  Loss: 0.9648 (0.9648)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  Acc@task: 79.1667 (79.1667)  time: 0.6197  data: 0.4702  max mem: 1380\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:06  Loss: 0.7842 (0.7332)  Acc@1: 79.1667 (79.9242)  Acc@5: 95.8333 (93.9394)  Acc@task: 70.8333 (72.3485)  time: 0.2045  data: 0.0431  max mem: 1380\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:04  Loss: 0.8046 (0.8868)  Acc@1: 79.1667 (75.9921)  Acc@5: 91.6667 (93.6508)  Acc@task: 70.8333 (72.8175)  time: 0.1638  data: 0.0004  max mem: 1380\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:02  Loss: 1.1579 (0.9727)  Acc@1: 70.8333 (74.7312)  Acc@5: 91.6667 (93.1452)  Acc@task: 70.8333 (72.4462)  time: 0.1638  data: 0.0005  max mem: 1380\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 1.1366 (0.9800)  Acc@1: 70.8333 (74.4919)  Acc@5: 95.8333 (93.3943)  Acc@task: 70.8333 (72.2561)  time: 0.1622  data: 0.0005  max mem: 1380\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 1.1080 (0.9706)  Acc@1: 70.8333 (74.7000)  Acc@5: 95.8333 (93.4000)  Acc@task: 70.8333 (72.3000)  time: 0.1596  data: 0.0005  max mem: 1380\n",
            "Test: [Task 7] Total time: 0:00:07 (0.1746 s / it)\n",
            "* Acc@task 72.300 Acc@1 74.700 Acc@5 93.400 loss 0.971\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:20  Loss: 0.9308 (0.9308)  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  Acc@task: 79.1667 (79.1667)  time: 0.4874  data: 0.3148  max mem: 1380\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:06  Loss: 0.9838 (1.1496)  Acc@1: 75.0000 (71.2121)  Acc@5: 91.6667 (91.2879)  Acc@task: 70.8333 (68.1818)  time: 0.1927  data: 0.0290  max mem: 1380\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:03  Loss: 0.9371 (1.0892)  Acc@1: 75.0000 (71.6270)  Acc@5: 95.8333 (93.2540)  Acc@task: 70.8333 (68.2540)  time: 0.1638  data: 0.0004  max mem: 1380\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:02  Loss: 0.8494 (1.0440)  Acc@1: 75.0000 (71.9086)  Acc@5: 95.8333 (93.8172)  Acc@task: 70.8333 (69.3548)  time: 0.1633  data: 0.0003  max mem: 1380\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 1.0641 (1.0732)  Acc@1: 70.8333 (70.9350)  Acc@5: 95.8333 (93.4959)  Acc@task: 66.6667 (68.3943)  time: 0.1622  data: 0.0003  max mem: 1380\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 1.0641 (1.0699)  Acc@1: 68.7500 (70.9000)  Acc@5: 95.8333 (93.6000)  Acc@task: 66.6667 (68.7000)  time: 0.1597  data: 0.0003  max mem: 1380\n",
            "Test: [Task 8] Total time: 0:00:07 (0.1717 s / it)\n",
            "* Acc@task 68.700 Acc@1 70.900 Acc@5 93.600 loss 1.070\n",
            "Test: [Task 9]  [ 0/42]  eta: 0:00:24  Loss: 8.6468 (8.6468)  Acc@1: 0.0000 (0.0000)  Acc@5: 12.5000 (12.5000)  Acc@task: 70.8333 (70.8333)  time: 0.5936  data: 0.4360  max mem: 1380\n",
            "Test: [Task 9]  [10/42]  eta: 0:00:06  Loss: 8.5091 (8.3476)  Acc@1: 0.0000 (0.0000)  Acc@5: 8.3333 (9.0909)  Acc@task: 70.8333 (71.2121)  time: 0.2022  data: 0.0401  max mem: 1380\n",
            "Test: [Task 9]  [20/42]  eta: 0:00:04  Loss: 8.0636 (8.2155)  Acc@1: 0.0000 (0.0000)  Acc@5: 8.3333 (8.5317)  Acc@task: 66.6667 (68.2540)  time: 0.1642  data: 0.0007  max mem: 1380\n",
            "Test: [Task 9]  [30/42]  eta: 0:00:02  Loss: 8.0636 (8.2811)  Acc@1: 0.0000 (0.0000)  Acc@5: 8.3333 (8.7366)  Acc@task: 66.6667 (67.4731)  time: 0.1635  data: 0.0012  max mem: 1380\n",
            "Test: [Task 9]  [40/42]  eta: 0:00:00  Loss: 8.2641 (8.2925)  Acc@1: 0.0000 (0.0000)  Acc@5: 8.3333 (9.0447)  Acc@task: 66.6667 (67.8862)  time: 0.1620  data: 0.0009  max mem: 1380\n",
            "Test: [Task 9]  [41/42]  eta: 0:00:00  Loss: 8.2641 (8.2908)  Acc@1: 0.0000 (0.0000)  Acc@5: 8.3333 (9.1000)  Acc@task: 66.6667 (68.3000)  time: 0.1595  data: 0.0008  max mem: 1380\n",
            "Test: [Task 9] Total time: 0:00:07 (0.1742 s / it)\n",
            "* Acc@task 68.300 Acc@1 0.000 Acc@5 9.100 loss 8.291\n",
            "[Average accuracy till task9]\tAcc@task: 75.6778\tAcc@1: 67.4222\tAcc@5: 85.3444\tLoss: 1.7683\tForgetting: 2.8125\tBackward: 73.7750\n",
            "torch.Size([106320, 384])\n",
            "Averaged stats: Lr: 0.005000  Loss: 0.1640  Acc@1: 97.5000 (94.2813)  Acc@5: 100.0000 (99.1250)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:23  Loss: 0.9150 (0.9150)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 79.1667 (79.1667)  time: 0.5688  data: 0.4018  max mem: 1380\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.7887 (0.7867)  Acc@1: 79.1667 (79.9242)  Acc@5: 100.0000 (97.7273)  Acc@task: 79.1667 (78.0303)  time: 0.1988  data: 0.0368  max mem: 1380\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.7524 (0.8432)  Acc@1: 79.1667 (79.7619)  Acc@5: 95.8333 (95.8333)  Acc@task: 79.1667 (78.5714)  time: 0.1631  data: 0.0003  max mem: 1380\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.6732 (0.7791)  Acc@1: 83.3333 (80.6452)  Acc@5: 95.8333 (96.1022)  Acc@task: 79.1667 (79.5699)  time: 0.1630  data: 0.0003  max mem: 1380\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.5894 (0.7433)  Acc@1: 83.3333 (81.0976)  Acc@5: 100.0000 (96.4431)  Acc@task: 83.3333 (80.6911)  time: 0.1616  data: 0.0003  max mem: 1380\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.5412 (0.7356)  Acc@1: 83.3333 (81.3000)  Acc@5: 100.0000 (96.5000)  Acc@task: 83.3333 (80.9000)  time: 0.1590  data: 0.0003  max mem: 1380\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1740 s / it)\n",
            "* Acc@task 80.900 Acc@1 81.300 Acc@5 96.500 loss 0.736\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:38  Loss: 1.5279 (1.5279)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  Acc@task: 66.6667 (66.6667)  time: 0.9262  data: 0.7242  max mem: 1380\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:07  Loss: 1.1695 (1.1358)  Acc@1: 75.0000 (72.3485)  Acc@5: 95.8333 (95.4545)  Acc@task: 79.1667 (76.8939)  time: 0.2319  data: 0.0671  max mem: 1380\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 1.1459 (1.2949)  Acc@1: 70.8333 (69.4444)  Acc@5: 95.8333 (94.2460)  Acc@task: 75.0000 (75.1984)  time: 0.1631  data: 0.0009  max mem: 1380\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 1.1026 (1.2379)  Acc@1: 70.8333 (70.4301)  Acc@5: 95.8333 (94.7581)  Acc@task: 75.0000 (75.8065)  time: 0.1637  data: 0.0004  max mem: 1380\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.9516 (1.1682)  Acc@1: 70.8333 (71.3415)  Acc@5: 95.8333 (94.7154)  Acc@task: 75.0000 (76.8293)  time: 0.1623  data: 0.0002  max mem: 1380\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.9516 (1.1711)  Acc@1: 70.8333 (71.3000)  Acc@5: 95.8333 (94.6000)  Acc@task: 75.0000 (76.8000)  time: 0.1598  data: 0.0002  max mem: 1380\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1817 s / it)\n",
            "* Acc@task 76.800 Acc@1 71.300 Acc@5 94.600 loss 1.171\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:19  Loss: 0.8102 (0.8102)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 87.5000 (87.5000)  time: 0.4596  data: 0.3039  max mem: 1380\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 0.9282 (0.9759)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (93.9394)  Acc@task: 83.3333 (80.6818)  time: 0.1909  data: 0.0281  max mem: 1380\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:03  Loss: 0.9282 (0.9607)  Acc@1: 75.0000 (76.3889)  Acc@5: 95.8333 (94.4444)  Acc@task: 79.1667 (80.9524)  time: 0.1639  data: 0.0004  max mem: 1380\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.8223 (0.9024)  Acc@1: 79.1667 (77.0161)  Acc@5: 95.8333 (95.1613)  Acc@task: 79.1667 (80.5108)  time: 0.1630  data: 0.0003  max mem: 1380\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.7414 (0.8919)  Acc@1: 79.1667 (77.7439)  Acc@5: 95.8333 (95.2236)  Acc@task: 83.3333 (81.5041)  time: 0.1620  data: 0.0004  max mem: 1380\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.7187 (0.8817)  Acc@1: 79.1667 (77.8000)  Acc@5: 95.8333 (95.3000)  Acc@task: 83.3333 (81.4000)  time: 0.1594  data: 0.0004  max mem: 1380\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1722 s / it)\n",
            "* Acc@task 81.400 Acc@1 77.800 Acc@5 95.300 loss 0.882\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:33  Loss: 1.0291 (1.0291)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  Acc@task: 62.5000 (62.5000)  time: 0.8003  data: 0.6040  max mem: 1380\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:07  Loss: 0.7143 (0.8299)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.0758)  Acc@task: 75.0000 (74.2424)  time: 0.2221  data: 0.0555  max mem: 1380\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.8650 (0.8958)  Acc@1: 79.1667 (78.3730)  Acc@5: 91.6667 (94.2460)  Acc@task: 75.0000 (75.5952)  time: 0.1634  data: 0.0005  max mem: 1380\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.9047 (0.8613)  Acc@1: 75.0000 (78.7634)  Acc@5: 91.6667 (94.2204)  Acc@task: 79.1667 (76.6129)  time: 0.1629  data: 0.0003  max mem: 1380\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.9137 (0.9391)  Acc@1: 79.1667 (77.9472)  Acc@5: 91.6667 (93.4959)  Acc@task: 79.1667 (77.2358)  time: 0.1623  data: 0.0003  max mem: 1380\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.8822 (0.9218)  Acc@1: 79.1667 (78.2000)  Acc@5: 95.8333 (93.6000)  Acc@task: 79.1667 (77.4000)  time: 0.1597  data: 0.0003  max mem: 1380\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1794 s / it)\n",
            "* Acc@task 77.400 Acc@1 78.200 Acc@5 93.600 loss 0.922\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:24  Loss: 0.4071 (0.4071)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  Acc@task: 79.1667 (79.1667)  time: 0.5902  data: 0.4247  max mem: 1380\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 0.7083 (0.7132)  Acc@1: 79.1667 (78.7879)  Acc@5: 95.8333 (97.3485)  Acc@task: 83.3333 (81.4394)  time: 0.2016  data: 0.0389  max mem: 1380\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.8358 (0.7945)  Acc@1: 79.1667 (78.3730)  Acc@5: 95.8333 (96.2302)  Acc@task: 83.3333 (80.1587)  time: 0.1635  data: 0.0003  max mem: 1380\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.8792 (0.8083)  Acc@1: 75.0000 (78.2258)  Acc@5: 95.8333 (95.5645)  Acc@task: 79.1667 (79.5699)  time: 0.1641  data: 0.0005  max mem: 1380\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.8485 (0.8831)  Acc@1: 75.0000 (76.9309)  Acc@5: 95.8333 (95.4268)  Acc@task: 79.1667 (78.7602)  time: 0.1634  data: 0.0006  max mem: 1380\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.8829 (0.9254)  Acc@1: 75.0000 (76.5000)  Acc@5: 91.6667 (95.1000)  Acc@task: 75.0000 (78.6000)  time: 0.1610  data: 0.0006  max mem: 1380\n",
            "Test: [Task 5] Total time: 0:00:07 (0.1755 s / it)\n",
            "* Acc@task 78.600 Acc@1 76.500 Acc@5 95.100 loss 0.925\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:24  Loss: 0.7210 (0.7210)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 91.6667 (91.6667)  time: 0.5755  data: 0.3980  max mem: 1380\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:06  Loss: 1.2107 (1.1491)  Acc@1: 66.6667 (70.4545)  Acc@5: 95.8333 (93.9394)  Acc@task: 83.3333 (82.5758)  time: 0.2046  data: 0.0402  max mem: 1380\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:04  Loss: 1.1715 (1.1634)  Acc@1: 66.6667 (69.6429)  Acc@5: 95.8333 (94.0476)  Acc@task: 79.1667 (80.5556)  time: 0.1660  data: 0.0024  max mem: 1380\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 1.1715 (1.2009)  Acc@1: 66.6667 (69.3548)  Acc@5: 95.8333 (93.8172)  Acc@task: 79.1667 (78.0914)  time: 0.1641  data: 0.0003  max mem: 1380\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.2482 (1.2218)  Acc@1: 70.8333 (70.1220)  Acc@5: 91.6667 (93.4959)  Acc@task: 70.8333 (76.6260)  time: 0.1630  data: 0.0002  max mem: 1380\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.1462 (1.2191)  Acc@1: 70.8333 (70.1000)  Acc@5: 91.6667 (93.5000)  Acc@task: 70.8333 (76.7000)  time: 0.1606  data: 0.0002  max mem: 1380\n",
            "Test: [Task 6] Total time: 0:00:07 (0.1752 s / it)\n",
            "* Acc@task 76.700 Acc@1 70.100 Acc@5 93.500 loss 1.219\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:23  Loss: 1.1062 (1.1062)  Acc@1: 66.6667 (66.6667)  Acc@5: 95.8333 (95.8333)  Acc@task: 79.1667 (79.1667)  time: 0.5576  data: 0.3990  max mem: 1380\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:06  Loss: 1.1062 (0.9602)  Acc@1: 75.0000 (71.5909)  Acc@5: 95.8333 (94.3182)  Acc@task: 70.8333 (72.3485)  time: 0.1984  data: 0.0369  max mem: 1380\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:04  Loss: 1.1049 (1.0867)  Acc@1: 75.0000 (70.4365)  Acc@5: 95.8333 (93.8492)  Acc@task: 70.8333 (72.8175)  time: 0.1636  data: 0.0006  max mem: 1380\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:02  Loss: 1.3375 (1.1571)  Acc@1: 70.8333 (70.1613)  Acc@5: 91.6667 (92.4731)  Acc@task: 70.8333 (72.4462)  time: 0.1643  data: 0.0006  max mem: 1380\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 1.2061 (1.1464)  Acc@1: 70.8333 (70.1220)  Acc@5: 91.6667 (92.5813)  Acc@task: 70.8333 (72.2561)  time: 0.1627  data: 0.0006  max mem: 1380\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 1.0185 (1.1314)  Acc@1: 70.8333 (70.4000)  Acc@5: 91.6667 (92.6000)  Acc@task: 70.8333 (72.3000)  time: 0.1601  data: 0.0006  max mem: 1380\n",
            "Test: [Task 7] Total time: 0:00:07 (0.1734 s / it)\n",
            "* Acc@task 72.300 Acc@1 70.400 Acc@5 92.600 loss 1.131\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:18  Loss: 1.0259 (1.0259)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  Acc@task: 79.1667 (79.1667)  time: 0.4336  data: 0.2712  max mem: 1380\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:06  Loss: 1.0485 (1.1905)  Acc@1: 75.0000 (70.8333)  Acc@5: 91.6667 (91.6667)  Acc@task: 70.8333 (68.1818)  time: 0.1883  data: 0.0262  max mem: 1380\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:03  Loss: 0.9462 (1.1551)  Acc@1: 70.8333 (70.0397)  Acc@5: 95.8333 (93.0556)  Acc@task: 70.8333 (68.2540)  time: 0.1638  data: 0.0010  max mem: 1380\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:02  Loss: 0.8907 (1.0915)  Acc@1: 70.8333 (71.3710)  Acc@5: 95.8333 (93.6828)  Acc@task: 70.8333 (69.3548)  time: 0.1634  data: 0.0003  max mem: 1380\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 0.9989 (1.1221)  Acc@1: 70.8333 (71.2398)  Acc@5: 91.6667 (93.2927)  Acc@task: 66.6667 (68.3943)  time: 0.1623  data: 0.0004  max mem: 1380\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 1.0740 (1.1210)  Acc@1: 70.8333 (71.3000)  Acc@5: 91.6667 (93.4000)  Acc@task: 66.6667 (68.7000)  time: 0.1596  data: 0.0004  max mem: 1380\n",
            "Test: [Task 8] Total time: 0:00:07 (0.1703 s / it)\n",
            "* Acc@task 68.700 Acc@1 71.300 Acc@5 93.400 loss 1.121\n",
            "Test: [Task 9]  [ 0/42]  eta: 0:00:17  Loss: 0.8470 (0.8470)  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  Acc@task: 70.8333 (70.8333)  time: 0.4285  data: 0.2673  max mem: 1380\n",
            "Test: [Task 9]  [10/42]  eta: 0:00:05  Loss: 0.9455 (0.9547)  Acc@1: 75.0000 (73.4849)  Acc@5: 95.8333 (96.2121)  Acc@task: 70.8333 (71.2121)  time: 0.1871  data: 0.0250  max mem: 1380\n",
            "Test: [Task 9]  [20/42]  eta: 0:00:03  Loss: 0.8987 (0.8832)  Acc@1: 75.0000 (75.5952)  Acc@5: 95.8333 (95.8333)  Acc@task: 66.6667 (68.2540)  time: 0.1646  data: 0.0010  max mem: 1380\n",
            "Test: [Task 9]  [30/42]  eta: 0:00:02  Loss: 1.0348 (1.0160)  Acc@1: 70.8333 (72.7151)  Acc@5: 91.6667 (94.0860)  Acc@task: 66.6667 (67.4731)  time: 0.1641  data: 0.0015  max mem: 1380\n",
            "Test: [Task 9]  [40/42]  eta: 0:00:00  Loss: 0.8694 (0.9614)  Acc@1: 70.8333 (73.7805)  Acc@5: 91.6667 (94.7154)  Acc@task: 66.6667 (67.8862)  time: 0.1627  data: 0.0012  max mem: 1380\n",
            "Test: [Task 9]  [41/42]  eta: 0:00:00  Loss: 0.8694 (0.9468)  Acc@1: 70.8333 (74.1000)  Acc@5: 91.6667 (94.8000)  Acc@task: 66.6667 (68.3000)  time: 0.1602  data: 0.0011  max mem: 1380\n",
            "Test: [Task 9] Total time: 0:00:07 (0.1708 s / it)\n",
            "* Acc@task 68.300 Acc@1 74.100 Acc@5 94.800 loss 0.947\n",
            "[Average accuracy till task9]\tAcc@task: 75.6778\tAcc@1: 74.5556\tAcc@5: 94.3778\tLoss: 1.0060\tForgetting: 3.5500\tBackward: 4.1000\n",
            "Loading checkpoint from: ./output/cifar100_full_dino_1epoch_25pct/checkpoint/task10_checkpoint.pth\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/engines/hide_promtp_wtp_and_tap_engine.py:540: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  loss = torch.nn.functional.cross_entropy(sim, torch.range(0, sim.shape[0] - 1).long().to(device))\n",
            "Train: Epoch[1/1]  [ 0/52]  eta: 0:00:37  Lr: 0.002812  Loss: 3.9067  Acc@1: 8.3333 (8.3333)  Acc@5: 62.5000 (62.5000)  time: 0.7116  data: 0.4408  max mem: 1380\n",
            "Train: Epoch[1/1]  [10/52]  eta: 0:00:12  Lr: 0.002812  Loss: 3.0905  Acc@1: 12.5000 (13.2576)  Acc@5: 62.5000 (60.6061)  time: 0.3047  data: 0.0404  max mem: 1380\n",
            "Train: Epoch[1/1]  [20/52]  eta: 0:00:09  Lr: 0.002812  Loss: 2.4346  Acc@1: 12.5000 (16.6667)  Acc@5: 66.6667 (63.2937)  time: 0.2638  data: 0.0003  max mem: 1380\n",
            "Train: Epoch[1/1]  [30/52]  eta: 0:00:06  Lr: 0.002812  Loss: 1.8895  Acc@1: 37.5000 (24.8656)  Acc@5: 75.0000 (69.8925)  time: 0.2632  data: 0.0004  max mem: 1380\n",
            "Train: Epoch[1/1]  [40/52]  eta: 0:00:03  Lr: 0.002812  Loss: 1.5842  Acc@1: 41.6667 (30.7927)  Acc@5: 83.3333 (74.4919)  time: 0.2655  data: 0.0005  max mem: 1380\n",
            "Train: Epoch[1/1]  [50/52]  eta: 0:00:00  Lr: 0.002812  Loss: 1.1789  Acc@1: 54.1667 (36.4379)  Acc@5: 91.6667 (77.7778)  time: 0.2660  data: 0.0006  max mem: 1380\n",
            "Train: Epoch[1/1]  [51/52]  eta: 0:00:00  Lr: 0.002812  Loss: 1.7817  Acc@1: 54.1667 (36.4744)  Acc@5: 87.5000 (77.8229)  time: 0.2577  data: 0.0006  max mem: 1380\n",
            "Train: Epoch[1/1] Total time: 0:00:14 (0.2716 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.7817  Acc@1: 54.1667 (36.4744)  Acc@5: 87.5000 (77.8229)\n",
            "torch.Size([5, 2, 5, 6, 64])\n",
            "torch.Size([5, 2, 1, 5, 6, 64])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:32  Loss: 0.9249 (0.9249)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.7692  data: 0.5869  max mem: 1380\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:07  Loss: 0.7884 (0.7884)  Acc@1: 79.1667 (79.9242)  Acc@5: 100.0000 (97.7273)  Acc@task: 79.1667 (78.4091)  time: 0.2195  data: 0.0543  max mem: 1380\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.6936 (0.8381)  Acc@1: 79.1667 (79.9603)  Acc@5: 95.8333 (95.8333)  Acc@task: 79.1667 (78.9683)  time: 0.1638  data: 0.0011  max mem: 1380\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.6146 (0.7778)  Acc@1: 83.3333 (80.7796)  Acc@5: 95.8333 (96.1022)  Acc@task: 83.3333 (79.7043)  time: 0.1633  data: 0.0015  max mem: 1380\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6018 (0.7444)  Acc@1: 83.3333 (81.0976)  Acc@5: 100.0000 (96.3415)  Acc@task: 83.3333 (80.4878)  time: 0.1630  data: 0.0011  max mem: 1380\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.5708 (0.7367)  Acc@1: 83.3333 (81.3000)  Acc@5: 100.0000 (96.4000)  Acc@task: 87.5000 (80.7000)  time: 0.1602  data: 0.0010  max mem: 1380\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1788 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@task 80.700 Acc@1 81.300 Acc@5 96.400 loss 0.737\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:17  Loss: 1.5276 (1.5276)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  Acc@task: 66.6667 (66.6667)  time: 0.4125  data: 0.2548  max mem: 1380\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 1.1497 (1.1670)  Acc@1: 70.8333 (72.3485)  Acc@5: 95.8333 (95.0758)  Acc@task: 75.0000 (75.3788)  time: 0.1876  data: 0.0240  max mem: 1380\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:03  Loss: 1.1456 (1.3368)  Acc@1: 70.8333 (68.8492)  Acc@5: 95.8333 (94.0476)  Acc@task: 75.0000 (73.0159)  time: 0.1642  data: 0.0006  max mem: 1380\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 1.1024 (1.2634)  Acc@1: 70.8333 (69.8925)  Acc@5: 95.8333 (94.6237)  Acc@task: 75.0000 (74.5968)  time: 0.1633  data: 0.0003  max mem: 1380\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.9546 (1.1924)  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (94.7154)  Acc@task: 75.0000 (75.5081)  time: 0.1630  data: 0.0002  max mem: 1380\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.9546 (1.1948)  Acc@1: 70.8333 (70.8000)  Acc@5: 95.8333 (94.6000)  Acc@task: 75.0000 (75.5000)  time: 0.1604  data: 0.0002  max mem: 1380\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1705 s / it)\n",
            "* Acc@task 75.500 Acc@1 70.800 Acc@5 94.600 loss 1.195\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:33  Loss: 0.8101 (0.8101)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 87.5000 (87.5000)  time: 0.7867  data: 0.5757  max mem: 1380\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:07  Loss: 0.9209 (0.9738)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (93.9394)  Acc@task: 83.3333 (81.8182)  time: 0.2206  data: 0.0539  max mem: 1380\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.9209 (0.9624)  Acc@1: 75.0000 (76.3889)  Acc@5: 95.8333 (94.4444)  Acc@task: 83.3333 (81.7460)  time: 0.1642  data: 0.0014  max mem: 1380\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.8225 (0.9047)  Acc@1: 79.1667 (77.0161)  Acc@5: 95.8333 (95.1613)  Acc@task: 79.1667 (80.7796)  time: 0.1640  data: 0.0006  max mem: 1380\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.7409 (0.8944)  Acc@1: 79.1667 (77.6423)  Acc@5: 95.8333 (95.2236)  Acc@task: 83.3333 (81.4024)  time: 0.1631  data: 0.0002  max mem: 1380\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.7209 (0.8840)  Acc@1: 79.1667 (77.7000)  Acc@5: 95.8333 (95.3000)  Acc@task: 83.3333 (81.3000)  time: 0.1604  data: 0.0002  max mem: 1380\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1794 s / it)\n",
            "* Acc@task 81.300 Acc@1 77.700 Acc@5 95.300 loss 0.884\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:23  Loss: 1.0315 (1.0315)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  Acc@task: 62.5000 (62.5000)  time: 0.5574  data: 0.3879  max mem: 1380\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:06  Loss: 0.8066 (0.8426)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.0758)  Acc@task: 70.8333 (70.8333)  time: 0.1989  data: 0.0355  max mem: 1380\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.8784 (0.9025)  Acc@1: 79.1667 (78.3730)  Acc@5: 91.6667 (94.2460)  Acc@task: 75.0000 (73.4127)  time: 0.1644  data: 0.0003  max mem: 1380\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.8958 (0.8629)  Acc@1: 75.0000 (78.8979)  Acc@5: 91.6667 (94.2204)  Acc@task: 75.0000 (74.1936)  time: 0.1653  data: 0.0003  max mem: 1380\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.8958 (0.9396)  Acc@1: 79.1667 (78.0488)  Acc@5: 91.6667 (93.4959)  Acc@task: 75.0000 (74.5935)  time: 0.1640  data: 0.0003  max mem: 1380\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.8803 (0.9223)  Acc@1: 79.1667 (78.3000)  Acc@5: 95.8333 (93.6000)  Acc@task: 75.0000 (74.8000)  time: 0.1615  data: 0.0003  max mem: 1380\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1761 s / it)\n",
            "* Acc@task 74.800 Acc@1 78.300 Acc@5 93.600 loss 0.922\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:31  Loss: 0.4070 (0.4070)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  Acc@task: 79.1667 (79.1667)  time: 0.7496  data: 0.5552  max mem: 1380\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 0.7067 (0.7162)  Acc@1: 79.1667 (78.4091)  Acc@5: 95.8333 (97.3485)  Acc@task: 83.3333 (80.3030)  time: 0.2172  data: 0.0516  max mem: 1380\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.8345 (0.7879)  Acc@1: 79.1667 (78.3730)  Acc@5: 95.8333 (96.2302)  Acc@task: 79.1667 (78.5714)  time: 0.1645  data: 0.0009  max mem: 1380\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.8792 (0.8060)  Acc@1: 79.1667 (78.2258)  Acc@5: 95.8333 (95.5645)  Acc@task: 79.1667 (78.6290)  time: 0.1642  data: 0.0005  max mem: 1380\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.8752 (0.8812)  Acc@1: 75.0000 (77.0325)  Acc@5: 95.8333 (95.4268)  Acc@task: 75.0000 (78.0488)  time: 0.1632  data: 0.0003  max mem: 1380\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.8780 (0.9235)  Acc@1: 75.0000 (76.6000)  Acc@5: 91.6667 (95.1000)  Acc@task: 75.0000 (77.7000)  time: 0.1605  data: 0.0002  max mem: 1380\n",
            "Test: [Task 5] Total time: 0:00:07 (0.1788 s / it)\n",
            "* Acc@task 77.700 Acc@1 76.600 Acc@5 95.100 loss 0.923\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:18  Loss: 0.7209 (0.7209)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 91.6667 (91.6667)  time: 0.4294  data: 0.2746  max mem: 1380\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:05  Loss: 1.2078 (1.1591)  Acc@1: 66.6667 (70.0758)  Acc@5: 95.8333 (93.9394)  Acc@task: 79.1667 (79.1667)  time: 0.1866  data: 0.0257  max mem: 1380\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:03  Loss: 1.1714 (1.1678)  Acc@1: 66.6667 (69.4444)  Acc@5: 95.8333 (94.0476)  Acc@task: 79.1667 (78.7698)  time: 0.1636  data: 0.0006  max mem: 1380\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 1.1714 (1.2069)  Acc@1: 66.6667 (69.2204)  Acc@5: 95.8333 (93.5484)  Acc@task: 70.8333 (75.6720)  time: 0.1641  data: 0.0004  max mem: 1380\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.2476 (1.2259)  Acc@1: 70.8333 (70.0203)  Acc@5: 91.6667 (93.2927)  Acc@task: 70.8333 (74.7968)  time: 0.1628  data: 0.0004  max mem: 1380\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.1461 (1.2231)  Acc@1: 70.8333 (70.0000)  Acc@5: 91.6667 (93.2000)  Acc@task: 70.8333 (74.9000)  time: 0.1604  data: 0.0004  max mem: 1380\n",
            "Test: [Task 6] Total time: 0:00:07 (0.1712 s / it)\n",
            "* Acc@task 74.900 Acc@1 70.000 Acc@5 93.200 loss 1.223\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:33  Loss: 1.1062 (1.1062)  Acc@1: 66.6667 (66.6667)  Acc@5: 95.8333 (95.8333)  Acc@task: 79.1667 (79.1667)  time: 0.7900  data: 0.5922  max mem: 1380\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:07  Loss: 1.1062 (0.9705)  Acc@1: 75.0000 (71.2121)  Acc@5: 95.8333 (93.9394)  Acc@task: 75.0000 (71.5909)  time: 0.2210  data: 0.0548  max mem: 1380\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:04  Loss: 1.1348 (1.0868)  Acc@1: 75.0000 (70.4365)  Acc@5: 91.6667 (93.6508)  Acc@task: 75.0000 (72.4206)  time: 0.1641  data: 0.0007  max mem: 1380\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:02  Loss: 1.3059 (1.1569)  Acc@1: 70.8333 (70.1613)  Acc@5: 91.6667 (92.3387)  Acc@task: 75.0000 (72.3118)  time: 0.1640  data: 0.0003  max mem: 1380\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 1.2060 (1.1458)  Acc@1: 70.8333 (70.1220)  Acc@5: 91.6667 (92.3781)  Acc@task: 75.0000 (72.4594)  time: 0.1635  data: 0.0003  max mem: 1380\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 1.0228 (1.1306)  Acc@1: 70.8333 (70.4000)  Acc@5: 91.6667 (92.4000)  Acc@task: 75.0000 (72.6000)  time: 0.1609  data: 0.0002  max mem: 1380\n",
            "Test: [Task 7] Total time: 0:00:07 (0.1796 s / it)\n",
            "* Acc@task 72.600 Acc@1 70.400 Acc@5 92.400 loss 1.131\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:17  Loss: 1.0252 (1.0252)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  Acc@task: 79.1667 (79.1667)  time: 0.4243  data: 0.2607  max mem: 1380\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:06  Loss: 1.0328 (1.1581)  Acc@1: 75.0000 (70.8333)  Acc@5: 91.6667 (91.6667)  Acc@task: 70.8333 (71.2121)  time: 0.1894  data: 0.0271  max mem: 1380\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:03  Loss: 0.9876 (1.1109)  Acc@1: 75.0000 (71.0317)  Acc@5: 95.8333 (93.2540)  Acc@task: 70.8333 (71.4286)  time: 0.1652  data: 0.0020  max mem: 1380\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:02  Loss: 0.8811 (1.0582)  Acc@1: 75.0000 (72.0430)  Acc@5: 95.8333 (93.9516)  Acc@task: 70.8333 (71.5054)  time: 0.1636  data: 0.0004  max mem: 1380\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 0.9956 (1.0975)  Acc@1: 70.8333 (71.6463)  Acc@5: 95.8333 (93.4959)  Acc@task: 66.6667 (70.2236)  time: 0.1628  data: 0.0007  max mem: 1380\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 1.0565 (1.0970)  Acc@1: 70.8333 (71.7000)  Acc@5: 95.8333 (93.6000)  Acc@task: 66.6667 (70.3000)  time: 0.1602  data: 0.0006  max mem: 1380\n",
            "Test: [Task 8] Total time: 0:00:07 (0.1718 s / it)\n",
            "* Acc@task 70.300 Acc@1 71.700 Acc@5 93.600 loss 1.097\n",
            "Test: [Task 9]  [ 0/42]  eta: 0:00:31  Loss: 0.8443 (0.8443)  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.7511  data: 0.5779  max mem: 1380\n",
            "Test: [Task 9]  [10/42]  eta: 0:00:06  Loss: 0.9256 (0.9424)  Acc@1: 75.0000 (73.4849)  Acc@5: 95.8333 (96.2121)  Acc@task: 70.8333 (76.1364)  time: 0.2165  data: 0.0530  max mem: 1380\n",
            "Test: [Task 9]  [20/42]  eta: 0:00:04  Loss: 0.8135 (0.8482)  Acc@1: 75.0000 (76.3889)  Acc@5: 95.8333 (96.0317)  Acc@task: 75.0000 (76.3889)  time: 0.1643  data: 0.0004  max mem: 1380\n",
            "Test: [Task 9]  [30/42]  eta: 0:00:02  Loss: 0.9058 (0.9582)  Acc@1: 75.0000 (74.1936)  Acc@5: 95.8333 (94.4892)  Acc@task: 75.0000 (75.9409)  time: 0.1646  data: 0.0003  max mem: 1380\n",
            "Test: [Task 9]  [40/42]  eta: 0:00:00  Loss: 0.8749 (0.9017)  Acc@1: 75.0000 (75.4065)  Acc@5: 95.8333 (95.2236)  Acc@task: 79.1667 (76.7276)  time: 0.1631  data: 0.0002  max mem: 1380\n",
            "Test: [Task 9]  [41/42]  eta: 0:00:00  Loss: 0.8749 (0.8885)  Acc@1: 75.0000 (75.7000)  Acc@5: 95.8333 (95.3000)  Acc@task: 79.1667 (77.0000)  time: 0.1605  data: 0.0002  max mem: 1380\n",
            "Test: [Task 9] Total time: 0:00:07 (0.1784 s / it)\n",
            "* Acc@task 77.000 Acc@1 75.700 Acc@5 95.300 loss 0.888\n",
            "Test: [Task 10]  [ 0/42]  eta: 0:00:18  Loss: 9.5899 (9.5899)  Acc@1: 0.0000 (0.0000)  Acc@5: 0.0000 (0.0000)  Acc@task: 62.5000 (62.5000)  time: 0.4516  data: 0.2900  max mem: 1380\n",
            "Test: [Task 10]  [10/42]  eta: 0:00:06  Loss: 9.4368 (9.3432)  Acc@1: 0.0000 (0.0000)  Acc@5: 4.1667 (3.7879)  Acc@task: 58.3333 (56.8182)  time: 0.1886  data: 0.0268  max mem: 1380\n",
            "Test: [Task 10]  [20/42]  eta: 0:00:03  Loss: 9.4245 (9.3443)  Acc@1: 0.0000 (0.0000)  Acc@5: 4.1667 (3.7698)  Acc@task: 58.3333 (58.7302)  time: 0.1631  data: 0.0007  max mem: 1380\n",
            "Test: [Task 10]  [30/42]  eta: 0:00:02  Loss: 9.4845 (9.4495)  Acc@1: 0.0000 (0.0000)  Acc@5: 4.1667 (3.6290)  Acc@task: 58.3333 (58.6022)  time: 0.1632  data: 0.0009  max mem: 1380\n",
            "Test: [Task 10]  [40/42]  eta: 0:00:00  Loss: 9.6460 (9.4523)  Acc@1: 0.0000 (0.0000)  Acc@5: 4.1667 (3.8618)  Acc@task: 54.1667 (57.4187)  time: 0.1619  data: 0.0008  max mem: 1380\n",
            "Test: [Task 10]  [41/42]  eta: 0:00:00  Loss: 9.6460 (9.4641)  Acc@1: 0.0000 (0.0000)  Acc@5: 4.1667 (3.8000)  Acc@task: 54.1667 (57.7000)  time: 0.1592  data: 0.0008  max mem: 1380\n",
            "Test: [Task 10] Total time: 0:00:07 (0.1703 s / it)\n",
            "* Acc@task 57.700 Acc@1 0.000 Acc@5 3.800 loss 9.464\n",
            "[Average accuracy till task10]\tAcc@task: 74.2500\tAcc@1: 67.2500\tAcc@5: 85.3300\tLoss: 1.8465\tForgetting: 3.7000\tBackward: 72.8778\n",
            "torch.Size([118320, 384])\n",
            "Averaged stats: Lr: 0.005000  Loss: 0.1195  Acc@1: 96.6667 (93.4259)  Acc@5: 100.0000 (99.0556)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:20  Loss: 0.9534 (0.9534)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (83.3333)  time: 0.4982  data: 0.3436  max mem: 1380\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.9401 (0.9642)  Acc@1: 75.0000 (75.7576)  Acc@5: 95.8333 (96.9697)  Acc@task: 79.1667 (78.4091)  time: 0.1928  data: 0.0319  max mem: 1380\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:03  Loss: 0.8749 (1.0307)  Acc@1: 75.0000 (76.7857)  Acc@5: 95.8333 (95.4365)  Acc@task: 79.1667 (78.9683)  time: 0.1634  data: 0.0005  max mem: 1380\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.7324 (0.9547)  Acc@1: 75.0000 (77.4194)  Acc@5: 95.8333 (95.6989)  Acc@task: 83.3333 (79.7043)  time: 0.1632  data: 0.0003  max mem: 1380\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6772 (0.9076)  Acc@1: 79.1667 (78.0488)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (80.4878)  time: 0.1621  data: 0.0003  max mem: 1380\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6208 (0.8968)  Acc@1: 83.3333 (78.3000)  Acc@5: 95.8333 (95.9000)  Acc@task: 87.5000 (80.7000)  time: 0.1597  data: 0.0003  max mem: 1380\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1726 s / it)\n",
            "* Acc@task 80.700 Acc@1 78.300 Acc@5 95.900 loss 0.897\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:28  Loss: 1.5278 (1.5278)  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  Acc@task: 66.6667 (66.6667)  time: 0.6846  data: 0.4897  max mem: 1380\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 1.1471 (1.1245)  Acc@1: 79.1667 (75.7576)  Acc@5: 95.8333 (95.8333)  Acc@task: 75.0000 (75.3788)  time: 0.2112  data: 0.0466  max mem: 1380\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 1.1462 (1.2940)  Acc@1: 75.0000 (71.6270)  Acc@5: 95.8333 (94.0476)  Acc@task: 75.0000 (73.0159)  time: 0.1635  data: 0.0018  max mem: 1380\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 1.0714 (1.1983)  Acc@1: 70.8333 (72.8495)  Acc@5: 91.6667 (94.0860)  Acc@task: 75.0000 (74.5968)  time: 0.1625  data: 0.0008  max mem: 1380\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.8837 (1.1303)  Acc@1: 75.0000 (73.1707)  Acc@5: 95.8333 (94.4106)  Acc@task: 75.0000 (75.5081)  time: 0.1618  data: 0.0003  max mem: 1380\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.8837 (1.1266)  Acc@1: 75.0000 (73.2000)  Acc@5: 95.8333 (94.3000)  Acc@task: 75.0000 (75.5000)  time: 0.1591  data: 0.0003  max mem: 1380\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1763 s / it)\n",
            "* Acc@task 75.500 Acc@1 73.200 Acc@5 94.300 loss 1.127\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:26  Loss: 0.7129 (0.7129)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 87.5000 (87.5000)  time: 0.6230  data: 0.4714  max mem: 1380\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 0.7129 (0.8328)  Acc@1: 79.1667 (81.0606)  Acc@5: 95.8333 (96.2121)  Acc@task: 83.3333 (81.8182)  time: 0.2047  data: 0.0434  max mem: 1380\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.7464 (0.8191)  Acc@1: 79.1667 (80.9524)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (81.7460)  time: 0.1638  data: 0.0005  max mem: 1380\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.7194 (0.7492)  Acc@1: 83.3333 (81.8548)  Acc@5: 95.8333 (96.3710)  Acc@task: 79.1667 (80.7796)  time: 0.1641  data: 0.0003  max mem: 1380\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.5097 (0.7562)  Acc@1: 83.3333 (81.9106)  Acc@5: 95.8333 (96.2398)  Acc@task: 83.3333 (81.4024)  time: 0.1626  data: 0.0003  max mem: 1380\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.4939 (0.7479)  Acc@1: 87.5000 (82.0000)  Acc@5: 95.8333 (96.3000)  Acc@task: 83.3333 (81.3000)  time: 0.1601  data: 0.0003  max mem: 1380\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1765 s / it)\n",
            "* Acc@task 81.300 Acc@1 82.000 Acc@5 96.300 loss 0.748\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:32  Loss: 1.3381 (1.3381)  Acc@1: 58.3333 (58.3333)  Acc@5: 83.3333 (83.3333)  Acc@task: 62.5000 (62.5000)  time: 0.7786  data: 0.5595  max mem: 1380\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:07  Loss: 1.1148 (1.1519)  Acc@1: 75.0000 (72.7273)  Acc@5: 91.6667 (90.5303)  Acc@task: 70.8333 (70.8333)  time: 0.2214  data: 0.0525  max mem: 1380\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 1.1547 (1.1907)  Acc@1: 75.0000 (71.8254)  Acc@5: 91.6667 (91.0714)  Acc@task: 75.0000 (73.4127)  time: 0.1644  data: 0.0011  max mem: 1380\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 1.1904 (1.1437)  Acc@1: 70.8333 (72.8495)  Acc@5: 91.6667 (91.8011)  Acc@task: 75.0000 (74.1936)  time: 0.1632  data: 0.0004  max mem: 1380\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 1.0264 (1.2059)  Acc@1: 75.0000 (72.5610)  Acc@5: 91.6667 (91.2602)  Acc@task: 75.0000 (74.5935)  time: 0.1632  data: 0.0003  max mem: 1380\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 1.0264 (1.1887)  Acc@1: 75.0000 (72.9000)  Acc@5: 91.6667 (91.3000)  Acc@task: 75.0000 (74.8000)  time: 0.1604  data: 0.0003  max mem: 1380\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1794 s / it)\n",
            "* Acc@task 74.800 Acc@1 72.900 Acc@5 91.300 loss 1.189\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:17  Loss: 0.5046 (0.5046)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 79.1667 (79.1667)  time: 0.4102  data: 0.2538  max mem: 1380\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 0.7489 (0.7875)  Acc@1: 79.1667 (76.8939)  Acc@5: 95.8333 (96.2121)  Acc@task: 83.3333 (80.3030)  time: 0.1884  data: 0.0271  max mem: 1380\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:03  Loss: 0.9557 (0.9055)  Acc@1: 75.0000 (75.3968)  Acc@5: 95.8333 (95.0397)  Acc@task: 79.1667 (78.5714)  time: 0.1653  data: 0.0024  max mem: 1380\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 1.0418 (0.9300)  Acc@1: 75.0000 (75.8065)  Acc@5: 91.6667 (94.2204)  Acc@task: 79.1667 (78.6290)  time: 0.1639  data: 0.0003  max mem: 1380\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 1.0122 (1.0076)  Acc@1: 70.8333 (74.1870)  Acc@5: 91.6667 (93.9024)  Acc@task: 75.0000 (78.0488)  time: 0.1633  data: 0.0004  max mem: 1380\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 1.0721 (1.0477)  Acc@1: 70.8333 (73.6000)  Acc@5: 91.6667 (93.6000)  Acc@task: 75.0000 (77.7000)  time: 0.1608  data: 0.0003  max mem: 1380\n",
            "Test: [Task 5] Total time: 0:00:07 (0.1723 s / it)\n",
            "* Acc@task 77.700 Acc@1 73.600 Acc@5 93.600 loss 1.048\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:32  Loss: 0.5975 (0.5975)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  Acc@task: 91.6667 (91.6667)  time: 0.7840  data: 0.5883  max mem: 1380\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:07  Loss: 1.1643 (1.0965)  Acc@1: 75.0000 (73.4849)  Acc@5: 95.8333 (93.9394)  Acc@task: 79.1667 (79.1667)  time: 0.2207  data: 0.0544  max mem: 1380\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:04  Loss: 1.0346 (1.0910)  Acc@1: 70.8333 (72.4206)  Acc@5: 95.8333 (94.2460)  Acc@task: 79.1667 (78.7698)  time: 0.1647  data: 0.0007  max mem: 1380\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 1.0602 (1.1347)  Acc@1: 70.8333 (71.9086)  Acc@5: 95.8333 (94.2204)  Acc@task: 70.8333 (75.6720)  time: 0.1638  data: 0.0004  max mem: 1380\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.1326 (1.1484)  Acc@1: 70.8333 (72.3577)  Acc@5: 95.8333 (94.2073)  Acc@task: 70.8333 (74.7968)  time: 0.1628  data: 0.0003  max mem: 1380\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.2104 (1.1499)  Acc@1: 70.8333 (72.3000)  Acc@5: 95.8333 (94.2000)  Acc@task: 70.8333 (74.9000)  time: 0.1602  data: 0.0002  max mem: 1380\n",
            "Test: [Task 6] Total time: 0:00:07 (0.1793 s / it)\n",
            "* Acc@task 74.900 Acc@1 72.300 Acc@5 94.200 loss 1.150\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:23  Loss: 1.0865 (1.0865)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  Acc@task: 79.1667 (79.1667)  time: 0.5634  data: 0.3901  max mem: 1380\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:06  Loss: 1.0716 (0.9010)  Acc@1: 75.0000 (75.7576)  Acc@5: 95.8333 (93.9394)  Acc@task: 75.0000 (71.5909)  time: 0.1991  data: 0.0358  max mem: 1380\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:04  Loss: 1.0716 (1.0574)  Acc@1: 75.0000 (72.2222)  Acc@5: 91.6667 (92.6587)  Acc@task: 75.0000 (72.4206)  time: 0.1642  data: 0.0005  max mem: 1380\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:02  Loss: 1.2673 (1.1401)  Acc@1: 66.6667 (70.8333)  Acc@5: 91.6667 (92.0699)  Acc@task: 75.0000 (72.3118)  time: 0.1646  data: 0.0007  max mem: 1380\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 1.2554 (1.1459)  Acc@1: 70.8333 (70.9350)  Acc@5: 91.6667 (91.7683)  Acc@task: 75.0000 (72.4594)  time: 0.1631  data: 0.0004  max mem: 1380\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 1.1101 (1.1277)  Acc@1: 70.8333 (71.3000)  Acc@5: 91.6667 (91.9000)  Acc@task: 75.0000 (72.6000)  time: 0.1604  data: 0.0004  max mem: 1380\n",
            "Test: [Task 7] Total time: 0:00:07 (0.1752 s / it)\n",
            "* Acc@task 72.600 Acc@1 71.300 Acc@5 91.900 loss 1.128\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:27  Loss: 1.2563 (1.2563)  Acc@1: 75.0000 (75.0000)  Acc@5: 87.5000 (87.5000)  Acc@task: 79.1667 (79.1667)  time: 0.6628  data: 0.5071  max mem: 1380\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:06  Loss: 0.9934 (1.3110)  Acc@1: 70.8333 (69.6970)  Acc@5: 91.6667 (90.9091)  Acc@task: 70.8333 (71.2121)  time: 0.2078  data: 0.0465  max mem: 1380\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:04  Loss: 0.9184 (1.1876)  Acc@1: 70.8333 (71.2302)  Acc@5: 91.6667 (92.4603)  Acc@task: 70.8333 (71.4286)  time: 0.1640  data: 0.0005  max mem: 1380\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:02  Loss: 1.0552 (1.1459)  Acc@1: 75.0000 (71.2366)  Acc@5: 95.8333 (93.2796)  Acc@task: 70.8333 (71.5054)  time: 0.1649  data: 0.0005  max mem: 1380\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 1.2021 (1.1739)  Acc@1: 70.8333 (70.6301)  Acc@5: 91.6667 (92.6829)  Acc@task: 66.6667 (70.2236)  time: 0.1636  data: 0.0003  max mem: 1380\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 1.2021 (1.1695)  Acc@1: 70.8333 (70.6000)  Acc@5: 91.6667 (92.8000)  Acc@task: 66.6667 (70.3000)  time: 0.1611  data: 0.0002  max mem: 1380\n",
            "Test: [Task 8] Total time: 0:00:07 (0.1764 s / it)\n",
            "* Acc@task 70.300 Acc@1 70.600 Acc@5 92.800 loss 1.169\n",
            "Test: [Task 9]  [ 0/42]  eta: 0:00:24  Loss: 0.8349 (0.8349)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.5888  data: 0.4198  max mem: 1380\n",
            "Test: [Task 9]  [10/42]  eta: 0:00:06  Loss: 0.9443 (0.9873)  Acc@1: 79.1667 (76.8939)  Acc@5: 95.8333 (95.8333)  Acc@task: 70.8333 (76.1364)  time: 0.2018  data: 0.0384  max mem: 1380\n",
            "Test: [Task 9]  [20/42]  eta: 0:00:04  Loss: 0.8811 (0.8808)  Acc@1: 79.1667 (78.1746)  Acc@5: 95.8333 (95.6349)  Acc@task: 75.0000 (76.3889)  time: 0.1645  data: 0.0004  max mem: 1380\n",
            "Test: [Task 9]  [30/42]  eta: 0:00:02  Loss: 0.9560 (0.9924)  Acc@1: 75.0000 (75.4032)  Acc@5: 91.6667 (93.6828)  Acc@task: 75.0000 (75.9409)  time: 0.1643  data: 0.0009  max mem: 1380\n",
            "Test: [Task 9]  [40/42]  eta: 0:00:00  Loss: 0.8691 (0.9249)  Acc@1: 75.0000 (76.7276)  Acc@5: 91.6667 (94.4106)  Acc@task: 79.1667 (76.7276)  time: 0.1629  data: 0.0008  max mem: 1380\n",
            "Test: [Task 9]  [41/42]  eta: 0:00:00  Loss: 0.8691 (0.9129)  Acc@1: 75.0000 (76.9000)  Acc@5: 91.6667 (94.5000)  Acc@task: 79.1667 (77.0000)  time: 0.1604  data: 0.0008  max mem: 1380\n",
            "Test: [Task 9] Total time: 0:00:07 (0.1747 s / it)\n",
            "* Acc@task 77.000 Acc@1 76.900 Acc@5 94.500 loss 0.913\n",
            "Test: [Task 10]  [ 0/42]  eta: 0:00:17  Loss: 0.9798 (0.9798)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  Acc@task: 62.5000 (62.5000)  time: 0.4284  data: 0.2517  max mem: 1380\n",
            "Test: [Task 10]  [10/42]  eta: 0:00:06  Loss: 1.4944 (1.4145)  Acc@1: 62.5000 (64.3939)  Acc@5: 95.8333 (94.6970)  Acc@task: 58.3333 (56.8182)  time: 0.1917  data: 0.0284  max mem: 1380\n",
            "Test: [Task 10]  [20/42]  eta: 0:00:03  Loss: 1.4944 (1.4163)  Acc@1: 62.5000 (65.6746)  Acc@5: 91.6667 (94.0476)  Acc@task: 58.3333 (58.7302)  time: 0.1668  data: 0.0032  max mem: 1380\n",
            "Test: [Task 10]  [30/42]  eta: 0:00:02  Loss: 1.4768 (1.4759)  Acc@1: 58.3333 (63.7097)  Acc@5: 91.6667 (93.5484)  Acc@task: 58.3333 (58.6022)  time: 0.1648  data: 0.0003  max mem: 1380\n",
            "Test: [Task 10]  [40/42]  eta: 0:00:00  Loss: 1.4143 (1.4667)  Acc@1: 62.5000 (64.1260)  Acc@5: 91.6667 (93.2927)  Acc@task: 54.1667 (57.4187)  time: 0.1636  data: 0.0002  max mem: 1380\n",
            "Test: [Task 10]  [41/42]  eta: 0:00:00  Loss: 1.4459 (1.4666)  Acc@1: 62.5000 (64.2000)  Acc@5: 91.6667 (93.4000)  Acc@task: 54.1667 (57.7000)  time: 0.1611  data: 0.0002  max mem: 1380\n",
            "Test: [Task 10] Total time: 0:00:07 (0.1723 s / it)\n",
            "* Acc@task 57.700 Acc@1 64.200 Acc@5 93.400 loss 1.467\n",
            "[Average accuracy till task10]\tAcc@task: 74.2500\tAcc@1: 73.5300\tAcc@5: 93.8200\tLoss: 1.0834\tForgetting: 3.4556\tBackward: 3.6556\n",
            "Total training time: 0:17:29\n",
            "[rank0]:[W1006 11:14:30.637824411 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
          ]
        }
      ],
      "source": [
        "!torchrun --nproc_per_node=1 main.py cifar100_hideprompt_5e --original_model vit_small_patch16_224.dino --model vit_small_patch16_224.dino --batch-size 24 --epochs 1 --seed 20 --ca_lr 0.005 --crct_epochs 1 --prompt_momentum 0.1 --reg 0.1 --length 5 --larger_prompt_lr --data-path ./datasets/ --trained_original_model ./output/cifar100_full_dino_1epoch_10pct --output_dir ./output/cifar100_full_dino_1epoch_final_25pct --pct 0.10\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}