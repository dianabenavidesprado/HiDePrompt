{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1c65321c",
      "metadata": {
        "id": "1c65321c",
        "outputId": "4b091cba-75a6-4588-c841-aec3934a6b2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32512"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import os\n",
        "os.system('cmd command')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4842b918",
      "metadata": {
        "id": "4842b918",
        "outputId": "c66c8adf-1141-4986-a44b-2a7b0c3f0ec7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "31ecb10e",
      "metadata": {
        "id": "31ecb10e",
        "outputId": "47ca170c-57e1-4c37-adae-4de8a5ee4a0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/HiDePrompt\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/HiDePrompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd565e2a",
      "metadata": {
        "id": "cd565e2a"
      },
      "outputs": [],
      "source": [
        "!pip install -q condacolab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a2473b5",
      "metadata": {
        "id": "6a2473b5",
        "outputId": "e435931a-a6cb-4f7a-f09c-fd79432ddd86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⏬ Downloading https://github.com/jaimergp/miniforge/releases/download/24.11.2-1_colab/Miniforge3-colab-24.11.2-1_colab-Linux-x86_64.sh...\n",
            "📦 Installing...\n",
            "📌 Adjusting configuration...\n",
            "🩹 Patching environment...\n",
            "⏲ Done in 0:00:11\n",
            "🔁 Restarting kernel...\n"
          ]
        }
      ],
      "source": [
        "import condacolab\n",
        "condacolab.install()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/HiDePrompt/HiDePrompt"
      ],
      "metadata": {
        "id": "4aBTrmkLHGUx",
        "outputId": "76c589bd-788b-44b7-f1cb-4d7dd17bc1d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "4aBTrmkLHGUx",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59ddd81a",
      "metadata": {
        "id": "59ddd81a",
        "outputId": "2d958915-469d-49c8-90be-f3d2d67d4cfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm (from -r requirements.txt (line 1))\n",
            "  Downloading timm-1.0.20-py3-none-any.whl.metadata (61 kB)\n",
            "Collecting pillow (from -r requirements.txt (line 2))\n",
            "  Downloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting matplotlib (from -r requirements.txt (line 3))\n",
            "  Downloading matplotlib-3.10.6-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
            "Collecting torchprofile (from -r requirements.txt (line 4))\n",
            "  Downloading torchprofile-0.0.4-py3-none-any.whl.metadata (303 bytes)\n",
            "Collecting torch (from -r requirements.txt (line 5))\n",
            "  Downloading torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
            "Collecting torchvision (from -r requirements.txt (line 6))\n",
            "  Downloading torchvision-0.23.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (2.3.0)\n",
            "Collecting scipy (from -r requirements.txt (line 8))\n",
            "  Downloading scipy-1.16.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
            "Collecting scikit-learn (from -r requirements.txt (line 9))\n",
            "  Downloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
            "Collecting numpy (from -r requirements.txt (line 10))\n",
            "  Downloading numpy-2.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "Collecting pyyaml (from timm->-r requirements.txt (line 1))\n",
            "  Downloading pyyaml-6.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)\n",
            "Collecting huggingface_hub (from timm->-r requirements.txt (line 1))\n",
            "  Downloading huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting safetensors (from timm->-r requirements.txt (line 1))\n",
            "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib->-r requirements.txt (line 3))\n",
            "  Downloading contourpy-1.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib->-r requirements.txt (line 3))\n",
            "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib->-r requirements.txt (line 3))\n",
            "  Downloading fonttools-4.60.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (111 kB)\n",
            "Collecting kiwisolver>=1.3.1 (from matplotlib->-r requirements.txt (line 3))\n",
            "  Downloading kiwisolver-1.4.9-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 3)) (24.2)\n",
            "Collecting pyparsing>=2.3.1 (from matplotlib->-r requirements.txt (line 3))\n",
            "  Downloading pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting python-dateutil>=2.7 (from matplotlib->-r requirements.txt (line 3))\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting filelock (from torch->-r requirements.txt (line 5))\n",
            "  Downloading filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting typing-extensions>=4.10.0 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting sympy>=1.13.3 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting networkx (from torch->-r requirements.txt (line 5))\n",
            "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting jinja2 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting fsspec (from torch->-r requirements.txt (line 5))\n",
            "  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.9.90 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Collecting nvidia-nccl-cu12==2.27.3 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.8.90 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.4.0 (from torch->-r requirements.txt (line 5))\n",
            "  Downloading triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/site-packages (from triton==3.4.0->torch->-r requirements.txt (line 5)) (65.6.3)\n",
            "Collecting joblib>=1.2.0 (from scikit-learn->-r requirements.txt (line 9))\n",
            "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn->-r requirements.txt (line 9))\n",
            "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting six>=1.5 (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 3))\n",
            "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch->-r requirements.txt (line 5))\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/site-packages (from huggingface_hub->timm->-r requirements.txt (line 1)) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/site-packages (from huggingface_hub->timm->-r requirements.txt (line 1)) (4.67.1)\n",
            "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface_hub->timm->-r requirements.txt (line 1))\n",
            "  Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch->-r requirements.txt (line 5))\n",
            "  Downloading markupsafe-3.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 1)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 1)) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 1)) (2024.12.14)\n",
            "Downloading timm-1.0.20-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m141.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.10.6-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m179.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchprofile-0.0.4-py3-none-any.whl (7.7 kB)\n",
            "Downloading torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl (888.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m888.1/888.1 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m116.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "Downloading triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.5/155.5 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.23.0-cp311-cp311-manylinux_2_28_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m101.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.16.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.9/35.9 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m107.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m112.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading contourpy-1.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (355 kB)\n",
            "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading fonttools-4.60.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (5.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m96.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
            "Downloading kiwisolver-1.4.9-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
            "Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
            "Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\n",
            "Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
            "Downloading huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyyaml-6.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (806 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.6/806.6 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
            "Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markupsafe-3.0.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\n",
            "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: nvidia-cusparselt-cu12, mpmath, typing-extensions, triton, threadpoolctl, sympy, six, safetensors, pyyaml, pyparsing, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, kiwisolver, joblib, hf-xet, fsspec, fonttools, filelock, cycler, scipy, python-dateutil, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, jinja2, huggingface_hub, contourpy, scikit-learn, nvidia-cusolver-cu12, matplotlib, torch, torchvision, torchprofile, timm\n",
            "Successfully installed MarkupSafe-3.0.3 contourpy-1.3.3 cycler-0.12.1 filelock-3.19.1 fonttools-4.60.0 fsspec-2025.9.0 hf-xet-1.1.10 huggingface_hub-0.35.3 jinja2-3.1.6 joblib-1.5.2 kiwisolver-1.4.9 matplotlib-3.10.6 mpmath-1.3.0 networkx-3.5 numpy-2.3.3 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 pillow-11.3.0 pyparsing-3.2.5 python-dateutil-2.9.0.post0 pyyaml-6.0.3 safetensors-0.6.2 scikit-learn-1.7.2 scipy-1.16.2 six-1.17.0 sympy-1.14.0 threadpoolctl-3.6.0 timm-1.0.20 torch-2.8.0 torchprofile-0.0.4 torchvision-0.23.0 triton-3.4.0 typing-extensions-4.15.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "cycler",
                  "dateutil",
                  "kiwisolver",
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy",
                  "pyparsing",
                  "six"
                ]
              },
              "id": "5e9c917c5f81486e8bc6a6c737f1745b"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/HiDePrompt/HiDePrompt/"
      ],
      "metadata": {
        "id": "nTIVenS1Ih7Q",
        "outputId": "b62c261e-8ae1-4ac5-993f-fb76ecf96dfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "nTIVenS1Ih7Q",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "383f6206",
      "metadata": {
        "id": "383f6206",
        "outputId": "f96cc02d-bc1b-4b77-a93d-62cb86879239",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(subparser_name='cifar100_hideprompt_5e', pct=1.0, batch_size=24, epochs=1, original_model='vit_small_patch16_224.dino', model='vit_small_patch16_224.dino', input_size=224, pretrained=True, drop=0.0, drop_path=0.0, opt='adam', opt_eps=1e-08, opt_betas=(0.9, 0.999), clip_grad=1.0, momentum=0.9, weight_decay=0.0, reinit_optimizer=True, sched='constant', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=0, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, unscale_lr=True, color_jitter=None, aa=None, smoothing=0.1, train_interpolation='bicubic', reprob=0.0, remode='pixel', recount=1, data_path='./datasets/', dataset='Split-CIFAR100', shuffle=False, output_dir='./output/cifar100_full_dino_1epoch_100pct', device='cuda', seed=20, eval=False, num_workers=4, pin_mem=True, world_size=1, dist_url='env://', num_tasks=10, train_mask=True, task_inc=False, use_g_prompt=False, g_prompt_length=5, g_prompt_layer_idx=[], use_prefix_tune_for_g_prompt=False, use_e_prompt=True, e_prompt_layer_idx=[0, 1, 2, 3, 4], use_prefix_tune_for_e_prompt=True, larger_prompt_lr=False, prompt_pool=True, size=10, length=20, top_k=1, initializer='uniform', prompt_key=False, prompt_key_init='uniform', use_prompt_mask=True, mask_first_epoch=False, shared_prompt_pool=True, shared_prompt_key=False, batchwise_prompt=False, embedding_key='cls', predefined_key='', pull_constraint=True, pull_constraint_coeff=1.0, same_key_value=False, global_pool='token', head_type='token', freeze=['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed'], crct_epochs=30, train_inference_task_only=True, original_model_mlp_structure=[2], ca_lr=0.005, milestones=[10], trained_original_model='', prompt_momentum=0.01, reg=0.01, not_train_ca=False, ca_epochs=30, ca_storage_efficient_method='multi-centroid', n_centroids=10, print_freq=10, config='cifar100_hideprompt_5e')\n",
            "| distributed init (rank 0): env://\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "[rank0]:[W1003 14:57:25.410418110 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/helpers.py:7: FutureWarning: Importing from timm.models.helpers is deprecated, please import via timm.models\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:886: UserWarning: Overwriting vit_tiny_patch16_224 in registry with vits.hide_prompt_vision_transformer.vit_tiny_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:895: UserWarning: Overwriting vit_tiny_patch16_384 in registry with vits.hide_prompt_vision_transformer.vit_tiny_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:904: UserWarning: Overwriting vit_small_patch32_224 in registry with vits.hide_prompt_vision_transformer.vit_small_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:913: UserWarning: Overwriting vit_small_patch32_384 in registry with vits.hide_prompt_vision_transformer.vit_small_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:922: UserWarning: Overwriting vit_small_patch16_224 in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:932: UserWarning: Overwriting vit_small_patch16_384 in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:942: UserWarning: Overwriting vit_base_patch32_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:952: UserWarning: Overwriting vit_base_patch32_384 in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:962: UserWarning: Overwriting vit_base_patch16_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:972: UserWarning: Overwriting vit_base_patch16_384 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:982: UserWarning: Overwriting vit_base_patch8_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:992: UserWarning: Overwriting vit_large_patch32_224 in registry with vits.hide_prompt_vision_transformer.vit_large_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1001: UserWarning: Overwriting vit_large_patch32_384 in registry with vits.hide_prompt_vision_transformer.vit_large_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1011: UserWarning: Overwriting vit_large_patch16_224 in registry with vits.hide_prompt_vision_transformer.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1021: UserWarning: Overwriting vit_large_patch16_384 in registry with vits.hide_prompt_vision_transformer.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1031: UserWarning: Overwriting vit_large_patch14_224 in registry with vits.hide_prompt_vision_transformer.vit_large_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1040: UserWarning: Overwriting vit_huge_patch14_224 in registry with vits.hide_prompt_vision_transformer.vit_huge_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1049: UserWarning: Overwriting vit_giant_patch14_224 in registry with vits.hide_prompt_vision_transformer.vit_giant_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1058: UserWarning: Overwriting vit_gigantic_patch14_224 in registry with vits.hide_prompt_vision_transformer.vit_gigantic_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1067: UserWarning: Overwriting vit_tiny_patch16_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_tiny_patch16_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1078: UserWarning: Overwriting vit_small_patch32_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_small_patch32_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1089: UserWarning: Overwriting vit_small_patch16_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1100: UserWarning: Overwriting vit_base_patch32_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1111: UserWarning: Overwriting vit_base_patch16_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1122: UserWarning: Overwriting vit_base_patch8_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_base_patch8_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1133: UserWarning: Overwriting vit_large_patch32_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_large_patch32_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1144: UserWarning: Overwriting vit_large_patch16_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_large_patch16_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1155: UserWarning: Overwriting vit_huge_patch14_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_huge_patch14_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1166: UserWarning: Overwriting vit_base_patch16_224_sam in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_sam. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1175: UserWarning: Overwriting vit_base_patch32_224_sam in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_224_sam. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1184: UserWarning: Overwriting vit_small_patch16_224_dino in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_224_dino. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1193: UserWarning: Overwriting vit_small_patch8_224_dino in registry with vits.hide_prompt_vision_transformer.vit_small_patch8_224_dino. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1211: UserWarning: Overwriting vit_base_patch8_224_dino in registry with vits.hide_prompt_vision_transformer.vit_base_patch8_224_dino. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1220: UserWarning: Overwriting vit_base_patch16_224_miil_in21k in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_miil_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1230: UserWarning: Overwriting vit_base_patch16_224_miil in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_miil. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1242: UserWarning: Overwriting vit_base_patch32_plus_256 in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_plus_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1251: UserWarning: Overwriting vit_base_patch16_plus_240 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_plus_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1260: UserWarning: Overwriting vit_base_patch16_rpn_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_rpn_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1271: UserWarning: Overwriting vit_small_patch16_36x1_224 in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_36x1_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1282: UserWarning: Overwriting vit_small_patch16_18x2_224 in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1294: UserWarning: Overwriting vit_base_patch16_18x2_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1331: UserWarning: Overwriting vit_base_patch16_224_dino in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_dino. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "Original train size:  50000\n",
            "Sampled train size:  50000\n",
            "Original train size:  50000\n",
            "Sampled train size:  50000\n",
            "100\n",
            "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, 13, 14, 15, 16, 17, 18, 19], [20, 21, 22, 23, 24, 25, 26, 27, 28, 29], [30, 31, 32, 33, 34, 35, 36, 37, 38, 39], [40, 41, 42, 43, 44, 45, 46, 47, 48, 49], [50, 51, 52, 53, 54, 55, 56, 57, 58, 59], [60, 61, 62, 63, 64, 65, 66, 67, 68, 69], [70, 71, 72, 73, 74, 75, 76, 77, 78, 79], [80, 81, 82, 83, 84, 85, 86, 87, 88, 89], [90, 91, 92, 93, 94, 95, 96, 97, 98, 99]]\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Creating original model: vit_small_patch16_224.dino\n",
            "[Sequential(\n",
            "  (0): Linear(in_features=384, out_features=768, bias=True)\n",
            "  (1): GELU(approximate='none')\n",
            "  (2): Dropout(p=0.0, inplace=False)\n",
            "), Sequential(\n",
            "  (0): Linear(in_features=768, out_features=384, bias=True)\n",
            "  (1): Dropout(p=0.0, inplace=False)\n",
            ")]\n",
            "Namespace(subparser_name='cifar100_hideprompt_5e', pct=1.0, batch_size=24, epochs=1, original_model='vit_small_patch16_224.dino', model='vit_small_patch16_224.dino', input_size=224, pretrained=True, drop=0.0, drop_path=0.0, opt='adam', opt_eps=1e-08, opt_betas=(0.9, 0.999), clip_grad=1.0, momentum=0.9, weight_decay=0.0, reinit_optimizer=True, sched='constant', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=0, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, unscale_lr=True, color_jitter=None, aa=None, smoothing=0.1, train_interpolation='bicubic', reprob=0.0, remode='pixel', recount=1, data_path='./datasets/', dataset='Split-CIFAR100', shuffle=False, output_dir='./output/cifar100_full_dino_1epoch_100pct', device='cuda', seed=20, eval=False, num_workers=4, pin_mem=True, world_size=1, dist_url='env://', num_tasks=10, train_mask=True, task_inc=False, use_g_prompt=False, g_prompt_length=5, g_prompt_layer_idx=[], use_prefix_tune_for_g_prompt=False, use_e_prompt=True, e_prompt_layer_idx=[0, 1, 2, 3, 4], use_prefix_tune_for_e_prompt=True, larger_prompt_lr=False, prompt_pool=True, size=10, length=20, top_k=1, initializer='uniform', prompt_key=False, prompt_key_init='uniform', use_prompt_mask=True, mask_first_epoch=False, shared_prompt_pool=True, shared_prompt_key=False, batchwise_prompt=False, embedding_key='cls', predefined_key='', pull_constraint=True, pull_constraint_coeff=1.0, same_key_value=False, global_pool='token', head_type='token', freeze=['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed'], crct_epochs=30, train_inference_task_only=True, original_model_mlp_structure=[2], ca_lr=0.005, milestones=[10], trained_original_model='', prompt_momentum=0.01, reg=0.01, not_train_ca=False, ca_epochs=30, ca_storage_efficient_method='multi-centroid', n_centroids=10, print_freq=10, config='cifar100_hideprompt_5e', rank=0, gpu=0, distributed=True, dist_backend='nccl', nb_classes=100)\n",
            "number of params: 630244\n",
            "Start training for 1 epochs\n",
            "Train: Epoch[1/1]  [  0/209]  eta: 0:05:14  Lr: 0.000047  Loss: 2.2702  Acc@1: 16.6667 (16.6667)  Acc@5: 50.0000 (50.0000)  time: 1.5069  data: 0.4946  max mem: 196\n",
            "Train: Epoch[1/1]  [ 10/209]  eta: 0:00:41  Lr: 0.000047  Loss: 2.3101  Acc@1: 12.5000 (11.7424)  Acc@5: 58.3333 (55.6818)  time: 0.2106  data: 0.0459  max mem: 217\n",
            "Train: Epoch[1/1]  [ 20/209]  eta: 0:00:28  Lr: 0.000047  Loss: 2.1748  Acc@1: 12.5000 (13.6905)  Acc@5: 62.5000 (60.9127)  time: 0.0828  data: 0.0011  max mem: 217\n",
            "Train: Epoch[1/1]  [ 30/209]  eta: 0:00:22  Lr: 0.000047  Loss: 1.9911  Acc@1: 12.5000 (14.9194)  Acc@5: 66.6667 (64.3817)  time: 0.0831  data: 0.0013  max mem: 217\n",
            "Train: Epoch[1/1]  [ 40/209]  eta: 0:00:19  Lr: 0.000047  Loss: 2.0689  Acc@1: 20.8333 (18.5976)  Acc@5: 70.8333 (67.0732)  time: 0.0822  data: 0.0018  max mem: 217\n",
            "Train: Epoch[1/1]  [ 50/209]  eta: 0:00:17  Lr: 0.000047  Loss: 2.1137  Acc@1: 29.1667 (21.6503)  Acc@5: 79.1667 (70.0980)  time: 0.0825  data: 0.0017  max mem: 217\n",
            "Train: Epoch[1/1]  [ 60/209]  eta: 0:00:15  Lr: 0.000047  Loss: 1.9793  Acc@1: 29.1667 (24.5902)  Acc@5: 83.3333 (72.6093)  time: 0.0829  data: 0.0014  max mem: 217\n",
            "Train: Epoch[1/1]  [ 70/209]  eta: 0:00:14  Lr: 0.000047  Loss: 1.9495  Acc@1: 54.1667 (28.8146)  Acc@5: 87.5000 (75.1761)  time: 0.0834  data: 0.0015  max mem: 217\n",
            "Train: Epoch[1/1]  [ 80/209]  eta: 0:00:12  Lr: 0.000047  Loss: 1.9083  Acc@1: 54.1667 (31.2243)  Acc@5: 91.6667 (77.0576)  time: 0.0837  data: 0.0015  max mem: 217\n",
            "Train: Epoch[1/1]  [ 90/209]  eta: 0:00:11  Lr: 0.000047  Loss: 1.6413  Acc@1: 50.0000 (33.8828)  Acc@5: 91.6667 (78.5256)  time: 0.0855  data: 0.0017  max mem: 217\n",
            "Train: Epoch[1/1]  [100/209]  eta: 0:00:10  Lr: 0.000047  Loss: 1.7442  Acc@1: 58.3333 (36.7574)  Acc@5: 91.6667 (79.9918)  time: 0.0893  data: 0.0019  max mem: 217\n",
            "Train: Epoch[1/1]  [110/209]  eta: 0:00:09  Lr: 0.000047  Loss: 1.5463  Acc@1: 62.5000 (38.8889)  Acc@5: 91.6667 (81.1562)  time: 0.0964  data: 0.0017  max mem: 217\n",
            "Train: Epoch[1/1]  [120/209]  eta: 0:00:08  Lr: 0.000047  Loss: 1.7033  Acc@1: 66.6667 (41.2190)  Acc@5: 91.6667 (82.2314)  time: 0.1067  data: 0.0018  max mem: 217\n",
            "Train: Epoch[1/1]  [130/209]  eta: 0:00:07  Lr: 0.000047  Loss: 1.5668  Acc@1: 66.6667 (43.3524)  Acc@5: 95.8333 (83.3651)  time: 0.1100  data: 0.0056  max mem: 217\n",
            "Train: Epoch[1/1]  [140/209]  eta: 0:00:06  Lr: 0.000047  Loss: 1.2963  Acc@1: 70.8333 (45.2423)  Acc@5: 95.8333 (84.0721)  time: 0.1073  data: 0.0090  max mem: 217\n",
            "Train: Epoch[1/1]  [150/209]  eta: 0:00:05  Lr: 0.000047  Loss: 1.1702  Acc@1: 70.8333 (47.3234)  Acc@5: 95.8333 (84.9062)  time: 0.1013  data: 0.0050  max mem: 217\n",
            "Train: Epoch[1/1]  [160/209]  eta: 0:00:04  Lr: 0.000047  Loss: 0.9167  Acc@1: 75.0000 (48.8872)  Acc@5: 95.8333 (85.6366)  time: 0.0916  data: 0.0013  max mem: 217\n",
            "Train: Epoch[1/1]  [170/209]  eta: 0:00:03  Lr: 0.000047  Loss: 1.0693  Acc@1: 75.0000 (50.3899)  Acc@5: 100.0000 (86.4035)  time: 0.0861  data: 0.0011  max mem: 217\n",
            "Train: Epoch[1/1]  [180/209]  eta: 0:00:02  Lr: 0.000047  Loss: 0.9159  Acc@1: 75.0000 (51.8186)  Acc@5: 100.0000 (87.0396)  time: 0.0862  data: 0.0011  max mem: 217\n",
            "Train: Epoch[1/1]  [190/209]  eta: 0:00:01  Lr: 0.000047  Loss: 0.9501  Acc@1: 79.1667 (53.2504)  Acc@5: 95.8333 (87.5436)  time: 0.0875  data: 0.0012  max mem: 217\n",
            "Train: Epoch[1/1]  [200/209]  eta: 0:00:00  Lr: 0.000047  Loss: 0.7696  Acc@1: 83.3333 (54.7264)  Acc@5: 95.8333 (88.0597)  time: 0.0866  data: 0.0012  max mem: 217\n",
            "Train: Epoch[1/1]  [208/209]  eta: 0:00:00  Lr: 0.000047  Loss: 0.7616  Acc@1: 79.1667 (55.5600)  Acc@5: 100.0000 (88.4200)  time: 0.0854  data: 0.0009  max mem: 217\n",
            "Train: Epoch[1/1] Total time: 0:00:20 (0.0971 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.7616  Acc@1: 79.1667 (55.5600)  Acc@5: 100.0000 (88.4200)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:17  Loss: 2.1097 (2.1097)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.4244  data: 0.3549  max mem: 217\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 2.1491 (2.1355)  Acc@1: 91.6667 (90.1515)  Acc@5: 100.0000 (98.4848)  time: 0.1149  data: 0.0333  max mem: 218\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 2.1491 (2.1561)  Acc@1: 91.6667 (90.0794)  Acc@5: 100.0000 (98.2143)  time: 0.0833  data: 0.0008  max mem: 218\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 2.1025 (2.1281)  Acc@1: 91.6667 (90.9946)  Acc@5: 95.8333 (97.9839)  time: 0.0837  data: 0.0005  max mem: 218\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 2.1206 (2.1192)  Acc@1: 91.6667 (91.0569)  Acc@5: 100.0000 (98.0691)  time: 0.0853  data: 0.0004  max mem: 218\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 2.1135 (2.1163)  Acc@1: 91.6667 (91.0000)  Acc@5: 100.0000 (98.1000)  time: 0.0882  data: 0.0004  max mem: 218\n",
            "Test: [Task 1] Total time: 0:00:04 (0.0960 s / it)\n",
            "* Acc@1 91.000 Acc@5 98.100 loss 2.116\n",
            "[Average accuracy till task1]\tAcc@1: 91.0000\tAcc@5: 98.1000\tLoss: 2.1163\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:20  Loss: 2.1097 (2.1097)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.4990  data: 0.4158  max mem: 343\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 2.1491 (2.1355)  Acc@1: 91.6667 (90.1515)  Acc@5: 100.0000 (98.4848)  time: 0.1286  data: 0.0389  max mem: 343\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 2.1491 (2.1561)  Acc@1: 91.6667 (90.0794)  Acc@5: 100.0000 (98.2143)  time: 0.0915  data: 0.0008  max mem: 343\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 2.1025 (2.1281)  Acc@1: 91.6667 (90.9946)  Acc@5: 95.8333 (97.9839)  time: 0.0918  data: 0.0004  max mem: 343\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 2.1206 (2.1192)  Acc@1: 91.6667 (91.0569)  Acc@5: 100.0000 (98.0691)  time: 0.0925  data: 0.0003  max mem: 343\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 2.1135 (2.1163)  Acc@1: 91.6667 (91.0000)  Acc@5: 100.0000 (98.1000)  time: 0.0910  data: 0.0003  max mem: 343\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1041 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@1 91.000 Acc@5 98.100 loss 2.116\n",
            "[Average accuracy till task1]\tAcc@1: 91.0000\tAcc@5: 98.1000\tLoss: 2.1163\n",
            "Train: Epoch[1/1]  [  0/209]  eta: 0:02:56  Lr: 0.000047  Loss: 2.2282  Acc@1: 8.3333 (8.3333)  Acc@5: 79.1667 (79.1667)  time: 0.8467  data: 0.6753  max mem: 343\n",
            "Train: Epoch[1/1]  [ 10/209]  eta: 0:00:36  Lr: 0.000047  Loss: 2.4469  Acc@1: 20.8333 (16.6667)  Acc@5: 58.3333 (60.2273)  time: 0.1829  data: 0.0628  max mem: 343\n",
            "Train: Epoch[1/1]  [ 20/209]  eta: 0:00:28  Lr: 0.000047  Loss: 2.2320  Acc@1: 16.6667 (15.2778)  Acc@5: 62.5000 (64.6825)  time: 0.1140  data: 0.0036  max mem: 343\n",
            "Train: Epoch[1/1]  [ 30/209]  eta: 0:00:23  Lr: 0.000047  Loss: 1.9668  Acc@1: 20.8333 (18.2796)  Acc@5: 70.8333 (67.8763)  time: 0.1030  data: 0.0040  max mem: 343\n",
            "Train: Epoch[1/1]  [ 40/209]  eta: 0:00:20  Lr: 0.000047  Loss: 1.9793  Acc@1: 29.1667 (22.2561)  Acc@5: 75.0000 (71.1382)  time: 0.0944  data: 0.0017  max mem: 343\n",
            "Train: Epoch[1/1]  [ 50/209]  eta: 0:00:18  Lr: 0.000047  Loss: 1.8811  Acc@1: 33.3333 (25.5719)  Acc@5: 83.3333 (74.5098)  time: 0.0944  data: 0.0011  max mem: 343\n",
            "Train: Epoch[1/1]  [ 60/209]  eta: 0:00:16  Lr: 0.000047  Loss: 1.7774  Acc@1: 41.6667 (28.6202)  Acc@5: 87.5000 (76.2978)  time: 0.0947  data: 0.0009  max mem: 343\n",
            "Train: Epoch[1/1]  [ 70/209]  eta: 0:00:15  Lr: 0.000047  Loss: 1.7563  Acc@1: 45.8333 (31.9249)  Acc@5: 87.5000 (77.9343)  time: 0.0949  data: 0.0009  max mem: 343\n",
            "Train: Epoch[1/1]  [ 80/209]  eta: 0:00:14  Lr: 0.000047  Loss: 1.9071  Acc@1: 50.0000 (34.3107)  Acc@5: 87.5000 (79.1667)  time: 0.0944  data: 0.0010  max mem: 343\n",
            "Train: Epoch[1/1]  [ 90/209]  eta: 0:00:12  Lr: 0.000047  Loss: 1.7941  Acc@1: 54.1667 (36.5843)  Acc@5: 91.6667 (80.6319)  time: 0.0938  data: 0.0006  max mem: 343\n",
            "Train: Epoch[1/1]  [100/209]  eta: 0:00:11  Lr: 0.000047  Loss: 1.5646  Acc@1: 54.1667 (38.6551)  Acc@5: 91.6667 (81.8482)  time: 0.0945  data: 0.0010  max mem: 343\n",
            "Train: Epoch[1/1]  [110/209]  eta: 0:00:10  Lr: 0.000047  Loss: 1.3693  Acc@1: 58.3333 (41.0285)  Acc@5: 91.6667 (82.8829)  time: 0.0955  data: 0.0018  max mem: 343\n",
            "Train: Epoch[1/1]  [120/209]  eta: 0:00:09  Lr: 0.000047  Loss: 1.3150  Acc@1: 62.5000 (42.9063)  Acc@5: 91.6667 (83.7466)  time: 0.0945  data: 0.0012  max mem: 343\n",
            "Train: Epoch[1/1]  [130/209]  eta: 0:00:08  Lr: 0.000047  Loss: 1.5186  Acc@1: 62.5000 (44.5929)  Acc@5: 95.8333 (84.7646)  time: 0.0963  data: 0.0009  max mem: 343\n",
            "Train: Epoch[1/1]  [140/209]  eta: 0:00:07  Lr: 0.000047  Loss: 1.2461  Acc@1: 66.6667 (46.1584)  Acc@5: 95.8333 (85.4610)  time: 0.1012  data: 0.0019  max mem: 343\n",
            "Train: Epoch[1/1]  [150/209]  eta: 0:00:06  Lr: 0.000047  Loss: 1.2915  Acc@1: 66.6667 (47.5717)  Acc@5: 95.8333 (86.1479)  time: 0.1057  data: 0.0015  max mem: 343\n",
            "Train: Epoch[1/1]  [160/209]  eta: 0:00:05  Lr: 0.000047  Loss: 1.3415  Acc@1: 66.6667 (48.7578)  Acc@5: 95.8333 (86.7754)  time: 0.1094  data: 0.0017  max mem: 343\n",
            "Train: Epoch[1/1]  [170/209]  eta: 0:00:04  Lr: 0.000047  Loss: 1.3242  Acc@1: 66.6667 (50.0731)  Acc@5: 95.8333 (87.3538)  time: 0.1074  data: 0.0028  max mem: 343\n",
            "Train: Epoch[1/1]  [180/209]  eta: 0:00:03  Lr: 0.000047  Loss: 1.1096  Acc@1: 70.8333 (51.1510)  Acc@5: 95.8333 (87.8913)  time: 0.1009  data: 0.0018  max mem: 343\n",
            "Train: Epoch[1/1]  [190/209]  eta: 0:00:01  Lr: 0.000047  Loss: 0.9438  Acc@1: 70.8333 (52.3778)  Acc@5: 95.8333 (88.3290)  time: 0.0949  data: 0.0013  max mem: 343\n",
            "Train: Epoch[1/1]  [200/209]  eta: 0:00:00  Lr: 0.000047  Loss: 1.0561  Acc@1: 75.0000 (53.6070)  Acc@5: 95.8333 (88.7438)  time: 0.0917  data: 0.0013  max mem: 343\n",
            "Train: Epoch[1/1]  [208/209]  eta: 0:00:00  Lr: 0.000047  Loss: 1.0447  Acc@1: 79.1667 (54.1400)  Acc@5: 95.8333 (89.0000)  time: 0.0884  data: 0.0007  max mem: 343\n",
            "Train: Epoch[1/1] Total time: 0:00:21 (0.1026 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.0447  Acc@1: 79.1667 (54.1400)  Acc@5: 95.8333 (89.0000)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:21  Loss: 2.1240 (2.1240)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.5091  data: 0.4044  max mem: 343\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 2.2462 (2.1972)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (98.1061)  time: 0.1290  data: 0.0376  max mem: 343\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 2.2462 (2.2120)  Acc@1: 75.0000 (78.9683)  Acc@5: 95.8333 (97.6190)  time: 0.0903  data: 0.0008  max mem: 343\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 2.1544 (2.1831)  Acc@1: 87.5000 (81.4516)  Acc@5: 95.8333 (97.5806)  time: 0.0895  data: 0.0005  max mem: 343\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 2.1488 (2.1712)  Acc@1: 87.5000 (82.8252)  Acc@5: 95.8333 (97.6626)  time: 0.0897  data: 0.0003  max mem: 343\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 2.1420 (2.1675)  Acc@1: 87.5000 (82.7000)  Acc@5: 95.8333 (97.6000)  time: 0.0881  data: 0.0003  max mem: 343\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1012 s / it)\n",
            "* Acc@1 82.700 Acc@5 97.600 loss 2.167\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:19  Loss: 2.6577 (2.6577)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  time: 0.4748  data: 0.3857  max mem: 343\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:03  Loss: 2.7094 (2.6784)  Acc@1: 75.0000 (73.8636)  Acc@5: 91.6667 (91.6667)  time: 0.1238  data: 0.0363  max mem: 343\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 2.6474 (2.6678)  Acc@1: 75.0000 (74.6032)  Acc@5: 91.6667 (92.2619)  time: 0.0883  data: 0.0010  max mem: 343\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 2.5991 (2.6572)  Acc@1: 75.0000 (74.4624)  Acc@5: 95.8333 (92.7419)  time: 0.0889  data: 0.0007  max mem: 343\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 2.5803 (2.6551)  Acc@1: 70.8333 (73.8821)  Acc@5: 95.8333 (93.6992)  time: 0.0895  data: 0.0016  max mem: 343\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 2.5789 (2.6495)  Acc@1: 70.8333 (74.0000)  Acc@5: 95.8333 (93.8000)  time: 0.0878  data: 0.0016  max mem: 343\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1007 s / it)\n",
            "* Acc@1 74.000 Acc@5 93.800 loss 2.649\n",
            "[Average accuracy till task2]\tAcc@1: 78.3500\tAcc@5: 95.7000\tLoss: 2.4085\tForgetting: 8.3000\tBackward: -8.3000\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([4800, 384])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 1.0852  Acc@1: 87.5000 (87.9167)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000467  Loss: 1.0199  Acc@1: 87.5000 (85.0000)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000464  Loss: 1.1532  Acc@1: 79.1667 (83.3333)  Acc@5: 100.0000 (98.3333)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 0.9882  Acc@1: 87.5000 (84.5833)  Acc@5: 100.0000 (98.7500)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000448  Loss: 0.6962  Acc@1: 91.6667 (91.2500)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000437  Loss: 0.9002  Acc@1: 87.5000 (87.9167)  Acc@5: 100.0000 (98.7500)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 0.6557  Acc@1: 87.5000 (89.5833)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000409  Loss: 0.8047  Acc@1: 91.6667 (89.1667)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000391  Loss: 0.8390  Acc@1: 87.5000 (88.3333)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 1.0268  Acc@1: 87.5000 (85.8333)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000352  Loss: 0.8302  Acc@1: 83.3333 (86.6667)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000330  Loss: 0.6253  Acc@1: 91.6667 (94.1667)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 0.8014  Acc@1: 87.5000 (88.7500)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000283  Loss: 0.7100  Acc@1: 91.6667 (88.7500)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000259  Loss: 0.7533  Acc@1: 91.6667 (93.3333)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 0.7723  Acc@1: 91.6667 (92.5000)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000210  Loss: 0.6395  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000186  Loss: 0.8454  Acc@1: 95.8333 (93.3333)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.8207  Acc@1: 95.8333 (93.7500)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000139  Loss: 0.6297  Acc@1: 95.8333 (96.2500)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000117  Loss: 0.6608  Acc@1: 95.8333 (94.1667)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 0.3328  Acc@1: 95.8333 (95.0000)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000078  Loss: 0.5336  Acc@1: 95.8333 (95.4167)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000060  Loss: 0.5346  Acc@1: 95.8333 (95.0000)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 0.5385  Acc@1: 95.8333 (95.0000)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000031  Loss: 0.6695  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000020  Loss: 0.5642  Acc@1: 95.8333 (93.7500)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.5994  Acc@1: 95.8333 (94.5833)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000005  Loss: 0.7949  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([4800, 384])\n",
            "Averaged stats: Lr: 0.000001  Loss: 0.6626  Acc@1: 91.6667 (93.3333)  Acc@5: 100.0000 (99.1667)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:19  Loss: 1.1243 (1.1243)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.4642  data: 0.3709  max mem: 349\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 1.2523 (1.2343)  Acc@1: 87.5000 (84.8485)  Acc@5: 100.0000 (99.6212)  time: 0.1223  data: 0.0362  max mem: 349\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 1.2523 (1.2381)  Acc@1: 87.5000 (86.3095)  Acc@5: 100.0000 (99.2063)  time: 0.0870  data: 0.0018  max mem: 349\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 1.1859 (1.2104)  Acc@1: 91.6667 (87.7688)  Acc@5: 100.0000 (98.9247)  time: 0.0866  data: 0.0008  max mem: 349\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 1.1569 (1.1911)  Acc@1: 91.6667 (88.9228)  Acc@5: 100.0000 (98.9837)  time: 0.0878  data: 0.0005  max mem: 349\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 1.1345 (1.1859)  Acc@1: 91.6667 (89.0000)  Acc@5: 100.0000 (99.0000)  time: 0.0859  data: 0.0005  max mem: 349\n",
            "Test: [Task 1] Total time: 0:00:04 (0.0992 s / it)\n",
            "* Acc@1 89.000 Acc@5 99.000 loss 1.186\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:35  Loss: 1.5267 (1.5267)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  time: 0.8344  data: 0.7417  max mem: 349\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:05  Loss: 1.5020 (1.4801)  Acc@1: 83.3333 (83.7121)  Acc@5: 95.8333 (96.9697)  time: 0.1580  data: 0.0692  max mem: 349\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 1.4639 (1.4842)  Acc@1: 87.5000 (84.7222)  Acc@5: 95.8333 (97.2222)  time: 0.0890  data: 0.0018  max mem: 349\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 1.4009 (1.4731)  Acc@1: 83.3333 (84.6774)  Acc@5: 100.0000 (97.4462)  time: 0.0894  data: 0.0018  max mem: 349\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 1.3602 (1.4659)  Acc@1: 83.3333 (84.9594)  Acc@5: 100.0000 (97.7642)  time: 0.0907  data: 0.0015  max mem: 349\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 1.3587 (1.4630)  Acc@1: 83.3333 (85.1000)  Acc@5: 100.0000 (97.8000)  time: 0.0884  data: 0.0014  max mem: 349\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1093 s / it)\n",
            "* Acc@1 85.100 Acc@5 97.800 loss 1.463\n",
            "[Average accuracy till task2]\tAcc@1: 87.0500\tAcc@5: 98.4000\tLoss: 1.3245\tForgetting: 2.0000\tBackward: -2.0000\n",
            "Train: Epoch[1/1]  [  0/209]  eta: 0:02:11  Lr: 0.000047  Loss: 2.2410  Acc@1: 16.6667 (16.6667)  Acc@5: 58.3333 (58.3333)  time: 0.6309  data: 0.5297  max mem: 349\n",
            "Train: Epoch[1/1]  [ 10/209]  eta: 0:00:27  Lr: 0.000047  Loss: 2.2368  Acc@1: 12.5000 (16.6667)  Acc@5: 62.5000 (61.3636)  time: 0.1407  data: 0.0490  max mem: 349\n",
            "Train: Epoch[1/1]  [ 20/209]  eta: 0:00:22  Lr: 0.000047  Loss: 2.2745  Acc@1: 20.8333 (19.8413)  Acc@5: 62.5000 (64.6825)  time: 0.0918  data: 0.0012  max mem: 349\n",
            "Train: Epoch[1/1]  [ 30/209]  eta: 0:00:19  Lr: 0.000047  Loss: 2.1255  Acc@1: 25.0000 (22.7151)  Acc@5: 66.6667 (67.2043)  time: 0.0925  data: 0.0012  max mem: 349\n",
            "Train: Epoch[1/1]  [ 40/209]  eta: 0:00:17  Lr: 0.000047  Loss: 1.9747  Acc@1: 37.5000 (27.3374)  Acc@5: 75.0000 (71.2398)  time: 0.0924  data: 0.0008  max mem: 349\n",
            "Train: Epoch[1/1]  [ 50/209]  eta: 0:00:16  Lr: 0.000047  Loss: 1.7808  Acc@1: 41.6667 (30.1471)  Acc@5: 83.3333 (74.0196)  time: 0.0919  data: 0.0006  max mem: 349\n",
            "Train: Epoch[1/1]  [ 60/209]  eta: 0:00:15  Lr: 0.000047  Loss: 1.6674  Acc@1: 45.8333 (33.6749)  Acc@5: 87.5000 (76.9126)  time: 0.0929  data: 0.0008  max mem: 349\n",
            "Train: Epoch[1/1]  [ 70/209]  eta: 0:00:14  Lr: 0.000047  Loss: 1.6376  Acc@1: 50.0000 (36.6197)  Acc@5: 91.6667 (79.5188)  time: 0.0962  data: 0.0014  max mem: 349\n",
            "Train: Epoch[1/1]  [ 80/209]  eta: 0:00:12  Lr: 0.000047  Loss: 1.6262  Acc@1: 58.3333 (39.2490)  Acc@5: 95.8333 (81.4300)  time: 0.0956  data: 0.0015  max mem: 349\n",
            "Train: Epoch[1/1]  [ 90/209]  eta: 0:00:11  Lr: 0.000047  Loss: 1.3541  Acc@1: 58.3333 (41.7582)  Acc@5: 95.8333 (83.1502)  time: 0.0943  data: 0.0008  max mem: 349\n",
            "Train: Epoch[1/1]  [100/209]  eta: 0:00:10  Lr: 0.000047  Loss: 1.3169  Acc@1: 66.6667 (44.4719)  Acc@5: 95.8333 (84.4059)  time: 0.1011  data: 0.0010  max mem: 349\n",
            "Train: Epoch[1/1]  [110/209]  eta: 0:00:10  Lr: 0.000047  Loss: 1.2447  Acc@1: 66.6667 (46.5090)  Acc@5: 95.8333 (85.4730)  time: 0.1090  data: 0.0016  max mem: 349\n",
            "Train: Epoch[1/1]  [120/209]  eta: 0:00:09  Lr: 0.000047  Loss: 1.2576  Acc@1: 66.6667 (48.3815)  Acc@5: 100.0000 (86.5014)  time: 0.1111  data: 0.0029  max mem: 349\n",
            "Train: Epoch[1/1]  [130/209]  eta: 0:00:08  Lr: 0.000047  Loss: 1.0587  Acc@1: 75.0000 (50.6043)  Acc@5: 100.0000 (87.2774)  time: 0.1117  data: 0.0035  max mem: 349\n",
            "Train: Epoch[1/1]  [140/209]  eta: 0:00:07  Lr: 0.000047  Loss: 1.0699  Acc@1: 75.0000 (51.8322)  Acc@5: 95.8333 (87.9728)  time: 0.1039  data: 0.0024  max mem: 349\n",
            "Train: Epoch[1/1]  [150/209]  eta: 0:00:05  Lr: 0.000047  Loss: 1.1034  Acc@1: 70.8333 (53.0905)  Acc@5: 95.8333 (88.5486)  time: 0.0945  data: 0.0012  max mem: 349\n",
            "Train: Epoch[1/1]  [160/209]  eta: 0:00:04  Lr: 0.000047  Loss: 0.9354  Acc@1: 75.0000 (54.8395)  Acc@5: 95.8333 (89.0269)  time: 0.0945  data: 0.0006  max mem: 349\n",
            "Train: Epoch[1/1]  [170/209]  eta: 0:00:03  Lr: 0.000047  Loss: 1.0263  Acc@1: 79.1667 (56.2378)  Acc@5: 95.8333 (89.5224)  time: 0.0947  data: 0.0008  max mem: 349\n",
            "Train: Epoch[1/1]  [180/209]  eta: 0:00:02  Lr: 0.000047  Loss: 0.8455  Acc@1: 75.0000 (57.3895)  Acc@5: 100.0000 (90.0092)  time: 0.0939  data: 0.0010  max mem: 349\n",
            "Train: Epoch[1/1]  [190/209]  eta: 0:00:01  Lr: 0.000047  Loss: 0.9199  Acc@1: 79.1667 (58.6606)  Acc@5: 100.0000 (90.4450)  time: 0.0933  data: 0.0008  max mem: 349\n",
            "Train: Epoch[1/1]  [200/209]  eta: 0:00:00  Lr: 0.000047  Loss: 1.0599  Acc@1: 75.0000 (59.5771)  Acc@5: 100.0000 (90.8375)  time: 0.0931  data: 0.0010  max mem: 349\n",
            "Train: Epoch[1/1]  [208/209]  eta: 0:00:00  Lr: 0.000047  Loss: 0.6903  Acc@1: 79.1667 (60.4000)  Acc@5: 95.8333 (91.0000)  time: 0.0900  data: 0.0008  max mem: 349\n",
            "Train: Epoch[1/1] Total time: 0:00:20 (0.0998 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.6903  Acc@1: 79.1667 (60.4000)  Acc@5: 95.8333 (91.0000)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:22  Loss: 1.2490 (1.2490)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.5245  data: 0.4340  max mem: 349\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 1.3906 (1.3225)  Acc@1: 87.5000 (83.7121)  Acc@5: 100.0000 (99.6212)  time: 0.1310  data: 0.0402  max mem: 349\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 1.3906 (1.3314)  Acc@1: 83.3333 (85.7143)  Acc@5: 100.0000 (99.2063)  time: 0.0907  data: 0.0009  max mem: 349\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 1.2853 (1.3048)  Acc@1: 87.5000 (87.0968)  Acc@5: 100.0000 (98.7903)  time: 0.0904  data: 0.0010  max mem: 349\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 1.2350 (1.2867)  Acc@1: 87.5000 (87.9065)  Acc@5: 100.0000 (98.7805)  time: 0.0911  data: 0.0007  max mem: 349\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 1.2333 (1.2816)  Acc@1: 91.6667 (88.0000)  Acc@5: 100.0000 (98.8000)  time: 0.0894  data: 0.0007  max mem: 349\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1031 s / it)\n",
            "* Acc@1 88.000 Acc@5 98.800 loss 1.282\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:28  Loss: 1.6714 (1.6714)  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  time: 0.6813  data: 0.5692  max mem: 349\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 1.6192 (1.5951)  Acc@1: 79.1667 (80.6818)  Acc@5: 95.8333 (96.2121)  time: 0.1529  data: 0.0627  max mem: 349\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 1.5803 (1.5973)  Acc@1: 79.1667 (80.9524)  Acc@5: 95.8333 (95.6349)  time: 0.0948  data: 0.0065  max mem: 349\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 1.5188 (1.5834)  Acc@1: 83.3333 (82.1237)  Acc@5: 95.8333 (95.8333)  time: 0.0896  data: 0.0011  max mem: 349\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 1.4954 (1.5769)  Acc@1: 83.3333 (82.3171)  Acc@5: 100.0000 (96.4431)  time: 0.0903  data: 0.0007  max mem: 349\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 1.4725 (1.5729)  Acc@1: 87.5000 (82.5000)  Acc@5: 100.0000 (96.5000)  time: 0.0888  data: 0.0005  max mem: 349\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1081 s / it)\n",
            "* Acc@1 82.500 Acc@5 96.500 loss 1.573\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:19  Loss: 2.3829 (2.3829)  Acc@1: 20.8333 (20.8333)  Acc@5: 91.6667 (91.6667)  time: 0.4595  data: 0.3653  max mem: 349\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 2.4813 (2.4711)  Acc@1: 41.6667 (42.8030)  Acc@5: 95.8333 (93.5606)  time: 0.1222  data: 0.0352  max mem: 349\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 2.4885 (2.4721)  Acc@1: 41.6667 (43.4524)  Acc@5: 95.8333 (93.6508)  time: 0.0884  data: 0.0013  max mem: 349\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 2.3691 (2.4388)  Acc@1: 54.1667 (46.1022)  Acc@5: 91.6667 (92.8763)  time: 0.0888  data: 0.0004  max mem: 349\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 2.4306 (2.4615)  Acc@1: 50.0000 (46.0366)  Acc@5: 91.6667 (92.4797)  time: 0.0894  data: 0.0003  max mem: 349\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 2.4310 (2.4676)  Acc@1: 50.0000 (45.8000)  Acc@5: 91.6667 (92.4000)  time: 0.0878  data: 0.0003  max mem: 349\n",
            "Test: [Task 3] Total time: 0:00:04 (0.0993 s / it)\n",
            "* Acc@1 45.800 Acc@5 92.400 loss 2.468\n",
            "[Average accuracy till task3]\tAcc@1: 72.1000\tAcc@5: 95.9000\tLoss: 1.7740\tForgetting: 1.5000\tBackward: 2.7500\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([7200, 384])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 1.2564  Acc@1: 79.1667 (78.1250)  Acc@5: 100.0000 (98.9583)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000467  Loss: 1.0136  Acc@1: 83.3333 (83.7500)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000464  Loss: 0.9205  Acc@1: 83.3333 (85.0000)  Acc@5: 100.0000 (99.3750)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 0.9349  Acc@1: 83.3333 (85.2083)  Acc@5: 100.0000 (99.3750)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000448  Loss: 0.8998  Acc@1: 87.5000 (88.5417)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000437  Loss: 0.8854  Acc@1: 87.5000 (89.1667)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 0.5869  Acc@1: 91.6667 (90.2083)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000409  Loss: 0.7062  Acc@1: 91.6667 (90.6250)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000391  Loss: 0.7993  Acc@1: 87.5000 (88.9583)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 0.5890  Acc@1: 91.6667 (92.9167)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000352  Loss: 0.7102  Acc@1: 87.5000 (89.5833)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000330  Loss: 0.5410  Acc@1: 91.6667 (92.5000)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 0.6663  Acc@1: 87.5000 (89.7917)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000283  Loss: 0.7397  Acc@1: 91.6667 (90.6250)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000259  Loss: 0.6044  Acc@1: 91.6667 (91.4583)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 0.5580  Acc@1: 91.6667 (91.8750)  Acc@5: 100.0000 (99.3750)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000210  Loss: 0.4715  Acc@1: 91.6667 (92.0833)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000186  Loss: 0.6607  Acc@1: 91.6667 (92.2917)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.5944  Acc@1: 91.6667 (93.3333)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000139  Loss: 0.6298  Acc@1: 95.8333 (91.8750)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000117  Loss: 0.4018  Acc@1: 91.6667 (91.8750)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 0.8084  Acc@1: 95.8333 (93.7500)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000078  Loss: 0.7601  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000060  Loss: 0.5794  Acc@1: 91.6667 (91.4583)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 0.4494  Acc@1: 95.8333 (93.1250)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000031  Loss: 0.4140  Acc@1: 95.8333 (94.1667)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000020  Loss: 0.5835  Acc@1: 91.6667 (91.0417)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.4367  Acc@1: 91.6667 (93.5417)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000005  Loss: 0.5066  Acc@1: 95.8333 (92.0833)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([7200, 384])\n",
            "Averaged stats: Lr: 0.000001  Loss: 0.4947  Acc@1: 91.6667 (93.5417)  Acc@5: 100.0000 (99.7917)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:25  Loss: 0.7014 (0.7014)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.6156  data: 0.5173  max mem: 355\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 0.9217 (0.8936)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (98.8636)  time: 0.1381  data: 0.0494  max mem: 355\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.9217 (0.8883)  Acc@1: 79.1667 (81.3492)  Acc@5: 100.0000 (98.6111)  time: 0.0957  data: 0.0091  max mem: 355\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.8439 (0.8651)  Acc@1: 91.6667 (83.4677)  Acc@5: 100.0000 (98.6559)  time: 0.0975  data: 0.0116  max mem: 355\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.8418 (0.8453)  Acc@1: 91.6667 (84.5528)  Acc@5: 100.0000 (98.7805)  time: 0.0923  data: 0.0051  max mem: 355\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.7726 (0.8384)  Acc@1: 91.6667 (84.7000)  Acc@5: 100.0000 (98.8000)  time: 0.0908  data: 0.0051  max mem: 355\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1092 s / it)\n",
            "* Acc@1 84.700 Acc@5 98.800 loss 0.838\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:22  Loss: 1.1096 (1.1096)  Acc@1: 83.3333 (83.3333)  Acc@5: 91.6667 (91.6667)  time: 0.5300  data: 0.4518  max mem: 355\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 1.0367 (1.0076)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (97.3485)  time: 0.1293  data: 0.0426  max mem: 355\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 0.9923 (1.0157)  Acc@1: 83.3333 (83.1349)  Acc@5: 100.0000 (97.2222)  time: 0.0886  data: 0.0010  max mem: 355\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.9701 (1.0046)  Acc@1: 83.3333 (83.7366)  Acc@5: 100.0000 (97.3118)  time: 0.0884  data: 0.0003  max mem: 355\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.8937 (0.9928)  Acc@1: 83.3333 (83.9431)  Acc@5: 100.0000 (97.7642)  time: 0.0894  data: 0.0003  max mem: 355\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.8795 (0.9886)  Acc@1: 83.3333 (84.1000)  Acc@5: 100.0000 (97.8000)  time: 0.0877  data: 0.0002  max mem: 355\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1012 s / it)\n",
            "* Acc@1 84.100 Acc@5 97.800 loss 0.989\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:23  Loss: 0.9152 (0.9152)  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)  time: 0.5524  data: 0.4766  max mem: 355\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:04  Loss: 1.0457 (1.0724)  Acc@1: 79.1667 (81.0606)  Acc@5: 95.8333 (97.3485)  time: 0.1324  data: 0.0444  max mem: 355\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 1.0337 (1.0316)  Acc@1: 83.3333 (81.3492)  Acc@5: 95.8333 (97.6190)  time: 0.0896  data: 0.0007  max mem: 355\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.9353 (1.0008)  Acc@1: 83.3333 (82.2581)  Acc@5: 100.0000 (97.7151)  time: 0.0893  data: 0.0003  max mem: 355\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.9892 (1.0181)  Acc@1: 83.3333 (82.5203)  Acc@5: 95.8333 (97.4594)  time: 0.0907  data: 0.0003  max mem: 355\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.9951 (1.0225)  Acc@1: 83.3333 (82.4000)  Acc@5: 95.8333 (97.5000)  time: 0.0888  data: 0.0003  max mem: 355\n",
            "Test: [Task 3] Total time: 0:00:04 (0.1025 s / it)\n",
            "* Acc@1 82.400 Acc@5 97.500 loss 1.022\n",
            "[Average accuracy till task3]\tAcc@1: 83.7333\tAcc@5: 98.0333\tLoss: 0.9498\tForgetting: 3.6500\tBackward: -3.6500\n",
            "Train: Epoch[1/1]  [  0/209]  eta: 0:01:31  Lr: 0.000047  Loss: 2.4030  Acc@1: 20.8333 (20.8333)  Acc@5: 45.8333 (45.8333)  time: 0.4376  data: 0.3372  max mem: 355\n",
            "Train: Epoch[1/1]  [ 10/209]  eta: 0:00:26  Lr: 0.000047  Loss: 2.2166  Acc@1: 4.1667 (7.5758)  Acc@5: 45.8333 (50.0000)  time: 0.1356  data: 0.0352  max mem: 355\n",
            "Train: Epoch[1/1]  [ 20/209]  eta: 0:00:23  Lr: 0.000047  Loss: 2.2641  Acc@1: 8.3333 (10.5159)  Acc@5: 58.3333 (54.5635)  time: 0.1066  data: 0.0032  max mem: 355\n",
            "Train: Epoch[1/1]  [ 30/209]  eta: 0:00:21  Lr: 0.000047  Loss: 2.1603  Acc@1: 12.5000 (12.7688)  Acc@5: 62.5000 (58.4677)  time: 0.1094  data: 0.0032  max mem: 355\n",
            "Train: Epoch[1/1]  [ 40/209]  eta: 0:00:20  Lr: 0.000047  Loss: 2.0068  Acc@1: 20.8333 (17.0732)  Acc@5: 70.8333 (63.5163)  time: 0.1209  data: 0.0151  max mem: 355\n",
            "Train: Epoch[1/1]  [ 50/209]  eta: 0:00:19  Lr: 0.000047  Loss: 2.1139  Acc@1: 33.3333 (21.0784)  Acc@5: 83.3333 (67.4837)  time: 0.1237  data: 0.0136  max mem: 355\n",
            "Train: Epoch[1/1]  [ 60/209]  eta: 0:00:17  Lr: 0.000047  Loss: 1.8360  Acc@1: 41.6667 (25.2732)  Acc@5: 87.5000 (71.0383)  time: 0.1056  data: 0.0020  max mem: 355\n",
            "Train: Epoch[1/1]  [ 70/209]  eta: 0:00:15  Lr: 0.000047  Loss: 1.7723  Acc@1: 45.8333 (28.1103)  Acc@5: 91.6667 (73.8263)  time: 0.0937  data: 0.0015  max mem: 355\n",
            "Train: Epoch[1/1]  [ 80/209]  eta: 0:00:14  Lr: 0.000047  Loss: 1.7205  Acc@1: 50.0000 (31.4300)  Acc@5: 91.6667 (76.0802)  time: 0.0930  data: 0.0009  max mem: 355\n",
            "Train: Epoch[1/1]  [ 90/209]  eta: 0:00:12  Lr: 0.000047  Loss: 1.7169  Acc@1: 54.1667 (34.4780)  Acc@5: 91.6667 (77.8388)  time: 0.0935  data: 0.0011  max mem: 355\n",
            "Train: Epoch[1/1]  [100/209]  eta: 0:00:11  Lr: 0.000047  Loss: 1.5520  Acc@1: 58.3333 (37.3762)  Acc@5: 95.8333 (79.6205)  time: 0.0965  data: 0.0014  max mem: 355\n",
            "Train: Epoch[1/1]  [110/209]  eta: 0:00:10  Lr: 0.000047  Loss: 1.4286  Acc@1: 62.5000 (39.6396)  Acc@5: 95.8333 (81.0435)  time: 0.0973  data: 0.0015  max mem: 355\n",
            "Train: Epoch[1/1]  [120/209]  eta: 0:00:09  Lr: 0.000047  Loss: 1.4318  Acc@1: 66.6667 (42.1832)  Acc@5: 95.8333 (82.1970)  time: 0.0936  data: 0.0010  max mem: 355\n",
            "Train: Epoch[1/1]  [130/209]  eta: 0:00:08  Lr: 0.000047  Loss: 1.3162  Acc@1: 66.6667 (44.0204)  Acc@5: 95.8333 (83.1743)  time: 0.0920  data: 0.0009  max mem: 355\n",
            "Train: Epoch[1/1]  [140/209]  eta: 0:00:07  Lr: 0.000047  Loss: 1.3141  Acc@1: 66.6667 (45.9220)  Acc@5: 95.8333 (84.1312)  time: 0.0923  data: 0.0010  max mem: 355\n",
            "Train: Epoch[1/1]  [150/209]  eta: 0:00:06  Lr: 0.000047  Loss: 1.2498  Acc@1: 75.0000 (47.9581)  Acc@5: 95.8333 (85.0717)  time: 0.0922  data: 0.0008  max mem: 355\n",
            "Train: Epoch[1/1]  [160/209]  eta: 0:00:05  Lr: 0.000047  Loss: 1.1095  Acc@1: 75.0000 (49.3530)  Acc@5: 100.0000 (85.8437)  time: 0.0933  data: 0.0005  max mem: 355\n",
            "Train: Epoch[1/1]  [170/209]  eta: 0:00:04  Lr: 0.000047  Loss: 0.9597  Acc@1: 75.0000 (50.9503)  Acc@5: 95.8333 (86.5010)  time: 0.1020  data: 0.0007  max mem: 355\n",
            "Train: Epoch[1/1]  [180/209]  eta: 0:00:02  Lr: 0.000047  Loss: 1.0137  Acc@1: 79.1667 (52.4171)  Acc@5: 95.8333 (87.0396)  time: 0.1097  data: 0.0010  max mem: 355\n",
            "Train: Epoch[1/1]  [190/209]  eta: 0:00:01  Lr: 0.000047  Loss: 1.2384  Acc@1: 75.0000 (53.5777)  Acc@5: 95.8333 (87.4346)  time: 0.1085  data: 0.0012  max mem: 355\n",
            "Train: Epoch[1/1]  [200/209]  eta: 0:00:00  Lr: 0.000047  Loss: 1.2181  Acc@1: 75.0000 (54.6227)  Acc@5: 95.8333 (87.8731)  time: 0.1131  data: 0.0090  max mem: 355\n",
            "Train: Epoch[1/1]  [208/209]  eta: 0:00:00  Lr: 0.000047  Loss: 0.7943  Acc@1: 75.0000 (55.4000)  Acc@5: 100.0000 (88.2200)  time: 0.1033  data: 0.0086  max mem: 355\n",
            "Train: Epoch[1/1] Total time: 0:00:21 (0.1040 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.7943  Acc@1: 75.0000 (55.4000)  Acc@5: 100.0000 (88.2200)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:16  Loss: 0.8519 (0.8519)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.3903  data: 0.3117  max mem: 355\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 0.9925 (0.9845)  Acc@1: 83.3333 (80.3030)  Acc@5: 100.0000 (98.4848)  time: 0.1212  data: 0.0369  max mem: 355\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.9925 (0.9922)  Acc@1: 79.1667 (81.5476)  Acc@5: 100.0000 (98.0159)  time: 0.0926  data: 0.0053  max mem: 355\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.9176 (0.9762)  Acc@1: 83.3333 (83.0645)  Acc@5: 95.8333 (97.9839)  time: 0.0908  data: 0.0008  max mem: 355\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.9137 (0.9575)  Acc@1: 91.6667 (84.2480)  Acc@5: 100.0000 (98.2724)  time: 0.0907  data: 0.0003  max mem: 355\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.8897 (0.9495)  Acc@1: 91.6667 (84.4000)  Acc@5: 100.0000 (98.3000)  time: 0.0889  data: 0.0003  max mem: 355\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1003 s / it)\n",
            "* Acc@1 84.400 Acc@5 98.300 loss 0.949\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:15  Loss: 1.1892 (1.1892)  Acc@1: 87.5000 (87.5000)  Acc@5: 87.5000 (87.5000)  time: 0.3640  data: 0.2642  max mem: 355\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 1.1433 (1.1208)  Acc@1: 83.3333 (82.1970)  Acc@5: 95.8333 (96.5909)  time: 0.1255  data: 0.0399  max mem: 355\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 1.1283 (1.1287)  Acc@1: 83.3333 (83.9286)  Acc@5: 95.8333 (96.0317)  time: 0.0960  data: 0.0090  max mem: 355\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 1.0661 (1.1162)  Acc@1: 83.3333 (84.1398)  Acc@5: 95.8333 (96.3710)  time: 0.0902  data: 0.0006  max mem: 355\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 1.0267 (1.1078)  Acc@1: 87.5000 (84.5528)  Acc@5: 100.0000 (96.7480)  time: 0.0901  data: 0.0005  max mem: 355\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 1.0007 (1.1017)  Acc@1: 87.5000 (84.8000)  Acc@5: 100.0000 (96.8000)  time: 0.0885  data: 0.0005  max mem: 355\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1010 s / it)\n",
            "* Acc@1 84.800 Acc@5 96.800 loss 1.102\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:17  Loss: 1.0901 (1.0901)  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  time: 0.4271  data: 0.3396  max mem: 355\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 1.2166 (1.2485)  Acc@1: 75.0000 (75.7576)  Acc@5: 95.8333 (96.5909)  time: 0.1223  data: 0.0354  max mem: 355\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 1.2166 (1.2087)  Acc@1: 79.1667 (77.1825)  Acc@5: 95.8333 (97.2222)  time: 0.0990  data: 0.0110  max mem: 355\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 1.1024 (1.1633)  Acc@1: 79.1667 (78.4946)  Acc@5: 100.0000 (97.0430)  time: 0.1027  data: 0.0146  max mem: 355\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 1.1215 (1.1808)  Acc@1: 79.1667 (78.9634)  Acc@5: 95.8333 (96.4431)  time: 0.0973  data: 0.0097  max mem: 355\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 1.1694 (1.1876)  Acc@1: 79.1667 (78.8000)  Acc@5: 95.8333 (96.4000)  time: 0.0929  data: 0.0064  max mem: 355\n",
            "Test: [Task 3] Total time: 0:00:04 (0.1089 s / it)\n",
            "* Acc@1 78.800 Acc@5 96.400 loss 1.188\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:30  Loss: 3.1892 (3.1892)  Acc@1: 12.5000 (12.5000)  Acc@5: 66.6667 (66.6667)  time: 0.7368  data: 0.6491  max mem: 355\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:04  Loss: 2.9743 (2.9725)  Acc@1: 12.5000 (16.2879)  Acc@5: 79.1667 (79.5455)  time: 0.1480  data: 0.0599  max mem: 355\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 2.9743 (2.9978)  Acc@1: 16.6667 (16.4683)  Acc@5: 79.1667 (78.3730)  time: 0.0882  data: 0.0012  max mem: 355\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 2.9810 (2.9735)  Acc@1: 16.6667 (18.1452)  Acc@5: 79.1667 (79.5699)  time: 0.0877  data: 0.0010  max mem: 355\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 2.9810 (2.9872)  Acc@1: 20.8333 (18.8008)  Acc@5: 79.1667 (78.6585)  time: 0.0886  data: 0.0004  max mem: 355\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 2.9570 (2.9786)  Acc@1: 20.8333 (19.2000)  Acc@5: 79.1667 (78.8000)  time: 0.0866  data: 0.0004  max mem: 355\n",
            "Test: [Task 4] Total time: 0:00:04 (0.1055 s / it)\n",
            "* Acc@1 19.200 Acc@5 78.800 loss 2.979\n",
            "[Average accuracy till task4]\tAcc@1: 66.8000\tAcc@5: 92.5750\tLoss: 1.5544\tForgetting: 2.2000\tBackward: 12.4000\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([9576, 384])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 1.0368  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (96.2500)\n",
            "torch.Size([9576, 384])\n",
            "Averaged stats: Lr: 0.000467  Loss: 0.8241  Acc@1: 87.5000 (84.0278)  Acc@5: 100.0000 (98.4722)\n",
            "torch.Size([9576, 384])\n",
            "Averaged stats: Lr: 0.000464  Loss: 1.1625  Acc@1: 87.5000 (87.2222)  Acc@5: 100.0000 (98.7500)\n",
            "torch.Size([9576, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 0.7201  Acc@1: 87.5000 (88.1944)  Acc@5: 100.0000 (98.7500)\n",
            "torch.Size([9576, 384])\n",
            "Averaged stats: Lr: 0.000448  Loss: 0.6114  Acc@1: 87.5000 (88.3333)  Acc@5: 100.0000 (99.0278)\n",
            "torch.Size([9576, 384])\n",
            "Averaged stats: Lr: 0.000437  Loss: 0.7368  Acc@1: 87.5000 (89.3056)  Acc@5: 100.0000 (99.4444)\n",
            "torch.Size([9576, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 0.6608  Acc@1: 87.5000 (89.7222)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([9576, 384])\n",
            "Averaged stats: Lr: 0.000409  Loss: 0.7090  Acc@1: 91.6667 (92.5000)  Acc@5: 100.0000 (99.8611)\n",
            "torch.Size([9576, 384])\n",
            "Averaged stats: Lr: 0.000391  Loss: 0.7418  Acc@1: 91.6667 (90.4167)  Acc@5: 100.0000 (99.7222)\n",
            "torch.Size([9576, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 0.6289  Acc@1: 91.6667 (91.5278)  Acc@5: 100.0000 (99.7222)\n",
            "torch.Size([9576, 384])\n",
            "Averaged stats: Lr: 0.000352  Loss: 0.5760  Acc@1: 91.6667 (90.8333)  Acc@5: 100.0000 (99.8611)\n",
            "torch.Size([9576, 384])\n",
            "Averaged stats: Lr: 0.000330  Loss: 0.4517  Acc@1: 91.6667 (92.2222)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([9576, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 0.4853  Acc@1: 91.6667 (93.8889)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([9576, 384])\n",
            "Averaged stats: Lr: 0.000283  Loss: 0.6447  Acc@1: 91.6667 (92.9167)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([9576, 384])\n",
            "Averaged stats: Lr: 0.000259  Loss: 0.4177  Acc@1: 91.6667 (93.0556)  Acc@5: 100.0000 (99.7222)\n",
            "torch.Size([9576, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 0.4471  Acc@1: 91.6667 (90.9722)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([9576, 384])\n",
            "Averaged stats: Lr: 0.000210  Loss: 0.4417  Acc@1: 91.6667 (92.2222)  Acc@5: 100.0000 (99.8611)\n",
            "torch.Size([9576, 384])\n",
            "Averaged stats: Lr: 0.000186  Loss: 0.3688  Acc@1: 95.8333 (93.8889)  Acc@5: 100.0000 (99.8611)\n",
            "torch.Size([9576, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.5138  Acc@1: 95.8333 (92.7778)  Acc@5: 100.0000 (99.8611)\n",
            "torch.Size([9576, 384])\n",
            "Averaged stats: Lr: 0.000139  Loss: 0.5205  Acc@1: 91.6667 (92.3611)  Acc@5: 100.0000 (99.7222)\n",
            "torch.Size([9576, 384])\n",
            "Averaged stats: Lr: 0.000117  Loss: 0.5151  Acc@1: 95.8333 (93.6111)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([9576, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 0.3716  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([9576, 384])\n",
            "Averaged stats: Lr: 0.000078  Loss: 0.5339  Acc@1: 91.6667 (93.4722)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([9576, 384])\n",
            "Averaged stats: Lr: 0.000060  Loss: 0.5138  Acc@1: 95.8333 (93.3333)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([9576, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 0.4439  Acc@1: 95.8333 (93.6111)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([9576, 384])\n",
            "Averaged stats: Lr: 0.000031  Loss: 0.5145  Acc@1: 91.6667 (90.6944)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([9576, 384])\n",
            "Averaged stats: Lr: 0.000020  Loss: 0.5675  Acc@1: 95.8333 (93.1944)  Acc@5: 100.0000 (99.8611)\n",
            "torch.Size([9576, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.5423  Acc@1: 91.6667 (93.8889)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([9576, 384])\n",
            "Averaged stats: Lr: 0.000005  Loss: 0.5132  Acc@1: 95.8333 (93.3333)  Acc@5: 100.0000 (99.7222)\n",
            "torch.Size([9576, 384])\n",
            "Averaged stats: Lr: 0.000001  Loss: 0.3842  Acc@1: 91.6667 (93.0556)  Acc@5: 100.0000 (100.0000)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:22  Loss: 0.5894 (0.5894)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.5309  data: 0.4617  max mem: 356\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 0.7540 (0.7604)  Acc@1: 79.1667 (81.0606)  Acc@5: 100.0000 (97.3485)  time: 0.1313  data: 0.0428  max mem: 356\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.7593 (0.7520)  Acc@1: 79.1667 (83.1349)  Acc@5: 100.0000 (97.6190)  time: 0.0902  data: 0.0009  max mem: 356\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.6875 (0.7303)  Acc@1: 87.5000 (84.1398)  Acc@5: 100.0000 (97.9839)  time: 0.0888  data: 0.0009  max mem: 356\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6622 (0.7105)  Acc@1: 87.5000 (84.9594)  Acc@5: 100.0000 (98.2724)  time: 0.0897  data: 0.0006  max mem: 356\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6574 (0.7028)  Acc@1: 87.5000 (85.1000)  Acc@5: 100.0000 (98.3000)  time: 0.0883  data: 0.0004  max mem: 356\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1021 s / it)\n",
            "* Acc@1 85.100 Acc@5 98.300 loss 0.703\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:15  Loss: 1.0505 (1.0505)  Acc@1: 87.5000 (87.5000)  Acc@5: 91.6667 (91.6667)  time: 0.3584  data: 0.2745  max mem: 356\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:03  Loss: 0.9221 (0.9067)  Acc@1: 83.3333 (81.4394)  Acc@5: 95.8333 (96.2121)  time: 0.1232  data: 0.0381  max mem: 356\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 0.9221 (0.9276)  Acc@1: 79.1667 (80.5556)  Acc@5: 95.8333 (96.2302)  time: 0.0955  data: 0.0075  max mem: 356\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.9151 (0.9101)  Acc@1: 79.1667 (81.0484)  Acc@5: 95.8333 (96.6398)  time: 0.0916  data: 0.0005  max mem: 356\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.7976 (0.8928)  Acc@1: 83.3333 (81.1992)  Acc@5: 100.0000 (97.0528)  time: 0.0919  data: 0.0004  max mem: 356\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.7837 (0.8869)  Acc@1: 83.3333 (81.4000)  Acc@5: 100.0000 (97.1000)  time: 0.0900  data: 0.0003  max mem: 356\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1025 s / it)\n",
            "* Acc@1 81.400 Acc@5 97.100 loss 0.887\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:32  Loss: 0.6587 (0.6587)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.7797  data: 0.6515  max mem: 356\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:05  Loss: 0.8334 (0.8743)  Acc@1: 79.1667 (80.6818)  Acc@5: 95.8333 (96.9697)  time: 0.1569  data: 0.0659  max mem: 356\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 0.8261 (0.8245)  Acc@1: 79.1667 (81.5476)  Acc@5: 95.8333 (97.4206)  time: 0.0931  data: 0.0051  max mem: 356\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.7323 (0.7888)  Acc@1: 83.3333 (82.6613)  Acc@5: 95.8333 (97.4462)  time: 0.0919  data: 0.0020  max mem: 356\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.7646 (0.8035)  Acc@1: 83.3333 (83.1301)  Acc@5: 95.8333 (97.0528)  time: 0.0920  data: 0.0011  max mem: 356\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.7708 (0.8088)  Acc@1: 83.3333 (83.0000)  Acc@5: 95.8333 (97.1000)  time: 0.0905  data: 0.0010  max mem: 356\n",
            "Test: [Task 3] Total time: 0:00:04 (0.1106 s / it)\n",
            "* Acc@1 83.000 Acc@5 97.100 loss 0.809\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:17  Loss: 1.3212 (1.3212)  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.8333)  time: 0.4229  data: 0.3444  max mem: 356\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:04  Loss: 0.9114 (0.9522)  Acc@1: 83.3333 (81.8182)  Acc@5: 100.0000 (98.1061)  time: 0.1250  data: 0.0357  max mem: 356\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 0.9300 (0.9665)  Acc@1: 79.1667 (79.9603)  Acc@5: 100.0000 (98.0159)  time: 0.0933  data: 0.0026  max mem: 356\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 0.9111 (0.9387)  Acc@1: 79.1667 (80.9140)  Acc@5: 100.0000 (97.8495)  time: 0.0917  data: 0.0004  max mem: 356\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.9111 (0.9559)  Acc@1: 79.1667 (80.1829)  Acc@5: 95.8333 (97.5610)  time: 0.0920  data: 0.0003  max mem: 356\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.8850 (0.9489)  Acc@1: 79.1667 (80.3000)  Acc@5: 100.0000 (97.6000)  time: 0.0903  data: 0.0003  max mem: 356\n",
            "Test: [Task 4] Total time: 0:00:04 (0.1019 s / it)\n",
            "* Acc@1 80.300 Acc@5 97.600 loss 0.949\n",
            "[Average accuracy till task4]\tAcc@1: 82.4500\tAcc@5: 97.5250\tLoss: 0.8368\tForgetting: 3.2000\tBackward: -3.0000\n",
            "Train: Epoch[1/1]  [  0/209]  eta: 0:01:34  Lr: 0.000047  Loss: 2.4273  Acc@1: 8.3333 (8.3333)  Acc@5: 45.8333 (45.8333)  time: 0.4508  data: 0.3384  max mem: 356\n",
            "Train: Epoch[1/1]  [ 10/209]  eta: 0:00:26  Lr: 0.000047  Loss: 2.2982  Acc@1: 20.8333 (21.9697)  Acc@5: 66.6667 (62.8788)  time: 0.1349  data: 0.0385  max mem: 356\n",
            "Train: Epoch[1/1]  [ 20/209]  eta: 0:00:21  Lr: 0.000047  Loss: 2.1525  Acc@1: 25.0000 (23.8095)  Acc@5: 66.6667 (66.6667)  time: 0.0977  data: 0.0049  max mem: 356\n",
            "Train: Epoch[1/1]  [ 30/209]  eta: 0:00:19  Lr: 0.000047  Loss: 2.0194  Acc@1: 29.1667 (25.0000)  Acc@5: 75.0000 (70.2957)  time: 0.0925  data: 0.0011  max mem: 356\n",
            "Train: Epoch[1/1]  [ 40/209]  eta: 0:00:17  Lr: 0.000047  Loss: 1.8065  Acc@1: 33.3333 (30.6911)  Acc@5: 79.1667 (74.0854)  time: 0.0933  data: 0.0010  max mem: 356\n",
            "Train: Epoch[1/1]  [ 50/209]  eta: 0:00:16  Lr: 0.000047  Loss: 1.8494  Acc@1: 45.8333 (33.7418)  Acc@5: 87.5000 (76.8791)  time: 0.0984  data: 0.0015  max mem: 356\n",
            "Train: Epoch[1/1]  [ 60/209]  eta: 0:00:15  Lr: 0.000047  Loss: 1.6482  Acc@1: 50.0000 (36.6120)  Acc@5: 87.5000 (79.1667)  time: 0.1075  data: 0.0028  max mem: 356\n",
            "Train: Epoch[1/1]  [ 70/209]  eta: 0:00:14  Lr: 0.000047  Loss: 1.4622  Acc@1: 50.0000 (38.8498)  Acc@5: 91.6667 (81.2207)  time: 0.1103  data: 0.0031  max mem: 356\n",
            "Train: Epoch[1/1]  [ 80/209]  eta: 0:00:13  Lr: 0.000047  Loss: 1.4938  Acc@1: 54.1667 (41.1008)  Acc@5: 95.8333 (82.9218)  time: 0.1068  data: 0.0021  max mem: 356\n",
            "Train: Epoch[1/1]  [ 90/209]  eta: 0:00:12  Lr: 0.000047  Loss: 1.3470  Acc@1: 58.3333 (43.2692)  Acc@5: 95.8333 (84.4322)  time: 0.1009  data: 0.0019  max mem: 356\n",
            "Train: Epoch[1/1]  [100/209]  eta: 0:00:11  Lr: 0.000047  Loss: 1.3884  Acc@1: 66.6667 (45.5858)  Acc@5: 95.8333 (85.5198)  time: 0.0962  data: 0.0016  max mem: 356\n",
            "Train: Epoch[1/1]  [110/209]  eta: 0:00:10  Lr: 0.000047  Loss: 1.3563  Acc@1: 66.6667 (47.4850)  Acc@5: 95.8333 (86.4490)  time: 0.0975  data: 0.0014  max mem: 356\n",
            "Train: Epoch[1/1]  [120/209]  eta: 0:00:09  Lr: 0.000047  Loss: 1.1379  Acc@1: 66.6667 (49.1736)  Acc@5: 95.8333 (87.3967)  time: 0.0954  data: 0.0014  max mem: 356\n",
            "Train: Epoch[1/1]  [130/209]  eta: 0:00:08  Lr: 0.000047  Loss: 0.9911  Acc@1: 70.8333 (51.1768)  Acc@5: 95.8333 (88.0089)  time: 0.0916  data: 0.0008  max mem: 356\n",
            "Train: Epoch[1/1]  [140/209]  eta: 0:00:06  Lr: 0.000047  Loss: 1.1933  Acc@1: 75.0000 (52.7482)  Acc@5: 95.8333 (88.6820)  time: 0.0913  data: 0.0007  max mem: 356\n",
            "Train: Epoch[1/1]  [150/209]  eta: 0:00:05  Lr: 0.000047  Loss: 0.9891  Acc@1: 70.8333 (54.0011)  Acc@5: 95.8333 (89.1280)  time: 0.0936  data: 0.0005  max mem: 356\n",
            "Train: Epoch[1/1]  [160/209]  eta: 0:00:04  Lr: 0.000047  Loss: 0.9896  Acc@1: 70.8333 (55.0725)  Acc@5: 95.8333 (89.5704)  time: 0.0938  data: 0.0006  max mem: 356\n",
            "Train: Epoch[1/1]  [170/209]  eta: 0:00:03  Lr: 0.000047  Loss: 0.9689  Acc@1: 70.8333 (56.1891)  Acc@5: 95.8333 (90.0585)  time: 0.0915  data: 0.0010  max mem: 356\n",
            "Train: Epoch[1/1]  [180/209]  eta: 0:00:02  Lr: 0.000047  Loss: 0.7977  Acc@1: 75.0000 (57.3204)  Acc@5: 100.0000 (90.5847)  time: 0.0913  data: 0.0009  max mem: 356\n",
            "Train: Epoch[1/1]  [190/209]  eta: 0:00:01  Lr: 0.000047  Loss: 0.7049  Acc@1: 79.1667 (58.2679)  Acc@5: 100.0000 (90.9031)  time: 0.0912  data: 0.0008  max mem: 356\n",
            "Train: Epoch[1/1]  [200/209]  eta: 0:00:00  Lr: 0.000047  Loss: 1.0817  Acc@1: 75.0000 (59.1003)  Acc@5: 100.0000 (91.2935)  time: 0.0955  data: 0.0009  max mem: 356\n",
            "Train: Epoch[1/1]  [208/209]  eta: 0:00:00  Lr: 0.000047  Loss: 0.7477  Acc@1: 75.0000 (59.6600)  Acc@5: 100.0000 (91.4800)  time: 0.0918  data: 0.0007  max mem: 356\n",
            "Train: Epoch[1/1] Total time: 0:00:20 (0.0989 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.7477  Acc@1: 75.0000 (59.6600)  Acc@5: 100.0000 (91.4800)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:33  Loss: 0.7419 (0.7419)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.8050  data: 0.7131  max mem: 356\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:05  Loss: 0.8238 (0.8110)  Acc@1: 83.3333 (81.0606)  Acc@5: 100.0000 (99.2424)  time: 0.1571  data: 0.0671  max mem: 356\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.8238 (0.8145)  Acc@1: 87.5000 (83.5317)  Acc@5: 100.0000 (98.8095)  time: 0.0906  data: 0.0016  max mem: 356\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.7782 (0.7993)  Acc@1: 87.5000 (84.0054)  Acc@5: 100.0000 (98.7903)  time: 0.0891  data: 0.0011  max mem: 356\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.7180 (0.7779)  Acc@1: 87.5000 (84.7561)  Acc@5: 100.0000 (98.8821)  time: 0.0896  data: 0.0008  max mem: 356\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.7180 (0.7700)  Acc@1: 87.5000 (84.9000)  Acc@5: 100.0000 (98.9000)  time: 0.0883  data: 0.0008  max mem: 356\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1087 s / it)\n",
            "* Acc@1 84.900 Acc@5 98.900 loss 0.770\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:18  Loss: 1.0963 (1.0963)  Acc@1: 83.3333 (83.3333)  Acc@5: 91.6667 (91.6667)  time: 0.4340  data: 0.3538  max mem: 356\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:03  Loss: 0.9610 (0.9417)  Acc@1: 83.3333 (82.5758)  Acc@5: 95.8333 (96.5909)  time: 0.1207  data: 0.0343  max mem: 356\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 0.9610 (0.9671)  Acc@1: 83.3333 (81.9444)  Acc@5: 95.8333 (95.8333)  time: 0.0884  data: 0.0014  max mem: 356\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.9459 (0.9518)  Acc@1: 83.3333 (81.9892)  Acc@5: 95.8333 (95.8333)  time: 0.0881  data: 0.0004  max mem: 356\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.8562 (0.9380)  Acc@1: 83.3333 (82.1138)  Acc@5: 95.8333 (96.4431)  time: 0.0887  data: 0.0003  max mem: 356\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.8278 (0.9320)  Acc@1: 83.3333 (82.3000)  Acc@5: 100.0000 (96.5000)  time: 0.0867  data: 0.0003  max mem: 356\n",
            "Test: [Task 2] Total time: 0:00:04 (0.0982 s / it)\n",
            "* Acc@1 82.300 Acc@5 96.500 loss 0.932\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:20  Loss: 0.7186 (0.7186)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  time: 0.4805  data: 0.4095  max mem: 356\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 0.8993 (0.9473)  Acc@1: 79.1667 (80.3030)  Acc@5: 95.8333 (96.2121)  time: 0.1240  data: 0.0382  max mem: 356\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 0.8918 (0.8858)  Acc@1: 79.1667 (81.7460)  Acc@5: 95.8333 (96.6270)  time: 0.0875  data: 0.0007  max mem: 356\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.7561 (0.8496)  Acc@1: 83.3333 (82.9301)  Acc@5: 100.0000 (97.3118)  time: 0.0873  data: 0.0005  max mem: 356\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.8048 (0.8603)  Acc@1: 83.3333 (83.2317)  Acc@5: 95.8333 (96.7480)  time: 0.0885  data: 0.0004  max mem: 356\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.8255 (0.8689)  Acc@1: 83.3333 (83.1000)  Acc@5: 95.8333 (96.8000)  time: 0.0865  data: 0.0004  max mem: 356\n",
            "Test: [Task 3] Total time: 0:00:04 (0.0995 s / it)\n",
            "* Acc@1 83.100 Acc@5 96.800 loss 0.869\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:35  Loss: 1.6386 (1.6386)  Acc@1: 58.3333 (58.3333)  Acc@5: 83.3333 (83.3333)  time: 0.8540  data: 0.7497  max mem: 356\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:05  Loss: 1.0583 (1.1069)  Acc@1: 75.0000 (74.6212)  Acc@5: 95.8333 (95.8333)  time: 0.1612  data: 0.0718  max mem: 356\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 1.1037 (1.1354)  Acc@1: 70.8333 (73.0159)  Acc@5: 95.8333 (95.2381)  time: 0.0919  data: 0.0062  max mem: 356\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 1.1037 (1.1092)  Acc@1: 75.0000 (74.8656)  Acc@5: 95.8333 (95.4301)  time: 0.0929  data: 0.0071  max mem: 356\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 1.0664 (1.1222)  Acc@1: 75.0000 (74.5935)  Acc@5: 95.8333 (95.2236)  time: 0.0909  data: 0.0035  max mem: 356\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 1.0632 (1.1155)  Acc@1: 79.1667 (74.8000)  Acc@5: 95.8333 (95.2000)  time: 0.0895  data: 0.0035  max mem: 356\n",
            "Test: [Task 4] Total time: 0:00:04 (0.1114 s / it)\n",
            "* Acc@1 74.800 Acc@5 95.200 loss 1.116\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:16  Loss: 3.0590 (3.0590)  Acc@1: 0.0000 (0.0000)  Acc@5: 75.0000 (75.0000)  time: 0.4009  data: 0.3241  max mem: 356\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:03  Loss: 3.1091 (3.1059)  Acc@1: 8.3333 (6.4394)  Acc@5: 66.6667 (66.6667)  time: 0.1205  data: 0.0357  max mem: 356\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 3.1318 (3.1260)  Acc@1: 4.1667 (4.1667)  Acc@5: 66.6667 (67.4603)  time: 0.0900  data: 0.0037  max mem: 356\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 3.1828 (3.1550)  Acc@1: 0.0000 (3.6290)  Acc@5: 66.6667 (65.9946)  time: 0.0878  data: 0.0005  max mem: 356\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 3.2414 (3.1923)  Acc@1: 0.0000 (3.4553)  Acc@5: 62.5000 (64.2276)  time: 0.0883  data: 0.0003  max mem: 356\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 3.2585 (3.1993)  Acc@1: 0.0000 (3.4000)  Acc@5: 62.5000 (64.0000)  time: 0.0865  data: 0.0003  max mem: 356\n",
            "Test: [Task 5] Total time: 0:00:04 (0.0980 s / it)\n",
            "* Acc@1 3.400 Acc@5 64.000 loss 3.199\n",
            "[Average accuracy till task5]\tAcc@1: 65.7000\tAcc@5: 90.2800\tLoss: 1.3771\tForgetting: 2.1500\tBackward: 23.7750\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([11976, 384])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 1.1029  Acc@1: 70.8333 (73.5417)  Acc@5: 95.8333 (95.6250)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000467  Loss: 0.6997  Acc@1: 83.3333 (80.0000)  Acc@5: 100.0000 (98.8542)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000464  Loss: 0.7287  Acc@1: 83.3333 (84.2708)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 0.8640  Acc@1: 87.5000 (86.9792)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000448  Loss: 0.6582  Acc@1: 87.5000 (87.9167)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000437  Loss: 0.8138  Acc@1: 91.6667 (89.6875)  Acc@5: 100.0000 (99.8958)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 0.6144  Acc@1: 87.5000 (89.7917)  Acc@5: 100.0000 (99.8958)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000409  Loss: 0.5838  Acc@1: 91.6667 (91.9792)  Acc@5: 100.0000 (99.8958)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000391  Loss: 0.4248  Acc@1: 91.6667 (92.5000)  Acc@5: 100.0000 (99.8958)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 0.4836  Acc@1: 91.6667 (90.5208)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000352  Loss: 0.5463  Acc@1: 91.6667 (89.2708)  Acc@5: 100.0000 (99.8958)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000330  Loss: 0.4189  Acc@1: 95.8333 (91.3542)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 0.6518  Acc@1: 91.6667 (92.0833)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000283  Loss: 0.3480  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000259  Loss: 0.5334  Acc@1: 87.5000 (90.2083)  Acc@5: 100.0000 (99.6875)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 0.8075  Acc@1: 91.6667 (93.3333)  Acc@5: 100.0000 (99.8958)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000210  Loss: 0.3886  Acc@1: 91.6667 (93.1250)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000186  Loss: 0.4898  Acc@1: 91.6667 (92.3958)  Acc@5: 100.0000 (99.8958)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.5710  Acc@1: 91.6667 (92.3958)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000139  Loss: 0.5931  Acc@1: 95.8333 (93.5417)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000117  Loss: 0.5536  Acc@1: 91.6667 (93.8542)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 0.6159  Acc@1: 95.8333 (93.3333)  Acc@5: 100.0000 (99.6875)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000078  Loss: 0.3056  Acc@1: 95.8333 (92.3958)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000060  Loss: 0.4634  Acc@1: 91.6667 (93.0208)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 0.5229  Acc@1: 95.8333 (93.3333)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000031  Loss: 0.4930  Acc@1: 91.6667 (93.0208)  Acc@5: 100.0000 (99.8958)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000020  Loss: 0.4044  Acc@1: 91.6667 (92.1875)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.5860  Acc@1: 91.6667 (93.6458)  Acc@5: 100.0000 (99.8958)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000005  Loss: 0.3056  Acc@1: 95.8333 (94.2708)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([11976, 384])\n",
            "Averaged stats: Lr: 0.000001  Loss: 0.5180  Acc@1: 91.6667 (93.2292)  Acc@5: 100.0000 (99.4792)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:20  Loss: 0.6708 (0.6708)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.4952  data: 0.3998  max mem: 356\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 0.6957 (0.7378)  Acc@1: 79.1667 (79.5455)  Acc@5: 100.0000 (97.7273)  time: 0.1305  data: 0.0410  max mem: 356\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.7078 (0.7319)  Acc@1: 83.3333 (81.3492)  Acc@5: 100.0000 (97.6190)  time: 0.0982  data: 0.0101  max mem: 356\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.6796 (0.7088)  Acc@1: 83.3333 (81.9893)  Acc@5: 100.0000 (97.9839)  time: 0.1015  data: 0.0138  max mem: 356\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.5958 (0.6831)  Acc@1: 87.5000 (83.1301)  Acc@5: 100.0000 (98.3740)  time: 0.0953  data: 0.0070  max mem: 356\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.5958 (0.6745)  Acc@1: 87.5000 (83.3000)  Acc@5: 100.0000 (98.4000)  time: 0.0910  data: 0.0036  max mem: 356\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1092 s / it)\n",
            "* Acc@1 83.300 Acc@5 98.400 loss 0.674\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:21  Loss: 1.0288 (1.0288)  Acc@1: 83.3333 (83.3333)  Acc@5: 91.6667 (91.6667)  time: 0.5168  data: 0.4300  max mem: 356\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 0.8210 (0.8422)  Acc@1: 83.3333 (79.5455)  Acc@5: 95.8333 (96.2121)  time: 0.1295  data: 0.0404  max mem: 356\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 0.8210 (0.8789)  Acc@1: 79.1667 (78.9683)  Acc@5: 95.8333 (96.0317)  time: 0.0906  data: 0.0010  max mem: 356\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.8217 (0.8626)  Acc@1: 79.1667 (79.0323)  Acc@5: 95.8333 (96.1022)  time: 0.0907  data: 0.0005  max mem: 356\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.7680 (0.8443)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (96.3415)  time: 0.0916  data: 0.0004  max mem: 356\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.7584 (0.8387)  Acc@1: 79.1667 (79.4000)  Acc@5: 95.8333 (96.4000)  time: 0.0899  data: 0.0004  max mem: 356\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1027 s / it)\n",
            "* Acc@1 79.400 Acc@5 96.400 loss 0.839\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:18  Loss: 0.5935 (0.5935)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.4516  data: 0.3812  max mem: 356\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:04  Loss: 0.7985 (0.8356)  Acc@1: 75.0000 (77.2727)  Acc@5: 95.8333 (96.9697)  time: 0.1293  data: 0.0404  max mem: 356\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 0.7867 (0.7828)  Acc@1: 79.1667 (79.3651)  Acc@5: 95.8333 (97.2222)  time: 0.0941  data: 0.0034  max mem: 356\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.7177 (0.7515)  Acc@1: 79.1667 (79.9731)  Acc@5: 95.8333 (97.1774)  time: 0.0913  data: 0.0004  max mem: 356\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.7437 (0.7645)  Acc@1: 83.3333 (80.9959)  Acc@5: 95.8333 (96.6463)  time: 0.0920  data: 0.0003  max mem: 356\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.7732 (0.7696)  Acc@1: 83.3333 (80.8000)  Acc@5: 95.8333 (96.7000)  time: 0.0900  data: 0.0003  max mem: 356\n",
            "Test: [Task 3] Total time: 0:00:04 (0.1029 s / it)\n",
            "* Acc@1 80.800 Acc@5 96.700 loss 0.770\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:24  Loss: 1.1640 (1.1640)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  time: 0.5767  data: 0.4922  max mem: 356\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:04  Loss: 0.8162 (0.8206)  Acc@1: 79.1667 (82.5758)  Acc@5: 100.0000 (98.1061)  time: 0.1360  data: 0.0458  max mem: 356\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 0.8162 (0.8287)  Acc@1: 79.1667 (81.1508)  Acc@5: 100.0000 (97.6190)  time: 0.0919  data: 0.0011  max mem: 356\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 0.7663 (0.7934)  Acc@1: 79.1667 (82.2581)  Acc@5: 95.8333 (97.7151)  time: 0.0917  data: 0.0013  max mem: 356\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7255 (0.8126)  Acc@1: 79.1667 (81.1992)  Acc@5: 95.8333 (97.5610)  time: 0.0921  data: 0.0012  max mem: 356\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.7090 (0.8052)  Acc@1: 79.1667 (81.4000)  Acc@5: 100.0000 (97.6000)  time: 0.0905  data: 0.0010  max mem: 356\n",
            "Test: [Task 4] Total time: 0:00:04 (0.1067 s / it)\n",
            "* Acc@1 81.400 Acc@5 97.600 loss 0.805\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:22  Loss: 0.7736 (0.7736)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.5380  data: 0.4520  max mem: 356\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:04  Loss: 0.8708 (0.8793)  Acc@1: 79.1667 (79.5455)  Acc@5: 100.0000 (98.4848)  time: 0.1445  data: 0.0571  max mem: 356\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 0.9041 (0.9177)  Acc@1: 79.1667 (79.3651)  Acc@5: 95.8333 (97.4206)  time: 0.0981  data: 0.0091  max mem: 356\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 0.9676 (0.9320)  Acc@1: 79.1667 (78.7634)  Acc@5: 95.8333 (97.4462)  time: 0.0912  data: 0.0004  max mem: 356\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 1.0218 (0.9722)  Acc@1: 75.0000 (77.5407)  Acc@5: 95.8333 (97.1545)  time: 0.0917  data: 0.0004  max mem: 356\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 1.0304 (0.9855)  Acc@1: 75.0000 (77.3000)  Acc@5: 95.8333 (97.0000)  time: 0.0898  data: 0.0004  max mem: 356\n",
            "Test: [Task 5] Total time: 0:00:04 (0.1069 s / it)\n",
            "* Acc@1 77.300 Acc@5 97.000 loss 0.985\n",
            "[Average accuracy till task5]\tAcc@1: 80.4400\tAcc@5: 97.2200\tLoss: 0.8147\tForgetting: 3.9000\tBackward: -3.4750\n",
            "Train: Epoch[1/1]  [  0/209]  eta: 0:01:57  Lr: 0.000047  Loss: 2.2774  Acc@1: 12.5000 (12.5000)  Acc@5: 54.1667 (54.1667)  time: 0.5633  data: 0.4732  max mem: 356\n",
            "Train: Epoch[1/1]  [ 10/209]  eta: 0:00:27  Lr: 0.000047  Loss: 2.2176  Acc@1: 8.3333 (11.7424)  Acc@5: 54.1667 (56.0606)  time: 0.1368  data: 0.0444  max mem: 356\n",
            "Train: Epoch[1/1]  [ 20/209]  eta: 0:00:21  Lr: 0.000047  Loss: 2.0883  Acc@1: 16.6667 (18.0556)  Acc@5: 66.6667 (65.2778)  time: 0.0927  data: 0.0011  max mem: 356\n",
            "Train: Epoch[1/1]  [ 30/209]  eta: 0:00:19  Lr: 0.000047  Loss: 2.0963  Acc@1: 25.0000 (20.5645)  Acc@5: 79.1667 (70.8333)  time: 0.0916  data: 0.0007  max mem: 356\n",
            "Train: Epoch[1/1]  [ 40/209]  eta: 0:00:17  Lr: 0.000047  Loss: 1.9084  Acc@1: 33.3333 (25.3049)  Acc@5: 83.3333 (74.6951)  time: 0.0918  data: 0.0007  max mem: 356\n",
            "Train: Epoch[1/1]  [ 50/209]  eta: 0:00:16  Lr: 0.000047  Loss: 1.6670  Acc@1: 37.5000 (28.5131)  Acc@5: 91.6667 (78.2680)  time: 0.0915  data: 0.0006  max mem: 356\n",
            "Train: Epoch[1/1]  [ 60/209]  eta: 0:00:14  Lr: 0.000047  Loss: 1.7793  Acc@1: 50.0000 (32.4454)  Acc@5: 91.6667 (80.3279)  time: 0.0913  data: 0.0004  max mem: 356\n",
            "Train: Epoch[1/1]  [ 70/209]  eta: 0:00:13  Lr: 0.000047  Loss: 1.7146  Acc@1: 54.1667 (35.7981)  Acc@5: 91.6667 (82.1009)  time: 0.0915  data: 0.0005  max mem: 356\n",
            "Train: Epoch[1/1]  [ 80/209]  eta: 0:00:12  Lr: 0.000047  Loss: 1.4979  Acc@1: 58.3333 (38.6317)  Acc@5: 91.6667 (83.5905)  time: 0.0954  data: 0.0016  max mem: 356\n",
            "Train: Epoch[1/1]  [ 90/209]  eta: 0:00:11  Lr: 0.000047  Loss: 1.6287  Acc@1: 58.3333 (40.8425)  Acc@5: 91.6667 (84.7527)  time: 0.1013  data: 0.0017  max mem: 356\n",
            "Train: Epoch[1/1]  [100/209]  eta: 0:00:10  Lr: 0.000047  Loss: 1.4476  Acc@1: 62.5000 (42.9868)  Acc@5: 95.8333 (85.8911)  time: 0.1086  data: 0.0013  max mem: 356\n",
            "Train: Epoch[1/1]  [110/209]  eta: 0:00:10  Lr: 0.000047  Loss: 1.2738  Acc@1: 66.6667 (45.0826)  Acc@5: 95.8333 (86.8243)  time: 0.1207  data: 0.0084  max mem: 356\n",
            "Train: Epoch[1/1]  [120/209]  eta: 0:00:09  Lr: 0.000047  Loss: 1.4814  Acc@1: 66.6667 (46.5565)  Acc@5: 95.8333 (87.7410)  time: 0.1213  data: 0.0101  max mem: 356\n",
            "Train: Epoch[1/1]  [130/209]  eta: 0:00:08  Lr: 0.000047  Loss: 1.2310  Acc@1: 66.6667 (48.0916)  Acc@5: 100.0000 (88.4224)  time: 0.1045  data: 0.0034  max mem: 356\n",
            "Train: Epoch[1/1]  [140/209]  eta: 0:00:07  Lr: 0.000047  Loss: 1.0599  Acc@1: 66.6667 (49.3499)  Acc@5: 100.0000 (89.0071)  time: 0.0926  data: 0.0013  max mem: 356\n",
            "Train: Epoch[1/1]  [150/209]  eta: 0:00:05  Lr: 0.000047  Loss: 1.0873  Acc@1: 70.8333 (50.8278)  Acc@5: 100.0000 (89.6799)  time: 0.0906  data: 0.0009  max mem: 356\n",
            "Train: Epoch[1/1]  [160/209]  eta: 0:00:04  Lr: 0.000047  Loss: 1.2434  Acc@1: 70.8333 (51.7081)  Acc@5: 100.0000 (90.1656)  time: 0.0902  data: 0.0006  max mem: 356\n",
            "Train: Epoch[1/1]  [170/209]  eta: 0:00:03  Lr: 0.000047  Loss: 1.3278  Acc@1: 66.6667 (52.4366)  Acc@5: 95.8333 (90.4483)  time: 0.0902  data: 0.0006  max mem: 356\n",
            "Train: Epoch[1/1]  [180/209]  eta: 0:00:02  Lr: 0.000047  Loss: 1.1116  Acc@1: 70.8333 (53.4991)  Acc@5: 95.8333 (90.8840)  time: 0.0899  data: 0.0006  max mem: 356\n",
            "Train: Epoch[1/1]  [190/209]  eta: 0:00:01  Lr: 0.000047  Loss: 1.0337  Acc@1: 70.8333 (54.6030)  Acc@5: 100.0000 (91.2958)  time: 0.0899  data: 0.0003  max mem: 356\n",
            "Train: Epoch[1/1]  [200/209]  eta: 0:00:00  Lr: 0.000047  Loss: 1.0892  Acc@1: 75.0000 (55.7214)  Acc@5: 100.0000 (91.6667)  time: 0.0898  data: 0.0003  max mem: 356\n",
            "Train: Epoch[1/1]  [208/209]  eta: 0:00:00  Lr: 0.000047  Loss: 0.9175  Acc@1: 75.0000 (56.3600)  Acc@5: 100.0000 (91.9200)  time: 0.0867  data: 0.0002  max mem: 356\n",
            "Train: Epoch[1/1] Total time: 0:00:20 (0.0986 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.9175  Acc@1: 75.0000 (56.3600)  Acc@5: 100.0000 (91.9200)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:15  Loss: 0.7449 (0.7449)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  time: 0.3772  data: 0.2937  max mem: 356\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 0.7536 (0.8042)  Acc@1: 79.1667 (78.7879)  Acc@5: 95.8333 (97.7273)  time: 0.1209  data: 0.0340  max mem: 356\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.7566 (0.8021)  Acc@1: 79.1667 (80.9524)  Acc@5: 100.0000 (97.4206)  time: 0.0916  data: 0.0046  max mem: 356\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.7219 (0.7781)  Acc@1: 83.3333 (81.3172)  Acc@5: 100.0000 (97.8495)  time: 0.0885  data: 0.0010  max mem: 356\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.7073 (0.7509)  Acc@1: 87.5000 (82.7236)  Acc@5: 100.0000 (98.1707)  time: 0.0889  data: 0.0006  max mem: 356\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6268 (0.7418)  Acc@1: 87.5000 (82.9000)  Acc@5: 100.0000 (98.2000)  time: 0.0872  data: 0.0006  max mem: 356\n",
            "Test: [Task 1] Total time: 0:00:04 (0.0997 s / it)\n",
            "* Acc@1 82.900 Acc@5 98.200 loss 0.742\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:33  Loss: 1.0413 (1.0413)  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  time: 0.7923  data: 0.6760  max mem: 356\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 0.9378 (0.8913)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (96.2121)  time: 0.1517  data: 0.0621  max mem: 356\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 0.9378 (0.9313)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  time: 0.0874  data: 0.0005  max mem: 356\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.8960 (0.9175)  Acc@1: 79.1667 (78.8979)  Acc@5: 95.8333 (95.9677)  time: 0.0876  data: 0.0004  max mem: 356\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.8210 (0.9018)  Acc@1: 79.1667 (79.2683)  Acc@5: 95.8333 (96.1382)  time: 0.0884  data: 0.0003  max mem: 356\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.8066 (0.8958)  Acc@1: 79.1667 (79.5000)  Acc@5: 95.8333 (96.2000)  time: 0.0864  data: 0.0003  max mem: 356\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1061 s / it)\n",
            "* Acc@1 79.500 Acc@5 96.200 loss 0.896\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:20  Loss: 0.6713 (0.6713)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  time: 0.4895  data: 0.3938  max mem: 356\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:04  Loss: 0.9028 (0.9212)  Acc@1: 79.1667 (78.0303)  Acc@5: 95.8333 (95.0758)  time: 0.1251  data: 0.0366  max mem: 356\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 0.8710 (0.8720)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (96.4286)  time: 0.0874  data: 0.0007  max mem: 356\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.7927 (0.8391)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (96.9086)  time: 0.0869  data: 0.0005  max mem: 356\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.8357 (0.8572)  Acc@1: 79.1667 (78.7602)  Acc@5: 95.8333 (96.6463)  time: 0.0881  data: 0.0004  max mem: 356\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.8772 (0.8644)  Acc@1: 79.1667 (78.7000)  Acc@5: 95.8333 (96.6000)  time: 0.0862  data: 0.0004  max mem: 356\n",
            "Test: [Task 3] Total time: 0:00:04 (0.0986 s / it)\n",
            "* Acc@1 78.700 Acc@5 96.600 loss 0.864\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:20  Loss: 1.3636 (1.3636)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  time: 0.4870  data: 0.4062  max mem: 356\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:03  Loss: 0.8783 (0.9212)  Acc@1: 79.1667 (78.7879)  Acc@5: 95.8333 (97.3485)  time: 0.1246  data: 0.0374  max mem: 356\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 0.8946 (0.9259)  Acc@1: 79.1667 (77.7778)  Acc@5: 95.8333 (97.2222)  time: 0.0873  data: 0.0005  max mem: 356\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 0.8446 (0.8949)  Acc@1: 79.1667 (79.0323)  Acc@5: 95.8333 (97.1774)  time: 0.0875  data: 0.0008  max mem: 356\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.8446 (0.9136)  Acc@1: 79.1667 (78.6585)  Acc@5: 95.8333 (96.9512)  time: 0.0884  data: 0.0006  max mem: 356\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.8288 (0.9068)  Acc@1: 79.1667 (78.9000)  Acc@5: 95.8333 (97.0000)  time: 0.0862  data: 0.0005  max mem: 356\n",
            "Test: [Task 4] Total time: 0:00:04 (0.1002 s / it)\n",
            "* Acc@1 78.900 Acc@5 97.000 loss 0.907\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:30  Loss: 0.8234 (0.8234)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.7201  data: 0.6136  max mem: 356\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:04  Loss: 0.9460 (0.9591)  Acc@1: 79.1667 (78.4091)  Acc@5: 100.0000 (97.7273)  time: 0.1449  data: 0.0565  max mem: 356\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 0.9935 (0.9840)  Acc@1: 75.0000 (76.7857)  Acc@5: 95.8333 (96.8254)  time: 0.0893  data: 0.0032  max mem: 356\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 1.0145 (0.9967)  Acc@1: 75.0000 (76.7473)  Acc@5: 95.8333 (96.9086)  time: 0.0896  data: 0.0034  max mem: 356\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 1.0989 (1.0420)  Acc@1: 70.8333 (75.5081)  Acc@5: 95.8333 (96.2398)  time: 0.0883  data: 0.0007  max mem: 356\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 1.1199 (1.0555)  Acc@1: 70.8333 (75.4000)  Acc@5: 95.8333 (96.0000)  time: 0.0869  data: 0.0007  max mem: 356\n",
            "Test: [Task 5] Total time: 0:00:04 (0.1052 s / it)\n",
            "* Acc@1 75.400 Acc@5 96.000 loss 1.055\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:16  Loss: 3.5349 (3.5349)  Acc@1: 8.3333 (8.3333)  Acc@5: 37.5000 (37.5000)  time: 0.3915  data: 0.3137  max mem: 356\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:04  Loss: 3.7825 (3.7592)  Acc@1: 0.0000 (1.5152)  Acc@5: 41.6667 (42.0455)  time: 0.1252  data: 0.0389  max mem: 356\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 3.7707 (3.7565)  Acc@1: 0.0000 (1.7857)  Acc@5: 37.5000 (40.0794)  time: 0.0928  data: 0.0063  max mem: 356\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 3.7789 (3.7778)  Acc@1: 0.0000 (1.6129)  Acc@5: 37.5000 (39.9194)  time: 0.0877  data: 0.0007  max mem: 356\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 3.7851 (3.7757)  Acc@1: 0.0000 (1.4228)  Acc@5: 37.5000 (39.8374)  time: 0.0885  data: 0.0003  max mem: 356\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 3.7789 (3.7757)  Acc@1: 0.0000 (1.4000)  Acc@5: 37.5000 (39.8000)  time: 0.0866  data: 0.0003  max mem: 356\n",
            "Test: [Task 6] Total time: 0:00:04 (0.0990 s / it)\n",
            "* Acc@1 1.400 Acc@5 39.800 loss 3.776\n",
            "[Average accuracy till task6]\tAcc@1: 66.1333\tAcc@5: 87.3000\tLoss: 1.3733\tForgetting: 3.5600\tBackward: 32.4000\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([14376, 384])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 1.2573  Acc@1: 79.1667 (76.1667)  Acc@5: 95.8333 (93.7500)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000467  Loss: 0.7892  Acc@1: 83.3333 (81.5000)  Acc@5: 100.0000 (97.2500)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000464  Loss: 0.7623  Acc@1: 83.3333 (84.8333)  Acc@5: 100.0000 (99.0000)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 0.6324  Acc@1: 87.5000 (86.7500)  Acc@5: 100.0000 (99.3333)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000448  Loss: 0.7804  Acc@1: 87.5000 (88.0000)  Acc@5: 100.0000 (99.4167)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000437  Loss: 0.5263  Acc@1: 83.3333 (88.7500)  Acc@5: 100.0000 (99.6667)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 0.5472  Acc@1: 91.6667 (90.3333)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000409  Loss: 0.6483  Acc@1: 91.6667 (91.5000)  Acc@5: 100.0000 (99.8333)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000391  Loss: 0.5218  Acc@1: 91.6667 (89.9167)  Acc@5: 100.0000 (99.7500)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 0.6895  Acc@1: 87.5000 (90.5833)  Acc@5: 100.0000 (99.7500)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000352  Loss: 0.6179  Acc@1: 91.6667 (91.5833)  Acc@5: 100.0000 (99.8333)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000330  Loss: 0.5301  Acc@1: 91.6667 (91.5833)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 0.6428  Acc@1: 91.6667 (90.3333)  Acc@5: 100.0000 (99.8333)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000283  Loss: 0.6028  Acc@1: 91.6667 (92.0000)  Acc@5: 100.0000 (99.9167)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000259  Loss: 0.4855  Acc@1: 91.6667 (93.1667)  Acc@5: 100.0000 (99.6667)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 0.6019  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (99.7500)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000210  Loss: 0.4451  Acc@1: 91.6667 (90.0833)  Acc@5: 100.0000 (99.6667)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000186  Loss: 0.6018  Acc@1: 87.5000 (90.9167)  Acc@5: 100.0000 (99.7500)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.3687  Acc@1: 91.6667 (92.3333)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000139  Loss: 0.7353  Acc@1: 91.6667 (93.3333)  Acc@5: 100.0000 (99.9167)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000117  Loss: 0.6088  Acc@1: 91.6667 (91.9167)  Acc@5: 100.0000 (99.7500)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 0.5459  Acc@1: 91.6667 (92.2500)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000078  Loss: 0.3749  Acc@1: 91.6667 (92.8333)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000060  Loss: 0.5341  Acc@1: 95.8333 (93.8333)  Acc@5: 100.0000 (99.8333)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 0.6331  Acc@1: 91.6667 (92.9167)  Acc@5: 100.0000 (99.7500)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000031  Loss: 0.6664  Acc@1: 91.6667 (91.5833)  Acc@5: 100.0000 (99.7500)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000020  Loss: 0.5351  Acc@1: 91.6667 (92.2500)  Acc@5: 100.0000 (99.6667)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.4987  Acc@1: 91.6667 (92.2500)  Acc@5: 100.0000 (99.7500)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000005  Loss: 0.2740  Acc@1: 91.6667 (91.9167)  Acc@5: 100.0000 (99.9167)\n",
            "torch.Size([14376, 384])\n",
            "Averaged stats: Lr: 0.000001  Loss: 0.5750  Acc@1: 91.6667 (92.5833)  Acc@5: 100.0000 (99.7500)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:23  Loss: 0.8323 (0.8323)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  time: 0.5705  data: 0.4949  max mem: 357\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 0.7910 (0.7501)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (97.7273)  time: 0.1342  data: 0.0459  max mem: 357\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.7274 (0.7417)  Acc@1: 79.1667 (79.7619)  Acc@5: 100.0000 (97.2222)  time: 0.0897  data: 0.0007  max mem: 357\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.6716 (0.7108)  Acc@1: 79.1667 (80.7796)  Acc@5: 100.0000 (97.7151)  time: 0.0892  data: 0.0004  max mem: 357\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6219 (0.6901)  Acc@1: 83.3333 (81.7073)  Acc@5: 100.0000 (97.8659)  time: 0.0904  data: 0.0004  max mem: 357\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6114 (0.6816)  Acc@1: 83.3333 (81.9000)  Acc@5: 100.0000 (97.9000)  time: 0.0886  data: 0.0003  max mem: 357\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1037 s / it)\n",
            "* Acc@1 81.900 Acc@5 97.900 loss 0.682\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:26  Loss: 0.9697 (0.9697)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  time: 0.6373  data: 0.5347  max mem: 357\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 0.8506 (0.8195)  Acc@1: 79.1667 (79.9242)  Acc@5: 95.8333 (96.2121)  time: 0.1482  data: 0.0624  max mem: 357\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 0.8506 (0.8613)  Acc@1: 79.1667 (77.7778)  Acc@5: 95.8333 (95.0397)  time: 0.0990  data: 0.0128  max mem: 357\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.8171 (0.8484)  Acc@1: 75.0000 (78.2258)  Acc@5: 95.8333 (95.2957)  time: 0.1013  data: 0.0144  max mem: 357\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.7510 (0.8254)  Acc@1: 75.0000 (78.3537)  Acc@5: 95.8333 (95.6301)  time: 0.0981  data: 0.0094  max mem: 357\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.7399 (0.8203)  Acc@1: 75.0000 (78.5000)  Acc@5: 95.8333 (95.7000)  time: 0.0964  data: 0.0090  max mem: 357\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1128 s / it)\n",
            "* Acc@1 78.500 Acc@5 95.700 loss 0.820\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:21  Loss: 0.5018 (0.5018)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.5023  data: 0.4189  max mem: 357\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:04  Loss: 0.7554 (0.7847)  Acc@1: 75.0000 (77.2727)  Acc@5: 95.8333 (96.5909)  time: 0.1293  data: 0.0392  max mem: 357\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 0.7401 (0.7271)  Acc@1: 79.1667 (79.9603)  Acc@5: 95.8333 (96.6270)  time: 0.0914  data: 0.0008  max mem: 357\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.6308 (0.7001)  Acc@1: 79.1667 (80.1075)  Acc@5: 95.8333 (97.0430)  time: 0.0908  data: 0.0004  max mem: 357\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.6912 (0.7134)  Acc@1: 79.1667 (80.9959)  Acc@5: 95.8333 (96.8496)  time: 0.0916  data: 0.0004  max mem: 357\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.6912 (0.7180)  Acc@1: 79.1667 (80.9000)  Acc@5: 95.8333 (96.9000)  time: 0.0901  data: 0.0004  max mem: 357\n",
            "Test: [Task 3] Total time: 0:00:04 (0.1029 s / it)\n",
            "* Acc@1 80.900 Acc@5 96.900 loss 0.718\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:17  Loss: 1.1513 (1.1513)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  time: 0.4224  data: 0.3436  max mem: 357\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:03  Loss: 0.7962 (0.8059)  Acc@1: 79.1667 (79.5455)  Acc@5: 95.8333 (97.3485)  time: 0.1246  data: 0.0339  max mem: 357\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 0.7817 (0.7951)  Acc@1: 79.1667 (79.7619)  Acc@5: 95.8333 (97.0238)  time: 0.0934  data: 0.0018  max mem: 357\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 0.7202 (0.7580)  Acc@1: 79.1667 (80.9140)  Acc@5: 95.8333 (97.1774)  time: 0.0920  data: 0.0006  max mem: 357\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.6995 (0.7775)  Acc@1: 79.1667 (79.9797)  Acc@5: 95.8333 (97.0528)  time: 0.0921  data: 0.0004  max mem: 357\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.6800 (0.7702)  Acc@1: 79.1667 (80.2000)  Acc@5: 95.8333 (97.1000)  time: 0.0905  data: 0.0004  max mem: 357\n",
            "Test: [Task 4] Total time: 0:00:04 (0.1019 s / it)\n",
            "* Acc@1 80.200 Acc@5 97.100 loss 0.770\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:29  Loss: 0.7115 (0.7115)  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  time: 0.7132  data: 0.5735  max mem: 357\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:04  Loss: 0.8082 (0.8031)  Acc@1: 75.0000 (76.8939)  Acc@5: 100.0000 (98.4848)  time: 0.1496  data: 0.0541  max mem: 357\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 0.8303 (0.8381)  Acc@1: 75.0000 (77.1825)  Acc@5: 95.8333 (97.0238)  time: 0.0935  data: 0.0035  max mem: 357\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 0.8466 (0.8448)  Acc@1: 79.1667 (77.6882)  Acc@5: 95.8333 (97.0430)  time: 0.0950  data: 0.0058  max mem: 357\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.9302 (0.8777)  Acc@1: 75.0000 (76.8293)  Acc@5: 95.8333 (96.6463)  time: 0.0940  data: 0.0036  max mem: 357\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.9416 (0.8911)  Acc@1: 75.0000 (76.6000)  Acc@5: 95.8333 (96.5000)  time: 0.0918  data: 0.0022  max mem: 357\n",
            "Test: [Task 5] Total time: 0:00:04 (0.1099 s / it)\n",
            "* Acc@1 76.600 Acc@5 96.500 loss 0.891\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:14  Loss: 0.8168 (0.8168)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  time: 0.3414  data: 0.2696  max mem: 357\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:03  Loss: 0.9837 (1.0289)  Acc@1: 70.8333 (71.5909)  Acc@5: 95.8333 (95.4545)  time: 0.1245  data: 0.0379  max mem: 357\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 1.0276 (1.0354)  Acc@1: 70.8333 (72.0238)  Acc@5: 95.8333 (95.6349)  time: 0.0959  data: 0.0075  max mem: 357\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 1.0502 (1.0544)  Acc@1: 70.8333 (71.6398)  Acc@5: 95.8333 (96.1022)  time: 0.0902  data: 0.0003  max mem: 357\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.0520 (1.0612)  Acc@1: 70.8333 (71.8496)  Acc@5: 95.8333 (95.8333)  time: 0.0912  data: 0.0003  max mem: 357\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.0520 (1.0627)  Acc@1: 70.8333 (71.7000)  Acc@5: 95.8333 (95.9000)  time: 0.0888  data: 0.0003  max mem: 357\n",
            "Test: [Task 6] Total time: 0:00:04 (0.1007 s / it)\n",
            "* Acc@1 71.700 Acc@5 95.900 loss 1.063\n",
            "[Average accuracy till task6]\tAcc@1: 78.3000\tAcc@5: 96.6667\tLoss: 0.8240\tForgetting: 3.9400\tBackward: -3.6000\n",
            "Train: Epoch[1/1]  [  0/209]  eta: 0:01:46  Lr: 0.000047  Loss: 2.4092  Acc@1: 12.5000 (12.5000)  Acc@5: 41.6667 (41.6667)  time: 0.5117  data: 0.4001  max mem: 357\n",
            "Train: Epoch[1/1]  [ 10/209]  eta: 0:00:26  Lr: 0.000047  Loss: 2.2341  Acc@1: 12.5000 (13.6364)  Acc@5: 50.0000 (51.5152)  time: 0.1321  data: 0.0382  max mem: 357\n",
            "Train: Epoch[1/1]  [ 20/209]  eta: 0:00:21  Lr: 0.000047  Loss: 2.0885  Acc@1: 20.8333 (19.2460)  Acc@5: 58.3333 (59.1270)  time: 0.0921  data: 0.0014  max mem: 357\n",
            "Train: Epoch[1/1]  [ 30/209]  eta: 0:00:18  Lr: 0.000047  Loss: 2.1899  Acc@1: 25.0000 (22.1774)  Acc@5: 70.8333 (63.4409)  time: 0.0904  data: 0.0010  max mem: 357\n",
            "Train: Epoch[1/1]  [ 40/209]  eta: 0:00:17  Lr: 0.000047  Loss: 2.0010  Acc@1: 29.1667 (26.0163)  Acc@5: 79.1667 (67.8862)  time: 0.0911  data: 0.0012  max mem: 357\n",
            "Train: Epoch[1/1]  [ 50/209]  eta: 0:00:15  Lr: 0.000047  Loss: 1.8410  Acc@1: 45.8333 (30.2288)  Acc@5: 83.3333 (71.8954)  time: 0.0909  data: 0.0011  max mem: 357\n",
            "Train: Epoch[1/1]  [ 60/209]  eta: 0:00:14  Lr: 0.000047  Loss: 1.8255  Acc@1: 41.6667 (32.1721)  Acc@5: 87.5000 (74.6585)  time: 0.0941  data: 0.0011  max mem: 357\n",
            "Train: Epoch[1/1]  [ 70/209]  eta: 0:00:13  Lr: 0.000047  Loss: 1.4394  Acc@1: 45.8333 (35.6221)  Acc@5: 91.6667 (77.6995)  time: 0.0998  data: 0.0015  max mem: 357\n",
            "Train: Epoch[1/1]  [ 80/209]  eta: 0:00:12  Lr: 0.000047  Loss: 1.5700  Acc@1: 62.5000 (38.8374)  Acc@5: 95.8333 (79.8868)  time: 0.1034  data: 0.0016  max mem: 357\n",
            "Train: Epoch[1/1]  [ 90/209]  eta: 0:00:11  Lr: 0.000047  Loss: 1.4118  Acc@1: 58.3333 (40.8425)  Acc@5: 95.8333 (81.5476)  time: 0.1043  data: 0.0012  max mem: 357\n",
            "Train: Epoch[1/1]  [100/209]  eta: 0:00:11  Lr: 0.000047  Loss: 1.5959  Acc@1: 58.3333 (42.8630)  Acc@5: 95.8333 (82.9208)  time: 0.1066  data: 0.0017  max mem: 357\n",
            "Train: Epoch[1/1]  [110/209]  eta: 0:00:10  Lr: 0.000047  Loss: 1.3758  Acc@1: 62.5000 (44.4820)  Acc@5: 95.8333 (84.1967)  time: 0.1043  data: 0.0020  max mem: 357\n",
            "Train: Epoch[1/1]  [120/209]  eta: 0:00:08  Lr: 0.000047  Loss: 1.1860  Acc@1: 62.5000 (46.4876)  Acc@5: 95.8333 (85.2961)  time: 0.0964  data: 0.0020  max mem: 357\n",
            "Train: Epoch[1/1]  [130/209]  eta: 0:00:07  Lr: 0.000047  Loss: 1.2671  Acc@1: 70.8333 (48.1870)  Acc@5: 95.8333 (86.1323)  time: 0.0914  data: 0.0016  max mem: 357\n",
            "Train: Epoch[1/1]  [140/209]  eta: 0:00:06  Lr: 0.000047  Loss: 1.0473  Acc@1: 75.0000 (50.0296)  Acc@5: 100.0000 (87.0272)  time: 0.0894  data: 0.0008  max mem: 357\n",
            "Train: Epoch[1/1]  [150/209]  eta: 0:00:05  Lr: 0.000047  Loss: 1.1717  Acc@1: 70.8333 (51.3797)  Acc@5: 95.8333 (87.6104)  time: 0.0899  data: 0.0010  max mem: 357\n",
            "Train: Epoch[1/1]  [160/209]  eta: 0:00:04  Lr: 0.000047  Loss: 1.0154  Acc@1: 70.8333 (52.5880)  Acc@5: 100.0000 (88.3023)  time: 0.0900  data: 0.0012  max mem: 357\n",
            "Train: Epoch[1/1]  [170/209]  eta: 0:00:03  Lr: 0.000047  Loss: 1.0104  Acc@1: 70.8333 (53.7524)  Acc@5: 95.8333 (88.6940)  time: 0.0895  data: 0.0010  max mem: 357\n",
            "Train: Epoch[1/1]  [180/209]  eta: 0:00:02  Lr: 0.000047  Loss: 0.8312  Acc@1: 70.8333 (54.8573)  Acc@5: 95.8333 (89.2035)  time: 0.0891  data: 0.0005  max mem: 357\n",
            "Train: Epoch[1/1]  [190/209]  eta: 0:00:01  Lr: 0.000047  Loss: 0.9608  Acc@1: 70.8333 (55.8682)  Acc@5: 100.0000 (89.7033)  time: 0.0898  data: 0.0004  max mem: 357\n",
            "Train: Epoch[1/1]  [200/209]  eta: 0:00:00  Lr: 0.000047  Loss: 1.0434  Acc@1: 70.8333 (56.7371)  Acc@5: 100.0000 (90.0705)  time: 0.0897  data: 0.0005  max mem: 357\n",
            "Train: Epoch[1/1]  [208/209]  eta: 0:00:00  Lr: 0.000047  Loss: 0.9515  Acc@1: 75.0000 (57.4200)  Acc@5: 100.0000 (90.3800)  time: 0.0864  data: 0.0004  max mem: 357\n",
            "Train: Epoch[1/1] Total time: 0:00:20 (0.0966 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.9515  Acc@1: 75.0000 (57.4200)  Acc@5: 100.0000 (90.3800)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:23  Loss: 0.9510 (0.9510)  Acc@1: 66.6667 (66.6667)  Acc@5: 91.6667 (91.6667)  time: 0.5531  data: 0.4407  max mem: 357\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 0.8847 (0.8372)  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (96.9697)  time: 0.1463  data: 0.0592  max mem: 357\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.8194 (0.8324)  Acc@1: 79.1667 (77.7778)  Acc@5: 95.8333 (96.4286)  time: 0.0992  data: 0.0140  max mem: 357\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.7460 (0.8041)  Acc@1: 79.1667 (78.4946)  Acc@5: 95.8333 (96.7742)  time: 0.0956  data: 0.0093  max mem: 357\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.7357 (0.7788)  Acc@1: 83.3333 (79.1667)  Acc@5: 100.0000 (97.3577)  time: 0.0936  data: 0.0062  max mem: 357\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.7026 (0.7693)  Acc@1: 83.3333 (79.4000)  Acc@5: 100.0000 (97.4000)  time: 0.0919  data: 0.0061  max mem: 357\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1088 s / it)\n",
            "* Acc@1 79.400 Acc@5 97.400 loss 0.769\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:16  Loss: 1.0067 (1.0067)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  time: 0.3945  data: 0.3086  max mem: 357\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:03  Loss: 0.9023 (0.8853)  Acc@1: 79.1667 (76.1364)  Acc@5: 95.8333 (96.2121)  time: 0.1193  data: 0.0356  max mem: 357\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 0.9023 (0.9285)  Acc@1: 75.0000 (76.5873)  Acc@5: 95.8333 (95.0397)  time: 0.0902  data: 0.0046  max mem: 357\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.9151 (0.9183)  Acc@1: 75.0000 (76.3441)  Acc@5: 95.8333 (95.1613)  time: 0.0888  data: 0.0006  max mem: 357\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.8292 (0.9006)  Acc@1: 75.0000 (76.7276)  Acc@5: 95.8333 (95.5285)  time: 0.0887  data: 0.0003  max mem: 357\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.8191 (0.8944)  Acc@1: 75.0000 (76.9000)  Acc@5: 95.8333 (95.6000)  time: 0.0870  data: 0.0003  max mem: 357\n",
            "Test: [Task 2] Total time: 0:00:04 (0.0982 s / it)\n",
            "* Acc@1 76.900 Acc@5 95.600 loss 0.894\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:17  Loss: 0.5437 (0.5437)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.4127  data: 0.3369  max mem: 357\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:03  Loss: 0.8357 (0.8385)  Acc@1: 75.0000 (75.7576)  Acc@5: 95.8333 (95.4545)  time: 0.1243  data: 0.0401  max mem: 357\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 0.7873 (0.8013)  Acc@1: 75.0000 (77.9762)  Acc@5: 95.8333 (95.4365)  time: 0.0914  data: 0.0054  max mem: 357\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.7543 (0.7710)  Acc@1: 79.1667 (78.6290)  Acc@5: 95.8333 (95.9677)  time: 0.0878  data: 0.0007  max mem: 357\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.7559 (0.7837)  Acc@1: 79.1667 (78.7602)  Acc@5: 95.8333 (95.5285)  time: 0.0886  data: 0.0006  max mem: 357\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.7593 (0.7854)  Acc@1: 79.1667 (78.7000)  Acc@5: 95.8333 (95.6000)  time: 0.0867  data: 0.0006  max mem: 357\n",
            "Test: [Task 3] Total time: 0:00:04 (0.0990 s / it)\n",
            "* Acc@1 78.700 Acc@5 95.600 loss 0.785\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:15  Loss: 1.3339 (1.3339)  Acc@1: 66.6667 (66.6667)  Acc@5: 87.5000 (87.5000)  time: 0.3725  data: 0.2966  max mem: 357\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:03  Loss: 0.8000 (0.8494)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (96.5909)  time: 0.1154  data: 0.0310  max mem: 357\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 0.8368 (0.8485)  Acc@1: 79.1667 (78.3730)  Acc@5: 95.8333 (96.4286)  time: 0.0888  data: 0.0028  max mem: 357\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 0.8173 (0.8166)  Acc@1: 79.1667 (80.2419)  Acc@5: 95.8333 (96.5054)  time: 0.0883  data: 0.0009  max mem: 357\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.8173 (0.8380)  Acc@1: 79.1667 (79.8781)  Acc@5: 95.8333 (96.4431)  time: 0.0882  data: 0.0006  max mem: 357\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.7349 (0.8315)  Acc@1: 79.1667 (80.1000)  Acc@5: 95.8333 (96.5000)  time: 0.0870  data: 0.0006  max mem: 357\n",
            "Test: [Task 4] Total time: 0:00:04 (0.0986 s / it)\n",
            "* Acc@1 80.100 Acc@5 96.500 loss 0.832\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:30  Loss: 0.7851 (0.7851)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.7245  data: 0.6117  max mem: 357\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:04  Loss: 0.8269 (0.8385)  Acc@1: 79.1667 (79.5455)  Acc@5: 100.0000 (98.4848)  time: 0.1475  data: 0.0566  max mem: 357\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 0.8666 (0.8783)  Acc@1: 75.0000 (77.5794)  Acc@5: 95.8333 (97.2222)  time: 0.0884  data: 0.0013  max mem: 357\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 0.8941 (0.8809)  Acc@1: 79.1667 (77.1505)  Acc@5: 95.8333 (97.5806)  time: 0.0876  data: 0.0009  max mem: 357\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.9416 (0.9086)  Acc@1: 75.0000 (76.5244)  Acc@5: 95.8333 (97.3577)  time: 0.0885  data: 0.0004  max mem: 357\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.9570 (0.9205)  Acc@1: 70.8333 (76.4000)  Acc@5: 95.8333 (97.2000)  time: 0.0866  data: 0.0004  max mem: 357\n",
            "Test: [Task 5] Total time: 0:00:04 (0.1051 s / it)\n",
            "* Acc@1 76.400 Acc@5 97.200 loss 0.921\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:21  Loss: 0.8636 (0.8636)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  time: 0.5087  data: 0.4402  max mem: 357\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:04  Loss: 1.1662 (1.1886)  Acc@1: 66.6667 (67.4242)  Acc@5: 91.6667 (93.5606)  time: 0.1273  data: 0.0409  max mem: 357\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 1.1914 (1.1962)  Acc@1: 66.6667 (68.0556)  Acc@5: 91.6667 (93.4524)  time: 0.0876  data: 0.0010  max mem: 357\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 1.2763 (1.2214)  Acc@1: 66.6667 (67.3387)  Acc@5: 91.6667 (93.2796)  time: 0.0871  data: 0.0008  max mem: 357\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.3082 (1.2189)  Acc@1: 66.6667 (67.4797)  Acc@5: 95.8333 (94.0041)  time: 0.0884  data: 0.0004  max mem: 357\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.2763 (1.2199)  Acc@1: 66.6667 (67.4000)  Acc@5: 95.8333 (94.0000)  time: 0.0865  data: 0.0004  max mem: 357\n",
            "Test: [Task 6] Total time: 0:00:04 (0.0994 s / it)\n",
            "* Acc@1 67.400 Acc@5 94.000 loss 1.220\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:22  Loss: 4.0407 (4.0407)  Acc@1: 0.0000 (0.0000)  Acc@5: 37.5000 (37.5000)  time: 0.5263  data: 0.4477  max mem: 357\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:04  Loss: 3.9043 (3.9061)  Acc@1: 0.0000 (1.1364)  Acc@5: 29.1667 (29.5455)  time: 0.1284  data: 0.0417  max mem: 357\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:02  Loss: 3.8387 (3.8691)  Acc@1: 0.0000 (0.9921)  Acc@5: 29.1667 (29.1667)  time: 0.0877  data: 0.0009  max mem: 357\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:01  Loss: 3.8438 (3.8722)  Acc@1: 0.0000 (1.2097)  Acc@5: 29.1667 (28.8979)  time: 0.0873  data: 0.0006  max mem: 357\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 3.8519 (3.8808)  Acc@1: 0.0000 (1.1179)  Acc@5: 29.1667 (29.1667)  time: 0.0886  data: 0.0005  max mem: 357\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 3.8466 (3.8688)  Acc@1: 0.0000 (1.2000)  Acc@5: 29.1667 (29.3000)  time: 0.0867  data: 0.0005  max mem: 357\n",
            "Test: [Task 7] Total time: 0:00:04 (0.1017 s / it)\n",
            "* Acc@1 1.200 Acc@5 29.300 loss 3.869\n",
            "[Average accuracy till task7]\tAcc@1: 65.7286\tAcc@5: 86.5143\tLoss: 1.3271\tForgetting: 3.9833\tBackward: 37.3500\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([16776, 384])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 0.9999  Acc@1: 75.0000 (77.0139)  Acc@5: 95.8333 (93.4028)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000467  Loss: 0.5549  Acc@1: 83.3333 (81.3194)  Acc@5: 100.0000 (97.7778)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000464  Loss: 0.8765  Acc@1: 83.3333 (85.2083)  Acc@5: 100.0000 (98.6806)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 0.8536  Acc@1: 91.6667 (87.7083)  Acc@5: 100.0000 (99.3750)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000448  Loss: 0.8380  Acc@1: 87.5000 (89.7917)  Acc@5: 100.0000 (99.2361)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000437  Loss: 0.5642  Acc@1: 91.6667 (89.5833)  Acc@5: 100.0000 (99.4444)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 0.6726  Acc@1: 87.5000 (89.6528)  Acc@5: 100.0000 (99.5139)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000409  Loss: 0.5188  Acc@1: 87.5000 (89.9306)  Acc@5: 100.0000 (99.5139)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000391  Loss: 0.4918  Acc@1: 91.6667 (91.2500)  Acc@5: 100.0000 (99.8611)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 0.6548  Acc@1: 91.6667 (89.7917)  Acc@5: 100.0000 (99.5139)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000352  Loss: 0.6096  Acc@1: 91.6667 (91.8750)  Acc@5: 100.0000 (99.7222)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000330  Loss: 0.5004  Acc@1: 91.6667 (93.1250)  Acc@5: 100.0000 (99.5139)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 0.6071  Acc@1: 91.6667 (91.2500)  Acc@5: 100.0000 (99.7222)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000283  Loss: 0.5807  Acc@1: 91.6667 (92.0139)  Acc@5: 100.0000 (99.7222)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000259  Loss: 0.3955  Acc@1: 95.8333 (91.8750)  Acc@5: 100.0000 (99.6528)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 0.4909  Acc@1: 95.8333 (92.0833)  Acc@5: 100.0000 (99.6528)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000210  Loss: 0.3628  Acc@1: 91.6667 (91.1111)  Acc@5: 100.0000 (99.7222)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000186  Loss: 0.2804  Acc@1: 95.8333 (91.9444)  Acc@5: 100.0000 (99.7222)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.3518  Acc@1: 95.8333 (92.3611)  Acc@5: 100.0000 (99.6528)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000139  Loss: 0.5156  Acc@1: 91.6667 (92.0139)  Acc@5: 100.0000 (99.8611)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000117  Loss: 0.3877  Acc@1: 91.6667 (93.0556)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 0.4953  Acc@1: 91.6667 (92.8472)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000078  Loss: 0.2932  Acc@1: 95.8333 (92.3611)  Acc@5: 100.0000 (99.8611)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000060  Loss: 0.2860  Acc@1: 87.5000 (92.2917)  Acc@5: 100.0000 (99.8611)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 0.3869  Acc@1: 95.8333 (94.3750)  Acc@5: 100.0000 (99.9306)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000031  Loss: 0.4553  Acc@1: 95.8333 (92.5694)  Acc@5: 100.0000 (100.0000)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000020  Loss: 0.4808  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (99.6528)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.4328  Acc@1: 91.6667 (91.5972)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000005  Loss: 0.4483  Acc@1: 91.6667 (92.3611)  Acc@5: 100.0000 (99.9306)\n",
            "torch.Size([16776, 384])\n",
            "Averaged stats: Lr: 0.000001  Loss: 0.5538  Acc@1: 91.6667 (92.0833)  Acc@5: 100.0000 (99.6528)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:23  Loss: 0.8434 (0.8434)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  time: 0.5596  data: 0.4841  max mem: 358\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 0.7886 (0.7462)  Acc@1: 79.1667 (77.2727)  Acc@5: 100.0000 (97.7273)  time: 0.1336  data: 0.0448  max mem: 358\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.7342 (0.7395)  Acc@1: 79.1667 (79.5635)  Acc@5: 95.8333 (97.2222)  time: 0.0901  data: 0.0007  max mem: 358\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.6946 (0.7061)  Acc@1: 83.3333 (80.2419)  Acc@5: 95.8333 (97.5806)  time: 0.0896  data: 0.0004  max mem: 358\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6074 (0.6819)  Acc@1: 83.3333 (80.8943)  Acc@5: 95.8333 (97.5610)  time: 0.0905  data: 0.0003  max mem: 358\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6020 (0.6728)  Acc@1: 83.3333 (81.1000)  Acc@5: 95.8333 (97.6000)  time: 0.0889  data: 0.0003  max mem: 358\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1030 s / it)\n",
            "* Acc@1 81.100 Acc@5 97.600 loss 0.673\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:25  Loss: 0.9648 (0.9648)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  time: 0.6011  data: 0.4802  max mem: 358\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 0.8293 (0.8194)  Acc@1: 79.1667 (77.6515)  Acc@5: 95.8333 (95.8333)  time: 0.1371  data: 0.0467  max mem: 358\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 0.8293 (0.8554)  Acc@1: 79.1667 (76.9841)  Acc@5: 95.8333 (94.8413)  time: 0.0995  data: 0.0124  max mem: 358\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.8613 (0.8482)  Acc@1: 75.0000 (77.1505)  Acc@5: 95.8333 (95.1613)  time: 0.1027  data: 0.0136  max mem: 358\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.7522 (0.8272)  Acc@1: 79.1667 (77.5407)  Acc@5: 95.8333 (95.5285)  time: 0.0940  data: 0.0031  max mem: 358\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.7460 (0.8220)  Acc@1: 79.1667 (77.6000)  Acc@5: 95.8333 (95.6000)  time: 0.0924  data: 0.0030  max mem: 358\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1104 s / it)\n",
            "* Acc@1 77.600 Acc@5 95.600 loss 0.822\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:23  Loss: 0.5278 (0.5278)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.5501  data: 0.4549  max mem: 358\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:04  Loss: 0.7307 (0.7798)  Acc@1: 75.0000 (77.2727)  Acc@5: 95.8333 (96.5909)  time: 0.1332  data: 0.0425  max mem: 358\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 0.7170 (0.7196)  Acc@1: 79.1667 (80.1587)  Acc@5: 95.8333 (96.4286)  time: 0.0911  data: 0.0008  max mem: 358\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.6604 (0.6934)  Acc@1: 83.3333 (80.2419)  Acc@5: 95.8333 (96.7742)  time: 0.0915  data: 0.0003  max mem: 358\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.6732 (0.7110)  Acc@1: 79.1667 (80.4878)  Acc@5: 95.8333 (96.3415)  time: 0.0923  data: 0.0003  max mem: 358\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.6732 (0.7128)  Acc@1: 79.1667 (80.3000)  Acc@5: 95.8333 (96.4000)  time: 0.0903  data: 0.0003  max mem: 358\n",
            "Test: [Task 3] Total time: 0:00:04 (0.1044 s / it)\n",
            "* Acc@1 80.300 Acc@5 96.400 loss 0.713\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:19  Loss: 1.1881 (1.1881)  Acc@1: 66.6667 (66.6667)  Acc@5: 87.5000 (87.5000)  time: 0.4751  data: 0.3913  max mem: 358\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:04  Loss: 0.8095 (0.8363)  Acc@1: 75.0000 (76.5152)  Acc@5: 95.8333 (96.5909)  time: 0.1276  data: 0.0366  max mem: 358\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 0.8095 (0.8176)  Acc@1: 79.1667 (77.3810)  Acc@5: 95.8333 (96.4286)  time: 0.0921  data: 0.0007  max mem: 358\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 0.7513 (0.7700)  Acc@1: 79.1667 (78.8979)  Acc@5: 95.8333 (96.9086)  time: 0.0916  data: 0.0003  max mem: 358\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7513 (0.7866)  Acc@1: 75.0000 (78.1504)  Acc@5: 95.8333 (96.7480)  time: 0.0920  data: 0.0003  max mem: 358\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.7238 (0.7790)  Acc@1: 75.0000 (78.3000)  Acc@5: 95.8333 (96.8000)  time: 0.0904  data: 0.0003  max mem: 358\n",
            "Test: [Task 4] Total time: 0:00:04 (0.1027 s / it)\n",
            "* Acc@1 78.300 Acc@5 96.800 loss 0.779\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:16  Loss: 0.6951 (0.6951)  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  time: 0.3977  data: 0.3158  max mem: 358\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:03  Loss: 0.7972 (0.7697)  Acc@1: 75.0000 (77.6515)  Acc@5: 100.0000 (98.1061)  time: 0.1237  data: 0.0360  max mem: 358\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 0.7985 (0.8001)  Acc@1: 79.1667 (76.5873)  Acc@5: 95.8333 (96.8254)  time: 0.0941  data: 0.0044  max mem: 358\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 0.7888 (0.8016)  Acc@1: 79.1667 (77.2849)  Acc@5: 95.8333 (96.5054)  time: 0.0926  data: 0.0007  max mem: 358\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.8582 (0.8327)  Acc@1: 79.1667 (76.5244)  Acc@5: 95.8333 (96.5447)  time: 0.0926  data: 0.0005  max mem: 358\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.8771 (0.8464)  Acc@1: 75.0000 (76.3000)  Acc@5: 95.8333 (96.4000)  time: 0.0910  data: 0.0004  max mem: 358\n",
            "Test: [Task 5] Total time: 0:00:04 (0.1030 s / it)\n",
            "* Acc@1 76.300 Acc@5 96.400 loss 0.846\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:32  Loss: 0.7494 (0.7494)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  time: 0.7806  data: 0.6662  max mem: 358\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:04  Loss: 0.8807 (0.9333)  Acc@1: 75.0000 (73.1061)  Acc@5: 95.8333 (95.0758)  time: 0.1543  data: 0.0617  max mem: 358\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 0.9529 (0.9357)  Acc@1: 75.0000 (73.8095)  Acc@5: 95.8333 (95.4365)  time: 0.0914  data: 0.0009  max mem: 358\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 0.9552 (0.9560)  Acc@1: 70.8333 (73.5215)  Acc@5: 95.8333 (95.4301)  time: 0.0908  data: 0.0006  max mem: 358\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 0.9665 (0.9633)  Acc@1: 70.8333 (73.7805)  Acc@5: 95.8333 (95.6301)  time: 0.0908  data: 0.0004  max mem: 358\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 0.9665 (0.9665)  Acc@1: 70.8333 (73.6000)  Acc@5: 95.8333 (95.6000)  time: 0.0891  data: 0.0003  max mem: 358\n",
            "Test: [Task 6] Total time: 0:00:04 (0.1092 s / it)\n",
            "* Acc@1 73.600 Acc@5 95.600 loss 0.967\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:15  Loss: 1.1675 (1.1675)  Acc@1: 75.0000 (75.0000)  Acc@5: 87.5000 (87.5000)  time: 0.3577  data: 0.2778  max mem: 358\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:03  Loss: 0.9855 (1.0384)  Acc@1: 70.8333 (71.2121)  Acc@5: 95.8333 (95.0758)  time: 0.1223  data: 0.0368  max mem: 358\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:02  Loss: 0.9480 (1.0471)  Acc@1: 70.8333 (69.8413)  Acc@5: 95.8333 (94.8413)  time: 0.0946  data: 0.0067  max mem: 358\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:01  Loss: 0.9800 (1.0741)  Acc@1: 66.6667 (69.4892)  Acc@5: 95.8333 (93.9516)  time: 0.0907  data: 0.0005  max mem: 358\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 0.9964 (1.0825)  Acc@1: 66.6667 (69.8171)  Acc@5: 91.6667 (93.4959)  time: 0.0910  data: 0.0003  max mem: 358\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 0.9964 (1.0795)  Acc@1: 68.7500 (69.8000)  Acc@5: 91.6667 (93.5000)  time: 0.0889  data: 0.0003  max mem: 358\n",
            "Test: [Task 7] Total time: 0:00:04 (0.1004 s / it)\n",
            "* Acc@1 69.800 Acc@5 93.500 loss 1.080\n",
            "[Average accuracy till task7]\tAcc@1: 76.7143\tAcc@5: 95.9857\tLoss: 0.8399\tForgetting: 4.0333\tBackward: -3.4333\n",
            "Train: Epoch[1/1]  [  0/209]  eta: 0:01:49  Lr: 0.000047  Loss: 2.3245  Acc@1: 16.6667 (16.6667)  Acc@5: 54.1667 (54.1667)  time: 0.5257  data: 0.4016  max mem: 358\n",
            "Train: Epoch[1/1]  [ 10/209]  eta: 0:00:27  Lr: 0.000047  Loss: 2.0969  Acc@1: 16.6667 (15.9091)  Acc@5: 58.3333 (59.8485)  time: 0.1368  data: 0.0413  max mem: 358\n",
            "Train: Epoch[1/1]  [ 20/209]  eta: 0:00:21  Lr: 0.000047  Loss: 2.0314  Acc@1: 20.8333 (22.8175)  Acc@5: 66.6667 (65.6746)  time: 0.0954  data: 0.0034  max mem: 358\n",
            "Train: Epoch[1/1]  [ 30/209]  eta: 0:00:20  Lr: 0.000047  Loss: 2.0220  Acc@1: 33.3333 (27.1505)  Acc@5: 75.0000 (71.2366)  time: 0.0986  data: 0.0025  max mem: 358\n",
            "Train: Epoch[1/1]  [ 40/209]  eta: 0:00:19  Lr: 0.000047  Loss: 1.9031  Acc@1: 37.5000 (31.0976)  Acc@5: 87.5000 (75.1016)  time: 0.1090  data: 0.0022  max mem: 358\n",
            "Train: Epoch[1/1]  [ 50/209]  eta: 0:00:17  Lr: 0.000047  Loss: 1.7064  Acc@1: 45.8333 (34.9673)  Acc@5: 87.5000 (78.4314)  time: 0.1097  data: 0.0012  max mem: 358\n",
            "Train: Epoch[1/1]  [ 60/209]  eta: 0:00:16  Lr: 0.000047  Loss: 1.8444  Acc@1: 50.0000 (38.5929)  Acc@5: 95.8333 (80.8743)  time: 0.1038  data: 0.0016  max mem: 358\n",
            "Train: Epoch[1/1]  [ 70/209]  eta: 0:00:15  Lr: 0.000047  Loss: 1.3813  Acc@1: 58.3333 (41.7840)  Acc@5: 95.8333 (82.8639)  time: 0.1000  data: 0.0023  max mem: 358\n",
            "Train: Epoch[1/1]  [ 80/209]  eta: 0:00:13  Lr: 0.000047  Loss: 1.4733  Acc@1: 58.3333 (44.0329)  Acc@5: 95.8333 (84.1564)  time: 0.0945  data: 0.0020  max mem: 358\n",
            "Train: Epoch[1/1]  [ 90/209]  eta: 0:00:12  Lr: 0.000047  Loss: 1.5539  Acc@1: 58.3333 (46.6575)  Acc@5: 95.8333 (85.3938)  time: 0.0907  data: 0.0011  max mem: 358\n",
            "Train: Epoch[1/1]  [100/209]  eta: 0:00:11  Lr: 0.000047  Loss: 1.3142  Acc@1: 70.8333 (48.9274)  Acc@5: 95.8333 (86.5924)  time: 0.0914  data: 0.0008  max mem: 358\n",
            "Train: Epoch[1/1]  [110/209]  eta: 0:00:10  Lr: 0.000047  Loss: 1.5309  Acc@1: 70.8333 (50.7132)  Acc@5: 95.8333 (87.5000)  time: 0.0931  data: 0.0010  max mem: 358\n",
            "Train: Epoch[1/1]  [120/209]  eta: 0:00:09  Lr: 0.000047  Loss: 1.0607  Acc@1: 66.6667 (52.2383)  Acc@5: 95.8333 (88.2920)  time: 0.0936  data: 0.0014  max mem: 358\n",
            "Train: Epoch[1/1]  [130/209]  eta: 0:00:07  Lr: 0.000047  Loss: 1.3784  Acc@1: 66.6667 (53.6260)  Acc@5: 95.8333 (88.8359)  time: 0.0919  data: 0.0014  max mem: 358\n",
            "Train: Epoch[1/1]  [140/209]  eta: 0:00:06  Lr: 0.000047  Loss: 1.2677  Acc@1: 70.8333 (54.8463)  Acc@5: 95.8333 (89.2435)  time: 0.0903  data: 0.0013  max mem: 358\n",
            "Train: Epoch[1/1]  [150/209]  eta: 0:00:05  Lr: 0.000047  Loss: 1.0826  Acc@1: 66.6667 (55.9327)  Acc@5: 95.8333 (89.7351)  time: 0.0900  data: 0.0009  max mem: 358\n",
            "Train: Epoch[1/1]  [160/209]  eta: 0:00:04  Lr: 0.000047  Loss: 0.8123  Acc@1: 70.8333 (57.0135)  Acc@5: 95.8333 (90.1656)  time: 0.0899  data: 0.0008  max mem: 358\n",
            "Train: Epoch[1/1]  [170/209]  eta: 0:00:03  Lr: 0.000047  Loss: 0.9861  Acc@1: 75.0000 (58.3090)  Acc@5: 95.8333 (90.6676)  time: 0.0898  data: 0.0013  max mem: 358\n",
            "Train: Epoch[1/1]  [180/209]  eta: 0:00:02  Lr: 0.000047  Loss: 0.9547  Acc@1: 79.1667 (59.3692)  Acc@5: 100.0000 (90.9991)  time: 0.0933  data: 0.0014  max mem: 358\n",
            "Train: Epoch[1/1]  [190/209]  eta: 0:00:01  Lr: 0.000047  Loss: 1.0962  Acc@1: 79.1667 (60.3185)  Acc@5: 100.0000 (91.4049)  time: 0.1001  data: 0.0015  max mem: 358\n",
            "Train: Epoch[1/1]  [200/209]  eta: 0:00:00  Lr: 0.000047  Loss: 0.7878  Acc@1: 75.0000 (61.0697)  Acc@5: 100.0000 (91.7910)  time: 0.1026  data: 0.0012  max mem: 358\n",
            "Train: Epoch[1/1]  [208/209]  eta: 0:00:00  Lr: 0.000047  Loss: 0.4991  Acc@1: 79.1667 (61.8600)  Acc@5: 100.0000 (92.0800)  time: 0.0972  data: 0.0006  max mem: 358\n",
            "Train: Epoch[1/1] Total time: 0:00:20 (0.0990 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.4991  Acc@1: 79.1667 (61.8600)  Acc@5: 100.0000 (92.0800)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:22  Loss: 0.9000 (0.9000)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  time: 0.5363  data: 0.4366  max mem: 358\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 0.8650 (0.7923)  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (97.3485)  time: 0.1452  data: 0.0572  max mem: 358\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.8146 (0.7790)  Acc@1: 75.0000 (78.1746)  Acc@5: 95.8333 (97.0238)  time: 0.0967  data: 0.0099  max mem: 358\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.7024 (0.7475)  Acc@1: 83.3333 (79.0323)  Acc@5: 95.8333 (97.1774)  time: 0.0881  data: 0.0004  max mem: 358\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6912 (0.7205)  Acc@1: 83.3333 (79.9797)  Acc@5: 100.0000 (97.4594)  time: 0.0887  data: 0.0003  max mem: 358\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6724 (0.7116)  Acc@1: 83.3333 (80.2000)  Acc@5: 100.0000 (97.5000)  time: 0.0867  data: 0.0003  max mem: 358\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1049 s / it)\n",
            "* Acc@1 80.200 Acc@5 97.500 loss 0.712\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:22  Loss: 0.9629 (0.9629)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  time: 0.5444  data: 0.4488  max mem: 358\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 0.8854 (0.8838)  Acc@1: 75.0000 (76.1364)  Acc@5: 95.8333 (96.2121)  time: 0.1308  data: 0.0424  max mem: 358\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 0.8854 (0.9157)  Acc@1: 75.0000 (75.7937)  Acc@5: 95.8333 (95.0397)  time: 0.0879  data: 0.0012  max mem: 358\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.8921 (0.9196)  Acc@1: 70.8333 (75.1344)  Acc@5: 95.8333 (95.1613)  time: 0.0870  data: 0.0005  max mem: 358\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.8501 (0.9059)  Acc@1: 70.8333 (75.1016)  Acc@5: 95.8333 (95.4268)  time: 0.0881  data: 0.0003  max mem: 358\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.8364 (0.9005)  Acc@1: 70.8333 (75.2000)  Acc@5: 95.8333 (95.5000)  time: 0.0863  data: 0.0003  max mem: 358\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1003 s / it)\n",
            "* Acc@1 75.200 Acc@5 95.500 loss 0.901\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:16  Loss: 0.5254 (0.5254)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.4039  data: 0.3314  max mem: 358\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:04  Loss: 0.7635 (0.7868)  Acc@1: 79.1667 (77.2727)  Acc@5: 95.8333 (95.4545)  time: 0.1270  data: 0.0434  max mem: 358\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 0.7094 (0.7385)  Acc@1: 79.1667 (79.3651)  Acc@5: 95.8333 (95.4365)  time: 0.0937  data: 0.0074  max mem: 358\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.6862 (0.7134)  Acc@1: 79.1667 (79.4355)  Acc@5: 95.8333 (95.8333)  time: 0.0887  data: 0.0005  max mem: 358\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.6950 (0.7315)  Acc@1: 79.1667 (80.0813)  Acc@5: 95.8333 (95.3252)  time: 0.0888  data: 0.0006  max mem: 358\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.7143 (0.7323)  Acc@1: 79.1667 (80.0000)  Acc@5: 95.8333 (95.4000)  time: 0.0867  data: 0.0006  max mem: 358\n",
            "Test: [Task 3] Total time: 0:00:04 (0.1017 s / it)\n",
            "* Acc@1 80.000 Acc@5 95.400 loss 0.732\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:26  Loss: 1.1851 (1.1851)  Acc@1: 66.6667 (66.6667)  Acc@5: 91.6667 (91.6667)  time: 0.6360  data: 0.5379  max mem: 358\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:04  Loss: 0.7439 (0.8225)  Acc@1: 79.1667 (78.0303)  Acc@5: 95.8333 (97.3485)  time: 0.1470  data: 0.0606  max mem: 358\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 0.7964 (0.8137)  Acc@1: 79.1667 (77.7778)  Acc@5: 95.8333 (97.2222)  time: 0.0931  data: 0.0067  max mem: 358\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 0.7648 (0.7783)  Acc@1: 79.1667 (79.3011)  Acc@5: 95.8333 (97.1774)  time: 0.0882  data: 0.0009  max mem: 358\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7060 (0.7930)  Acc@1: 79.1667 (79.2683)  Acc@5: 95.8333 (96.8496)  time: 0.0886  data: 0.0007  max mem: 358\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.7044 (0.7849)  Acc@1: 79.1667 (79.4000)  Acc@5: 95.8333 (96.9000)  time: 0.0873  data: 0.0007  max mem: 358\n",
            "Test: [Task 4] Total time: 0:00:04 (0.1057 s / it)\n",
            "* Acc@1 79.400 Acc@5 96.900 loss 0.785\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:23  Loss: 0.7578 (0.7578)  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  time: 0.5571  data: 0.4853  max mem: 358\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:04  Loss: 0.8032 (0.8026)  Acc@1: 79.1667 (78.7879)  Acc@5: 100.0000 (98.1061)  time: 0.1326  data: 0.0457  max mem: 358\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 0.8084 (0.8320)  Acc@1: 75.0000 (76.7857)  Acc@5: 95.8333 (97.2222)  time: 0.0884  data: 0.0011  max mem: 358\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 0.8166 (0.8298)  Acc@1: 75.0000 (76.6129)  Acc@5: 95.8333 (97.0430)  time: 0.0872  data: 0.0005  max mem: 358\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.9036 (0.8624)  Acc@1: 70.8333 (75.6098)  Acc@5: 95.8333 (96.9512)  time: 0.0884  data: 0.0003  max mem: 358\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.9185 (0.8757)  Acc@1: 70.8333 (75.5000)  Acc@5: 95.8333 (96.8000)  time: 0.0866  data: 0.0003  max mem: 358\n",
            "Test: [Task 5] Total time: 0:00:04 (0.1011 s / it)\n",
            "* Acc@1 75.500 Acc@5 96.800 loss 0.876\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:21  Loss: 0.7683 (0.7683)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  time: 0.5042  data: 0.4331  max mem: 358\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:04  Loss: 0.9488 (1.0061)  Acc@1: 75.0000 (73.4849)  Acc@5: 95.8333 (94.6970)  time: 0.1271  data: 0.0400  max mem: 358\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 1.0116 (1.0116)  Acc@1: 70.8333 (72.2222)  Acc@5: 95.8333 (94.8413)  time: 0.0880  data: 0.0007  max mem: 358\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 1.0568 (1.0424)  Acc@1: 66.6667 (70.4301)  Acc@5: 95.8333 (94.6237)  time: 0.0872  data: 0.0007  max mem: 358\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 1.1061 (1.0410)  Acc@1: 70.8333 (70.9350)  Acc@5: 95.8333 (94.9187)  time: 0.0883  data: 0.0005  max mem: 358\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 1.1061 (1.0431)  Acc@1: 70.8333 (70.8000)  Acc@5: 95.8333 (94.9000)  time: 0.0865  data: 0.0004  max mem: 358\n",
            "Test: [Task 6] Total time: 0:00:04 (0.1013 s / it)\n",
            "* Acc@1 70.800 Acc@5 94.900 loss 1.043\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:26  Loss: 1.3611 (1.3611)  Acc@1: 66.6667 (66.6667)  Acc@5: 87.5000 (87.5000)  time: 0.6370  data: 0.5301  max mem: 358\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:04  Loss: 1.1306 (1.2075)  Acc@1: 66.6667 (68.1818)  Acc@5: 91.6667 (94.3182)  time: 0.1469  data: 0.0586  max mem: 358\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:02  Loss: 1.1251 (1.1984)  Acc@1: 66.6667 (67.8571)  Acc@5: 91.6667 (93.8492)  time: 0.0989  data: 0.0135  max mem: 358\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:01  Loss: 1.1154 (1.2257)  Acc@1: 66.6667 (67.7419)  Acc@5: 91.6667 (93.4140)  time: 0.0952  data: 0.0093  max mem: 358\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 1.1634 (1.2378)  Acc@1: 66.6667 (68.0894)  Acc@5: 91.6667 (93.0894)  time: 0.0895  data: 0.0018  max mem: 358\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 1.1634 (1.2356)  Acc@1: 66.6667 (68.0000)  Acc@5: 91.6667 (93.0000)  time: 0.0878  data: 0.0016  max mem: 358\n",
            "Test: [Task 7] Total time: 0:00:04 (0.1091 s / it)\n",
            "* Acc@1 68.000 Acc@5 93.000 loss 1.236\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:18  Loss: 4.1064 (4.1064)  Acc@1: 0.0000 (0.0000)  Acc@5: 33.3333 (33.3333)  time: 0.4366  data: 0.3537  max mem: 358\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:03  Loss: 4.2096 (4.1877)  Acc@1: 0.0000 (1.1364)  Acc@5: 25.0000 (25.0000)  time: 0.1201  data: 0.0339  max mem: 358\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:02  Loss: 4.2077 (4.1448)  Acc@1: 0.0000 (1.3889)  Acc@5: 25.0000 (26.7857)  time: 0.0882  data: 0.0012  max mem: 358\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:01  Loss: 4.0592 (4.1426)  Acc@1: 0.0000 (1.0753)  Acc@5: 25.0000 (24.8656)  time: 0.0881  data: 0.0004  max mem: 358\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 4.0592 (4.1471)  Acc@1: 0.0000 (0.9146)  Acc@5: 25.0000 (25.3049)  time: 0.0889  data: 0.0003  max mem: 358\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 4.0592 (4.1444)  Acc@1: 0.0000 (0.9000)  Acc@5: 25.0000 (25.2000)  time: 0.0872  data: 0.0003  max mem: 358\n",
            "Test: [Task 8] Total time: 0:00:04 (0.0984 s / it)\n",
            "* Acc@1 0.900 Acc@5 25.200 loss 4.144\n",
            "[Average accuracy till task8]\tAcc@1: 66.2500\tAcc@5: 86.9000\tLoss: 1.3035\tForgetting: 3.5857\tBackward: 41.8714\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([19176, 384])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 0.6545  Acc@1: 83.3333 (81.1905)  Acc@5: 95.8333 (93.8095)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000467  Loss: 0.6111  Acc@1: 79.1667 (82.5000)  Acc@5: 95.8333 (96.6667)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000464  Loss: 0.6432  Acc@1: 87.5000 (85.0595)  Acc@5: 100.0000 (98.6310)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 0.7506  Acc@1: 87.5000 (87.7976)  Acc@5: 100.0000 (98.4524)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000448  Loss: 0.6220  Acc@1: 83.3333 (87.4405)  Acc@5: 100.0000 (99.0476)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000437  Loss: 0.6895  Acc@1: 91.6667 (88.0952)  Acc@5: 100.0000 (99.2262)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 0.6674  Acc@1: 87.5000 (89.7619)  Acc@5: 100.0000 (99.3452)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000409  Loss: 0.4509  Acc@1: 87.5000 (88.9881)  Acc@5: 100.0000 (99.4643)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000391  Loss: 0.6411  Acc@1: 91.6667 (91.7857)  Acc@5: 100.0000 (99.6429)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 0.4678  Acc@1: 91.6667 (91.4881)  Acc@5: 100.0000 (99.7619)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000352  Loss: 0.3831  Acc@1: 95.8333 (91.7857)  Acc@5: 100.0000 (99.7024)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000330  Loss: 0.4520  Acc@1: 91.6667 (90.5952)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 0.3959  Acc@1: 91.6667 (91.0119)  Acc@5: 100.0000 (99.8214)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000283  Loss: 0.4804  Acc@1: 91.6667 (91.1310)  Acc@5: 100.0000 (99.5238)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000259  Loss: 0.4590  Acc@1: 87.5000 (92.1429)  Acc@5: 100.0000 (99.7619)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 0.6240  Acc@1: 91.6667 (91.5476)  Acc@5: 100.0000 (99.7024)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000210  Loss: 0.4951  Acc@1: 87.5000 (90.5357)  Acc@5: 100.0000 (99.5238)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000186  Loss: 0.6821  Acc@1: 91.6667 (90.7738)  Acc@5: 100.0000 (99.8214)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.6710  Acc@1: 91.6667 (90.7143)  Acc@5: 100.0000 (99.7619)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000139  Loss: 0.4944  Acc@1: 95.8333 (92.5000)  Acc@5: 100.0000 (99.5238)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000117  Loss: 0.3798  Acc@1: 91.6667 (92.2024)  Acc@5: 100.0000 (99.5238)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 0.5230  Acc@1: 91.6667 (91.7262)  Acc@5: 100.0000 (99.7619)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000078  Loss: 0.4508  Acc@1: 91.6667 (91.8452)  Acc@5: 100.0000 (99.9405)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000060  Loss: 0.4281  Acc@1: 91.6667 (92.8571)  Acc@5: 100.0000 (99.9405)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 0.4235  Acc@1: 91.6667 (93.3929)  Acc@5: 100.0000 (99.8810)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000031  Loss: 0.5161  Acc@1: 91.6667 (91.4286)  Acc@5: 100.0000 (99.3452)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000020  Loss: 0.3813  Acc@1: 91.6667 (92.5000)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.3895  Acc@1: 91.6667 (92.6190)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000005  Loss: 0.4039  Acc@1: 91.6667 (91.4286)  Acc@5: 100.0000 (99.5238)\n",
            "torch.Size([19176, 384])\n",
            "Averaged stats: Lr: 0.000001  Loss: 0.2991  Acc@1: 91.6667 (92.6190)  Acc@5: 100.0000 (99.8214)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:21  Loss: 0.8085 (0.8085)  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  time: 0.5039  data: 0.4173  max mem: 358\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:04  Loss: 0.8085 (0.7565)  Acc@1: 75.0000 (78.0303)  Acc@5: 95.8333 (96.5909)  time: 0.1286  data: 0.0389  max mem: 358\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.7774 (0.7434)  Acc@1: 79.1667 (78.9683)  Acc@5: 95.8333 (96.4286)  time: 0.0903  data: 0.0008  max mem: 358\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.6868 (0.7082)  Acc@1: 79.1667 (80.1075)  Acc@5: 95.8333 (96.6398)  time: 0.0899  data: 0.0006  max mem: 358\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.5900 (0.6795)  Acc@1: 83.3333 (80.9959)  Acc@5: 95.8333 (96.9512)  time: 0.0905  data: 0.0004  max mem: 358\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.5839 (0.6702)  Acc@1: 83.3333 (81.2000)  Acc@5: 95.8333 (97.0000)  time: 0.0888  data: 0.0004  max mem: 358\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1020 s / it)\n",
            "* Acc@1 81.200 Acc@5 97.000 loss 0.670\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:28  Loss: 1.0099 (1.0099)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  time: 0.6707  data: 0.5827  max mem: 358\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 0.8280 (0.8266)  Acc@1: 75.0000 (75.7576)  Acc@5: 95.8333 (95.0758)  time: 0.1435  data: 0.0541  max mem: 358\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 0.8280 (0.8629)  Acc@1: 75.0000 (74.4048)  Acc@5: 95.8333 (94.4444)  time: 0.0910  data: 0.0009  max mem: 358\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.8298 (0.8518)  Acc@1: 75.0000 (74.8656)  Acc@5: 95.8333 (95.0269)  time: 0.0913  data: 0.0008  max mem: 358\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.7738 (0.8336)  Acc@1: 75.0000 (75.3049)  Acc@5: 95.8333 (95.4268)  time: 0.0923  data: 0.0020  max mem: 358\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.7633 (0.8292)  Acc@1: 75.0000 (75.4000)  Acc@5: 95.8333 (95.5000)  time: 0.0907  data: 0.0020  max mem: 358\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1082 s / it)\n",
            "* Acc@1 75.400 Acc@5 95.500 loss 0.829\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:30  Loss: 0.5048 (0.5048)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.7227  data: 0.6392  max mem: 358\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:04  Loss: 0.7856 (0.8048)  Acc@1: 75.0000 (77.2727)  Acc@5: 95.8333 (95.8333)  time: 0.1492  data: 0.0597  max mem: 358\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 0.7494 (0.7427)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  time: 0.0914  data: 0.0012  max mem: 358\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.6895 (0.7196)  Acc@1: 79.1667 (79.0323)  Acc@5: 95.8333 (96.1022)  time: 0.0910  data: 0.0004  max mem: 358\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.6913 (0.7396)  Acc@1: 79.1667 (79.0650)  Acc@5: 95.8333 (95.7317)  time: 0.0915  data: 0.0003  max mem: 358\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.6913 (0.7409)  Acc@1: 79.1667 (78.9000)  Acc@5: 95.8333 (95.8000)  time: 0.0901  data: 0.0003  max mem: 358\n",
            "Test: [Task 3] Total time: 0:00:04 (0.1081 s / it)\n",
            "* Acc@1 78.900 Acc@5 95.800 loss 0.741\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:15  Loss: 1.1309 (1.1309)  Acc@1: 66.6667 (66.6667)  Acc@5: 87.5000 (87.5000)  time: 0.3806  data: 0.3027  max mem: 358\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:04  Loss: 0.8035 (0.8469)  Acc@1: 75.0000 (72.7273)  Acc@5: 95.8333 (96.2121)  time: 0.1310  data: 0.0448  max mem: 358\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 0.7841 (0.8190)  Acc@1: 75.0000 (74.4048)  Acc@5: 95.8333 (96.4286)  time: 0.0991  data: 0.0097  max mem: 358\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 0.7282 (0.7680)  Acc@1: 75.0000 (76.3441)  Acc@5: 95.8333 (96.7742)  time: 0.0922  data: 0.0004  max mem: 358\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.6956 (0.7788)  Acc@1: 75.0000 (76.0163)  Acc@5: 95.8333 (96.5447)  time: 0.0923  data: 0.0003  max mem: 358\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.6897 (0.7701)  Acc@1: 75.0000 (76.3000)  Acc@5: 95.8333 (96.6000)  time: 0.0900  data: 0.0003  max mem: 358\n",
            "Test: [Task 4] Total time: 0:00:04 (0.1041 s / it)\n",
            "* Acc@1 76.300 Acc@5 96.600 loss 0.770\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:22  Loss: 0.6388 (0.6388)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.5447  data: 0.4754  max mem: 358\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:04  Loss: 0.7161 (0.7164)  Acc@1: 79.1667 (79.9242)  Acc@5: 100.0000 (98.1061)  time: 0.1338  data: 0.0439  max mem: 358\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 0.7326 (0.7388)  Acc@1: 79.1667 (79.7619)  Acc@5: 95.8333 (96.8254)  time: 0.0918  data: 0.0009  max mem: 358\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 0.7460 (0.7401)  Acc@1: 79.1667 (79.8387)  Acc@5: 95.8333 (96.5054)  time: 0.0907  data: 0.0007  max mem: 358\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.7669 (0.7727)  Acc@1: 75.0000 (78.6585)  Acc@5: 95.8333 (96.6463)  time: 0.0907  data: 0.0003  max mem: 358\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.8161 (0.7877)  Acc@1: 75.0000 (78.5000)  Acc@5: 95.8333 (96.5000)  time: 0.0893  data: 0.0003  max mem: 358\n",
            "Test: [Task 5] Total time: 0:00:04 (0.1055 s / it)\n",
            "* Acc@1 78.500 Acc@5 96.500 loss 0.788\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:30  Loss: 0.7335 (0.7335)  Acc@1: 83.3333 (83.3333)  Acc@5: 91.6667 (91.6667)  time: 0.7373  data: 0.6386  max mem: 358\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:04  Loss: 0.8253 (0.8919)  Acc@1: 79.1667 (74.6212)  Acc@5: 95.8333 (94.6970)  time: 0.1501  data: 0.0589  max mem: 358\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 0.8754 (0.9051)  Acc@1: 75.0000 (74.2064)  Acc@5: 95.8333 (95.2381)  time: 0.0906  data: 0.0010  max mem: 358\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 0.9456 (0.9257)  Acc@1: 75.0000 (73.3871)  Acc@5: 95.8333 (95.0269)  time: 0.0900  data: 0.0007  max mem: 358\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 0.9800 (0.9322)  Acc@1: 75.0000 (73.3740)  Acc@5: 95.8333 (95.1220)  time: 0.0904  data: 0.0003  max mem: 358\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 0.9800 (0.9361)  Acc@1: 75.0000 (73.3000)  Acc@5: 95.8333 (95.0000)  time: 0.0890  data: 0.0003  max mem: 358\n",
            "Test: [Task 6] Total time: 0:00:04 (0.1077 s / it)\n",
            "* Acc@1 73.300 Acc@5 95.000 loss 0.936\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:15  Loss: 1.1532 (1.1532)  Acc@1: 79.1667 (79.1667)  Acc@5: 87.5000 (87.5000)  time: 0.3590  data: 0.2752  max mem: 358\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:03  Loss: 0.8976 (0.9538)  Acc@1: 75.0000 (74.6212)  Acc@5: 95.8333 (95.4545)  time: 0.1220  data: 0.0355  max mem: 358\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:02  Loss: 0.8947 (0.9647)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.0397)  time: 0.0939  data: 0.0061  max mem: 358\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:01  Loss: 0.9291 (1.0050)  Acc@1: 70.8333 (73.1183)  Acc@5: 95.8333 (93.6828)  time: 0.0900  data: 0.0006  max mem: 358\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 0.9590 (1.0145)  Acc@1: 70.8333 (72.7642)  Acc@5: 91.6667 (93.5976)  time: 0.0903  data: 0.0005  max mem: 358\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 0.9590 (1.0108)  Acc@1: 70.8333 (72.8000)  Acc@5: 93.7500 (93.6000)  time: 0.0882  data: 0.0004  max mem: 358\n",
            "Test: [Task 7] Total time: 0:00:04 (0.1000 s / it)\n",
            "* Acc@1 72.800 Acc@5 93.600 loss 1.011\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:19  Loss: 1.1168 (1.1168)  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.8333)  time: 0.4713  data: 0.3956  max mem: 358\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:04  Loss: 1.1884 (1.1904)  Acc@1: 70.8333 (69.3182)  Acc@5: 91.6667 (92.4242)  time: 0.1252  data: 0.0406  max mem: 358\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:02  Loss: 1.1863 (1.1649)  Acc@1: 66.6667 (68.6508)  Acc@5: 91.6667 (93.4524)  time: 0.0903  data: 0.0027  max mem: 358\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:01  Loss: 1.1652 (1.1486)  Acc@1: 66.6667 (68.5484)  Acc@5: 95.8333 (93.9516)  time: 0.0899  data: 0.0005  max mem: 358\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 1.1652 (1.1688)  Acc@1: 66.6667 (67.0732)  Acc@5: 91.6667 (92.9878)  time: 0.0898  data: 0.0006  max mem: 358\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 1.1652 (1.1636)  Acc@1: 66.6667 (67.1000)  Acc@5: 91.6667 (93.0000)  time: 0.0879  data: 0.0006  max mem: 358\n",
            "Test: [Task 8] Total time: 0:00:04 (0.1015 s / it)\n",
            "* Acc@1 67.100 Acc@5 93.000 loss 1.164\n",
            "[Average accuracy till task8]\tAcc@1: 75.4375\tAcc@5: 95.3750\tLoss: 0.8636\tForgetting: 4.1429\tBackward: -3.0286\n",
            "Train: Epoch[1/1]  [  0/209]  eta: 0:03:24  Lr: 0.000047  Loss: 2.3768  Acc@1: 0.0000 (0.0000)  Acc@5: 45.8333 (45.8333)  time: 0.9798  data: 0.7984  max mem: 358\n",
            "Train: Epoch[1/1]  [ 10/209]  eta: 0:00:37  Lr: 0.000047  Loss: 2.3396  Acc@1: 8.3333 (10.6061)  Acc@5: 62.5000 (54.9242)  time: 0.1879  data: 0.0758  max mem: 358\n",
            "Train: Epoch[1/1]  [ 20/209]  eta: 0:00:28  Lr: 0.000047  Loss: 1.9584  Acc@1: 16.6667 (17.4603)  Acc@5: 66.6667 (63.2937)  time: 0.1080  data: 0.0040  max mem: 358\n",
            "Train: Epoch[1/1]  [ 30/209]  eta: 0:00:23  Lr: 0.000047  Loss: 2.0997  Acc@1: 20.8333 (19.3548)  Acc@5: 75.0000 (66.8011)  time: 0.0998  data: 0.0032  max mem: 358\n",
            "Train: Epoch[1/1]  [ 40/209]  eta: 0:00:20  Lr: 0.000047  Loss: 1.8580  Acc@1: 29.1667 (23.6789)  Acc@5: 79.1667 (70.7317)  time: 0.0917  data: 0.0013  max mem: 358\n",
            "Train: Epoch[1/1]  [ 50/209]  eta: 0:00:18  Lr: 0.000047  Loss: 1.9898  Acc@1: 37.5000 (28.6765)  Acc@5: 87.5000 (74.1830)  time: 0.0901  data: 0.0012  max mem: 358\n",
            "Train: Epoch[1/1]  [ 60/209]  eta: 0:00:16  Lr: 0.000047  Loss: 1.6150  Acc@1: 50.0000 (32.6503)  Acc@5: 87.5000 (77.3224)  time: 0.0894  data: 0.0016  max mem: 358\n",
            "Train: Epoch[1/1]  [ 70/209]  eta: 0:00:15  Lr: 0.000047  Loss: 1.6579  Acc@1: 50.0000 (35.3873)  Acc@5: 91.6667 (79.5188)  time: 0.0898  data: 0.0015  max mem: 358\n",
            "Train: Epoch[1/1]  [ 80/209]  eta: 0:00:13  Lr: 0.000047  Loss: 1.4483  Acc@1: 54.1667 (38.7346)  Acc@5: 95.8333 (81.8416)  time: 0.0897  data: 0.0015  max mem: 358\n",
            "Train: Epoch[1/1]  [ 90/209]  eta: 0:00:12  Lr: 0.000047  Loss: 1.4674  Acc@1: 62.5000 (41.4377)  Acc@5: 95.8333 (83.2875)  time: 0.0895  data: 0.0016  max mem: 358\n",
            "Train: Epoch[1/1]  [100/209]  eta: 0:00:11  Lr: 0.000047  Loss: 1.2758  Acc@1: 62.5000 (43.6881)  Acc@5: 95.8333 (84.4885)  time: 0.0904  data: 0.0015  max mem: 358\n",
            "Train: Epoch[1/1]  [110/209]  eta: 0:00:10  Lr: 0.000047  Loss: 1.2741  Acc@1: 62.5000 (45.4204)  Acc@5: 95.8333 (85.6607)  time: 0.0910  data: 0.0014  max mem: 358\n",
            "Train: Epoch[1/1]  [120/209]  eta: 0:00:08  Lr: 0.000047  Loss: 1.3796  Acc@1: 62.5000 (47.4862)  Acc@5: 95.8333 (86.5358)  time: 0.0908  data: 0.0013  max mem: 358\n",
            "Train: Epoch[1/1]  [130/209]  eta: 0:00:07  Lr: 0.000047  Loss: 1.1714  Acc@1: 75.0000 (49.4911)  Acc@5: 100.0000 (87.5318)  time: 0.0906  data: 0.0014  max mem: 358\n",
            "Train: Epoch[1/1]  [140/209]  eta: 0:00:06  Lr: 0.000047  Loss: 1.2438  Acc@1: 70.8333 (51.1229)  Acc@5: 100.0000 (88.2388)  time: 0.0947  data: 0.0019  max mem: 358\n",
            "Train: Epoch[1/1]  [150/209]  eta: 0:00:05  Lr: 0.000047  Loss: 1.2784  Acc@1: 70.8333 (52.3455)  Acc@5: 95.8333 (88.6313)  time: 0.1020  data: 0.0019  max mem: 358\n",
            "Train: Epoch[1/1]  [160/209]  eta: 0:00:04  Lr: 0.000047  Loss: 0.9392  Acc@1: 79.1667 (54.0631)  Acc@5: 100.0000 (89.3116)  time: 0.1056  data: 0.0020  max mem: 358\n",
            "Train: Epoch[1/1]  [170/209]  eta: 0:00:03  Lr: 0.000047  Loss: 0.9115  Acc@1: 79.1667 (55.5068)  Acc@5: 100.0000 (89.8635)  time: 0.1052  data: 0.0019  max mem: 358\n",
            "Train: Epoch[1/1]  [180/209]  eta: 0:00:02  Lr: 0.000047  Loss: 1.1205  Acc@1: 79.1667 (56.9061)  Acc@5: 100.0000 (90.3085)  time: 0.1016  data: 0.0031  max mem: 358\n",
            "Train: Epoch[1/1]  [190/209]  eta: 0:00:01  Lr: 0.000047  Loss: 1.0797  Acc@1: 79.1667 (58.0061)  Acc@5: 100.0000 (90.6850)  time: 0.0942  data: 0.0031  max mem: 358\n",
            "Train: Epoch[1/1]  [200/209]  eta: 0:00:00  Lr: 0.000047  Loss: 0.8897  Acc@1: 79.1667 (59.0381)  Acc@5: 100.0000 (91.1277)  time: 0.0895  data: 0.0014  max mem: 358\n",
            "Train: Epoch[1/1]  [208/209]  eta: 0:00:00  Lr: 0.000047  Loss: 1.1533  Acc@1: 79.1667 (59.6800)  Acc@5: 100.0000 (91.4000)  time: 0.0861  data: 0.0009  max mem: 358\n",
            "Train: Epoch[1/1] Total time: 0:00:20 (0.0996 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 1.1533  Acc@1: 79.1667 (59.6800)  Acc@5: 100.0000 (91.4000)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:17  Loss: 0.7710 (0.7710)  Acc@1: 83.3333 (83.3333)  Acc@5: 91.6667 (91.6667)  time: 0.4117  data: 0.3402  max mem: 358\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 0.8607 (0.8038)  Acc@1: 75.0000 (76.5152)  Acc@5: 95.8333 (94.6970)  time: 0.1222  data: 0.0382  max mem: 358\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.7871 (0.7858)  Acc@1: 79.1667 (77.5794)  Acc@5: 95.8333 (94.6429)  time: 0.0906  data: 0.0044  max mem: 358\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.7422 (0.7489)  Acc@1: 79.1667 (79.3011)  Acc@5: 95.8333 (95.5645)  time: 0.0882  data: 0.0007  max mem: 358\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6691 (0.7203)  Acc@1: 83.3333 (80.3862)  Acc@5: 100.0000 (96.1382)  time: 0.0885  data: 0.0004  max mem: 358\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6566 (0.7107)  Acc@1: 87.5000 (80.5000)  Acc@5: 100.0000 (96.2000)  time: 0.0867  data: 0.0004  max mem: 358\n",
            "Test: [Task 1] Total time: 0:00:04 (0.0988 s / it)\n",
            "* Acc@1 80.500 Acc@5 96.200 loss 0.711\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:23  Loss: 1.1326 (1.1326)  Acc@1: 66.6667 (66.6667)  Acc@5: 91.6667 (91.6667)  time: 0.5537  data: 0.4773  max mem: 358\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 1.0007 (0.9870)  Acc@1: 75.0000 (72.7273)  Acc@5: 95.8333 (94.6970)  time: 0.1292  data: 0.0442  max mem: 358\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 0.9384 (1.0108)  Acc@1: 70.8333 (72.6191)  Acc@5: 95.8333 (93.6508)  time: 0.0869  data: 0.0006  max mem: 358\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.9384 (0.9970)  Acc@1: 70.8333 (72.9839)  Acc@5: 95.8333 (94.0860)  time: 0.0881  data: 0.0006  max mem: 358\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.8813 (0.9816)  Acc@1: 70.8333 (73.1707)  Acc@5: 95.8333 (94.2073)  time: 0.0886  data: 0.0007  max mem: 358\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.8587 (0.9754)  Acc@1: 70.8333 (73.2000)  Acc@5: 95.8333 (94.3000)  time: 0.0868  data: 0.0007  max mem: 358\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1023 s / it)\n",
            "* Acc@1 73.200 Acc@5 94.300 loss 0.975\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:33  Loss: 0.6078 (0.6078)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.7974  data: 0.6720  max mem: 358\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:04  Loss: 0.8614 (0.8856)  Acc@1: 75.0000 (73.8636)  Acc@5: 95.8333 (95.0758)  time: 0.1558  data: 0.0666  max mem: 358\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 0.8318 (0.8187)  Acc@1: 75.0000 (76.3889)  Acc@5: 95.8333 (94.8413)  time: 0.0950  data: 0.0067  max mem: 358\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.7370 (0.7955)  Acc@1: 79.1667 (77.0161)  Acc@5: 95.8333 (95.1613)  time: 0.0936  data: 0.0043  max mem: 358\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.7894 (0.8125)  Acc@1: 79.1667 (77.8455)  Acc@5: 95.8333 (95.0203)  time: 0.0888  data: 0.0009  max mem: 358\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.7894 (0.8143)  Acc@1: 79.1667 (77.7000)  Acc@5: 95.8333 (95.1000)  time: 0.0875  data: 0.0008  max mem: 358\n",
            "Test: [Task 3] Total time: 0:00:04 (0.1110 s / it)\n",
            "* Acc@1 77.700 Acc@5 95.100 loss 0.814\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:16  Loss: 1.0884 (1.0884)  Acc@1: 62.5000 (62.5000)  Acc@5: 87.5000 (87.5000)  time: 0.3994  data: 0.3206  max mem: 358\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:03  Loss: 0.8230 (0.8437)  Acc@1: 75.0000 (73.8636)  Acc@5: 95.8333 (96.2121)  time: 0.1210  data: 0.0362  max mem: 358\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 0.7771 (0.8224)  Acc@1: 75.0000 (75.5952)  Acc@5: 95.8333 (95.6349)  time: 0.0905  data: 0.0041  max mem: 358\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 0.7441 (0.7758)  Acc@1: 79.1667 (77.9570)  Acc@5: 95.8333 (96.2366)  time: 0.0881  data: 0.0005  max mem: 358\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7272 (0.7867)  Acc@1: 79.1667 (77.7439)  Acc@5: 95.8333 (96.3415)  time: 0.0885  data: 0.0003  max mem: 358\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.7272 (0.7777)  Acc@1: 79.1667 (78.0000)  Acc@5: 95.8333 (96.4000)  time: 0.0867  data: 0.0003  max mem: 358\n",
            "Test: [Task 4] Total time: 0:00:04 (0.0986 s / it)\n",
            "* Acc@1 78.000 Acc@5 96.400 loss 0.778\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:20  Loss: 0.7231 (0.7231)  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  time: 0.4859  data: 0.4081  max mem: 358\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:04  Loss: 0.8396 (0.8097)  Acc@1: 75.0000 (76.8939)  Acc@5: 95.8333 (96.9697)  time: 0.1253  data: 0.0379  max mem: 358\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 0.8396 (0.8276)  Acc@1: 75.0000 (77.1825)  Acc@5: 95.8333 (96.2302)  time: 0.0882  data: 0.0007  max mem: 358\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 0.7999 (0.8260)  Acc@1: 79.1667 (77.4194)  Acc@5: 95.8333 (95.9677)  time: 0.0876  data: 0.0005  max mem: 358\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.8801 (0.8645)  Acc@1: 75.0000 (75.1016)  Acc@5: 95.8333 (95.9350)  time: 0.0886  data: 0.0004  max mem: 358\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.9516 (0.8789)  Acc@1: 70.8333 (74.9000)  Acc@5: 95.8333 (95.8000)  time: 0.0866  data: 0.0003  max mem: 358\n",
            "Test: [Task 5] Total time: 0:00:04 (0.1011 s / it)\n",
            "* Acc@1 74.900 Acc@5 95.800 loss 0.879\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:29  Loss: 0.7402 (0.7402)  Acc@1: 83.3333 (83.3333)  Acc@5: 91.6667 (91.6667)  time: 0.6989  data: 0.6106  max mem: 358\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:04  Loss: 0.7955 (0.8674)  Acc@1: 75.0000 (74.6212)  Acc@5: 95.8333 (95.8333)  time: 0.1514  data: 0.0661  max mem: 358\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 0.8343 (0.8779)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  time: 0.0962  data: 0.0095  max mem: 358\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 0.8943 (0.9002)  Acc@1: 70.8333 (73.6559)  Acc@5: 95.8333 (96.1022)  time: 0.0922  data: 0.0038  max mem: 358\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 0.9243 (0.9162)  Acc@1: 75.0000 (73.6789)  Acc@5: 95.8333 (95.7317)  time: 0.0888  data: 0.0009  max mem: 358\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 0.9243 (0.9190)  Acc@1: 75.0000 (73.6000)  Acc@5: 95.8333 (95.7000)  time: 0.0874  data: 0.0009  max mem: 358\n",
            "Test: [Task 6] Total time: 0:00:04 (0.1087 s / it)\n",
            "* Acc@1 73.600 Acc@5 95.700 loss 0.919\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:18  Loss: 1.1101 (1.1101)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  time: 0.4425  data: 0.3702  max mem: 358\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:03  Loss: 0.9327 (0.9592)  Acc@1: 75.0000 (73.1061)  Acc@5: 95.8333 (95.8333)  time: 0.1233  data: 0.0372  max mem: 358\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:02  Loss: 0.8937 (0.9758)  Acc@1: 75.0000 (72.6190)  Acc@5: 95.8333 (96.0317)  time: 0.0896  data: 0.0023  max mem: 358\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:01  Loss: 0.9725 (1.0086)  Acc@1: 70.8333 (71.6398)  Acc@5: 95.8333 (94.4892)  time: 0.0879  data: 0.0005  max mem: 358\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 1.0108 (1.0143)  Acc@1: 70.8333 (71.9512)  Acc@5: 95.8333 (94.6138)  time: 0.0888  data: 0.0002  max mem: 358\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 1.0108 (1.0115)  Acc@1: 75.0000 (72.1000)  Acc@5: 95.8333 (94.6000)  time: 0.0869  data: 0.0002  max mem: 358\n",
            "Test: [Task 7] Total time: 0:00:04 (0.0995 s / it)\n",
            "* Acc@1 72.100 Acc@5 94.600 loss 1.011\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:17  Loss: 1.2560 (1.2560)  Acc@1: 66.6667 (66.6667)  Acc@5: 91.6667 (91.6667)  time: 0.4191  data: 0.3433  max mem: 358\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:03  Loss: 1.2560 (1.2904)  Acc@1: 62.5000 (61.7424)  Acc@5: 91.6667 (92.0455)  time: 0.1210  data: 0.0323  max mem: 358\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:02  Loss: 1.2497 (1.2709)  Acc@1: 62.5000 (63.0952)  Acc@5: 91.6667 (92.2619)  time: 0.0934  data: 0.0058  max mem: 358\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:01  Loss: 1.2080 (1.2515)  Acc@1: 62.5000 (63.5753)  Acc@5: 95.8333 (92.8763)  time: 0.0953  data: 0.0096  max mem: 358\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 1.2080 (1.2631)  Acc@1: 62.5000 (63.5163)  Acc@5: 91.6667 (92.1748)  time: 0.0915  data: 0.0048  max mem: 358\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 1.2080 (1.2578)  Acc@1: 62.5000 (63.5000)  Acc@5: 91.6667 (92.2000)  time: 0.0902  data: 0.0047  max mem: 358\n",
            "Test: [Task 8] Total time: 0:00:04 (0.1028 s / it)\n",
            "* Acc@1 63.500 Acc@5 92.200 loss 1.258\n",
            "Test: [Task 9]  [ 0/42]  eta: 0:00:33  Loss: 4.8926 (4.8926)  Acc@1: 0.0000 (0.0000)  Acc@5: 12.5000 (12.5000)  time: 0.7997  data: 0.6694  max mem: 358\n",
            "Test: [Task 9]  [10/42]  eta: 0:00:04  Loss: 4.6512 (4.6534)  Acc@1: 0.0000 (0.0000)  Acc@5: 12.5000 (12.8788)  time: 0.1544  data: 0.0614  max mem: 358\n",
            "Test: [Task 9]  [20/42]  eta: 0:00:02  Loss: 4.6382 (4.6332)  Acc@1: 0.0000 (0.0000)  Acc@5: 12.5000 (12.8968)  time: 0.0926  data: 0.0054  max mem: 358\n",
            "Test: [Task 9]  [30/42]  eta: 0:00:01  Loss: 4.6565 (4.6523)  Acc@1: 0.0000 (0.0000)  Acc@5: 8.3333 (12.3656)  time: 0.0946  data: 0.0083  max mem: 358\n",
            "Test: [Task 9]  [40/42]  eta: 0:00:00  Loss: 4.5846 (4.6342)  Acc@1: 0.0000 (0.0000)  Acc@5: 12.5000 (13.1098)  time: 0.0916  data: 0.0040  max mem: 358\n",
            "Test: [Task 9]  [41/42]  eta: 0:00:00  Loss: 4.5846 (4.6261)  Acc@1: 0.0000 (0.0000)  Acc@5: 12.5000 (13.3000)  time: 0.0904  data: 0.0040  max mem: 358\n",
            "Test: [Task 9] Total time: 0:00:04 (0.1109 s / it)\n",
            "* Acc@1 0.000 Acc@5 13.300 loss 4.626\n",
            "[Average accuracy till task9]\tAcc@1: 65.9444\tAcc@5: 85.9556\tLoss: 1.3302\tForgetting: 3.8875\tBackward: 44.5750\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([21576, 384])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 0.6644  Acc@1: 83.3333 (82.0833)  Acc@5: 95.8333 (92.7604)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000467  Loss: 0.6452  Acc@1: 83.3333 (80.8333)  Acc@5: 100.0000 (97.6042)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000464  Loss: 0.5630  Acc@1: 87.5000 (85.6771)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 0.8238  Acc@1: 87.5000 (86.8229)  Acc@5: 100.0000 (99.3229)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000448  Loss: 0.6463  Acc@1: 87.5000 (89.3750)  Acc@5: 100.0000 (99.7396)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000437  Loss: 0.7128  Acc@1: 91.6667 (90.7813)  Acc@5: 100.0000 (99.1667)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 0.5449  Acc@1: 91.6667 (90.8333)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000409  Loss: 0.4041  Acc@1: 91.6667 (90.1563)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000391  Loss: 0.3539  Acc@1: 91.6667 (92.8646)  Acc@5: 100.0000 (99.6354)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 0.5532  Acc@1: 95.8333 (92.5521)  Acc@5: 100.0000 (99.7396)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000352  Loss: 0.3357  Acc@1: 91.6667 (92.0313)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000330  Loss: 0.4240  Acc@1: 91.6667 (92.0833)  Acc@5: 100.0000 (99.8958)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 0.3307  Acc@1: 91.6667 (91.2500)  Acc@5: 100.0000 (99.7396)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000283  Loss: 0.3773  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000259  Loss: 0.4399  Acc@1: 91.6667 (91.7708)  Acc@5: 100.0000 (99.6354)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 0.4854  Acc@1: 91.6667 (92.1875)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000210  Loss: 0.4466  Acc@1: 91.6667 (93.2292)  Acc@5: 100.0000 (99.7396)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000186  Loss: 0.4104  Acc@1: 91.6667 (92.9167)  Acc@5: 100.0000 (99.8958)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.4576  Acc@1: 91.6667 (93.0208)  Acc@5: 100.0000 (99.9479)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000139  Loss: 0.6861  Acc@1: 91.6667 (92.1354)  Acc@5: 100.0000 (99.6875)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000117  Loss: 0.3124  Acc@1: 91.6667 (92.3958)  Acc@5: 100.0000 (99.8958)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 0.5474  Acc@1: 91.6667 (92.5000)  Acc@5: 100.0000 (99.7917)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000078  Loss: 0.5062  Acc@1: 91.6667 (92.6563)  Acc@5: 100.0000 (99.6875)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000060  Loss: 0.4880  Acc@1: 91.6667 (91.7708)  Acc@5: 100.0000 (99.7396)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 0.4792  Acc@1: 91.6667 (92.3958)  Acc@5: 100.0000 (99.5313)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000031  Loss: 0.3887  Acc@1: 91.6667 (92.6563)  Acc@5: 100.0000 (99.6354)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000020  Loss: 0.4798  Acc@1: 91.6667 (93.0208)  Acc@5: 100.0000 (99.8438)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.6926  Acc@1: 95.8333 (93.2813)  Acc@5: 100.0000 (99.7396)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000005  Loss: 0.2199  Acc@1: 91.6667 (92.5000)  Acc@5: 100.0000 (99.8438)\n",
            "torch.Size([21576, 384])\n",
            "Averaged stats: Lr: 0.000001  Loss: 0.2807  Acc@1: 91.6667 (92.5521)  Acc@5: 100.0000 (100.0000)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:17  Loss: 0.8575 (0.8575)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  time: 0.4225  data: 0.3416  max mem: 359\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:03  Loss: 0.8495 (0.8057)  Acc@1: 75.0000 (75.3788)  Acc@5: 95.8333 (94.3182)  time: 0.1237  data: 0.0352  max mem: 359\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.8254 (0.7825)  Acc@1: 79.1667 (77.9762)  Acc@5: 95.8333 (94.6429)  time: 0.0922  data: 0.0028  max mem: 359\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.7302 (0.7429)  Acc@1: 79.1667 (79.0323)  Acc@5: 95.8333 (95.2957)  time: 0.0905  data: 0.0011  max mem: 359\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6630 (0.7101)  Acc@1: 79.1667 (79.4715)  Acc@5: 95.8333 (95.9350)  time: 0.0909  data: 0.0008  max mem: 359\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6173 (0.6993)  Acc@1: 83.3333 (79.7000)  Acc@5: 95.8333 (96.0000)  time: 0.0894  data: 0.0007  max mem: 359\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1032 s / it)\n",
            "* Acc@1 79.700 Acc@5 96.000 loss 0.699\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:33  Loss: 1.0309 (1.0309)  Acc@1: 66.6667 (66.6667)  Acc@5: 87.5000 (87.5000)  time: 0.7884  data: 0.6948  max mem: 359\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:05  Loss: 0.8234 (0.8648)  Acc@1: 75.0000 (75.3788)  Acc@5: 95.8333 (94.6970)  time: 0.1563  data: 0.0643  max mem: 359\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 0.8234 (0.9167)  Acc@1: 75.0000 (73.6111)  Acc@5: 95.8333 (94.2460)  time: 0.0917  data: 0.0014  max mem: 359\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.8909 (0.9070)  Acc@1: 70.8333 (73.9247)  Acc@5: 95.8333 (94.7581)  time: 0.0907  data: 0.0009  max mem: 359\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.8232 (0.8892)  Acc@1: 75.0000 (74.2886)  Acc@5: 95.8333 (95.1220)  time: 0.0917  data: 0.0003  max mem: 359\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.8129 (0.8855)  Acc@1: 75.0000 (74.3000)  Acc@5: 95.8333 (95.1000)  time: 0.0899  data: 0.0003  max mem: 359\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1100 s / it)\n",
            "* Acc@1 74.300 Acc@5 95.100 loss 0.885\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:22  Loss: 0.5199 (0.5199)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.5324  data: 0.4586  max mem: 359\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:04  Loss: 0.7855 (0.8028)  Acc@1: 75.0000 (78.0303)  Acc@5: 95.8333 (95.0758)  time: 0.1328  data: 0.0429  max mem: 359\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 0.7609 (0.7429)  Acc@1: 79.1667 (80.3571)  Acc@5: 95.8333 (95.2381)  time: 0.0920  data: 0.0009  max mem: 359\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.6630 (0.7194)  Acc@1: 79.1667 (80.1075)  Acc@5: 95.8333 (95.6989)  time: 0.0909  data: 0.0004  max mem: 359\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.7026 (0.7378)  Acc@1: 83.3333 (80.1829)  Acc@5: 95.8333 (95.1220)  time: 0.0910  data: 0.0004  max mem: 359\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.6983 (0.7359)  Acc@1: 83.3333 (80.0000)  Acc@5: 95.8333 (95.2000)  time: 0.0894  data: 0.0003  max mem: 359\n",
            "Test: [Task 3] Total time: 0:00:04 (0.1037 s / it)\n",
            "* Acc@1 80.000 Acc@5 95.200 loss 0.736\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:22  Loss: 1.2488 (1.2488)  Acc@1: 66.6667 (66.6667)  Acc@5: 87.5000 (87.5000)  time: 0.5312  data: 0.4385  max mem: 359\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:04  Loss: 0.8513 (0.8799)  Acc@1: 70.8333 (72.3485)  Acc@5: 95.8333 (95.4545)  time: 0.1334  data: 0.0415  max mem: 359\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 0.8180 (0.8462)  Acc@1: 75.0000 (74.4048)  Acc@5: 95.8333 (95.8333)  time: 0.0919  data: 0.0011  max mem: 359\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 0.7571 (0.7900)  Acc@1: 75.0000 (76.2097)  Acc@5: 95.8333 (96.3710)  time: 0.0903  data: 0.0004  max mem: 359\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7417 (0.8005)  Acc@1: 75.0000 (75.7114)  Acc@5: 95.8333 (96.2398)  time: 0.0906  data: 0.0004  max mem: 359\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.7417 (0.7906)  Acc@1: 75.0000 (76.0000)  Acc@5: 95.8333 (96.3000)  time: 0.0890  data: 0.0004  max mem: 359\n",
            "Test: [Task 4] Total time: 0:00:04 (0.1050 s / it)\n",
            "* Acc@1 76.000 Acc@5 96.300 loss 0.791\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:34  Loss: 0.6689 (0.6689)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.8228  data: 0.7204  max mem: 359\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:05  Loss: 0.7054 (0.7294)  Acc@1: 75.0000 (79.5455)  Acc@5: 100.0000 (98.1061)  time: 0.1603  data: 0.0713  max mem: 359\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 0.7285 (0.7533)  Acc@1: 75.0000 (78.7698)  Acc@5: 95.8333 (97.2222)  time: 0.0935  data: 0.0037  max mem: 359\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 0.7285 (0.7563)  Acc@1: 79.1667 (79.0323)  Acc@5: 95.8333 (96.7742)  time: 0.0925  data: 0.0012  max mem: 359\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.7605 (0.7937)  Acc@1: 75.0000 (77.3374)  Acc@5: 95.8333 (96.7480)  time: 0.0913  data: 0.0007  max mem: 359\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.8377 (0.8093)  Acc@1: 75.0000 (77.2000)  Acc@5: 95.8333 (96.6000)  time: 0.0901  data: 0.0007  max mem: 359\n",
            "Test: [Task 5] Total time: 0:00:04 (0.1122 s / it)\n",
            "* Acc@1 77.200 Acc@5 96.600 loss 0.809\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:22  Loss: 0.7749 (0.7749)  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  time: 0.5249  data: 0.4447  max mem: 359\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:04  Loss: 0.7921 (0.8744)  Acc@1: 79.1667 (74.6212)  Acc@5: 95.8333 (95.8333)  time: 0.1313  data: 0.0413  max mem: 359\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 0.8599 (0.8973)  Acc@1: 75.0000 (73.8095)  Acc@5: 95.8333 (95.6349)  time: 0.0909  data: 0.0010  max mem: 359\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 0.9204 (0.9101)  Acc@1: 75.0000 (74.0591)  Acc@5: 95.8333 (95.2957)  time: 0.0899  data: 0.0008  max mem: 359\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 0.9471 (0.9215)  Acc@1: 75.0000 (73.5772)  Acc@5: 95.8333 (95.2236)  time: 0.0904  data: 0.0004  max mem: 359\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 0.9471 (0.9245)  Acc@1: 75.0000 (73.5000)  Acc@5: 95.8333 (95.2000)  time: 0.0889  data: 0.0004  max mem: 359\n",
            "Test: [Task 6] Total time: 0:00:04 (0.1029 s / it)\n",
            "* Acc@1 73.500 Acc@5 95.200 loss 0.924\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:19  Loss: 1.1089 (1.1089)  Acc@1: 70.8333 (70.8333)  Acc@5: 87.5000 (87.5000)  time: 0.4753  data: 0.3895  max mem: 359\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:03  Loss: 0.8700 (0.8991)  Acc@1: 75.0000 (74.6212)  Acc@5: 95.8333 (95.0758)  time: 0.1242  data: 0.0366  max mem: 359\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:02  Loss: 0.8285 (0.9046)  Acc@1: 75.0000 (74.6032)  Acc@5: 95.8333 (95.0397)  time: 0.0891  data: 0.0009  max mem: 359\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:01  Loss: 0.8908 (0.9475)  Acc@1: 70.8333 (73.6559)  Acc@5: 91.6667 (93.6828)  time: 0.0897  data: 0.0005  max mem: 359\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 0.9087 (0.9548)  Acc@1: 70.8333 (73.4756)  Acc@5: 91.6667 (93.5976)  time: 0.0903  data: 0.0004  max mem: 359\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 0.9087 (0.9518)  Acc@1: 75.0000 (73.6000)  Acc@5: 93.7500 (93.6000)  time: 0.0883  data: 0.0004  max mem: 359\n",
            "Test: [Task 7] Total time: 0:00:04 (0.1020 s / it)\n",
            "* Acc@1 73.600 Acc@5 93.600 loss 0.952\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:33  Loss: 0.9999 (0.9999)  Acc@1: 66.6667 (66.6667)  Acc@5: 95.8333 (95.8333)  time: 0.7901  data: 0.6985  max mem: 359\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:05  Loss: 0.9999 (1.0441)  Acc@1: 70.8333 (73.4849)  Acc@5: 91.6667 (92.4242)  time: 0.1600  data: 0.0723  max mem: 359\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:02  Loss: 0.9843 (1.0129)  Acc@1: 70.8333 (72.2222)  Acc@5: 91.6667 (93.6508)  time: 0.0981  data: 0.0130  max mem: 359\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:01  Loss: 0.9795 (0.9927)  Acc@1: 70.8333 (72.8495)  Acc@5: 95.8333 (93.9516)  time: 0.1012  data: 0.0150  max mem: 359\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 1.0363 (1.0141)  Acc@1: 70.8333 (71.6463)  Acc@5: 91.6667 (93.5976)  time: 0.0965  data: 0.0071  max mem: 359\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 1.0363 (1.0093)  Acc@1: 70.8333 (71.7000)  Acc@5: 91.6667 (93.6000)  time: 0.0950  data: 0.0071  max mem: 359\n",
            "Test: [Task 8] Total time: 0:00:04 (0.1155 s / it)\n",
            "* Acc@1 71.700 Acc@5 93.600 loss 1.009\n",
            "Test: [Task 9]  [ 0/42]  eta: 0:00:23  Loss: 0.9637 (0.9637)  Acc@1: 66.6667 (66.6667)  Acc@5: 95.8333 (95.8333)  time: 0.5689  data: 0.4747  max mem: 359\n",
            "Test: [Task 9]  [10/42]  eta: 0:00:04  Loss: 0.9637 (1.0157)  Acc@1: 70.8333 (69.6970)  Acc@5: 95.8333 (93.5606)  time: 0.1331  data: 0.0439  max mem: 359\n",
            "Test: [Task 9]  [20/42]  eta: 0:00:02  Loss: 1.0177 (1.0018)  Acc@1: 70.8333 (71.2302)  Acc@5: 95.8333 (94.2460)  time: 0.0886  data: 0.0008  max mem: 359\n",
            "Test: [Task 9]  [30/42]  eta: 0:00:01  Loss: 1.0911 (1.0452)  Acc@1: 66.6667 (70.0269)  Acc@5: 95.8333 (94.8925)  time: 0.0882  data: 0.0006  max mem: 359\n",
            "Test: [Task 9]  [40/42]  eta: 0:00:00  Loss: 0.9485 (1.0057)  Acc@1: 75.0000 (71.1382)  Acc@5: 95.8333 (95.1220)  time: 0.0892  data: 0.0003  max mem: 359\n",
            "Test: [Task 9]  [41/42]  eta: 0:00:00  Loss: 0.8926 (0.9943)  Acc@1: 75.0000 (71.5000)  Acc@5: 95.8333 (95.2000)  time: 0.0872  data: 0.0003  max mem: 359\n",
            "Test: [Task 9] Total time: 0:00:04 (0.1019 s / it)\n",
            "* Acc@1 71.500 Acc@5 95.200 loss 0.994\n",
            "[Average accuracy till task9]\tAcc@1: 75.2778\tAcc@5: 95.2000\tLoss: 0.8667\tForgetting: 3.9875\tBackward: -2.3375\n",
            "Train: Epoch[1/1]  [  0/209]  eta: 0:02:23  Lr: 0.000047  Loss: 2.4121  Acc@1: 0.0000 (0.0000)  Acc@5: 45.8333 (45.8333)  time: 0.6864  data: 0.5636  max mem: 359\n",
            "Train: Epoch[1/1]  [ 10/209]  eta: 0:00:29  Lr: 0.000047  Loss: 2.1634  Acc@1: 8.3333 (10.2273)  Acc@5: 62.5000 (60.2273)  time: 0.1459  data: 0.0527  max mem: 359\n",
            "Train: Epoch[1/1]  [ 20/209]  eta: 0:00:22  Lr: 0.000047  Loss: 2.1953  Acc@1: 16.6667 (15.2778)  Acc@5: 58.3333 (59.9206)  time: 0.0909  data: 0.0016  max mem: 359\n",
            "Train: Epoch[1/1]  [ 30/209]  eta: 0:00:19  Lr: 0.000047  Loss: 2.0092  Acc@1: 25.0000 (20.9677)  Acc@5: 75.0000 (66.9355)  time: 0.0892  data: 0.0009  max mem: 359\n",
            "Train: Epoch[1/1]  [ 40/209]  eta: 0:00:17  Lr: 0.000047  Loss: 1.7138  Acc@1: 33.3333 (25.7114)  Acc@5: 87.5000 (70.9350)  time: 0.0889  data: 0.0007  max mem: 359\n",
            "Train: Epoch[1/1]  [ 50/209]  eta: 0:00:16  Lr: 0.000047  Loss: 1.6646  Acc@1: 41.6667 (30.8007)  Acc@5: 87.5000 (74.5915)  time: 0.0906  data: 0.0011  max mem: 359\n",
            "Train: Epoch[1/1]  [ 60/209]  eta: 0:00:15  Lr: 0.000047  Loss: 1.7191  Acc@1: 54.1667 (35.2459)  Acc@5: 91.6667 (77.1858)  time: 0.0997  data: 0.0016  max mem: 359\n",
            "Train: Epoch[1/1]  [ 70/209]  eta: 0:00:14  Lr: 0.000047  Loss: 1.5149  Acc@1: 62.5000 (39.6714)  Acc@5: 95.8333 (79.8709)  time: 0.1066  data: 0.0027  max mem: 359\n",
            "Train: Epoch[1/1]  [ 80/209]  eta: 0:00:13  Lr: 0.000047  Loss: 1.3386  Acc@1: 66.6667 (43.6214)  Acc@5: 95.8333 (81.9959)  time: 0.1088  data: 0.0049  max mem: 359\n",
            "Train: Epoch[1/1]  [ 90/209]  eta: 0:00:12  Lr: 0.000047  Loss: 1.0646  Acc@1: 70.8333 (47.0238)  Acc@5: 100.0000 (83.6538)  time: 0.1078  data: 0.0042  max mem: 359\n",
            "Train: Epoch[1/1]  [100/209]  eta: 0:00:11  Lr: 0.000047  Loss: 1.3610  Acc@1: 70.8333 (49.4224)  Acc@5: 95.8333 (84.7772)  time: 0.1012  data: 0.0018  max mem: 359\n",
            "Train: Epoch[1/1]  [110/209]  eta: 0:00:10  Lr: 0.000047  Loss: 1.3354  Acc@1: 70.8333 (52.0646)  Acc@5: 95.8333 (85.9234)  time: 0.0954  data: 0.0015  max mem: 359\n",
            "Train: Epoch[1/1]  [120/209]  eta: 0:00:09  Lr: 0.000047  Loss: 1.0397  Acc@1: 79.1667 (54.4077)  Acc@5: 100.0000 (86.9490)  time: 0.0915  data: 0.0011  max mem: 359\n",
            "Train: Epoch[1/1]  [130/209]  eta: 0:00:07  Lr: 0.000047  Loss: 1.1813  Acc@1: 83.3333 (56.5204)  Acc@5: 100.0000 (87.8499)  time: 0.0896  data: 0.0009  max mem: 359\n",
            "Train: Epoch[1/1]  [140/209]  eta: 0:00:06  Lr: 0.000047  Loss: 1.0380  Acc@1: 79.1667 (58.1265)  Acc@5: 100.0000 (88.5638)  time: 0.0894  data: 0.0011  max mem: 359\n",
            "Train: Epoch[1/1]  [150/209]  eta: 0:00:05  Lr: 0.000047  Loss: 1.0900  Acc@1: 79.1667 (59.6027)  Acc@5: 100.0000 (89.2384)  time: 0.0889  data: 0.0008  max mem: 359\n",
            "Train: Epoch[1/1]  [160/209]  eta: 0:00:04  Lr: 0.000047  Loss: 0.6948  Acc@1: 83.3333 (61.0766)  Acc@5: 100.0000 (89.7774)  time: 0.0884  data: 0.0008  max mem: 359\n",
            "Train: Epoch[1/1]  [170/209]  eta: 0:00:03  Lr: 0.000047  Loss: 0.8053  Acc@1: 83.3333 (62.3538)  Acc@5: 100.0000 (90.2778)  time: 0.0892  data: 0.0010  max mem: 359\n",
            "Train: Epoch[1/1]  [180/209]  eta: 0:00:02  Lr: 0.000047  Loss: 0.6877  Acc@1: 83.3333 (63.4669)  Acc@5: 100.0000 (90.8149)  time: 0.0894  data: 0.0009  max mem: 359\n",
            "Train: Epoch[1/1]  [190/209]  eta: 0:00:01  Lr: 0.000047  Loss: 0.8107  Acc@1: 83.3333 (64.4197)  Acc@5: 100.0000 (91.1649)  time: 0.0894  data: 0.0010  max mem: 359\n",
            "Train: Epoch[1/1]  [200/209]  eta: 0:00:00  Lr: 0.000047  Loss: 0.8763  Acc@1: 83.3333 (65.4229)  Acc@5: 100.0000 (91.5216)  time: 0.0906  data: 0.0011  max mem: 359\n",
            "Train: Epoch[1/1]  [208/209]  eta: 0:00:00  Lr: 0.000047  Loss: 0.7664  Acc@1: 87.5000 (66.1600)  Acc@5: 100.0000 (91.7800)  time: 0.0874  data: 0.0007  max mem: 359\n",
            "Train: Epoch[1/1] Total time: 0:00:20 (0.0972 s / it)\n",
            "Averaged stats: Lr: 0.000047  Loss: 0.7664  Acc@1: 87.5000 (66.1600)  Acc@5: 100.0000 (91.7800)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:35  Loss: 0.8706 (0.8706)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  time: 0.8504  data: 0.7589  max mem: 359\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:05  Loss: 0.8630 (0.8113)  Acc@1: 75.0000 (76.1364)  Acc@5: 95.8333 (94.6970)  time: 0.1684  data: 0.0826  max mem: 359\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.8167 (0.7982)  Acc@1: 75.0000 (76.9841)  Acc@5: 95.8333 (94.2460)  time: 0.0950  data: 0.0083  max mem: 359\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.7321 (0.7584)  Acc@1: 79.1667 (78.4946)  Acc@5: 95.8333 (94.8925)  time: 0.0898  data: 0.0011  max mem: 359\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6444 (0.7253)  Acc@1: 83.3333 (79.0650)  Acc@5: 95.8333 (95.7317)  time: 0.0894  data: 0.0005  max mem: 359\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6232 (0.7146)  Acc@1: 83.3333 (79.3000)  Acc@5: 100.0000 (95.8000)  time: 0.0880  data: 0.0005  max mem: 359\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1121 s / it)\n",
            "* Acc@1 79.300 Acc@5 95.800 loss 0.715\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:15  Loss: 1.0894 (1.0894)  Acc@1: 66.6667 (66.6667)  Acc@5: 91.6667 (91.6667)  time: 0.3592  data: 0.2874  max mem: 359\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:03  Loss: 0.9097 (0.9413)  Acc@1: 75.0000 (72.7273)  Acc@5: 95.8333 (95.0758)  time: 0.1239  data: 0.0405  max mem: 359\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 0.9097 (0.9817)  Acc@1: 70.8333 (72.0238)  Acc@5: 95.8333 (94.2460)  time: 0.0944  data: 0.0082  max mem: 359\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.9518 (0.9772)  Acc@1: 70.8333 (72.4462)  Acc@5: 95.8333 (94.2204)  time: 0.0886  data: 0.0006  max mem: 359\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.9484 (0.9694)  Acc@1: 70.8333 (72.6626)  Acc@5: 95.8333 (94.5122)  time: 0.0888  data: 0.0004  max mem: 359\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.9219 (0.9660)  Acc@1: 70.8333 (72.7000)  Acc@5: 95.8333 (94.5000)  time: 0.0869  data: 0.0004  max mem: 359\n",
            "Test: [Task 2] Total time: 0:00:04 (0.0999 s / it)\n",
            "* Acc@1 72.700 Acc@5 94.500 loss 0.966\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:23  Loss: 0.5555 (0.5555)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.5649  data: 0.4953  max mem: 359\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:04  Loss: 0.8615 (0.8337)  Acc@1: 75.0000 (76.1364)  Acc@5: 95.8333 (95.8333)  time: 0.1321  data: 0.0459  max mem: 359\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 0.7794 (0.7835)  Acc@1: 79.1667 (78.7698)  Acc@5: 95.8333 (95.8333)  time: 0.0879  data: 0.0007  max mem: 359\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.7213 (0.7653)  Acc@1: 79.1667 (78.2258)  Acc@5: 95.8333 (96.1022)  time: 0.0876  data: 0.0005  max mem: 359\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.7631 (0.7847)  Acc@1: 79.1667 (78.0488)  Acc@5: 95.8333 (95.7317)  time: 0.0887  data: 0.0006  max mem: 359\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.7631 (0.7844)  Acc@1: 79.1667 (77.9000)  Acc@5: 95.8333 (95.8000)  time: 0.0868  data: 0.0006  max mem: 359\n",
            "Test: [Task 3] Total time: 0:00:04 (0.1017 s / it)\n",
            "* Acc@1 77.900 Acc@5 95.800 loss 0.784\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:30  Loss: 1.3110 (1.3110)  Acc@1: 66.6667 (66.6667)  Acc@5: 83.3333 (83.3333)  time: 0.7373  data: 0.6149  max mem: 359\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:04  Loss: 0.8475 (0.8925)  Acc@1: 75.0000 (73.1061)  Acc@5: 95.8333 (95.0758)  time: 0.1542  data: 0.0658  max mem: 359\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 0.8163 (0.8482)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.6349)  time: 0.1020  data: 0.0171  max mem: 359\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 0.7101 (0.8016)  Acc@1: 75.0000 (77.0161)  Acc@5: 95.8333 (95.9677)  time: 0.0987  data: 0.0119  max mem: 359\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.7948 (0.8131)  Acc@1: 75.0000 (76.6260)  Acc@5: 95.8333 (95.9350)  time: 0.0886  data: 0.0007  max mem: 359\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.7948 (0.8031)  Acc@1: 75.0000 (76.9000)  Acc@5: 95.8333 (96.0000)  time: 0.0873  data: 0.0006  max mem: 359\n",
            "Test: [Task 4] Total time: 0:00:04 (0.1126 s / it)\n",
            "* Acc@1 76.900 Acc@5 96.000 loss 0.803\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:20  Loss: 0.6256 (0.6256)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.4774  data: 0.3845  max mem: 359\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:03  Loss: 0.7094 (0.7365)  Acc@1: 79.1667 (78.4091)  Acc@5: 100.0000 (98.1061)  time: 0.1245  data: 0.0364  max mem: 359\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 0.7300 (0.7537)  Acc@1: 79.1667 (78.5714)  Acc@5: 95.8333 (97.4206)  time: 0.0881  data: 0.0011  max mem: 359\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 0.7198 (0.7530)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (96.6398)  time: 0.0874  data: 0.0005  max mem: 359\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.7709 (0.7887)  Acc@1: 79.1667 (78.2520)  Acc@5: 95.8333 (96.5447)  time: 0.0887  data: 0.0003  max mem: 359\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.8328 (0.8044)  Acc@1: 79.1667 (78.1000)  Acc@5: 95.8333 (96.4000)  time: 0.0868  data: 0.0003  max mem: 359\n",
            "Test: [Task 5] Total time: 0:00:04 (0.0994 s / it)\n",
            "* Acc@1 78.100 Acc@5 96.400 loss 0.804\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:21  Loss: 0.7978 (0.7978)  Acc@1: 83.3333 (83.3333)  Acc@5: 91.6667 (91.6667)  time: 0.5007  data: 0.4234  max mem: 359\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:04  Loss: 0.8017 (0.9091)  Acc@1: 79.1667 (74.6212)  Acc@5: 95.8333 (94.3182)  time: 0.1311  data: 0.0432  max mem: 359\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 0.9060 (0.9319)  Acc@1: 75.0000 (74.0079)  Acc@5: 91.6667 (94.2460)  time: 0.0909  data: 0.0028  max mem: 359\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 0.9286 (0.9472)  Acc@1: 75.0000 (73.9247)  Acc@5: 91.6667 (94.6237)  time: 0.0881  data: 0.0005  max mem: 359\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 0.9408 (0.9570)  Acc@1: 75.0000 (73.7805)  Acc@5: 95.8333 (94.7154)  time: 0.0891  data: 0.0005  max mem: 359\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 0.9408 (0.9592)  Acc@1: 75.0000 (73.8000)  Acc@5: 95.8333 (94.6000)  time: 0.0872  data: 0.0004  max mem: 359\n",
            "Test: [Task 6] Total time: 0:00:04 (0.1016 s / it)\n",
            "* Acc@1 73.800 Acc@5 94.600 loss 0.959\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:19  Loss: 1.2037 (1.2037)  Acc@1: 79.1667 (79.1667)  Acc@5: 87.5000 (87.5000)  time: 0.4563  data: 0.3675  max mem: 359\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:04  Loss: 0.8959 (0.9399)  Acc@1: 79.1667 (78.0303)  Acc@5: 91.6667 (93.9394)  time: 0.1261  data: 0.0392  max mem: 359\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:02  Loss: 0.8926 (0.9406)  Acc@1: 75.0000 (75.9921)  Acc@5: 91.6667 (94.0476)  time: 0.0910  data: 0.0036  max mem: 359\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:01  Loss: 0.9086 (0.9743)  Acc@1: 70.8333 (74.8656)  Acc@5: 95.8333 (93.4140)  time: 0.0892  data: 0.0008  max mem: 359\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 0.9025 (0.9782)  Acc@1: 70.8333 (74.8984)  Acc@5: 95.8333 (93.5976)  time: 0.0894  data: 0.0007  max mem: 359\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 0.9005 (0.9756)  Acc@1: 75.0000 (74.9000)  Acc@5: 95.8333 (93.6000)  time: 0.0876  data: 0.0007  max mem: 359\n",
            "Test: [Task 7] Total time: 0:00:04 (0.1016 s / it)\n",
            "* Acc@1 74.900 Acc@5 93.600 loss 0.976\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:35  Loss: 1.1201 (1.1201)  Acc@1: 66.6667 (66.6667)  Acc@5: 91.6667 (91.6667)  time: 0.8511  data: 0.7297  max mem: 359\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:05  Loss: 1.1201 (1.1567)  Acc@1: 66.6667 (67.0455)  Acc@5: 91.6667 (91.2879)  time: 0.1589  data: 0.0672  max mem: 359\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:02  Loss: 1.1588 (1.1162)  Acc@1: 66.6667 (68.2540)  Acc@5: 91.6667 (93.0556)  time: 0.0888  data: 0.0007  max mem: 359\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:01  Loss: 1.0999 (1.0983)  Acc@1: 66.6667 (68.2796)  Acc@5: 95.8333 (93.2796)  time: 0.0881  data: 0.0006  max mem: 359\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 1.0999 (1.1251)  Acc@1: 62.5000 (67.7846)  Acc@5: 91.6667 (92.4797)  time: 0.0892  data: 0.0004  max mem: 359\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 1.0999 (1.1214)  Acc@1: 62.5000 (67.9000)  Acc@5: 91.6667 (92.5000)  time: 0.0875  data: 0.0004  max mem: 359\n",
            "Test: [Task 8] Total time: 0:00:04 (0.1090 s / it)\n",
            "* Acc@1 67.900 Acc@5 92.500 loss 1.121\n",
            "Test: [Task 9]  [ 0/42]  eta: 0:00:25  Loss: 1.0283 (1.0283)  Acc@1: 66.6667 (66.6667)  Acc@5: 95.8333 (95.8333)  time: 0.6038  data: 0.5256  max mem: 359\n",
            "Test: [Task 9]  [10/42]  eta: 0:00:04  Loss: 1.1119 (1.1573)  Acc@1: 66.6667 (65.9091)  Acc@5: 91.6667 (92.4242)  time: 0.1372  data: 0.0486  max mem: 359\n",
            "Test: [Task 9]  [20/42]  eta: 0:00:02  Loss: 1.1692 (1.1566)  Acc@1: 66.6667 (65.8730)  Acc@5: 91.6667 (92.8571)  time: 0.0894  data: 0.0006  max mem: 359\n",
            "Test: [Task 9]  [30/42]  eta: 0:00:01  Loss: 1.2028 (1.2016)  Acc@1: 62.5000 (65.3226)  Acc@5: 95.8333 (93.2796)  time: 0.0885  data: 0.0004  max mem: 359\n",
            "Test: [Task 9]  [40/42]  eta: 0:00:00  Loss: 1.1104 (1.1573)  Acc@1: 66.6667 (66.1585)  Acc@5: 95.8333 (93.6992)  time: 0.0897  data: 0.0003  max mem: 359\n",
            "Test: [Task 9]  [41/42]  eta: 0:00:00  Loss: 1.0487 (1.1457)  Acc@1: 66.6667 (66.3000)  Acc@5: 95.8333 (93.8000)  time: 0.0877  data: 0.0003  max mem: 359\n",
            "Test: [Task 9] Total time: 0:00:04 (0.1033 s / it)\n",
            "* Acc@1 66.300 Acc@5 93.800 loss 1.146\n",
            "Test: [Task 10]  [ 0/42]  eta: 0:00:19  Loss: 4.6748 (4.6748)  Acc@1: 0.0000 (0.0000)  Acc@5: 8.3333 (8.3333)  time: 0.4540  data: 0.3432  max mem: 359\n",
            "Test: [Task 10]  [10/42]  eta: 0:00:04  Loss: 4.6748 (4.7493)  Acc@1: 0.0000 (0.0000)  Acc@5: 12.5000 (11.3636)  time: 0.1259  data: 0.0351  max mem: 359\n",
            "Test: [Task 10]  [20/42]  eta: 0:00:02  Loss: 4.6918 (4.7195)  Acc@1: 0.0000 (0.0000)  Acc@5: 12.5000 (12.3016)  time: 0.0909  data: 0.0024  max mem: 359\n",
            "Test: [Task 10]  [30/42]  eta: 0:00:01  Loss: 4.8162 (4.7427)  Acc@1: 0.0000 (0.0000)  Acc@5: 12.5000 (12.9032)  time: 0.0893  data: 0.0004  max mem: 359\n",
            "Test: [Task 10]  [40/42]  eta: 0:00:00  Loss: 4.8162 (4.7471)  Acc@1: 0.0000 (0.0000)  Acc@5: 12.5000 (13.0081)  time: 0.0900  data: 0.0008  max mem: 359\n",
            "Test: [Task 10]  [41/42]  eta: 0:00:00  Loss: 4.7983 (4.7381)  Acc@1: 0.0000 (0.0000)  Acc@5: 16.6667 (13.1000)  time: 0.0881  data: 0.0008  max mem: 359\n",
            "Test: [Task 10] Total time: 0:00:04 (0.1018 s / it)\n",
            "* Acc@1 0.000 Acc@5 13.100 loss 4.738\n",
            "[Average accuracy till task10]\tAcc@1: 66.7800\tAcc@5: 86.6100\tLoss: 1.3012\tForgetting: 3.5778\tBackward: 47.8778\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "module.mlp.norm.weight\n",
            "module.mlp.norm.bias\n",
            "module.mlp.net.0.0.weight\n",
            "module.mlp.net.0.0.bias\n",
            "module.mlp.net.1.0.weight\n",
            "module.mlp.net.1.0.bias\n",
            "module.head.weight\n",
            "module.head.bias\n",
            "torch.Size([23976, 384])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.000469  Loss: 0.8323  Acc@1: 79.1667 (82.0370)  Acc@5: 95.8333 (93.0556)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000467  Loss: 0.7465  Acc@1: 83.3333 (83.8426)  Acc@5: 100.0000 (97.6389)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000464  Loss: 0.7908  Acc@1: 83.3333 (85.6482)  Acc@5: 100.0000 (98.9352)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000457  Loss: 0.4988  Acc@1: 87.5000 (87.3148)  Acc@5: 100.0000 (99.3981)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000448  Loss: 0.4569  Acc@1: 87.5000 (89.3519)  Acc@5: 100.0000 (99.4444)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000437  Loss: 0.5899  Acc@1: 91.6667 (89.1204)  Acc@5: 100.0000 (99.7222)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000424  Loss: 0.6070  Acc@1: 91.6667 (90.2778)  Acc@5: 100.0000 (99.3056)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000409  Loss: 0.3519  Acc@1: 91.6667 (89.7222)  Acc@5: 100.0000 (99.7222)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000391  Loss: 0.4618  Acc@1: 87.5000 (90.3241)  Acc@5: 100.0000 (99.7222)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000372  Loss: 0.3741  Acc@1: 91.6667 (90.0000)  Acc@5: 100.0000 (99.7685)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000352  Loss: 0.5313  Acc@1: 87.5000 (91.5741)  Acc@5: 100.0000 (99.7222)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000330  Loss: 0.4388  Acc@1: 91.6667 (91.0648)  Acc@5: 100.0000 (99.7685)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000307  Loss: 0.3270  Acc@1: 95.8333 (92.1296)  Acc@5: 100.0000 (99.8148)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000283  Loss: 0.4059  Acc@1: 91.6667 (90.9259)  Acc@5: 100.0000 (99.6759)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000259  Loss: 0.5961  Acc@1: 91.6667 (91.2500)  Acc@5: 100.0000 (99.8611)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000234  Loss: 0.3537  Acc@1: 87.5000 (90.6019)  Acc@5: 100.0000 (99.7222)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000210  Loss: 0.4164  Acc@1: 91.6667 (91.0648)  Acc@5: 100.0000 (99.5833)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000186  Loss: 0.4559  Acc@1: 95.8333 (92.7778)  Acc@5: 100.0000 (99.8148)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000162  Loss: 0.5278  Acc@1: 91.6667 (91.2037)  Acc@5: 100.0000 (99.6759)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000139  Loss: 0.5827  Acc@1: 91.6667 (91.2037)  Acc@5: 100.0000 (99.7222)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000117  Loss: 0.5534  Acc@1: 91.6667 (91.1111)  Acc@5: 100.0000 (99.7685)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000097  Loss: 0.4864  Acc@1: 91.6667 (91.8982)  Acc@5: 100.0000 (99.6759)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000078  Loss: 0.4128  Acc@1: 95.8333 (92.4074)  Acc@5: 100.0000 (99.8611)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000060  Loss: 0.5038  Acc@1: 95.8333 (92.0833)  Acc@5: 100.0000 (99.9537)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000045  Loss: 0.4052  Acc@1: 91.6667 (91.6204)  Acc@5: 100.0000 (99.8611)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000031  Loss: 0.3292  Acc@1: 91.6667 (91.8519)  Acc@5: 100.0000 (99.8148)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000020  Loss: 0.4882  Acc@1: 87.5000 (91.7593)  Acc@5: 100.0000 (99.6759)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000011  Loss: 0.2655  Acc@1: 91.6667 (92.3148)  Acc@5: 100.0000 (99.7222)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000005  Loss: 0.2737  Acc@1: 91.6667 (92.0833)  Acc@5: 100.0000 (99.8611)\n",
            "torch.Size([23976, 384])\n",
            "Averaged stats: Lr: 0.000001  Loss: 0.3652  Acc@1: 87.5000 (91.3426)  Acc@5: 100.0000 (99.6296)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:38  Loss: 0.8525 (0.8525)  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  time: 0.9262  data: 0.8169  max mem: 359\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:05  Loss: 0.8612 (0.8227)  Acc@1: 75.0000 (76.5152)  Acc@5: 95.8333 (93.9394)  time: 0.1651  data: 0.0758  max mem: 359\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:02  Loss: 0.8348 (0.7943)  Acc@1: 79.1667 (78.5714)  Acc@5: 95.8333 (94.4444)  time: 0.0894  data: 0.0012  max mem: 359\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:01  Loss: 0.7300 (0.7554)  Acc@1: 79.1667 (78.6290)  Acc@5: 95.8333 (95.4301)  time: 0.0901  data: 0.0006  max mem: 359\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6811 (0.7228)  Acc@1: 83.3333 (79.2683)  Acc@5: 95.8333 (95.9350)  time: 0.0905  data: 0.0004  max mem: 359\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6356 (0.7111)  Acc@1: 83.3333 (79.5000)  Acc@5: 95.8333 (96.0000)  time: 0.0889  data: 0.0003  max mem: 359\n",
            "Test: [Task 1] Total time: 0:00:04 (0.1114 s / it)\n",
            "* Acc@1 79.500 Acc@5 96.000 loss 0.711\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:23  Loss: 1.0235 (1.0235)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  time: 0.5679  data: 0.4926  max mem: 359\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:04  Loss: 0.8367 (0.8529)  Acc@1: 79.1667 (77.6515)  Acc@5: 95.8333 (95.4545)  time: 0.1351  data: 0.0456  max mem: 359\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:02  Loss: 0.8367 (0.9140)  Acc@1: 75.0000 (74.6032)  Acc@5: 95.8333 (95.0397)  time: 0.0906  data: 0.0006  max mem: 359\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:01  Loss: 0.9055 (0.9095)  Acc@1: 70.8333 (74.5968)  Acc@5: 95.8333 (94.6237)  time: 0.0895  data: 0.0003  max mem: 359\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.8438 (0.8939)  Acc@1: 70.8333 (74.4919)  Acc@5: 95.8333 (94.9187)  time: 0.0901  data: 0.0003  max mem: 359\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.8204 (0.8906)  Acc@1: 70.8333 (74.5000)  Acc@5: 95.8333 (94.9000)  time: 0.0886  data: 0.0003  max mem: 359\n",
            "Test: [Task 2] Total time: 0:00:04 (0.1036 s / it)\n",
            "* Acc@1 74.500 Acc@5 94.900 loss 0.891\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:22  Loss: 0.4728 (0.4728)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  time: 0.5271  data: 0.4414  max mem: 359\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:04  Loss: 0.7990 (0.7793)  Acc@1: 75.0000 (77.2727)  Acc@5: 95.8333 (95.4545)  time: 0.1311  data: 0.0410  max mem: 359\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:02  Loss: 0.7242 (0.7237)  Acc@1: 79.1667 (79.5635)  Acc@5: 95.8333 (95.6349)  time: 0.0909  data: 0.0010  max mem: 359\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:01  Loss: 0.7196 (0.7082)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.6989)  time: 0.0908  data: 0.0019  max mem: 359\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.7229 (0.7244)  Acc@1: 79.1667 (79.5732)  Acc@5: 95.8333 (95.2236)  time: 0.0912  data: 0.0016  max mem: 359\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.7229 (0.7234)  Acc@1: 79.1667 (79.4000)  Acc@5: 95.8333 (95.3000)  time: 0.0891  data: 0.0015  max mem: 359\n",
            "Test: [Task 3] Total time: 0:00:04 (0.1048 s / it)\n",
            "* Acc@1 79.400 Acc@5 95.300 loss 0.723\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:33  Loss: 1.3775 (1.3775)  Acc@1: 62.5000 (62.5000)  Acc@5: 83.3333 (83.3333)  time: 0.7921  data: 0.6873  max mem: 359\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:05  Loss: 0.9546 (0.9836)  Acc@1: 70.8333 (66.6667)  Acc@5: 95.8333 (94.3182)  time: 0.1564  data: 0.0662  max mem: 359\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:02  Loss: 0.8934 (0.9346)  Acc@1: 70.8333 (70.2381)  Acc@5: 95.8333 (94.6429)  time: 0.0915  data: 0.0028  max mem: 359\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:01  Loss: 0.8131 (0.8764)  Acc@1: 75.0000 (72.4462)  Acc@5: 95.8333 (95.2957)  time: 0.0903  data: 0.0010  max mem: 359\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.8295 (0.8874)  Acc@1: 75.0000 (72.2561)  Acc@5: 95.8333 (95.0203)  time: 0.0908  data: 0.0003  max mem: 359\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.8138 (0.8770)  Acc@1: 75.0000 (72.6000)  Acc@5: 95.8333 (95.1000)  time: 0.0893  data: 0.0003  max mem: 359\n",
            "Test: [Task 4] Total time: 0:00:04 (0.1097 s / it)\n",
            "* Acc@1 72.600 Acc@5 95.100 loss 0.877\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:22  Loss: 0.7497 (0.7497)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.5348  data: 0.4543  max mem: 359\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:04  Loss: 0.7497 (0.7856)  Acc@1: 79.1667 (77.2727)  Acc@5: 100.0000 (98.1061)  time: 0.1303  data: 0.0421  max mem: 359\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:02  Loss: 0.7619 (0.8145)  Acc@1: 75.0000 (75.7937)  Acc@5: 95.8333 (97.0238)  time: 0.0897  data: 0.0006  max mem: 359\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:01  Loss: 0.7581 (0.8048)  Acc@1: 75.0000 (76.6129)  Acc@5: 95.8333 (96.2366)  time: 0.0899  data: 0.0005  max mem: 359\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.8180 (0.8403)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (96.3415)  time: 0.0906  data: 0.0004  max mem: 359\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.8586 (0.8571)  Acc@1: 75.0000 (74.8000)  Acc@5: 95.8333 (96.2000)  time: 0.0887  data: 0.0004  max mem: 359\n",
            "Test: [Task 5] Total time: 0:00:04 (0.1024 s / it)\n",
            "* Acc@1 74.800 Acc@5 96.200 loss 0.857\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:21  Loss: 0.7902 (0.7902)  Acc@1: 83.3333 (83.3333)  Acc@5: 91.6667 (91.6667)  time: 0.5035  data: 0.4300  max mem: 359\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:04  Loss: 0.8383 (0.8717)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  time: 0.1285  data: 0.0401  max mem: 359\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:02  Loss: 0.8683 (0.9056)  Acc@1: 70.8333 (73.2143)  Acc@5: 95.8333 (95.4365)  time: 0.0900  data: 0.0007  max mem: 359\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:01  Loss: 0.9550 (0.9234)  Acc@1: 75.0000 (73.7903)  Acc@5: 95.8333 (95.1613)  time: 0.0895  data: 0.0003  max mem: 359\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 0.9701 (0.9300)  Acc@1: 75.0000 (73.4756)  Acc@5: 95.8333 (94.8171)  time: 0.0897  data: 0.0004  max mem: 359\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 0.9701 (0.9330)  Acc@1: 75.0000 (73.4000)  Acc@5: 95.8333 (94.8000)  time: 0.0882  data: 0.0004  max mem: 359\n",
            "Test: [Task 6] Total time: 0:00:04 (0.1034 s / it)\n",
            "* Acc@1 73.400 Acc@5 94.800 loss 0.933\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:29  Loss: 1.2107 (1.2107)  Acc@1: 66.6667 (66.6667)  Acc@5: 91.6667 (91.6667)  time: 0.6929  data: 0.5938  max mem: 359\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:04  Loss: 0.8847 (0.9004)  Acc@1: 79.1667 (76.8939)  Acc@5: 91.6667 (94.3182)  time: 0.1492  data: 0.0621  max mem: 359\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:02  Loss: 0.8586 (0.9087)  Acc@1: 75.0000 (76.3889)  Acc@5: 95.8333 (94.6429)  time: 0.0940  data: 0.0067  max mem: 359\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:01  Loss: 0.9029 (0.9529)  Acc@1: 70.8333 (74.4624)  Acc@5: 95.8333 (93.5484)  time: 0.0918  data: 0.0027  max mem: 359\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 0.9321 (0.9577)  Acc@1: 70.8333 (74.2886)  Acc@5: 95.8333 (93.6992)  time: 0.0900  data: 0.0006  max mem: 359\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 0.9321 (0.9539)  Acc@1: 75.0000 (74.4000)  Acc@5: 95.8333 (93.7000)  time: 0.0888  data: 0.0006  max mem: 359\n",
            "Test: [Task 7] Total time: 0:00:04 (0.1080 s / it)\n",
            "* Acc@1 74.400 Acc@5 93.700 loss 0.954\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:20  Loss: 1.0297 (1.0297)  Acc@1: 66.6667 (66.6667)  Acc@5: 95.8333 (95.8333)  time: 0.4965  data: 0.4195  max mem: 359\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:04  Loss: 1.0297 (1.0673)  Acc@1: 70.8333 (73.1061)  Acc@5: 91.6667 (92.4242)  time: 0.1275  data: 0.0389  max mem: 359\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:02  Loss: 1.0395 (1.0257)  Acc@1: 66.6667 (71.0317)  Acc@5: 91.6667 (93.8492)  time: 0.0894  data: 0.0006  max mem: 359\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:01  Loss: 1.0395 (1.0014)  Acc@1: 70.8333 (71.3710)  Acc@5: 95.8333 (93.9516)  time: 0.0885  data: 0.0003  max mem: 359\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 1.0460 (1.0228)  Acc@1: 70.8333 (70.4268)  Acc@5: 91.6667 (93.3943)  time: 0.0893  data: 0.0003  max mem: 359\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 1.0460 (1.0183)  Acc@1: 70.8333 (70.6000)  Acc@5: 91.6667 (93.5000)  time: 0.0874  data: 0.0003  max mem: 359\n",
            "Test: [Task 8] Total time: 0:00:04 (0.1008 s / it)\n",
            "* Acc@1 70.600 Acc@5 93.500 loss 1.018\n",
            "Test: [Task 9]  [ 0/42]  eta: 0:00:16  Loss: 0.8055 (0.8055)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  time: 0.4025  data: 0.3227  max mem: 359\n",
            "Test: [Task 9]  [10/42]  eta: 0:00:04  Loss: 0.8662 (0.9138)  Acc@1: 70.8333 (72.7273)  Acc@5: 91.6667 (93.9394)  time: 0.1251  data: 0.0404  max mem: 359\n",
            "Test: [Task 9]  [20/42]  eta: 0:00:02  Loss: 0.8687 (0.8940)  Acc@1: 70.8333 (74.8016)  Acc@5: 91.6667 (94.2460)  time: 0.0930  data: 0.0063  max mem: 359\n",
            "Test: [Task 9]  [30/42]  eta: 0:00:01  Loss: 0.9308 (0.9358)  Acc@1: 75.0000 (73.5215)  Acc@5: 95.8333 (94.7581)  time: 0.0891  data: 0.0005  max mem: 359\n",
            "Test: [Task 9]  [40/42]  eta: 0:00:00  Loss: 0.8590 (0.8886)  Acc@1: 75.0000 (74.8984)  Acc@5: 95.8333 (94.9187)  time: 0.0898  data: 0.0005  max mem: 359\n",
            "Test: [Task 9]  [41/42]  eta: 0:00:00  Loss: 0.7570 (0.8763)  Acc@1: 75.0000 (75.2000)  Acc@5: 95.8333 (95.0000)  time: 0.0877  data: 0.0005  max mem: 359\n",
            "Test: [Task 9] Total time: 0:00:04 (0.1019 s / it)\n",
            "* Acc@1 75.200 Acc@5 95.000 loss 0.876\n",
            "Test: [Task 10]  [ 0/42]  eta: 0:00:40  Loss: 1.0281 (1.0281)  Acc@1: 70.8333 (70.8333)  Acc@5: 100.0000 (100.0000)  time: 0.9558  data: 0.8540  max mem: 359\n",
            "Test: [Task 10]  [10/42]  eta: 0:00:05  Loss: 1.1689 (1.1803)  Acc@1: 66.6667 (66.2879)  Acc@5: 95.8333 (93.9394)  time: 0.1716  data: 0.0845  max mem: 359\n",
            "Test: [Task 10]  [20/42]  eta: 0:00:02  Loss: 1.1596 (1.1550)  Acc@1: 62.5000 (65.6746)  Acc@5: 95.8333 (93.6508)  time: 0.0947  data: 0.0083  max mem: 359\n",
            "Test: [Task 10]  [30/42]  eta: 0:00:01  Loss: 1.1596 (1.1619)  Acc@1: 62.5000 (64.7849)  Acc@5: 95.8333 (93.8172)  time: 0.0961  data: 0.0097  max mem: 359\n",
            "Test: [Task 10]  [40/42]  eta: 0:00:00  Loss: 1.1930 (1.1669)  Acc@1: 62.5000 (64.3293)  Acc@5: 95.8333 (93.9024)  time: 0.0929  data: 0.0052  max mem: 359\n",
            "Test: [Task 10]  [41/42]  eta: 0:00:00  Loss: 1.1925 (1.1598)  Acc@1: 62.5000 (64.5000)  Acc@5: 95.8333 (93.9000)  time: 0.0911  data: 0.0052  max mem: 359\n",
            "Test: [Task 10] Total time: 0:00:04 (0.1160 s / it)\n",
            "* Acc@1 64.500 Acc@5 93.900 loss 1.160\n",
            "[Average accuracy till task10]\tAcc@1: 73.8900\tAcc@5: 94.8400\tLoss: 0.9000\tForgetting: 4.3889\tBackward: -2.4222\n",
            "Total training time: 0:21:59\n",
            "[rank0]:[W1003 15:19:34.494368040 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
          ]
        }
      ],
      "source": [
        "!torchrun --nproc_per_node=1 main.py cifar100_hideprompt_5e --original_model vit_small_patch16_224.dino --model vit_small_patch16_224.dino --batch-size 24 --data-path ./datasets/ --output_dir ./output/cifar100_full_dino_1epoch_25pct --epochs 1 --sched constant --seed 20 --train_inference_task_only --lr 0.0005 --pct 0.25\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "902dd0fc",
      "metadata": {
        "id": "902dd0fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12dcd6ef-9eb7-4c12-c2e4-a3511c26d0ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(subparser_name='cifar100_hideprompt_5e', pct=1.0, batch_size=24, epochs=1, original_model='vit_small_patch16_224.dino', model='vit_small_patch16_224.dino', input_size=224, pretrained=True, drop=0.0, drop_path=0.0, opt='adam', opt_eps=1e-08, opt_betas=(0.9, 0.999), clip_grad=1.0, momentum=0.9, weight_decay=0.0, reinit_optimizer=True, sched='step', lr=0.03, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=0, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, unscale_lr=True, color_jitter=None, aa=None, smoothing=0.1, train_interpolation='bicubic', reprob=0.0, remode='pixel', recount=1, data_path='./datasets/', dataset='Split-CIFAR100', shuffle=False, output_dir='./output/cifar100_full_dino_1epoch_final_100pct', device='cuda', seed=20, eval=False, num_workers=4, pin_mem=True, world_size=1, dist_url='env://', num_tasks=10, train_mask=True, task_inc=False, use_g_prompt=False, g_prompt_length=5, g_prompt_layer_idx=[], use_prefix_tune_for_g_prompt=False, use_e_prompt=True, e_prompt_layer_idx=[0, 1, 2, 3, 4], use_prefix_tune_for_e_prompt=True, larger_prompt_lr=True, prompt_pool=True, size=10, length=5, top_k=1, initializer='uniform', prompt_key=False, prompt_key_init='uniform', use_prompt_mask=True, mask_first_epoch=False, shared_prompt_pool=True, shared_prompt_key=False, batchwise_prompt=False, embedding_key='cls', predefined_key='', pull_constraint=True, pull_constraint_coeff=1.0, same_key_value=False, global_pool='token', head_type='token', freeze=['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed'], crct_epochs=1, train_inference_task_only=False, original_model_mlp_structure=[2], ca_lr=0.005, milestones=[10], trained_original_model='./output/cifar100_full_dino_1epoch_100pct', prompt_momentum=0.1, reg=0.1, not_train_ca=False, ca_epochs=30, ca_storage_efficient_method='multi-centroid', n_centroids=10, print_freq=10, config='cifar100_hideprompt_5e')\n",
            "| distributed init (rank 0): env://\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "[rank0]:[W1003 15:20:30.845468145 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/helpers.py:7: FutureWarning: Importing from timm.models.helpers is deprecated, please import via timm.models\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:886: UserWarning: Overwriting vit_tiny_patch16_224 in registry with vits.hide_prompt_vision_transformer.vit_tiny_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:895: UserWarning: Overwriting vit_tiny_patch16_384 in registry with vits.hide_prompt_vision_transformer.vit_tiny_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:904: UserWarning: Overwriting vit_small_patch32_224 in registry with vits.hide_prompt_vision_transformer.vit_small_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:913: UserWarning: Overwriting vit_small_patch32_384 in registry with vits.hide_prompt_vision_transformer.vit_small_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:922: UserWarning: Overwriting vit_small_patch16_224 in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:932: UserWarning: Overwriting vit_small_patch16_384 in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:942: UserWarning: Overwriting vit_base_patch32_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:952: UserWarning: Overwriting vit_base_patch32_384 in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:962: UserWarning: Overwriting vit_base_patch16_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:972: UserWarning: Overwriting vit_base_patch16_384 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:982: UserWarning: Overwriting vit_base_patch8_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch8_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:992: UserWarning: Overwriting vit_large_patch32_224 in registry with vits.hide_prompt_vision_transformer.vit_large_patch32_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1001: UserWarning: Overwriting vit_large_patch32_384 in registry with vits.hide_prompt_vision_transformer.vit_large_patch32_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1011: UserWarning: Overwriting vit_large_patch16_224 in registry with vits.hide_prompt_vision_transformer.vit_large_patch16_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1021: UserWarning: Overwriting vit_large_patch16_384 in registry with vits.hide_prompt_vision_transformer.vit_large_patch16_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1031: UserWarning: Overwriting vit_large_patch14_224 in registry with vits.hide_prompt_vision_transformer.vit_large_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1040: UserWarning: Overwriting vit_huge_patch14_224 in registry with vits.hide_prompt_vision_transformer.vit_huge_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1049: UserWarning: Overwriting vit_giant_patch14_224 in registry with vits.hide_prompt_vision_transformer.vit_giant_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1058: UserWarning: Overwriting vit_gigantic_patch14_224 in registry with vits.hide_prompt_vision_transformer.vit_gigantic_patch14_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1067: UserWarning: Overwriting vit_tiny_patch16_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_tiny_patch16_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1078: UserWarning: Overwriting vit_small_patch32_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_small_patch32_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1089: UserWarning: Overwriting vit_small_patch16_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1100: UserWarning: Overwriting vit_base_patch32_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1111: UserWarning: Overwriting vit_base_patch16_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1122: UserWarning: Overwriting vit_base_patch8_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_base_patch8_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1133: UserWarning: Overwriting vit_large_patch32_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_large_patch32_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1144: UserWarning: Overwriting vit_large_patch16_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_large_patch16_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1155: UserWarning: Overwriting vit_huge_patch14_224_in21k in registry with vits.hide_prompt_vision_transformer.vit_huge_patch14_224_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1166: UserWarning: Overwriting vit_base_patch16_224_sam in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_sam. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1175: UserWarning: Overwriting vit_base_patch32_224_sam in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_224_sam. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1184: UserWarning: Overwriting vit_small_patch16_224_dino in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_224_dino. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1193: UserWarning: Overwriting vit_small_patch8_224_dino in registry with vits.hide_prompt_vision_transformer.vit_small_patch8_224_dino. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1211: UserWarning: Overwriting vit_base_patch8_224_dino in registry with vits.hide_prompt_vision_transformer.vit_base_patch8_224_dino. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1220: UserWarning: Overwriting vit_base_patch16_224_miil_in21k in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_miil_in21k. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1230: UserWarning: Overwriting vit_base_patch16_224_miil in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_miil. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1242: UserWarning: Overwriting vit_base_patch32_plus_256 in registry with vits.hide_prompt_vision_transformer.vit_base_patch32_plus_256. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1251: UserWarning: Overwriting vit_base_patch16_plus_240 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_plus_240. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1260: UserWarning: Overwriting vit_base_patch16_rpn_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_rpn_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1271: UserWarning: Overwriting vit_small_patch16_36x1_224 in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_36x1_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1282: UserWarning: Overwriting vit_small_patch16_18x2_224 in registry with vits.hide_prompt_vision_transformer.vit_small_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1294: UserWarning: Overwriting vit_base_patch16_18x2_224 in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_18x2_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/vits/hide_prompt_vision_transformer.py:1331: UserWarning: Overwriting vit_base_patch16_224_dino in registry with vits.hide_prompt_vision_transformer.vit_base_patch16_224_dino. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  @register_model\n",
            "Original train size:  50000\n",
            "Sampled train size:  50000\n",
            "Original train size:  50000\n",
            "Sampled train size:  50000\n",
            "100\n",
            "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [10, 11, 12, 13, 14, 15, 16, 17, 18, 19], [20, 21, 22, 23, 24, 25, 26, 27, 28, 29], [30, 31, 32, 33, 34, 35, 36, 37, 38, 39], [40, 41, 42, 43, 44, 45, 46, 47, 48, 49], [50, 51, 52, 53, 54, 55, 56, 57, 58, 59], [60, 61, 62, 63, 64, 65, 66, 67, 68, 69], [70, 71, 72, 73, 74, 75, 76, 77, 78, 79], [80, 81, 82, 83, 84, 85, 86, 87, 88, 89], [90, 91, 92, 93, 94, 95, 96, 97, 98, 99]]\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Creating original model: vit_small_patch16_224.dino\n",
            "[Sequential(\n",
            "  (0): Linear(in_features=384, out_features=768, bias=True)\n",
            "  (1): GELU(approximate='none')\n",
            "  (2): Dropout(p=0.0, inplace=False)\n",
            "), Sequential(\n",
            "  (0): Linear(in_features=768, out_features=384, bias=True)\n",
            "  (1): Dropout(p=0.0, inplace=False)\n",
            ")]\n",
            "Creating model: vit_small_patch16_224.dino\n",
            "Namespace(subparser_name='cifar100_hideprompt_5e', pct=1.0, batch_size=24, epochs=1, original_model='vit_small_patch16_224.dino', model='vit_small_patch16_224.dino', input_size=224, pretrained=True, drop=0.0, drop_path=0.0, opt='adam', opt_eps=1e-08, opt_betas=(0.9, 0.999), clip_grad=1.0, momentum=0.9, weight_decay=0.0, reinit_optimizer=True, sched='step', lr=0.03, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=0, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, unscale_lr=True, color_jitter=None, aa=None, smoothing=0.1, train_interpolation='bicubic', reprob=0.0, remode='pixel', recount=1, data_path='./datasets/', dataset='Split-CIFAR100', shuffle=False, output_dir='./output/cifar100_full_dino_1epoch_final_100pct', device='cuda', seed=20, eval=False, num_workers=4, pin_mem=True, world_size=1, dist_url='env://', num_tasks=10, train_mask=True, task_inc=False, use_g_prompt=False, g_prompt_length=5, g_prompt_layer_idx=[], use_prefix_tune_for_g_prompt=False, use_e_prompt=True, e_prompt_layer_idx=[0, 1, 2, 3, 4], use_prefix_tune_for_e_prompt=True, larger_prompt_lr=True, prompt_pool=True, size=10, length=5, top_k=1, initializer='uniform', prompt_key=False, prompt_key_init='uniform', use_prompt_mask=True, mask_first_epoch=False, shared_prompt_pool=True, shared_prompt_key=False, batchwise_prompt=False, embedding_key='cls', predefined_key='', pull_constraint=True, pull_constraint_coeff=1.0, same_key_value=False, global_pool='token', head_type='token', freeze=['blocks', 'patch_embed', 'cls_token', 'norm', 'pos_embed'], crct_epochs=1, train_inference_task_only=False, original_model_mlp_structure=[2], ca_lr=0.005, milestones=[10], trained_original_model='./output/cifar100_full_dino_1epoch_100pct', prompt_momentum=0.1, reg=0.1, not_train_ca=False, ca_epochs=30, ca_storage_efficient_method='multi-centroid', n_centroids=10, print_freq=10, config='cifar100_hideprompt_5e', rank=0, gpu=0, distributed=True, dist_backend='nccl', nb_classes=100)\n",
            "number of params: 230500\n",
            "Start training for 1 epochs\n",
            "Loading checkpoint from: ./output/cifar100_full_dino_1epoch_100pct/checkpoint/task1_checkpoint.pth\n",
            "[rank0]:[W1003 15:20:41.400127487 reducer.cpp:1457] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/engines/hide_promtp_wtp_and_tap_engine.py:545: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  loss = torch.nn.functional.cross_entropy(sim, torch.range(0, sim.shape[0] - 1).long().to(device))\n",
            "Train: Epoch[1/1]  [  0/209]  eta: 0:06:44  Lr: 0.002812  Loss: 4.1140  Acc@1: 12.5000 (12.5000)  Acc@5: 45.8333 (45.8333)  time: 1.9362  data: 0.7140  max mem: 1368\n",
            "Train: Epoch[1/1]  [ 10/209]  eta: 0:01:20  Lr: 0.002812  Loss: 3.1619  Acc@1: 12.5000 (12.8788)  Acc@5: 45.8333 (50.0000)  time: 0.4065  data: 0.0652  max mem: 1370\n",
            "Train: Epoch[1/1]  [ 20/209]  eta: 0:01:03  Lr: 0.002812  Loss: 2.0835  Acc@1: 16.6667 (16.8651)  Acc@5: 62.5000 (60.5159)  time: 0.2573  data: 0.0006  max mem: 1370\n",
            "Train: Epoch[1/1]  [ 30/209]  eta: 0:00:55  Lr: 0.002812  Loss: 1.7330  Acc@1: 25.0000 (21.7742)  Acc@5: 75.0000 (65.9946)  time: 0.2600  data: 0.0009  max mem: 1370\n",
            "Train: Epoch[1/1]  [ 40/209]  eta: 0:00:50  Lr: 0.002812  Loss: 1.8413  Acc@1: 37.5000 (27.4390)  Acc@5: 83.3333 (70.8333)  time: 0.2603  data: 0.0008  max mem: 1370\n",
            "Train: Epoch[1/1]  [ 50/209]  eta: 0:00:46  Lr: 0.002812  Loss: 1.6337  Acc@1: 45.8333 (32.0261)  Acc@5: 87.5000 (74.2647)  time: 0.2625  data: 0.0005  max mem: 1370\n",
            "Train: Epoch[1/1]  [ 60/209]  eta: 0:00:42  Lr: 0.002812  Loss: 1.7921  Acc@1: 50.0000 (34.6995)  Acc@5: 87.5000 (76.0929)  time: 0.2653  data: 0.0003  max mem: 1370\n",
            "Train: Epoch[1/1]  [ 70/209]  eta: 0:00:39  Lr: 0.002812  Loss: 1.1460  Acc@1: 50.0000 (37.9108)  Acc@5: 91.6667 (78.4624)  time: 0.2694  data: 0.0005  max mem: 1370\n",
            "Train: Epoch[1/1]  [ 80/209]  eta: 0:00:36  Lr: 0.002812  Loss: 1.5301  Acc@1: 62.5000 (40.6893)  Acc@5: 91.6667 (80.4012)  time: 0.2726  data: 0.0010  max mem: 1370\n",
            "Train: Epoch[1/1]  [ 90/209]  eta: 0:00:33  Lr: 0.002812  Loss: 1.1025  Acc@1: 62.5000 (43.7729)  Acc@5: 91.6667 (81.8223)  time: 0.2757  data: 0.0008  max mem: 1370\n",
            "Train: Epoch[1/1]  [100/209]  eta: 0:00:30  Lr: 0.002812  Loss: 0.9555  Acc@1: 70.8333 (46.2459)  Acc@5: 95.8333 (83.2096)  time: 0.2794  data: 0.0003  max mem: 1370\n",
            "Train: Epoch[1/1]  [110/209]  eta: 0:00:28  Lr: 0.002812  Loss: 0.9783  Acc@1: 66.6667 (48.1231)  Acc@5: 95.8333 (84.4595)  time: 0.2825  data: 0.0004  max mem: 1370\n",
            "Train: Epoch[1/1]  [120/209]  eta: 0:00:25  Lr: 0.002812  Loss: 0.9646  Acc@1: 66.6667 (49.7934)  Acc@5: 95.8333 (85.4339)  time: 0.2849  data: 0.0006  max mem: 1370\n",
            "Train: Epoch[1/1]  [130/209]  eta: 0:00:22  Lr: 0.002812  Loss: 0.8712  Acc@1: 70.8333 (51.6858)  Acc@5: 95.8333 (86.3232)  time: 0.2876  data: 0.0008  max mem: 1370\n",
            "Train: Epoch[1/1]  [140/209]  eta: 0:00:19  Lr: 0.002812  Loss: 0.9396  Acc@1: 75.0000 (53.4279)  Acc@5: 95.8333 (86.9681)  time: 0.2905  data: 0.0006  max mem: 1370\n",
            "Train: Epoch[1/1]  [150/209]  eta: 0:00:16  Lr: 0.002812  Loss: 0.7339  Acc@1: 75.0000 (54.9945)  Acc@5: 95.8333 (87.5276)  time: 0.2924  data: 0.0004  max mem: 1370\n",
            "Train: Epoch[1/1]  [160/209]  eta: 0:00:14  Lr: 0.002812  Loss: 0.5751  Acc@1: 79.1667 (56.6770)  Acc@5: 95.8333 (88.1211)  time: 0.2931  data: 0.0004  max mem: 1370\n",
            "Train: Epoch[1/1]  [170/209]  eta: 0:00:11  Lr: 0.002812  Loss: 0.7202  Acc@1: 79.1667 (58.0409)  Acc@5: 95.8333 (88.6940)  time: 0.2912  data: 0.0004  max mem: 1370\n",
            "Train: Epoch[1/1]  [180/209]  eta: 0:00:08  Lr: 0.002812  Loss: 0.4635  Acc@1: 79.1667 (59.1160)  Acc@5: 95.8333 (89.0654)  time: 0.2877  data: 0.0006  max mem: 1370\n",
            "Train: Epoch[1/1]  [190/209]  eta: 0:00:05  Lr: 0.002812  Loss: 0.4515  Acc@1: 75.0000 (59.8822)  Acc@5: 95.8333 (89.4852)  time: 0.2850  data: 0.0005  max mem: 1370\n",
            "Train: Epoch[1/1]  [200/209]  eta: 0:00:02  Lr: 0.002812  Loss: 0.5691  Acc@1: 75.0000 (60.9867)  Acc@5: 95.8333 (89.8632)  time: 0.2830  data: 0.0004  max mem: 1370\n",
            "Train: Epoch[1/1]  [208/209]  eta: 0:00:00  Lr: 0.002812  Loss: 0.7125  Acc@1: 83.3333 (61.6600)  Acc@5: 100.0000 (90.1600)  time: 0.2764  data: 0.0003  max mem: 1370\n",
            "Train: Epoch[1/1] Total time: 0:00:59 (0.2854 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.7125  Acc@1: 83.3333 (61.6600)  Acc@5: 100.0000 (90.1600)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:21  Loss: 1.1808 (1.1808)  Acc@1: 70.8333 (70.8333)  Acc@5: 87.5000 (87.5000)  Acc@task: 100.0000 (100.0000)  time: 0.5149  data: 0.3440  max mem: 1370\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 1.0812 (1.1322)  Acc@1: 79.1667 (76.5152)  Acc@5: 87.5000 (90.5303)  Acc@task: 100.0000 (100.0000)  time: 0.2051  data: 0.0316  max mem: 1370\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 1.0812 (1.1626)  Acc@1: 75.0000 (75.1984)  Acc@5: 87.5000 (89.8810)  Acc@task: 100.0000 (100.0000)  time: 0.1740  data: 0.0005  max mem: 1370\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 1.0624 (1.1304)  Acc@1: 75.0000 (75.8065)  Acc@5: 87.5000 (89.6505)  Acc@task: 100.0000 (100.0000)  time: 0.1750  data: 0.0005  max mem: 1370\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 1.0382 (1.1110)  Acc@1: 75.0000 (76.1179)  Acc@5: 91.6667 (90.2439)  Acc@task: 100.0000 (100.0000)  time: 0.1757  data: 0.0004  max mem: 1370\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 1.0038 (1.0985)  Acc@1: 75.0000 (76.3000)  Acc@5: 91.6667 (90.4000)  Acc@task: 100.0000 (100.0000)  time: 0.1779  data: 0.0004  max mem: 1370\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1877 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@task 100.000 Acc@1 76.300 Acc@5 90.400 loss 1.099\n",
            "[Average accuracy till task1]\tAcc@task: 100.0000\tAcc@1: 76.3000\tAcc@5: 90.4000\tLoss: 1.0985\n",
            "Loading checkpoint from: ./output/cifar100_full_dino_1epoch_100pct/checkpoint/task2_checkpoint.pth\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/engines/hide_promtp_wtp_and_tap_engine.py:540: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  loss = torch.nn.functional.cross_entropy(sim, torch.range(0, sim.shape[0] - 1).long().to(device))\n",
            "Train: Epoch[1/1]  [  0/209]  eta: 0:02:20  Lr: 0.002812  Loss: 5.0238  Acc@1: 12.5000 (12.5000)  Acc@5: 62.5000 (62.5000)  time: 0.6744  data: 0.3350  max mem: 1371\n",
            "Train: Epoch[1/1]  [ 10/209]  eta: 0:01:01  Lr: 0.002812  Loss: 4.0505  Acc@1: 12.5000 (14.7727)  Acc@5: 54.1667 (53.7879)  time: 0.3088  data: 0.0308  max mem: 1374\n",
            "Train: Epoch[1/1]  [ 20/209]  eta: 0:00:55  Lr: 0.002812  Loss: 3.9287  Acc@1: 16.6667 (16.0714)  Acc@5: 62.5000 (60.5159)  time: 0.2726  data: 0.0004  max mem: 1374\n",
            "Train: Epoch[1/1]  [ 30/209]  eta: 0:00:51  Lr: 0.002812  Loss: 3.6541  Acc@1: 20.8333 (19.2204)  Acc@5: 66.6667 (64.3817)  time: 0.2740  data: 0.0004  max mem: 1374\n",
            "Train: Epoch[1/1]  [ 40/209]  eta: 0:00:47  Lr: 0.002812  Loss: 2.1095  Acc@1: 33.3333 (24.2886)  Acc@5: 75.0000 (67.3781)  time: 0.2755  data: 0.0005  max mem: 1374\n",
            "Train: Epoch[1/1]  [ 50/209]  eta: 0:00:44  Lr: 0.002812  Loss: 2.1709  Acc@1: 41.6667 (28.4314)  Acc@5: 79.1667 (70.8333)  time: 0.2762  data: 0.0007  max mem: 1374\n",
            "Train: Epoch[1/1]  [ 60/209]  eta: 0:00:41  Lr: 0.002812  Loss: 2.8912  Acc@1: 45.8333 (31.0109)  Acc@5: 83.3333 (73.2240)  time: 0.2776  data: 0.0005  max mem: 1374\n",
            "Train: Epoch[1/1]  [ 70/209]  eta: 0:00:39  Lr: 0.002812  Loss: 1.5357  Acc@1: 45.8333 (34.6244)  Acc@5: 87.5000 (75.9977)  time: 0.2794  data: 0.0004  max mem: 1374\n",
            "Train: Epoch[1/1]  [ 80/209]  eta: 0:00:36  Lr: 0.002812  Loss: 1.6365  Acc@1: 58.3333 (37.7572)  Acc@5: 91.6667 (77.9321)  time: 0.2800  data: 0.0004  max mem: 1374\n",
            "Train: Epoch[1/1]  [ 90/209]  eta: 0:00:33  Lr: 0.002812  Loss: 1.3249  Acc@1: 62.5000 (40.9341)  Acc@5: 95.8333 (79.5788)  time: 0.2804  data: 0.0006  max mem: 1374\n",
            "Train: Epoch[1/1]  [100/209]  eta: 0:00:30  Lr: 0.002812  Loss: 1.4713  Acc@1: 66.6667 (43.6056)  Acc@5: 95.8333 (81.1056)  time: 0.2805  data: 0.0007  max mem: 1374\n",
            "Train: Epoch[1/1]  [110/209]  eta: 0:00:27  Lr: 0.002812  Loss: 1.5056  Acc@1: 66.6667 (46.0210)  Acc@5: 95.8333 (82.3198)  time: 0.2802  data: 0.0007  max mem: 1374\n",
            "Train: Epoch[1/1]  [120/209]  eta: 0:00:25  Lr: 0.002812  Loss: 1.1746  Acc@1: 70.8333 (48.0372)  Acc@5: 91.6667 (83.2989)  time: 0.2812  data: 0.0005  max mem: 1374\n",
            "Train: Epoch[1/1]  [130/209]  eta: 0:00:22  Lr: 0.002812  Loss: 1.7151  Acc@1: 70.8333 (49.9682)  Acc@5: 95.8333 (84.2557)  time: 0.2811  data: 0.0004  max mem: 1374\n",
            "Train: Epoch[1/1]  [140/209]  eta: 0:00:19  Lr: 0.002812  Loss: 1.3376  Acc@1: 75.0000 (51.6844)  Acc@5: 95.8333 (85.1950)  time: 0.2797  data: 0.0006  max mem: 1374\n",
            "Train: Epoch[1/1]  [150/209]  eta: 0:00:16  Lr: 0.002812  Loss: 1.3181  Acc@1: 70.8333 (52.8974)  Acc@5: 95.8333 (85.8996)  time: 0.2793  data: 0.0008  max mem: 1374\n",
            "Train: Epoch[1/1]  [160/209]  eta: 0:00:13  Lr: 0.002812  Loss: 1.3320  Acc@1: 70.8333 (54.0890)  Acc@5: 95.8333 (86.4907)  time: 0.2793  data: 0.0006  max mem: 1374\n",
            "Train: Epoch[1/1]  [170/209]  eta: 0:00:10  Lr: 0.002812  Loss: 0.9883  Acc@1: 75.0000 (55.3363)  Acc@5: 95.8333 (87.1832)  time: 0.2795  data: 0.0004  max mem: 1374\n",
            "Train: Epoch[1/1]  [180/209]  eta: 0:00:08  Lr: 0.002812  Loss: 0.9717  Acc@1: 79.1667 (56.6529)  Acc@5: 100.0000 (87.7532)  time: 0.2790  data: 0.0004  max mem: 1374\n",
            "Train: Epoch[1/1]  [190/209]  eta: 0:00:05  Lr: 0.002812  Loss: 1.5797  Acc@1: 75.0000 (57.4389)  Acc@5: 100.0000 (88.2199)  time: 0.2779  data: 0.0004  max mem: 1374\n",
            "Train: Epoch[1/1]  [200/209]  eta: 0:00:02  Lr: 0.002812  Loss: 1.2335  Acc@1: 70.8333 (58.2711)  Acc@5: 100.0000 (88.7438)  time: 0.2776  data: 0.0006  max mem: 1374\n",
            "Train: Epoch[1/1]  [208/209]  eta: 0:00:00  Lr: 0.002812  Loss: 1.2318  Acc@1: 70.8333 (58.8400)  Acc@5: 100.0000 (88.9800)  time: 0.2683  data: 0.0005  max mem: 1374\n",
            "Train: Epoch[1/1] Total time: 0:00:58 (0.2798 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.2318  Acc@1: 70.8333 (58.8400)  Acc@5: 100.0000 (88.9800)\n",
            "torch.Size([5, 2, 5, 6, 64])\n",
            "torch.Size([5, 2, 1, 5, 6, 64])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:39  Loss: 1.2228 (1.2228)  Acc@1: 75.0000 (75.0000)  Acc@5: 83.3333 (83.3333)  Acc@task: 95.8333 (95.8333)  time: 0.9472  data: 0.6860  max mem: 1374\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:07  Loss: 1.1197 (1.2029)  Acc@1: 66.6667 (70.0758)  Acc@5: 87.5000 (89.7727)  Acc@task: 91.6667 (90.5303)  time: 0.2464  data: 0.0630  max mem: 1374\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 1.1211 (1.2196)  Acc@1: 66.6667 (70.8333)  Acc@5: 87.5000 (89.0873)  Acc@task: 91.6667 (91.6667)  time: 0.1765  data: 0.0005  max mem: 1374\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 1.1666 (1.1882)  Acc@1: 70.8333 (71.7742)  Acc@5: 91.6667 (89.2473)  Acc@task: 95.8333 (92.6075)  time: 0.1773  data: 0.0003  max mem: 1374\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 1.1339 (1.1654)  Acc@1: 75.0000 (72.6626)  Acc@5: 91.6667 (89.8374)  Acc@task: 95.8333 (93.4959)  time: 0.1780  data: 0.0004  max mem: 1374\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 1.0661 (1.1518)  Acc@1: 75.0000 (72.9000)  Acc@5: 91.6667 (90.0000)  Acc@task: 95.8333 (93.6000)  time: 0.1747  data: 0.0004  max mem: 1374\n",
            "Test: [Task 1] Total time: 0:00:08 (0.1982 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@task 93.600 Acc@1 72.900 Acc@5 90.000 loss 1.152\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:29  Loss: 1.8025 (1.8025)  Acc@1: 54.1667 (54.1667)  Acc@5: 83.3333 (83.3333)  Acc@task: 91.6667 (91.6667)  time: 0.7056  data: 0.5186  max mem: 1374\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:07  Loss: 1.4954 (1.5962)  Acc@1: 54.1667 (60.6061)  Acc@5: 87.5000 (87.5000)  Acc@task: 91.6667 (89.3939)  time: 0.2278  data: 0.0488  max mem: 1374\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 1.4506 (1.5355)  Acc@1: 62.5000 (61.7064)  Acc@5: 87.5000 (87.3016)  Acc@task: 91.6667 (90.4762)  time: 0.1789  data: 0.0012  max mem: 1374\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 1.4450 (1.5294)  Acc@1: 62.5000 (62.0968)  Acc@5: 87.5000 (87.5000)  Acc@task: 91.6667 (90.7258)  time: 0.1783  data: 0.0008  max mem: 1374\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 1.4349 (1.5003)  Acc@1: 62.5000 (63.1098)  Acc@5: 87.5000 (88.0081)  Acc@task: 91.6667 (90.4472)  time: 0.1787  data: 0.0009  max mem: 1374\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 1.3953 (1.4978)  Acc@1: 62.5000 (63.0000)  Acc@5: 87.5000 (88.0000)  Acc@task: 91.6667 (90.5000)  time: 0.1756  data: 0.0009  max mem: 1374\n",
            "Test: [Task 2] Total time: 0:00:08 (0.1923 s / it)\n",
            "* Acc@task 90.500 Acc@1 63.000 Acc@5 88.000 loss 1.498\n",
            "[Average accuracy till task2]\tAcc@task: 92.0500\tAcc@1: 67.9500\tAcc@5: 89.0000\tLoss: 1.3248\tForgetting: 0.0000\tBackward: 72.9000\n",
            "torch.Size([24000, 384])\n",
            "Averaged stats: Lr: 0.005000  Loss: 0.1534  Acc@1: 89.1667 (88.5000)  Acc@5: 100.0000 (99.8333)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:22  Loss: 0.6984 (0.6984)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 95.8333 (95.8333)  time: 0.5256  data: 0.3493  max mem: 1374\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.6658 (0.7291)  Acc@1: 87.5000 (83.7121)  Acc@5: 95.8333 (95.0758)  Acc@task: 91.6667 (90.5303)  time: 0.2105  data: 0.0321  max mem: 1374\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.6658 (0.7404)  Acc@1: 87.5000 (82.7381)  Acc@5: 95.8333 (95.2381)  Acc@task: 91.6667 (91.6667)  time: 0.1786  data: 0.0004  max mem: 1374\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.6716 (0.7207)  Acc@1: 79.1667 (83.0645)  Acc@5: 95.8333 (95.5645)  Acc@task: 95.8333 (92.6075)  time: 0.1791  data: 0.0004  max mem: 1374\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6383 (0.7015)  Acc@1: 83.3333 (83.1301)  Acc@5: 95.8333 (95.8333)  Acc@task: 95.8333 (93.4959)  time: 0.1803  data: 0.0003  max mem: 1374\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6049 (0.6915)  Acc@1: 83.3333 (83.4000)  Acc@5: 95.8333 (95.9000)  Acc@task: 95.8333 (93.6000)  time: 0.1768  data: 0.0003  max mem: 1374\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1887 s / it)\n",
            "* Acc@task 93.600 Acc@1 83.400 Acc@5 95.900 loss 0.692\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:19  Loss: 0.9193 (0.9193)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  Acc@task: 91.6667 (91.6667)  time: 0.4627  data: 0.2765  max mem: 1374\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.7424 (0.7784)  Acc@1: 79.1667 (80.3030)  Acc@5: 95.8333 (95.8333)  Acc@task: 91.6667 (89.3939)  time: 0.2041  data: 0.0264  max mem: 1374\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.6644 (0.7713)  Acc@1: 83.3333 (81.3492)  Acc@5: 95.8333 (94.6429)  Acc@task: 91.6667 (90.4762)  time: 0.1787  data: 0.0016  max mem: 1374\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.6643 (0.7616)  Acc@1: 83.3333 (81.4516)  Acc@5: 95.8333 (95.0269)  Acc@task: 91.6667 (90.7258)  time: 0.1797  data: 0.0013  max mem: 1374\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.6619 (0.7429)  Acc@1: 83.3333 (82.0122)  Acc@5: 95.8333 (95.0203)  Acc@task: 91.6667 (90.4472)  time: 0.1804  data: 0.0005  max mem: 1374\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.6584 (0.7400)  Acc@1: 83.3333 (82.0000)  Acc@5: 95.8333 (95.0000)  Acc@task: 91.6667 (90.5000)  time: 0.1771  data: 0.0004  max mem: 1374\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1876 s / it)\n",
            "* Acc@task 90.500 Acc@1 82.000 Acc@5 95.000 loss 0.740\n",
            "[Average accuracy till task2]\tAcc@task: 92.0500\tAcc@1: 82.7000\tAcc@5: 95.4500\tLoss: 0.7158\tForgetting: 0.0000\tBackward: 7.1000\n",
            "Loading checkpoint from: ./output/cifar100_full_dino_1epoch_100pct/checkpoint/task3_checkpoint.pth\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/engines/hide_promtp_wtp_and_tap_engine.py:540: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  loss = torch.nn.functional.cross_entropy(sim, torch.range(0, sim.shape[0] - 1).long().to(device))\n",
            "Train: Epoch[1/1]  [  0/209]  eta: 0:02:38  Lr: 0.002812  Loss: 3.6282  Acc@1: 12.5000 (12.5000)  Acc@5: 54.1667 (54.1667)  time: 0.7591  data: 0.4465  max mem: 1374\n",
            "Train: Epoch[1/1]  [ 10/209]  eta: 0:01:03  Lr: 0.002812  Loss: 3.1362  Acc@1: 8.3333 (10.6061)  Acc@5: 58.3333 (54.5455)  time: 0.3204  data: 0.0410  max mem: 1375\n",
            "Train: Epoch[1/1]  [ 20/209]  eta: 0:00:56  Lr: 0.002812  Loss: 2.2793  Acc@1: 12.5000 (15.2778)  Acc@5: 62.5000 (61.1111)  time: 0.2776  data: 0.0004  max mem: 1375\n",
            "Train: Epoch[1/1]  [ 30/209]  eta: 0:00:52  Lr: 0.002812  Loss: 2.2367  Acc@1: 25.0000 (19.8925)  Acc@5: 75.0000 (65.7258)  time: 0.2783  data: 0.0005  max mem: 1375\n",
            "Train: Epoch[1/1]  [ 40/209]  eta: 0:00:48  Lr: 0.002812  Loss: 1.8338  Acc@1: 37.5000 (25.1016)  Acc@5: 79.1667 (69.8171)  time: 0.2781  data: 0.0010  max mem: 1375\n",
            "Train: Epoch[1/1]  [ 50/209]  eta: 0:00:45  Lr: 0.002812  Loss: 1.4345  Acc@1: 45.8333 (30.6373)  Acc@5: 83.3333 (73.6111)  time: 0.2781  data: 0.0010  max mem: 1375\n",
            "Train: Epoch[1/1]  [ 60/209]  eta: 0:00:42  Lr: 0.002812  Loss: 1.9060  Acc@1: 54.1667 (34.8361)  Acc@5: 87.5000 (76.0929)  time: 0.2781  data: 0.0005  max mem: 1375\n",
            "Train: Epoch[1/1]  [ 70/209]  eta: 0:00:39  Lr: 0.002812  Loss: 1.3040  Acc@1: 58.3333 (39.0845)  Acc@5: 91.6667 (78.5211)  time: 0.2787  data: 0.0004  max mem: 1375\n",
            "Train: Epoch[1/1]  [ 80/209]  eta: 0:00:36  Lr: 0.002812  Loss: 1.4524  Acc@1: 70.8333 (43.1584)  Acc@5: 95.8333 (80.5556)  time: 0.2782  data: 0.0004  max mem: 1375\n",
            "Train: Epoch[1/1]  [ 90/209]  eta: 0:00:33  Lr: 0.002812  Loss: 1.1572  Acc@1: 70.8333 (46.1538)  Acc@5: 95.8333 (82.0513)  time: 0.2770  data: 0.0007  max mem: 1375\n",
            "Train: Epoch[1/1]  [100/209]  eta: 0:00:30  Lr: 0.002812  Loss: 1.0722  Acc@1: 75.0000 (49.0924)  Acc@5: 95.8333 (83.4571)  time: 0.2768  data: 0.0007  max mem: 1375\n",
            "Train: Epoch[1/1]  [110/209]  eta: 0:00:27  Lr: 0.002812  Loss: 1.1428  Acc@1: 79.1667 (51.5015)  Acc@5: 95.8333 (84.5345)  time: 0.2769  data: 0.0005  max mem: 1375\n",
            "Train: Epoch[1/1]  [120/209]  eta: 0:00:25  Lr: 0.002812  Loss: 0.9449  Acc@1: 79.1667 (53.4435)  Acc@5: 95.8333 (85.6061)  time: 0.2766  data: 0.0004  max mem: 1375\n",
            "Train: Epoch[1/1]  [130/209]  eta: 0:00:22  Lr: 0.002812  Loss: 0.9627  Acc@1: 79.1667 (55.3435)  Acc@5: 95.8333 (86.3550)  time: 0.2767  data: 0.0004  max mem: 1375\n",
            "Train: Epoch[1/1]  [140/209]  eta: 0:00:19  Lr: 0.002812  Loss: 1.2037  Acc@1: 79.1667 (57.0331)  Acc@5: 95.8333 (87.1158)  time: 0.2766  data: 0.0008  max mem: 1375\n",
            "Train: Epoch[1/1]  [150/209]  eta: 0:00:16  Lr: 0.002812  Loss: 0.7899  Acc@1: 75.0000 (58.1126)  Acc@5: 100.0000 (87.8587)  time: 0.2764  data: 0.0008  max mem: 1375\n",
            "Train: Epoch[1/1]  [160/209]  eta: 0:00:13  Lr: 0.002812  Loss: 0.9418  Acc@1: 70.8333 (59.2133)  Acc@5: 95.8333 (88.4058)  time: 0.2766  data: 0.0006  max mem: 1375\n",
            "Train: Epoch[1/1]  [170/209]  eta: 0:00:10  Lr: 0.002812  Loss: 0.9744  Acc@1: 79.1667 (60.4776)  Acc@5: 95.8333 (88.8158)  time: 0.2762  data: 0.0005  max mem: 1375\n",
            "Train: Epoch[1/1]  [180/209]  eta: 0:00:08  Lr: 0.002812  Loss: 0.8167  Acc@1: 83.3333 (61.6483)  Acc@5: 95.8333 (89.3416)  time: 0.2758  data: 0.0005  max mem: 1375\n",
            "Train: Epoch[1/1]  [190/209]  eta: 0:00:05  Lr: 0.002812  Loss: 0.8860  Acc@1: 83.3333 (62.5000)  Acc@5: 100.0000 (89.8124)  time: 0.2757  data: 0.0010  max mem: 1375\n",
            "Train: Epoch[1/1]  [200/209]  eta: 0:00:02  Lr: 0.002812  Loss: 0.7706  Acc@1: 83.3333 (63.6401)  Acc@5: 100.0000 (90.1534)  time: 0.2760  data: 0.0010  max mem: 1375\n",
            "Train: Epoch[1/1]  [208/209]  eta: 0:00:00  Lr: 0.002812  Loss: 0.5098  Acc@1: 83.3333 (64.1800)  Acc@5: 100.0000 (90.4800)  time: 0.2672  data: 0.0004  max mem: 1375\n",
            "Train: Epoch[1/1] Total time: 0:00:58 (0.2791 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.5098  Acc@1: 83.3333 (64.1800)  Acc@5: 100.0000 (90.4800)\n",
            "torch.Size([5, 2, 5, 6, 64])\n",
            "torch.Size([5, 2, 1, 5, 6, 64])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:20  Loss: 0.7175 (0.7175)  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  Acc@task: 91.6667 (91.6667)  time: 0.4872  data: 0.3206  max mem: 1375\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.7175 (0.7586)  Acc@1: 83.3333 (81.8182)  Acc@5: 95.8333 (95.0758)  Acc@task: 87.5000 (83.3333)  time: 0.2055  data: 0.0296  max mem: 1375\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.7075 (0.7711)  Acc@1: 83.3333 (81.3492)  Acc@5: 95.8333 (94.8413)  Acc@task: 87.5000 (84.9206)  time: 0.1773  data: 0.0005  max mem: 1375\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.7055 (0.7579)  Acc@1: 83.3333 (81.5860)  Acc@5: 95.8333 (95.1613)  Acc@task: 91.6667 (86.1559)  time: 0.1779  data: 0.0004  max mem: 1375\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.6537 (0.7367)  Acc@1: 83.3333 (81.8089)  Acc@5: 95.8333 (95.5285)  Acc@task: 91.6667 (87.2968)  time: 0.1793  data: 0.0003  max mem: 1375\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.6385 (0.7260)  Acc@1: 83.3333 (82.1000)  Acc@5: 95.8333 (95.6000)  Acc@task: 91.6667 (87.5000)  time: 0.1760  data: 0.0003  max mem: 1375\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1867 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@task 87.500 Acc@1 82.100 Acc@5 95.600 loss 0.726\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:20  Loss: 0.8767 (0.8767)  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (83.3333)  time: 0.4915  data: 0.3198  max mem: 1375\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.6633 (0.7445)  Acc@1: 79.1667 (80.3030)  Acc@5: 95.8333 (95.8333)  Acc@task: 87.5000 (86.3636)  time: 0.2074  data: 0.0317  max mem: 1375\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.6633 (0.7283)  Acc@1: 83.3333 (81.9444)  Acc@5: 95.8333 (95.2381)  Acc@task: 87.5000 (87.1032)  time: 0.1788  data: 0.0025  max mem: 1375\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.6772 (0.7302)  Acc@1: 83.3333 (81.8548)  Acc@5: 95.8333 (95.2957)  Acc@task: 87.5000 (87.5000)  time: 0.1796  data: 0.0016  max mem: 1375\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.6329 (0.7158)  Acc@1: 83.3333 (82.1138)  Acc@5: 95.8333 (95.3252)  Acc@task: 87.5000 (87.5000)  time: 0.1805  data: 0.0006  max mem: 1375\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.6315 (0.7110)  Acc@1: 83.3333 (82.2000)  Acc@5: 95.8333 (95.4000)  Acc@task: 87.5000 (87.6000)  time: 0.1772  data: 0.0005  max mem: 1375\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1881 s / it)\n",
            "* Acc@task 87.600 Acc@1 82.200 Acc@5 95.400 loss 0.711\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:20  Loss: 1.2741 (1.2741)  Acc@1: 50.0000 (50.0000)  Acc@5: 95.8333 (95.8333)  Acc@task: 95.8333 (95.8333)  time: 0.4962  data: 0.2931  max mem: 1375\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 1.6542 (1.6650)  Acc@1: 50.0000 (50.0000)  Acc@5: 87.5000 (85.6061)  Acc@task: 87.5000 (85.9849)  time: 0.2123  data: 0.0350  max mem: 1375\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 1.6392 (1.5353)  Acc@1: 50.0000 (55.3571)  Acc@5: 87.5000 (86.9048)  Acc@task: 83.3333 (85.9127)  time: 0.1822  data: 0.0048  max mem: 1375\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 1.4730 (1.5444)  Acc@1: 58.3333 (55.2419)  Acc@5: 87.5000 (86.4247)  Acc@task: 83.3333 (86.4247)  time: 0.1808  data: 0.0004  max mem: 1375\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 1.6148 (1.5446)  Acc@1: 54.1667 (56.0976)  Acc@5: 83.3333 (86.4837)  Acc@task: 83.3333 (86.4837)  time: 0.1807  data: 0.0004  max mem: 1375\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 1.6148 (1.5464)  Acc@1: 54.1667 (56.0000)  Acc@5: 87.5000 (86.7000)  Acc@task: 83.3333 (86.3000)  time: 0.1775  data: 0.0004  max mem: 1375\n",
            "Test: [Task 3] Total time: 0:00:08 (0.1911 s / it)\n",
            "* Acc@task 86.300 Acc@1 56.000 Acc@5 86.700 loss 1.546\n",
            "[Average accuracy till task3]\tAcc@task: 87.1333\tAcc@1: 73.4333\tAcc@5: 92.5667\tLoss: 0.9945\tForgetting: 0.0000\tBackward: 50.6500\n",
            "torch.Size([36000, 384])\n",
            "Averaged stats: Lr: 0.005000  Loss: 0.1273  Acc@1: 92.5000 (92.0833)  Acc@5: 100.0000 (99.7917)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:33  Loss: 0.2660 (0.2660)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  Acc@task: 91.6667 (91.6667)  time: 0.7946  data: 0.5921  max mem: 1375\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:07  Loss: 0.5612 (0.5358)  Acc@1: 87.5000 (87.8788)  Acc@5: 100.0000 (97.7273)  Acc@task: 87.5000 (83.3333)  time: 0.2392  data: 0.0577  max mem: 1375\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.5523 (0.4961)  Acc@1: 87.5000 (87.1032)  Acc@5: 95.8333 (97.4206)  Acc@task: 87.5000 (84.9206)  time: 0.1816  data: 0.0023  max mem: 1375\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.3779 (0.4837)  Acc@1: 87.5000 (87.7688)  Acc@5: 95.8333 (97.3118)  Acc@task: 91.6667 (86.1559)  time: 0.1796  data: 0.0004  max mem: 1375\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.3338 (0.4583)  Acc@1: 91.6667 (88.5163)  Acc@5: 100.0000 (97.7642)  Acc@task: 91.6667 (87.2968)  time: 0.1799  data: 0.0003  max mem: 1375\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.3267 (0.4494)  Acc@1: 91.6667 (88.7000)  Acc@5: 100.0000 (97.8000)  Acc@task: 91.6667 (87.5000)  time: 0.1766  data: 0.0003  max mem: 1375\n",
            "Test: [Task 1] Total time: 0:00:08 (0.1965 s / it)\n",
            "* Acc@task 87.500 Acc@1 88.700 Acc@5 97.800 loss 0.449\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:18  Loss: 0.6788 (0.6788)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (83.3333)  time: 0.4446  data: 0.2803  max mem: 1375\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.6383 (0.6431)  Acc@1: 83.3333 (84.0909)  Acc@5: 100.0000 (96.9697)  Acc@task: 87.5000 (86.3636)  time: 0.2078  data: 0.0346  max mem: 1375\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.5336 (0.6415)  Acc@1: 87.5000 (85.7143)  Acc@5: 95.8333 (95.6349)  Acc@task: 87.5000 (87.1032)  time: 0.1819  data: 0.0053  max mem: 1375\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.5670 (0.6374)  Acc@1: 83.3333 (84.5430)  Acc@5: 95.8333 (95.8333)  Acc@task: 87.5000 (87.5000)  time: 0.1797  data: 0.0009  max mem: 1375\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.5579 (0.6167)  Acc@1: 83.3333 (84.9594)  Acc@5: 95.8333 (95.9350)  Acc@task: 87.5000 (87.5000)  time: 0.1793  data: 0.0008  max mem: 1375\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.5229 (0.6074)  Acc@1: 87.5000 (85.1000)  Acc@5: 95.8333 (96.0000)  Acc@task: 87.5000 (87.6000)  time: 0.1762  data: 0.0008  max mem: 1375\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1889 s / it)\n",
            "* Acc@task 87.600 Acc@1 85.100 Acc@5 96.000 loss 0.607\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:20  Loss: 0.2380 (0.2380)  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)  Acc@task: 95.8333 (95.8333)  time: 0.4771  data: 0.3038  max mem: 1375\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 0.5591 (0.5972)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (96.5909)  Acc@task: 87.5000 (85.9849)  time: 0.2095  data: 0.0342  max mem: 1375\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.4792 (0.5234)  Acc@1: 87.5000 (86.3095)  Acc@5: 95.8333 (96.6270)  Acc@task: 83.3333 (85.9127)  time: 0.1804  data: 0.0038  max mem: 1375\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.4379 (0.5019)  Acc@1: 87.5000 (86.5591)  Acc@5: 95.8333 (97.0430)  Acc@task: 83.3333 (86.4247)  time: 0.1788  data: 0.0004  max mem: 1375\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.4521 (0.5297)  Acc@1: 87.5000 (86.6870)  Acc@5: 95.8333 (96.7480)  Acc@task: 83.3333 (86.4837)  time: 0.1788  data: 0.0003  max mem: 1375\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.4521 (0.5317)  Acc@1: 87.5000 (86.6000)  Acc@5: 100.0000 (96.8000)  Acc@task: 83.3333 (86.3000)  time: 0.1755  data: 0.0003  max mem: 1375\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1875 s / it)\n",
            "* Acc@task 86.300 Acc@1 86.600 Acc@5 96.800 loss 0.532\n",
            "[Average accuracy till task3]\tAcc@task: 87.1333\tAcc@1: 86.8000\tAcc@5: 96.8667\tLoss: 0.5295\tForgetting: 0.0000\tBackward: 7.7500\n",
            "Loading checkpoint from: ./output/cifar100_full_dino_1epoch_100pct/checkpoint/task4_checkpoint.pth\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/engines/hide_promtp_wtp_and_tap_engine.py:540: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  loss = torch.nn.functional.cross_entropy(sim, torch.range(0, sim.shape[0] - 1).long().to(device))\n",
            "Train: Epoch[1/1]  [  0/209]  eta: 0:02:35  Lr: 0.002812  Loss: 4.2797  Acc@1: 4.1667 (4.1667)  Acc@5: 50.0000 (50.0000)  time: 0.7448  data: 0.4711  max mem: 1375\n",
            "Train: Epoch[1/1]  [ 10/209]  eta: 0:01:03  Lr: 0.002812  Loss: 3.1356  Acc@1: 12.5000 (11.3636)  Acc@5: 66.6667 (63.6364)  time: 0.3205  data: 0.0437  max mem: 1375\n",
            "Train: Epoch[1/1]  [ 20/209]  eta: 0:00:56  Lr: 0.002812  Loss: 2.9851  Acc@1: 16.6667 (17.6587)  Acc@5: 70.8333 (67.2619)  time: 0.2770  data: 0.0009  max mem: 1375\n",
            "Train: Epoch[1/1]  [ 30/209]  eta: 0:00:52  Lr: 0.002812  Loss: 2.4308  Acc@1: 29.1667 (23.3871)  Acc@5: 75.0000 (72.0430)  time: 0.2770  data: 0.0007  max mem: 1375\n",
            "Train: Epoch[1/1]  [ 40/209]  eta: 0:00:48  Lr: 0.002812  Loss: 2.2590  Acc@1: 41.6667 (29.3699)  Acc@5: 83.3333 (75.9146)  time: 0.2775  data: 0.0005  max mem: 1375\n",
            "Train: Epoch[1/1]  [ 50/209]  eta: 0:00:45  Lr: 0.002812  Loss: 2.3123  Acc@1: 50.0000 (34.5588)  Acc@5: 91.6667 (78.9216)  time: 0.2770  data: 0.0005  max mem: 1375\n",
            "Train: Epoch[1/1]  [ 60/209]  eta: 0:00:42  Lr: 0.002812  Loss: 1.3746  Acc@1: 58.3333 (38.6612)  Acc@5: 91.6667 (81.3525)  time: 0.2773  data: 0.0005  max mem: 1375\n",
            "Train: Epoch[1/1]  [ 70/209]  eta: 0:00:39  Lr: 0.002812  Loss: 1.5355  Acc@1: 58.3333 (42.2535)  Acc@5: 95.8333 (83.3333)  time: 0.2772  data: 0.0010  max mem: 1375\n",
            "Train: Epoch[1/1]  [ 80/209]  eta: 0:00:36  Lr: 0.002812  Loss: 1.5461  Acc@1: 70.8333 (46.0391)  Acc@5: 95.8333 (84.8251)  time: 0.2767  data: 0.0009  max mem: 1375\n",
            "Train: Epoch[1/1]  [ 90/209]  eta: 0:00:33  Lr: 0.002812  Loss: 1.7087  Acc@1: 75.0000 (49.1758)  Acc@5: 95.8333 (86.0806)  time: 0.2765  data: 0.0003  max mem: 1375\n",
            "Train: Epoch[1/1]  [100/209]  eta: 0:00:30  Lr: 0.002812  Loss: 1.2835  Acc@1: 75.0000 (51.8152)  Acc@5: 95.8333 (87.0875)  time: 0.2765  data: 0.0003  max mem: 1375\n",
            "Train: Epoch[1/1]  [110/209]  eta: 0:00:27  Lr: 0.002812  Loss: 1.3747  Acc@1: 75.0000 (53.7913)  Acc@5: 95.8333 (87.9129)  time: 0.2765  data: 0.0004  max mem: 1375\n",
            "Train: Epoch[1/1]  [120/209]  eta: 0:00:24  Lr: 0.002812  Loss: 1.3239  Acc@1: 75.0000 (55.6818)  Acc@5: 95.8333 (88.6708)  time: 0.2762  data: 0.0009  max mem: 1375\n",
            "Train: Epoch[1/1]  [130/209]  eta: 0:00:22  Lr: 0.002812  Loss: 1.2642  Acc@1: 75.0000 (57.4427)  Acc@5: 95.8333 (89.3448)  time: 0.2761  data: 0.0008  max mem: 1375\n",
            "Train: Epoch[1/1]  [140/209]  eta: 0:00:19  Lr: 0.002812  Loss: 1.2394  Acc@1: 79.1667 (59.0721)  Acc@5: 95.8333 (89.9527)  time: 0.2762  data: 0.0004  max mem: 1375\n",
            "Train: Epoch[1/1]  [150/209]  eta: 0:00:16  Lr: 0.002812  Loss: 1.2035  Acc@1: 75.0000 (60.2373)  Acc@5: 95.8333 (90.3422)  time: 0.2767  data: 0.0004  max mem: 1375\n",
            "Train: Epoch[1/1]  [160/209]  eta: 0:00:13  Lr: 0.002812  Loss: 1.0025  Acc@1: 79.1667 (61.8530)  Acc@5: 100.0000 (90.8903)  time: 0.2768  data: 0.0005  max mem: 1375\n",
            "Train: Epoch[1/1]  [170/209]  eta: 0:00:10  Lr: 0.002812  Loss: 1.1404  Acc@1: 83.3333 (63.0117)  Acc@5: 100.0000 (91.2524)  time: 0.2765  data: 0.0010  max mem: 1375\n",
            "Train: Epoch[1/1]  [180/209]  eta: 0:00:08  Lr: 0.002812  Loss: 1.0062  Acc@1: 83.3333 (64.1344)  Acc@5: 100.0000 (91.6206)  time: 0.2766  data: 0.0009  max mem: 1375\n",
            "Train: Epoch[1/1]  [190/209]  eta: 0:00:05  Lr: 0.002812  Loss: 1.1951  Acc@1: 83.3333 (65.2051)  Acc@5: 100.0000 (91.9503)  time: 0.2768  data: 0.0004  max mem: 1375\n",
            "Train: Epoch[1/1]  [200/209]  eta: 0:00:02  Lr: 0.002812  Loss: 1.4680  Acc@1: 83.3333 (66.0862)  Acc@5: 100.0000 (92.2886)  time: 0.2767  data: 0.0004  max mem: 1375\n",
            "Train: Epoch[1/1]  [208/209]  eta: 0:00:00  Lr: 0.002812  Loss: 0.8187  Acc@1: 83.3333 (66.6400)  Acc@5: 100.0000 (92.5200)  time: 0.2685  data: 0.0004  max mem: 1375\n",
            "Train: Epoch[1/1] Total time: 0:00:58 (0.2791 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 0.8187  Acc@1: 83.3333 (66.6400)  Acc@5: 100.0000 (92.5200)\n",
            "torch.Size([5, 2, 5, 6, 64])\n",
            "torch.Size([5, 2, 1, 5, 6, 64])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:25  Loss: 0.2590 (0.2590)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  Acc@task: 91.6667 (91.6667)  time: 0.5971  data: 0.4239  max mem: 1375\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.5286 (0.5199)  Acc@1: 87.5000 (88.6364)  Acc@5: 100.0000 (97.3485)  Acc@task: 87.5000 (85.6061)  time: 0.2141  data: 0.0389  max mem: 1375\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.5286 (0.5040)  Acc@1: 87.5000 (87.5000)  Acc@5: 95.8333 (97.2222)  Acc@task: 87.5000 (86.7064)  time: 0.1775  data: 0.0006  max mem: 1375\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.3803 (0.4894)  Acc@1: 87.5000 (87.9032)  Acc@5: 100.0000 (97.3118)  Acc@task: 91.6667 (86.9624)  time: 0.1789  data: 0.0013  max mem: 1375\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.3166 (0.4649)  Acc@1: 91.6667 (88.5163)  Acc@5: 100.0000 (97.7642)  Acc@task: 87.5000 (87.7033)  time: 0.1790  data: 0.0010  max mem: 1375\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.2924 (0.4558)  Acc@1: 91.6667 (88.7000)  Acc@5: 100.0000 (97.8000)  Acc@task: 87.5000 (87.9000)  time: 0.1760  data: 0.0010  max mem: 1375\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1893 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@task 87.900 Acc@1 88.700 Acc@5 97.800 loss 0.456\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:24  Loss: 0.6726 (0.6726)  Acc@1: 75.0000 (75.0000)  Acc@5: 95.8333 (95.8333)  Acc@task: 87.5000 (87.5000)  time: 0.5716  data: 0.3987  max mem: 1375\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.6726 (0.6474)  Acc@1: 83.3333 (84.0909)  Acc@5: 100.0000 (96.9697)  Acc@task: 87.5000 (83.3333)  time: 0.2144  data: 0.0366  max mem: 1375\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.5442 (0.6659)  Acc@1: 87.5000 (85.1190)  Acc@5: 95.8333 (95.6349)  Acc@task: 83.3333 (82.9365)  time: 0.1779  data: 0.0004  max mem: 1375\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.5801 (0.6610)  Acc@1: 83.3333 (83.8710)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (83.6022)  time: 0.1785  data: 0.0004  max mem: 1375\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.5454 (0.6342)  Acc@1: 83.3333 (84.6545)  Acc@5: 95.8333 (96.0366)  Acc@task: 83.3333 (83.5366)  time: 0.1800  data: 0.0003  max mem: 1375\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.5287 (0.6242)  Acc@1: 87.5000 (84.8000)  Acc@5: 95.8333 (96.1000)  Acc@task: 83.3333 (83.7000)  time: 0.1761  data: 0.0003  max mem: 1375\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1892 s / it)\n",
            "* Acc@task 83.700 Acc@1 84.800 Acc@5 96.100 loss 0.624\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:19  Loss: 0.4074 (0.4074)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  Acc@task: 91.6667 (91.6667)  time: 0.4577  data: 0.2920  max mem: 1375\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 0.5548 (0.6103)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (96.5909)  Acc@task: 83.3333 (85.2273)  time: 0.2126  data: 0.0377  max mem: 1375\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.5061 (0.5187)  Acc@1: 87.5000 (86.3095)  Acc@5: 95.8333 (96.6270)  Acc@task: 83.3333 (85.7143)  time: 0.1843  data: 0.0068  max mem: 1375\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.4209 (0.4851)  Acc@1: 87.5000 (86.9624)  Acc@5: 100.0000 (97.1774)  Acc@task: 83.3333 (86.1559)  time: 0.1801  data: 0.0012  max mem: 1375\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.4112 (0.5127)  Acc@1: 87.5000 (86.7886)  Acc@5: 100.0000 (96.8496)  Acc@task: 87.5000 (86.3821)  time: 0.1796  data: 0.0006  max mem: 1375\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.4112 (0.5129)  Acc@1: 87.5000 (86.8000)  Acc@5: 100.0000 (96.9000)  Acc@task: 87.5000 (86.2000)  time: 0.1767  data: 0.0005  max mem: 1375\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1895 s / it)\n",
            "* Acc@task 86.200 Acc@1 86.800 Acc@5 96.900 loss 0.513\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:20  Loss: 2.5017 (2.5017)  Acc@1: 45.8333 (45.8333)  Acc@5: 70.8333 (70.8333)  Acc@task: 79.1667 (79.1667)  time: 0.4885  data: 0.3140  max mem: 1375\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:06  Loss: 1.7873 (1.9568)  Acc@1: 54.1667 (51.1364)  Acc@5: 87.5000 (85.2273)  Acc@task: 87.5000 (85.2273)  time: 0.2086  data: 0.0316  max mem: 1375\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 1.9028 (2.0233)  Acc@1: 45.8333 (49.0079)  Acc@5: 87.5000 (86.1111)  Acc@task: 83.3333 (83.3333)  time: 0.1795  data: 0.0019  max mem: 1375\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 1.9041 (1.9529)  Acc@1: 45.8333 (50.8065)  Acc@5: 91.6667 (87.6344)  Acc@task: 83.3333 (84.1398)  time: 0.1793  data: 0.0004  max mem: 1375\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 1.9497 (1.9869)  Acc@1: 50.0000 (50.7114)  Acc@5: 87.5000 (86.9919)  Acc@task: 83.3333 (83.6382)  time: 0.1804  data: 0.0003  max mem: 1375\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 1.9497 (1.9733)  Acc@1: 50.0000 (51.0000)  Acc@5: 87.5000 (87.1000)  Acc@task: 83.3333 (83.7000)  time: 0.1770  data: 0.0003  max mem: 1375\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1898 s / it)\n",
            "* Acc@task 83.700 Acc@1 51.000 Acc@5 87.100 loss 1.973\n",
            "[Average accuracy till task4]\tAcc@task: 85.3750\tAcc@1: 77.8250\tAcc@5: 94.4750\tLoss: 0.8916\tForgetting: 0.0000\tBackward: 47.1000\n",
            "torch.Size([48000, 384])\n",
            "Averaged stats: Lr: 0.005000  Loss: 0.1512  Acc@1: 95.0000 (94.1944)  Acc@5: 100.0000 (99.5000)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:35  Loss: 0.3105 (0.3105)  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)  Acc@task: 91.6667 (91.6667)  time: 0.8516  data: 0.6389  max mem: 1375\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:07  Loss: 0.5110 (0.5033)  Acc@1: 87.5000 (87.8788)  Acc@5: 100.0000 (98.4848)  Acc@task: 87.5000 (85.6061)  time: 0.2411  data: 0.0589  max mem: 1375\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.5109 (0.4885)  Acc@1: 87.5000 (87.6984)  Acc@5: 100.0000 (97.8175)  Acc@task: 87.5000 (86.7064)  time: 0.1791  data: 0.0006  max mem: 1375\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.2755 (0.4714)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (97.9839)  Acc@task: 91.6667 (86.9624)  time: 0.1781  data: 0.0004  max mem: 1375\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.2812 (0.4516)  Acc@1: 87.5000 (88.2114)  Acc@5: 100.0000 (98.2724)  Acc@task: 87.5000 (87.7033)  time: 0.1782  data: 0.0004  max mem: 1375\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.2812 (0.4433)  Acc@1: 87.5000 (88.3000)  Acc@5: 100.0000 (98.3000)  Acc@task: 87.5000 (87.9000)  time: 0.1751  data: 0.0003  max mem: 1375\n",
            "Test: [Task 1] Total time: 0:00:08 (0.1961 s / it)\n",
            "* Acc@task 87.900 Acc@1 88.300 Acc@5 98.300 loss 0.443\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:26  Loss: 0.6582 (0.6582)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  Acc@task: 87.5000 (87.5000)  time: 0.6406  data: 0.4712  max mem: 1375\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:07  Loss: 0.4720 (0.5203)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (97.3485)  Acc@task: 87.5000 (83.3333)  time: 0.2202  data: 0.0432  max mem: 1375\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.4553 (0.5371)  Acc@1: 87.5000 (86.9048)  Acc@5: 95.8333 (96.8254)  Acc@task: 83.3333 (82.9365)  time: 0.1771  data: 0.0006  max mem: 1375\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.4629 (0.5388)  Acc@1: 87.5000 (86.1559)  Acc@5: 95.8333 (97.0430)  Acc@task: 83.3333 (83.6022)  time: 0.1774  data: 0.0012  max mem: 1375\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.4629 (0.5196)  Acc@1: 87.5000 (86.1789)  Acc@5: 95.8333 (97.2561)  Acc@task: 83.3333 (83.5366)  time: 0.1786  data: 0.0011  max mem: 1375\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.4196 (0.5128)  Acc@1: 87.5000 (86.3000)  Acc@5: 100.0000 (97.3000)  Acc@task: 83.3333 (83.7000)  time: 0.1747  data: 0.0010  max mem: 1375\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1898 s / it)\n",
            "* Acc@task 83.700 Acc@1 86.300 Acc@5 97.300 loss 0.513\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:25  Loss: 0.3606 (0.3606)  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)  Acc@task: 91.6667 (91.6667)  time: 0.6126  data: 0.4482  max mem: 1375\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 0.4985 (0.5955)  Acc@1: 83.3333 (85.2273)  Acc@5: 95.8333 (96.9697)  Acc@task: 83.3333 (85.2273)  time: 0.2163  data: 0.0411  max mem: 1375\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.4585 (0.5042)  Acc@1: 87.5000 (87.3016)  Acc@5: 95.8333 (96.8254)  Acc@task: 83.3333 (85.7143)  time: 0.1766  data: 0.0004  max mem: 1375\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.4048 (0.4600)  Acc@1: 87.5000 (88.0376)  Acc@5: 95.8333 (97.1774)  Acc@task: 83.3333 (86.1559)  time: 0.1770  data: 0.0011  max mem: 1375\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.3596 (0.4789)  Acc@1: 87.5000 (87.8049)  Acc@5: 95.8333 (97.0528)  Acc@task: 87.5000 (86.3821)  time: 0.1775  data: 0.0010  max mem: 1375\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.3596 (0.4778)  Acc@1: 87.5000 (87.7000)  Acc@5: 100.0000 (97.1000)  Acc@task: 87.5000 (86.2000)  time: 0.1741  data: 0.0010  max mem: 1375\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1883 s / it)\n",
            "* Acc@task 86.200 Acc@1 87.700 Acc@5 97.100 loss 0.478\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:21  Loss: 1.4325 (1.4325)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  Acc@task: 79.1667 (79.1667)  time: 0.5012  data: 0.3317  max mem: 1375\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:06  Loss: 0.4966 (0.5789)  Acc@1: 87.5000 (85.2273)  Acc@5: 95.8333 (95.4545)  Acc@task: 87.5000 (85.2273)  time: 0.2062  data: 0.0318  max mem: 1375\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.5073 (0.6470)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.6349)  Acc@task: 83.3333 (83.3333)  time: 0.1768  data: 0.0018  max mem: 1375\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.6309 (0.6381)  Acc@1: 83.3333 (83.7366)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (84.1398)  time: 0.1777  data: 0.0012  max mem: 1375\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.5635 (0.6699)  Acc@1: 83.3333 (83.5366)  Acc@5: 95.8333 (95.3252)  Acc@task: 83.3333 (83.6382)  time: 0.1782  data: 0.0004  max mem: 1375\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.5128 (0.6552)  Acc@1: 83.3333 (83.8000)  Acc@5: 95.8333 (95.4000)  Acc@task: 83.3333 (83.7000)  time: 0.1750  data: 0.0004  max mem: 1375\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1863 s / it)\n",
            "* Acc@task 83.700 Acc@1 83.800 Acc@5 95.400 loss 0.655\n",
            "[Average accuracy till task4]\tAcc@task: 85.3750\tAcc@1: 86.5250\tAcc@5: 97.0250\tLoss: 0.5223\tForgetting: 0.1333\tBackward: 5.8000\n",
            "Loading checkpoint from: ./output/cifar100_full_dino_1epoch_100pct/checkpoint/task5_checkpoint.pth\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/engines/hide_promtp_wtp_and_tap_engine.py:540: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  loss = torch.nn.functional.cross_entropy(sim, torch.range(0, sim.shape[0] - 1).long().to(device))\n",
            "Train: Epoch[1/1]  [  0/209]  eta: 0:02:41  Lr: 0.002812  Loss: 3.7405  Acc@1: 8.3333 (8.3333)  Acc@5: 70.8333 (70.8333)  time: 0.7714  data: 0.4923  max mem: 1375\n",
            "Train: Epoch[1/1]  [ 10/209]  eta: 0:01:03  Lr: 0.002812  Loss: 3.3914  Acc@1: 12.5000 (12.1212)  Acc@5: 58.3333 (52.6515)  time: 0.3186  data: 0.0454  max mem: 1376\n",
            "Train: Epoch[1/1]  [ 20/209]  eta: 0:00:56  Lr: 0.002812  Loss: 2.7491  Acc@1: 12.5000 (19.4444)  Acc@5: 58.3333 (59.3254)  time: 0.2743  data: 0.0006  max mem: 1376\n",
            "Train: Epoch[1/1]  [ 30/209]  eta: 0:00:52  Lr: 0.002812  Loss: 2.9356  Acc@1: 33.3333 (25.4032)  Acc@5: 70.8333 (66.1290)  time: 0.2758  data: 0.0005  max mem: 1376\n",
            "Train: Epoch[1/1]  [ 40/209]  eta: 0:00:48  Lr: 0.002812  Loss: 2.6693  Acc@1: 41.6667 (31.4024)  Acc@5: 83.3333 (71.0366)  time: 0.2769  data: 0.0006  max mem: 1376\n",
            "Train: Epoch[1/1]  [ 50/209]  eta: 0:00:45  Lr: 0.002812  Loss: 2.0991  Acc@1: 54.1667 (36.5196)  Acc@5: 87.5000 (75.0000)  time: 0.2772  data: 0.0006  max mem: 1376\n",
            "Train: Epoch[1/1]  [ 60/209]  eta: 0:00:42  Lr: 0.002812  Loss: 2.1759  Acc@1: 58.3333 (40.4372)  Acc@5: 87.5000 (77.3907)  time: 0.2768  data: 0.0005  max mem: 1376\n",
            "Train: Epoch[1/1]  [ 70/209]  eta: 0:00:39  Lr: 0.002812  Loss: 1.3844  Acc@1: 66.6667 (44.2488)  Acc@5: 91.6667 (79.6362)  time: 0.2765  data: 0.0005  max mem: 1376\n",
            "Train: Epoch[1/1]  [ 80/209]  eta: 0:00:36  Lr: 0.002812  Loss: 1.6505  Acc@1: 70.8333 (47.5823)  Acc@5: 95.8333 (81.4300)  time: 0.2770  data: 0.0006  max mem: 1376\n",
            "Train: Epoch[1/1]  [ 90/209]  eta: 0:00:33  Lr: 0.002812  Loss: 1.3049  Acc@1: 70.8333 (50.2747)  Acc@5: 95.8333 (82.8297)  time: 0.2775  data: 0.0007  max mem: 1376\n",
            "Train: Epoch[1/1]  [100/209]  eta: 0:00:30  Lr: 0.002812  Loss: 1.8910  Acc@1: 70.8333 (52.3102)  Acc@5: 95.8333 (84.0347)  time: 0.2778  data: 0.0007  max mem: 1376\n",
            "Train: Epoch[1/1]  [110/209]  eta: 0:00:27  Lr: 0.002812  Loss: 1.8067  Acc@1: 70.8333 (54.2042)  Acc@5: 95.8333 (85.1351)  time: 0.2786  data: 0.0004  max mem: 1376\n",
            "Train: Epoch[1/1]  [120/209]  eta: 0:00:24  Lr: 0.002812  Loss: 1.1919  Acc@1: 75.0000 (56.1295)  Acc@5: 95.8333 (86.0882)  time: 0.2780  data: 0.0004  max mem: 1376\n",
            "Train: Epoch[1/1]  [130/209]  eta: 0:00:22  Lr: 0.002812  Loss: 1.2814  Acc@1: 75.0000 (57.6018)  Acc@5: 95.8333 (86.9911)  time: 0.2780  data: 0.0003  max mem: 1376\n",
            "Train: Epoch[1/1]  [140/209]  eta: 0:00:19  Lr: 0.002812  Loss: 1.4055  Acc@1: 79.1667 (59.3085)  Acc@5: 100.0000 (87.6478)  time: 0.2786  data: 0.0007  max mem: 1376\n",
            "Train: Epoch[1/1]  [150/209]  eta: 0:00:16  Lr: 0.002812  Loss: 1.1250  Acc@1: 83.3333 (60.8996)  Acc@5: 100.0000 (88.2726)  time: 0.2778  data: 0.0008  max mem: 1376\n",
            "Train: Epoch[1/1]  [160/209]  eta: 0:00:13  Lr: 0.002812  Loss: 1.3593  Acc@1: 79.1667 (62.0083)  Acc@5: 95.8333 (88.8199)  time: 0.2780  data: 0.0004  max mem: 1376\n",
            "Train: Epoch[1/1]  [170/209]  eta: 0:00:10  Lr: 0.002812  Loss: 1.4715  Acc@1: 79.1667 (63.2310)  Acc@5: 95.8333 (89.1813)  time: 0.2786  data: 0.0004  max mem: 1376\n",
            "Train: Epoch[1/1]  [180/209]  eta: 0:00:08  Lr: 0.002812  Loss: 1.4227  Acc@1: 79.1667 (64.2495)  Acc@5: 95.8333 (89.6639)  time: 0.2795  data: 0.0006  max mem: 1376\n",
            "Train: Epoch[1/1]  [190/209]  eta: 0:00:05  Lr: 0.002812  Loss: 1.6501  Acc@1: 79.1667 (65.0742)  Acc@5: 95.8333 (89.9651)  time: 0.2801  data: 0.0008  max mem: 1376\n",
            "Train: Epoch[1/1]  [200/209]  eta: 0:00:02  Lr: 0.002812  Loss: 1.3403  Acc@1: 83.3333 (66.0448)  Acc@5: 95.8333 (90.2985)  time: 0.2791  data: 0.0006  max mem: 1376\n",
            "Train: Epoch[1/1]  [208/209]  eta: 0:00:00  Lr: 0.002812  Loss: 1.2264  Acc@1: 83.3333 (66.5400)  Acc@5: 95.8333 (90.5400)  time: 0.2701  data: 0.0003  max mem: 1376\n",
            "Train: Epoch[1/1] Total time: 0:00:58 (0.2798 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.2264  Acc@1: 83.3333 (66.5400)  Acc@5: 95.8333 (90.5400)\n",
            "torch.Size([5, 2, 5, 6, 64])\n",
            "torch.Size([5, 2, 1, 5, 6, 64])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:27  Loss: 0.3271 (0.3271)  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)  Acc@task: 87.5000 (87.5000)  time: 0.6429  data: 0.4807  max mem: 1376\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:07  Loss: 0.4592 (0.4814)  Acc@1: 91.6667 (88.2576)  Acc@5: 100.0000 (98.1061)  Acc@task: 87.5000 (83.7121)  time: 0.2188  data: 0.0442  max mem: 1376\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.4592 (0.4916)  Acc@1: 87.5000 (88.2937)  Acc@5: 100.0000 (97.8175)  Acc@task: 83.3333 (84.5238)  time: 0.1766  data: 0.0006  max mem: 1376\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.2774 (0.4688)  Acc@1: 87.5000 (87.7688)  Acc@5: 100.0000 (98.3871)  Acc@task: 83.3333 (84.6774)  time: 0.1775  data: 0.0005  max mem: 1376\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.2837 (0.4491)  Acc@1: 87.5000 (88.4146)  Acc@5: 100.0000 (98.5772)  Acc@task: 87.5000 (85.7724)  time: 0.1782  data: 0.0003  max mem: 1376\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.2837 (0.4408)  Acc@1: 87.5000 (88.5000)  Acc@5: 100.0000 (98.6000)  Acc@task: 87.5000 (86.0000)  time: 0.1751  data: 0.0003  max mem: 1376\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1903 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@task 86.000 Acc@1 88.500 Acc@5 98.600 loss 0.441\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:40  Loss: 0.6602 (0.6602)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  Acc@task: 87.5000 (87.5000)  time: 0.9716  data: 0.7815  max mem: 1376\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:07  Loss: 0.5063 (0.5384)  Acc@1: 87.5000 (86.7424)  Acc@5: 100.0000 (97.7273)  Acc@task: 83.3333 (80.6818)  time: 0.2499  data: 0.0727  max mem: 1376\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.4677 (0.5398)  Acc@1: 87.5000 (86.9048)  Acc@5: 100.0000 (96.8254)  Acc@task: 79.1667 (80.5556)  time: 0.1778  data: 0.0014  max mem: 1376\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.4643 (0.5389)  Acc@1: 87.5000 (86.2903)  Acc@5: 95.8333 (96.9086)  Acc@task: 83.3333 (81.1828)  time: 0.1782  data: 0.0007  max mem: 1376\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.4640 (0.5190)  Acc@1: 87.5000 (86.2805)  Acc@5: 95.8333 (97.1545)  Acc@task: 83.3333 (81.3008)  time: 0.1783  data: 0.0003  max mem: 1376\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.4473 (0.5124)  Acc@1: 87.5000 (86.4000)  Acc@5: 95.8333 (97.2000)  Acc@task: 83.3333 (81.5000)  time: 0.1751  data: 0.0003  max mem: 1376\n",
            "Test: [Task 2] Total time: 0:00:08 (0.1981 s / it)\n",
            "* Acc@task 81.500 Acc@1 86.400 Acc@5 97.200 loss 0.512\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:27  Loss: 0.1651 (0.1651)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  Acc@task: 91.6667 (91.6667)  time: 0.6485  data: 0.4775  max mem: 1376\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:07  Loss: 0.5004 (0.5867)  Acc@1: 83.3333 (85.6061)  Acc@5: 95.8333 (96.5909)  Acc@task: 83.3333 (81.8182)  time: 0.2214  data: 0.0441  max mem: 1376\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.4680 (0.5004)  Acc@1: 87.5000 (87.5000)  Acc@5: 95.8333 (96.6270)  Acc@task: 83.3333 (82.9365)  time: 0.1782  data: 0.0006  max mem: 1376\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.4125 (0.4557)  Acc@1: 87.5000 (88.3065)  Acc@5: 95.8333 (96.7742)  Acc@task: 83.3333 (83.0645)  time: 0.1786  data: 0.0004  max mem: 1376\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.3523 (0.4753)  Acc@1: 87.5000 (88.1098)  Acc@5: 95.8333 (96.7480)  Acc@task: 83.3333 (83.6382)  time: 0.1800  data: 0.0004  max mem: 1376\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.3523 (0.4762)  Acc@1: 87.5000 (88.0000)  Acc@5: 95.8333 (96.8000)  Acc@task: 83.3333 (83.4000)  time: 0.1763  data: 0.0004  max mem: 1376\n",
            "Test: [Task 3] Total time: 0:00:08 (0.1928 s / it)\n",
            "* Acc@task 83.400 Acc@1 88.000 Acc@5 96.800 loss 0.476\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:35  Loss: 1.4907 (1.4907)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  Acc@task: 83.3333 (83.3333)  time: 0.8480  data: 0.5976  max mem: 1376\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:07  Loss: 0.4980 (0.5923)  Acc@1: 87.5000 (84.4697)  Acc@5: 95.8333 (95.4545)  Acc@task: 87.5000 (85.6061)  time: 0.2393  data: 0.0550  max mem: 1376\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.4980 (0.6236)  Acc@1: 83.3333 (83.7302)  Acc@5: 95.8333 (96.0317)  Acc@task: 83.3333 (84.3254)  time: 0.1780  data: 0.0005  max mem: 1376\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.5592 (0.5927)  Acc@1: 83.3333 (84.6774)  Acc@5: 95.8333 (96.5054)  Acc@task: 87.5000 (85.3495)  time: 0.1784  data: 0.0004  max mem: 1376\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.5624 (0.6332)  Acc@1: 83.3333 (84.4512)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (84.5528)  time: 0.1796  data: 0.0003  max mem: 1376\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.5624 (0.6193)  Acc@1: 83.3333 (84.7000)  Acc@5: 95.8333 (95.9000)  Acc@task: 83.3333 (84.7000)  time: 0.1761  data: 0.0003  max mem: 1376\n",
            "Test: [Task 4] Total time: 0:00:08 (0.1960 s / it)\n",
            "* Acc@task 84.700 Acc@1 84.700 Acc@5 95.900 loss 0.619\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:28  Loss: 1.5384 (1.5384)  Acc@1: 41.6667 (41.6667)  Acc@5: 91.6667 (91.6667)  Acc@task: 83.3333 (83.3333)  time: 0.6756  data: 0.5065  max mem: 1376\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:07  Loss: 2.3732 (2.3196)  Acc@1: 33.3333 (34.4697)  Acc@5: 91.6667 (88.6364)  Acc@task: 79.1667 (81.4394)  time: 0.2239  data: 0.0473  max mem: 1376\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 2.4520 (2.3656)  Acc@1: 33.3333 (35.7143)  Acc@5: 87.5000 (87.1032)  Acc@task: 79.1667 (81.3492)  time: 0.1780  data: 0.0013  max mem: 1376\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 2.4859 (2.4130)  Acc@1: 37.5000 (36.6935)  Acc@5: 83.3333 (86.1559)  Acc@task: 83.3333 (81.0484)  time: 0.1784  data: 0.0013  max mem: 1376\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 2.5608 (2.4688)  Acc@1: 33.3333 (35.6707)  Acc@5: 83.3333 (84.9594)  Acc@task: 79.1667 (79.7764)  time: 0.1796  data: 0.0010  max mem: 1376\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 2.5608 (2.4892)  Acc@1: 33.3333 (35.4000)  Acc@5: 83.3333 (84.7000)  Acc@task: 75.0000 (79.7000)  time: 0.1761  data: 0.0010  max mem: 1376\n",
            "Test: [Task 5] Total time: 0:00:08 (0.1917 s / it)\n",
            "* Acc@task 79.700 Acc@1 35.400 Acc@5 84.700 loss 2.489\n",
            "[Average accuracy till task5]\tAcc@task: 83.0600\tAcc@1: 76.6000\tAcc@5: 94.6400\tLoss: 0.9076\tForgetting: 0.0500\tBackward: 44.4000\n",
            "torch.Size([60000, 384])\n",
            "Averaged stats: Lr: 0.005000  Loss: 0.0801  Acc@1: 97.5000 (95.5208)  Acc@5: 100.0000 (99.7083)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:25  Loss: 0.3496 (0.3496)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 87.5000 (87.5000)  time: 0.6139  data: 0.4335  max mem: 1376\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.4679 (0.4063)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (98.8636)  Acc@task: 87.5000 (83.7121)  time: 0.2178  data: 0.0397  max mem: 1376\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.3664 (0.4139)  Acc@1: 91.6667 (88.4921)  Acc@5: 100.0000 (98.2143)  Acc@task: 83.3333 (84.5238)  time: 0.1775  data: 0.0004  max mem: 1376\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.2792 (0.3828)  Acc@1: 91.6667 (89.5161)  Acc@5: 100.0000 (98.6559)  Acc@task: 83.3333 (84.6774)  time: 0.1779  data: 0.0004  max mem: 1376\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.1740 (0.3568)  Acc@1: 95.8333 (90.2439)  Acc@5: 100.0000 (98.8821)  Acc@task: 87.5000 (85.7724)  time: 0.1792  data: 0.0003  max mem: 1376\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.1550 (0.3495)  Acc@1: 95.8333 (90.4000)  Acc@5: 100.0000 (98.9000)  Acc@task: 87.5000 (86.0000)  time: 0.1757  data: 0.0003  max mem: 1376\n",
            "Test: [Task 1] Total time: 0:00:08 (0.1906 s / it)\n",
            "* Acc@task 86.000 Acc@1 90.400 Acc@5 98.900 loss 0.350\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:42  Loss: 0.8034 (0.8034)  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.8333)  Acc@task: 87.5000 (87.5000)  time: 1.0067  data: 0.8149  max mem: 1376\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:08  Loss: 0.5002 (0.5423)  Acc@1: 83.3333 (84.0909)  Acc@5: 100.0000 (97.7273)  Acc@task: 83.3333 (80.6818)  time: 0.2536  data: 0.0757  max mem: 1376\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.4721 (0.5640)  Acc@1: 87.5000 (84.5238)  Acc@5: 95.8333 (96.6270)  Acc@task: 79.1667 (80.5556)  time: 0.1781  data: 0.0011  max mem: 1376\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.4988 (0.5472)  Acc@1: 87.5000 (84.5430)  Acc@5: 95.8333 (96.9086)  Acc@task: 83.3333 (81.1828)  time: 0.1784  data: 0.0004  max mem: 1376\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.4325 (0.5193)  Acc@1: 87.5000 (84.9594)  Acc@5: 95.8333 (97.1545)  Acc@task: 83.3333 (81.3008)  time: 0.1788  data: 0.0003  max mem: 1376\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.3762 (0.5119)  Acc@1: 87.5000 (85.0000)  Acc@5: 100.0000 (97.2000)  Acc@task: 83.3333 (81.5000)  time: 0.1756  data: 0.0003  max mem: 1376\n",
            "Test: [Task 2] Total time: 0:00:08 (0.1993 s / it)\n",
            "* Acc@task 81.500 Acc@1 85.000 Acc@5 97.200 loss 0.512\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:19  Loss: 0.0658 (0.0658)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  Acc@task: 91.6667 (91.6667)  time: 0.4591  data: 0.2905  max mem: 1376\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 0.6368 (0.5887)  Acc@1: 87.5000 (85.6061)  Acc@5: 100.0000 (97.3485)  Acc@task: 83.3333 (81.8182)  time: 0.2070  data: 0.0313  max mem: 1376\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.4925 (0.5168)  Acc@1: 87.5000 (87.1032)  Acc@5: 100.0000 (97.4206)  Acc@task: 83.3333 (82.9365)  time: 0.1795  data: 0.0029  max mem: 1376\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.4050 (0.4854)  Acc@1: 87.5000 (87.6344)  Acc@5: 100.0000 (97.7151)  Acc@task: 83.3333 (83.0645)  time: 0.1780  data: 0.0005  max mem: 1376\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.3584 (0.5105)  Acc@1: 87.5000 (87.1951)  Acc@5: 95.8333 (97.3577)  Acc@task: 83.3333 (83.6382)  time: 0.1786  data: 0.0006  max mem: 1376\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.3584 (0.5146)  Acc@1: 87.5000 (87.0000)  Acc@5: 95.8333 (97.4000)  Acc@task: 83.3333 (83.4000)  time: 0.1751  data: 0.0006  max mem: 1376\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1885 s / it)\n",
            "* Acc@task 83.400 Acc@1 87.000 Acc@5 97.400 loss 0.515\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:36  Loss: 1.1269 (1.1269)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  Acc@task: 83.3333 (83.3333)  time: 0.8594  data: 0.6838  max mem: 1376\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:07  Loss: 0.4800 (0.5525)  Acc@1: 83.3333 (85.2273)  Acc@5: 95.8333 (95.4545)  Acc@task: 87.5000 (85.6061)  time: 0.2388  data: 0.0631  max mem: 1376\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.5087 (0.5888)  Acc@1: 83.3333 (84.3254)  Acc@5: 95.8333 (96.0317)  Acc@task: 83.3333 (84.3254)  time: 0.1770  data: 0.0007  max mem: 1376\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.4348 (0.5336)  Acc@1: 83.3333 (85.3495)  Acc@5: 100.0000 (96.7742)  Acc@task: 87.5000 (85.3495)  time: 0.1779  data: 0.0004  max mem: 1376\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.3691 (0.5689)  Acc@1: 83.3333 (84.8577)  Acc@5: 95.8333 (96.3415)  Acc@task: 83.3333 (84.5528)  time: 0.1783  data: 0.0003  max mem: 1376\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.3691 (0.5558)  Acc@1: 83.3333 (85.1000)  Acc@5: 100.0000 (96.4000)  Acc@task: 83.3333 (84.7000)  time: 0.1751  data: 0.0003  max mem: 1376\n",
            "Test: [Task 4] Total time: 0:00:08 (0.1950 s / it)\n",
            "* Acc@task 84.700 Acc@1 85.100 Acc@5 96.400 loss 0.556\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:27  Loss: 0.1430 (0.1430)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.6633  data: 0.4926  max mem: 1376\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:07  Loss: 0.4798 (0.4883)  Acc@1: 87.5000 (86.3636)  Acc@5: 100.0000 (98.4848)  Acc@task: 79.1667 (81.4394)  time: 0.2221  data: 0.0451  max mem: 1376\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.5245 (0.5600)  Acc@1: 83.3333 (84.1270)  Acc@5: 100.0000 (97.8175)  Acc@task: 79.1667 (81.3492)  time: 0.1776  data: 0.0006  max mem: 1376\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.5167 (0.5444)  Acc@1: 83.3333 (83.4677)  Acc@5: 100.0000 (98.2527)  Acc@task: 83.3333 (81.0484)  time: 0.1781  data: 0.0008  max mem: 1376\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.5127 (0.5567)  Acc@1: 83.3333 (83.9431)  Acc@5: 95.8333 (97.7642)  Acc@task: 79.1667 (79.7764)  time: 0.1789  data: 0.0006  max mem: 1376\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.5127 (0.5730)  Acc@1: 83.3333 (83.5000)  Acc@5: 95.8333 (97.6000)  Acc@task: 75.0000 (79.7000)  time: 0.1754  data: 0.0006  max mem: 1376\n",
            "Test: [Task 5] Total time: 0:00:08 (0.1908 s / it)\n",
            "* Acc@task 79.700 Acc@1 83.500 Acc@5 97.600 loss 0.573\n",
            "[Average accuracy till task5]\tAcc@task: 83.0600\tAcc@1: 86.2000\tAcc@5: 97.5000\tLoss: 0.5010\tForgetting: 0.5000\tBackward: 4.7000\n",
            "Loading checkpoint from: ./output/cifar100_full_dino_1epoch_100pct/checkpoint/task6_checkpoint.pth\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/engines/hide_promtp_wtp_and_tap_engine.py:540: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  loss = torch.nn.functional.cross_entropy(sim, torch.range(0, sim.shape[0] - 1).long().to(device))\n",
            "Train: Epoch[1/1]  [  0/209]  eta: 0:02:44  Lr: 0.002812  Loss: 5.1079  Acc@1: 0.0000 (0.0000)  Acc@5: 45.8333 (45.8333)  time: 0.7875  data: 0.5192  max mem: 1376\n",
            "Train: Epoch[1/1]  [ 10/209]  eta: 0:01:04  Lr: 0.002812  Loss: 3.9601  Acc@1: 12.5000 (12.8788)  Acc@5: 58.3333 (57.5758)  time: 0.3220  data: 0.0476  max mem: 1376\n",
            "Train: Epoch[1/1]  [ 20/209]  eta: 0:00:56  Lr: 0.002812  Loss: 3.2402  Acc@1: 16.6667 (17.2619)  Acc@5: 66.6667 (64.0873)  time: 0.2766  data: 0.0005  max mem: 1376\n",
            "Train: Epoch[1/1]  [ 30/209]  eta: 0:00:52  Lr: 0.002812  Loss: 2.9627  Acc@1: 29.1667 (22.0430)  Acc@5: 75.0000 (68.9516)  time: 0.2778  data: 0.0005  max mem: 1376\n",
            "Train: Epoch[1/1]  [ 40/209]  eta: 0:00:49  Lr: 0.002812  Loss: 2.8323  Acc@1: 37.5000 (27.4390)  Acc@5: 83.3333 (74.2886)  time: 0.2792  data: 0.0007  max mem: 1376\n",
            "Train: Epoch[1/1]  [ 50/209]  eta: 0:00:45  Lr: 0.002812  Loss: 1.9996  Acc@1: 50.0000 (32.1895)  Acc@5: 87.5000 (77.0425)  time: 0.2797  data: 0.0008  max mem: 1376\n",
            "Train: Epoch[1/1]  [ 60/209]  eta: 0:00:42  Lr: 0.002812  Loss: 1.9740  Acc@1: 54.1667 (36.0656)  Acc@5: 87.5000 (79.4399)  time: 0.2785  data: 0.0004  max mem: 1376\n",
            "Train: Epoch[1/1]  [ 70/209]  eta: 0:00:39  Lr: 0.002812  Loss: 1.9111  Acc@1: 58.3333 (39.9648)  Acc@5: 95.8333 (81.8075)  time: 0.2784  data: 0.0004  max mem: 1376\n",
            "Train: Epoch[1/1]  [ 80/209]  eta: 0:00:36  Lr: 0.002812  Loss: 2.3066  Acc@1: 62.5000 (42.6440)  Acc@5: 95.8333 (83.3848)  time: 0.2791  data: 0.0004  max mem: 1376\n",
            "Train: Epoch[1/1]  [ 90/209]  eta: 0:00:33  Lr: 0.002812  Loss: 1.4883  Acc@1: 66.6667 (45.4212)  Acc@5: 95.8333 (84.7985)  time: 0.2790  data: 0.0007  max mem: 1376\n",
            "Train: Epoch[1/1]  [100/209]  eta: 0:00:30  Lr: 0.002812  Loss: 2.0324  Acc@1: 70.8333 (47.6073)  Acc@5: 95.8333 (85.9323)  time: 0.2788  data: 0.0008  max mem: 1376\n",
            "Train: Epoch[1/1]  [110/209]  eta: 0:00:28  Lr: 0.002812  Loss: 1.5437  Acc@1: 70.8333 (49.9625)  Acc@5: 95.8333 (87.0496)  time: 0.2786  data: 0.0006  max mem: 1376\n",
            "Train: Epoch[1/1]  [120/209]  eta: 0:00:25  Lr: 0.002812  Loss: 1.5994  Acc@1: 75.0000 (51.8595)  Acc@5: 100.0000 (88.0510)  time: 0.2791  data: 0.0004  max mem: 1376\n",
            "Train: Epoch[1/1]  [130/209]  eta: 0:00:22  Lr: 0.002812  Loss: 1.9283  Acc@1: 75.0000 (53.4351)  Acc@5: 100.0000 (88.7405)  time: 0.2796  data: 0.0004  max mem: 1376\n",
            "Train: Epoch[1/1]  [140/209]  eta: 0:00:19  Lr: 0.002812  Loss: 1.3654  Acc@1: 70.8333 (54.6690)  Acc@5: 100.0000 (89.4504)  time: 0.2794  data: 0.0005  max mem: 1376\n",
            "Train: Epoch[1/1]  [150/209]  eta: 0:00:16  Lr: 0.002812  Loss: 1.4566  Acc@1: 70.8333 (55.9603)  Acc@5: 100.0000 (90.0110)  time: 0.2793  data: 0.0007  max mem: 1376\n",
            "Train: Epoch[1/1]  [160/209]  eta: 0:00:13  Lr: 0.002812  Loss: 1.4279  Acc@1: 75.0000 (57.1429)  Acc@5: 100.0000 (90.5280)  time: 0.2794  data: 0.0006  max mem: 1376\n",
            "Train: Epoch[1/1]  [170/209]  eta: 0:00:10  Lr: 0.002812  Loss: 1.3208  Acc@1: 75.0000 (58.3090)  Acc@5: 100.0000 (90.9844)  time: 0.2792  data: 0.0004  max mem: 1376\n",
            "Train: Epoch[1/1]  [180/209]  eta: 0:00:08  Lr: 0.002812  Loss: 1.3714  Acc@1: 75.0000 (59.2311)  Acc@5: 100.0000 (91.4134)  time: 0.2784  data: 0.0004  max mem: 1376\n",
            "Train: Epoch[1/1]  [190/209]  eta: 0:00:05  Lr: 0.002812  Loss: 1.5652  Acc@1: 75.0000 (60.1876)  Acc@5: 100.0000 (91.7757)  time: 0.2786  data: 0.0008  max mem: 1376\n",
            "Train: Epoch[1/1]  [200/209]  eta: 0:00:02  Lr: 0.002812  Loss: 1.6131  Acc@1: 79.1667 (61.1111)  Acc@5: 100.0000 (92.1227)  time: 0.2793  data: 0.0009  max mem: 1376\n",
            "Train: Epoch[1/1]  [208/209]  eta: 0:00:00  Lr: 0.002812  Loss: 1.2321  Acc@1: 79.1667 (61.7800)  Acc@5: 100.0000 (92.3600)  time: 0.2706  data: 0.0007  max mem: 1376\n",
            "Train: Epoch[1/1] Total time: 0:00:58 (0.2811 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.2321  Acc@1: 79.1667 (61.7800)  Acc@5: 100.0000 (92.3600)\n",
            "torch.Size([5, 2, 5, 6, 64])\n",
            "torch.Size([5, 2, 1, 5, 6, 64])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:22  Loss: 0.3502 (0.3502)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.5357  data: 0.3583  max mem: 1376\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.4672 (0.4331)  Acc@1: 87.5000 (87.8788)  Acc@5: 100.0000 (98.8636)  Acc@task: 83.3333 (82.5758)  time: 0.2088  data: 0.0332  max mem: 1376\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.3710 (0.4467)  Acc@1: 87.5000 (88.2937)  Acc@5: 100.0000 (98.2143)  Acc@task: 83.3333 (82.5397)  time: 0.1765  data: 0.0006  max mem: 1376\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.2863 (0.4111)  Acc@1: 91.6667 (89.1129)  Acc@5: 100.0000 (98.5215)  Acc@task: 83.3333 (83.0645)  time: 0.1777  data: 0.0006  max mem: 1376\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.1919 (0.3845)  Acc@1: 91.6667 (89.7358)  Acc@5: 100.0000 (98.7805)  Acc@task: 87.5000 (83.7398)  time: 0.1786  data: 0.0004  max mem: 1376\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.1746 (0.3766)  Acc@1: 91.6667 (89.9000)  Acc@5: 100.0000 (98.8000)  Acc@task: 87.5000 (84.0000)  time: 0.1755  data: 0.0004  max mem: 1376\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1873 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@task 84.000 Acc@1 89.900 Acc@5 98.800 loss 0.377\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:21  Loss: 0.8474 (0.8474)  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (83.3333)  time: 0.5022  data: 0.3328  max mem: 1376\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.4826 (0.5511)  Acc@1: 83.3333 (83.7121)  Acc@5: 100.0000 (97.7273)  Acc@task: 83.3333 (81.0606)  time: 0.2135  data: 0.0388  max mem: 1376\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.4769 (0.5795)  Acc@1: 83.3333 (83.9286)  Acc@5: 95.8333 (96.6270)  Acc@task: 79.1667 (79.5635)  time: 0.1814  data: 0.0059  max mem: 1376\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.4979 (0.5677)  Acc@1: 83.3333 (83.6022)  Acc@5: 95.8333 (96.9086)  Acc@task: 79.1667 (80.3763)  time: 0.1788  data: 0.0017  max mem: 1376\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.4307 (0.5353)  Acc@1: 87.5000 (84.1463)  Acc@5: 95.8333 (97.1545)  Acc@task: 79.1667 (80.2846)  time: 0.1792  data: 0.0006  max mem: 1376\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.3825 (0.5275)  Acc@1: 87.5000 (84.2000)  Acc@5: 100.0000 (97.2000)  Acc@task: 83.3333 (80.4000)  time: 0.1764  data: 0.0005  max mem: 1376\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1895 s / it)\n",
            "* Acc@task 80.400 Acc@1 84.200 Acc@5 97.200 loss 0.528\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:20  Loss: 0.0703 (0.0703)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  Acc@task: 87.5000 (87.5000)  time: 0.4900  data: 0.3143  max mem: 1376\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 0.4964 (0.5668)  Acc@1: 87.5000 (85.9849)  Acc@5: 95.8333 (97.3485)  Acc@task: 83.3333 (82.1970)  time: 0.2069  data: 0.0291  max mem: 1376\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.4500 (0.4956)  Acc@1: 87.5000 (87.6984)  Acc@5: 95.8333 (97.4206)  Acc@task: 83.3333 (82.9365)  time: 0.1782  data: 0.0006  max mem: 1376\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.4056 (0.4687)  Acc@1: 87.5000 (88.0376)  Acc@5: 100.0000 (97.7151)  Acc@task: 83.3333 (82.7957)  time: 0.1787  data: 0.0005  max mem: 1376\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.3740 (0.4939)  Acc@1: 87.5000 (87.6016)  Acc@5: 95.8333 (97.4594)  Acc@task: 83.3333 (83.3333)  time: 0.1798  data: 0.0003  max mem: 1376\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.3740 (0.4972)  Acc@1: 87.5000 (87.4000)  Acc@5: 95.8333 (97.5000)  Acc@task: 83.3333 (83.2000)  time: 0.1760  data: 0.0003  max mem: 1376\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1890 s / it)\n",
            "* Acc@task 83.200 Acc@1 87.400 Acc@5 97.500 loss 0.497\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:42  Loss: 1.1312 (1.1312)  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  Acc@task: 70.8333 (70.8333)  time: 1.0024  data: 0.8020  max mem: 1376\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:08  Loss: 0.4830 (0.5548)  Acc@1: 87.5000 (85.6061)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (82.1970)  time: 0.2538  data: 0.0742  max mem: 1376\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.4830 (0.5789)  Acc@1: 87.5000 (85.1190)  Acc@5: 95.8333 (96.4286)  Acc@task: 83.3333 (82.5397)  time: 0.1785  data: 0.0010  max mem: 1376\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.4303 (0.5249)  Acc@1: 87.5000 (85.8871)  Acc@5: 100.0000 (97.0430)  Acc@task: 83.3333 (83.4677)  time: 0.1787  data: 0.0006  max mem: 1376\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.4303 (0.5549)  Acc@1: 87.5000 (85.3659)  Acc@5: 100.0000 (96.7480)  Acc@task: 83.3333 (82.6220)  time: 0.1792  data: 0.0004  max mem: 1376\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.4303 (0.5421)  Acc@1: 87.5000 (85.6000)  Acc@5: 100.0000 (96.8000)  Acc@task: 83.3333 (82.8000)  time: 0.1758  data: 0.0004  max mem: 1376\n",
            "Test: [Task 4] Total time: 0:00:08 (0.1997 s / it)\n",
            "* Acc@task 82.800 Acc@1 85.600 Acc@5 96.800 loss 0.542\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:41  Loss: 0.1368 (0.1368)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  Acc@task: 75.0000 (75.0000)  time: 0.9935  data: 0.7655  max mem: 1376\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:08  Loss: 0.4711 (0.4658)  Acc@1: 87.5000 (86.3636)  Acc@5: 100.0000 (98.4848)  Acc@task: 79.1667 (79.5455)  time: 0.2530  data: 0.0712  max mem: 1376\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.4753 (0.5355)  Acc@1: 83.3333 (84.5238)  Acc@5: 100.0000 (97.6190)  Acc@task: 79.1667 (79.7619)  time: 0.1785  data: 0.0013  max mem: 1376\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.4753 (0.5117)  Acc@1: 83.3333 (84.4086)  Acc@5: 100.0000 (98.1183)  Acc@task: 83.3333 (80.2419)  time: 0.1786  data: 0.0010  max mem: 1376\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.4772 (0.5195)  Acc@1: 87.5000 (84.9594)  Acc@5: 95.8333 (97.6626)  Acc@task: 79.1667 (79.1667)  time: 0.1792  data: 0.0007  max mem: 1376\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.4772 (0.5362)  Acc@1: 87.5000 (84.5000)  Acc@5: 95.8333 (97.5000)  Acc@task: 79.1667 (79.1000)  time: 0.1760  data: 0.0007  max mem: 1376\n",
            "Test: [Task 5] Total time: 0:00:08 (0.2008 s / it)\n",
            "* Acc@task 79.100 Acc@1 84.500 Acc@5 97.500 loss 0.536\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:35  Loss: 3.7318 (3.7318)  Acc@1: 33.3333 (33.3333)  Acc@5: 83.3333 (83.3333)  Acc@task: 87.5000 (87.5000)  time: 0.8477  data: 0.6845  max mem: 1376\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:07  Loss: 3.9783 (3.9362)  Acc@1: 20.8333 (21.5909)  Acc@5: 79.1667 (80.3030)  Acc@task: 79.1667 (79.5455)  time: 0.2387  data: 0.0625  max mem: 1376\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:04  Loss: 3.8767 (3.8219)  Acc@1: 16.6667 (21.0317)  Acc@5: 79.1667 (80.1587)  Acc@task: 79.1667 (78.9683)  time: 0.1775  data: 0.0005  max mem: 1376\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 3.8154 (3.8905)  Acc@1: 16.6667 (19.7581)  Acc@5: 79.1667 (78.7634)  Acc@task: 79.1667 (78.3602)  time: 0.1782  data: 0.0005  max mem: 1376\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 3.8135 (3.8889)  Acc@1: 16.6667 (19.8171)  Acc@5: 79.1667 (78.4553)  Acc@task: 75.0000 (78.2520)  time: 0.1788  data: 0.0004  max mem: 1376\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 3.8135 (3.8984)  Acc@1: 20.8333 (19.9000)  Acc@5: 79.1667 (78.4000)  Acc@task: 75.0000 (78.3000)  time: 0.1754  data: 0.0004  max mem: 1376\n",
            "Test: [Task 6] Total time: 0:00:08 (0.1953 s / it)\n",
            "* Acc@task 78.300 Acc@1 19.900 Acc@5 78.400 loss 3.898\n",
            "[Average accuracy till task6]\tAcc@task: 81.3000\tAcc@1: 75.2500\tAcc@5: 94.3667\tLoss: 1.0630\tForgetting: 0.5600\tBackward: 45.2400\n",
            "torch.Size([72000, 384])\n",
            "Averaged stats: Lr: 0.005000  Loss: 0.1125  Acc@1: 96.6667 (95.4167)  Acc@5: 100.0000 (99.7333)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:20  Loss: 0.3273 (0.3273)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.4900  data: 0.3215  max mem: 1376\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.4836 (0.4518)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (98.8636)  Acc@task: 83.3333 (82.5758)  time: 0.2075  data: 0.0316  max mem: 1376\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.3927 (0.4779)  Acc@1: 87.5000 (88.0952)  Acc@5: 100.0000 (97.6190)  Acc@task: 83.3333 (82.5397)  time: 0.1783  data: 0.0020  max mem: 1376\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.3164 (0.4470)  Acc@1: 91.6667 (88.1720)  Acc@5: 100.0000 (97.8495)  Acc@task: 83.3333 (83.0645)  time: 0.1779  data: 0.0013  max mem: 1376\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.2514 (0.4112)  Acc@1: 91.6667 (89.2276)  Acc@5: 100.0000 (98.2724)  Acc@task: 87.5000 (83.7398)  time: 0.1786  data: 0.0007  max mem: 1376\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.2014 (0.4042)  Acc@1: 91.6667 (89.3000)  Acc@5: 100.0000 (98.3000)  Acc@task: 87.5000 (84.0000)  time: 0.1753  data: 0.0007  max mem: 1376\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1870 s / it)\n",
            "* Acc@task 84.000 Acc@1 89.300 Acc@5 98.300 loss 0.404\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:23  Loss: 0.8286 (0.8286)  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  Acc@task: 83.3333 (83.3333)  time: 0.5587  data: 0.3668  max mem: 1376\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.4329 (0.5873)  Acc@1: 83.3333 (83.7121)  Acc@5: 100.0000 (97.7273)  Acc@task: 83.3333 (81.0606)  time: 0.2122  data: 0.0339  max mem: 1376\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.5904 (0.6340)  Acc@1: 83.3333 (83.1349)  Acc@5: 100.0000 (97.0238)  Acc@task: 79.1667 (79.5635)  time: 0.1777  data: 0.0005  max mem: 1376\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.6972 (0.6408)  Acc@1: 83.3333 (83.4677)  Acc@5: 95.8333 (97.0430)  Acc@task: 79.1667 (80.3763)  time: 0.1786  data: 0.0005  max mem: 1376\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.4575 (0.6028)  Acc@1: 87.5000 (83.9431)  Acc@5: 100.0000 (97.4594)  Acc@task: 79.1667 (80.2846)  time: 0.1791  data: 0.0004  max mem: 1376\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.4411 (0.5950)  Acc@1: 87.5000 (84.0000)  Acc@5: 100.0000 (97.5000)  Acc@task: 83.3333 (80.4000)  time: 0.1754  data: 0.0004  max mem: 1376\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1898 s / it)\n",
            "* Acc@task 80.400 Acc@1 84.000 Acc@5 97.500 loss 0.595\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:37  Loss: 0.1067 (0.1067)  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)  Acc@task: 87.5000 (87.5000)  time: 0.9046  data: 0.7004  max mem: 1376\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:07  Loss: 0.4852 (0.5949)  Acc@1: 83.3333 (83.7121)  Acc@5: 100.0000 (96.9697)  Acc@task: 83.3333 (82.1970)  time: 0.2440  data: 0.0648  max mem: 1376\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.4481 (0.5222)  Acc@1: 87.5000 (85.7143)  Acc@5: 95.8333 (97.0238)  Acc@task: 83.3333 (82.9365)  time: 0.1777  data: 0.0012  max mem: 1376\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.4232 (0.4950)  Acc@1: 87.5000 (85.6183)  Acc@5: 95.8333 (97.4462)  Acc@task: 83.3333 (82.7957)  time: 0.1784  data: 0.0008  max mem: 1376\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.3827 (0.5195)  Acc@1: 83.3333 (85.8740)  Acc@5: 95.8333 (97.1545)  Acc@task: 83.3333 (83.3333)  time: 0.1790  data: 0.0003  max mem: 1376\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.3827 (0.5222)  Acc@1: 83.3333 (85.7000)  Acc@5: 95.8333 (97.2000)  Acc@task: 83.3333 (83.2000)  time: 0.1755  data: 0.0003  max mem: 1376\n",
            "Test: [Task 3] Total time: 0:00:08 (0.1969 s / it)\n",
            "* Acc@task 83.200 Acc@1 85.700 Acc@5 97.200 loss 0.522\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:20  Loss: 1.0791 (1.0791)  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.8333)  Acc@task: 70.8333 (70.8333)  time: 0.4869  data: 0.3144  max mem: 1376\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:06  Loss: 0.5685 (0.5843)  Acc@1: 87.5000 (84.8485)  Acc@5: 95.8333 (96.9697)  Acc@task: 83.3333 (82.1970)  time: 0.2074  data: 0.0298  max mem: 1376\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.5323 (0.6007)  Acc@1: 87.5000 (84.7222)  Acc@5: 95.8333 (96.4286)  Acc@task: 83.3333 (82.5397)  time: 0.1783  data: 0.0009  max mem: 1376\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.4021 (0.5330)  Acc@1: 87.5000 (86.0215)  Acc@5: 100.0000 (97.0430)  Acc@task: 83.3333 (83.4677)  time: 0.1779  data: 0.0005  max mem: 1376\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.3744 (0.5667)  Acc@1: 87.5000 (85.6707)  Acc@5: 100.0000 (96.7480)  Acc@task: 83.3333 (82.6220)  time: 0.1795  data: 0.0006  max mem: 1376\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.3744 (0.5539)  Acc@1: 87.5000 (85.9000)  Acc@5: 100.0000 (96.8000)  Acc@task: 83.3333 (82.8000)  time: 0.1760  data: 0.0006  max mem: 1376\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1883 s / it)\n",
            "* Acc@task 82.800 Acc@1 85.900 Acc@5 96.800 loss 0.554\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:33  Loss: 0.1406 (0.1406)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  Acc@task: 75.0000 (75.0000)  time: 0.8075  data: 0.6081  max mem: 1376\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:07  Loss: 0.3570 (0.3933)  Acc@1: 87.5000 (87.8788)  Acc@5: 100.0000 (98.8636)  Acc@task: 79.1667 (79.5455)  time: 0.2359  data: 0.0556  max mem: 1376\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.4269 (0.4450)  Acc@1: 83.3333 (85.5159)  Acc@5: 100.0000 (98.6111)  Acc@task: 79.1667 (79.7619)  time: 0.1782  data: 0.0004  max mem: 1376\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.3926 (0.4242)  Acc@1: 87.5000 (86.6936)  Acc@5: 100.0000 (98.9247)  Acc@task: 83.3333 (80.2419)  time: 0.1783  data: 0.0004  max mem: 1376\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.4072 (0.4514)  Acc@1: 87.5000 (86.6870)  Acc@5: 100.0000 (98.4756)  Acc@task: 79.1667 (79.1667)  time: 0.1790  data: 0.0003  max mem: 1376\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.4072 (0.4644)  Acc@1: 87.5000 (86.3000)  Acc@5: 100.0000 (98.4000)  Acc@task: 79.1667 (79.1000)  time: 0.1753  data: 0.0003  max mem: 1376\n",
            "Test: [Task 5] Total time: 0:00:08 (0.1946 s / it)\n",
            "* Acc@task 79.100 Acc@1 86.300 Acc@5 98.400 loss 0.464\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:20  Loss: 0.2174 (0.2174)  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)  Acc@task: 87.5000 (87.5000)  time: 0.4796  data: 0.3090  max mem: 1376\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:06  Loss: 0.7309 (0.7000)  Acc@1: 87.5000 (83.3333)  Acc@5: 95.8333 (96.9697)  Acc@task: 79.1667 (79.5455)  time: 0.2060  data: 0.0284  max mem: 1376\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:04  Loss: 0.6874 (0.7341)  Acc@1: 79.1667 (80.7540)  Acc@5: 95.8333 (96.8254)  Acc@task: 79.1667 (78.9683)  time: 0.1778  data: 0.0004  max mem: 1376\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 0.7806 (0.7995)  Acc@1: 75.0000 (78.7634)  Acc@5: 95.8333 (97.0430)  Acc@task: 79.1667 (78.3602)  time: 0.1780  data: 0.0010  max mem: 1376\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 0.8882 (0.8223)  Acc@1: 75.0000 (78.0488)  Acc@5: 95.8333 (96.4431)  Acc@task: 75.0000 (78.2520)  time: 0.1787  data: 0.0009  max mem: 1376\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 0.8134 (0.8193)  Acc@1: 75.0000 (78.0000)  Acc@5: 95.8333 (96.5000)  Acc@task: 75.0000 (78.3000)  time: 0.1752  data: 0.0009  max mem: 1376\n",
            "Test: [Task 6] Total time: 0:00:07 (0.1865 s / it)\n",
            "* Acc@task 78.300 Acc@1 78.000 Acc@5 96.500 loss 0.819\n",
            "[Average accuracy till task6]\tAcc@task: 81.3000\tAcc@1: 84.8667\tAcc@5: 97.4500\tLoss: 0.5598\tForgetting: 1.0800\tBackward: 3.8000\n",
            "Loading checkpoint from: ./output/cifar100_full_dino_1epoch_100pct/checkpoint/task7_checkpoint.pth\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/engines/hide_promtp_wtp_and_tap_engine.py:540: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  loss = torch.nn.functional.cross_entropy(sim, torch.range(0, sim.shape[0] - 1).long().to(device))\n",
            "Train: Epoch[1/1]  [  0/209]  eta: 0:02:57  Lr: 0.002812  Loss: 4.3581  Acc@1: 8.3333 (8.3333)  Acc@5: 50.0000 (50.0000)  time: 0.8494  data: 0.5759  max mem: 1376\n",
            "Train: Epoch[1/1]  [ 10/209]  eta: 0:01:05  Lr: 0.002812  Loss: 4.0436  Acc@1: 12.5000 (15.1515)  Acc@5: 54.1667 (57.1970)  time: 0.3297  data: 0.0527  max mem: 1378\n",
            "Train: Epoch[1/1]  [ 20/209]  eta: 0:00:57  Lr: 0.002812  Loss: 3.2377  Acc@1: 16.6667 (16.6667)  Acc@5: 54.1667 (60.1190)  time: 0.2775  data: 0.0004  max mem: 1378\n",
            "Train: Epoch[1/1]  [ 30/209]  eta: 0:00:53  Lr: 0.002812  Loss: 2.4970  Acc@1: 25.0000 (24.1935)  Acc@5: 75.0000 (66.6667)  time: 0.2785  data: 0.0004  max mem: 1378\n",
            "Train: Epoch[1/1]  [ 40/209]  eta: 0:00:49  Lr: 0.002812  Loss: 2.0598  Acc@1: 45.8333 (30.1829)  Acc@5: 83.3333 (71.8496)  time: 0.2800  data: 0.0007  max mem: 1378\n",
            "Train: Epoch[1/1]  [ 50/209]  eta: 0:00:46  Lr: 0.002812  Loss: 2.2027  Acc@1: 50.0000 (35.0490)  Acc@5: 87.5000 (75.4902)  time: 0.2801  data: 0.0007  max mem: 1378\n",
            "Train: Epoch[1/1]  [ 60/209]  eta: 0:00:43  Lr: 0.002812  Loss: 2.0837  Acc@1: 58.3333 (39.8224)  Acc@5: 91.6667 (78.3470)  time: 0.2802  data: 0.0004  max mem: 1378\n",
            "Train: Epoch[1/1]  [ 70/209]  eta: 0:00:39  Lr: 0.002812  Loss: 1.7574  Acc@1: 66.6667 (43.2512)  Acc@5: 91.6667 (80.6925)  time: 0.2801  data: 0.0004  max mem: 1378\n",
            "Train: Epoch[1/1]  [ 80/209]  eta: 0:00:36  Lr: 0.002812  Loss: 1.9959  Acc@1: 66.6667 (46.1934)  Acc@5: 95.8333 (82.5103)  time: 0.2800  data: 0.0003  max mem: 1378\n",
            "Train: Epoch[1/1]  [ 90/209]  eta: 0:00:34  Lr: 0.002812  Loss: 1.9856  Acc@1: 70.8333 (49.4048)  Acc@5: 95.8333 (83.9286)  time: 0.2804  data: 0.0006  max mem: 1378\n",
            "Train: Epoch[1/1]  [100/209]  eta: 0:00:31  Lr: 0.002812  Loss: 1.6516  Acc@1: 79.1667 (52.3102)  Acc@5: 95.8333 (85.2310)  time: 0.2799  data: 0.0008  max mem: 1378\n",
            "Train: Epoch[1/1]  [110/209]  eta: 0:00:28  Lr: 0.002812  Loss: 1.4266  Acc@1: 79.1667 (54.5420)  Acc@5: 95.8333 (86.2237)  time: 0.2795  data: 0.0005  max mem: 1378\n",
            "Train: Epoch[1/1]  [120/209]  eta: 0:00:25  Lr: 0.002812  Loss: 1.7839  Acc@1: 75.0000 (56.1295)  Acc@5: 95.8333 (87.0523)  time: 0.2799  data: 0.0004  max mem: 1378\n",
            "Train: Epoch[1/1]  [130/209]  eta: 0:00:22  Lr: 0.002812  Loss: 1.6228  Acc@1: 70.8333 (57.3155)  Acc@5: 95.8333 (87.9135)  time: 0.2796  data: 0.0005  max mem: 1378\n",
            "Train: Epoch[1/1]  [140/209]  eta: 0:00:19  Lr: 0.002812  Loss: 1.1057  Acc@1: 75.0000 (59.1017)  Acc@5: 100.0000 (88.5934)  time: 0.2794  data: 0.0008  max mem: 1378\n",
            "Train: Epoch[1/1]  [150/209]  eta: 0:00:16  Lr: 0.002812  Loss: 1.5623  Acc@1: 79.1667 (60.2373)  Acc@5: 100.0000 (89.2384)  time: 0.2802  data: 0.0007  max mem: 1378\n",
            "Train: Epoch[1/1]  [160/209]  eta: 0:00:13  Lr: 0.002812  Loss: 1.3587  Acc@1: 79.1667 (61.5683)  Acc@5: 100.0000 (89.8551)  time: 0.2802  data: 0.0004  max mem: 1378\n",
            "Train: Epoch[1/1]  [170/209]  eta: 0:00:11  Lr: 0.002812  Loss: 1.9372  Acc@1: 79.1667 (62.6949)  Acc@5: 100.0000 (90.2534)  time: 0.2802  data: 0.0004  max mem: 1378\n",
            "Train: Epoch[1/1]  [180/209]  eta: 0:00:08  Lr: 0.002812  Loss: 1.4947  Acc@1: 83.3333 (63.7661)  Acc@5: 95.8333 (90.7228)  time: 0.2798  data: 0.0004  max mem: 1378\n",
            "Train: Epoch[1/1]  [190/209]  eta: 0:00:05  Lr: 0.002812  Loss: 1.4455  Acc@1: 83.3333 (64.6815)  Acc@5: 100.0000 (91.0558)  time: 0.2794  data: 0.0008  max mem: 1378\n",
            "Train: Epoch[1/1]  [200/209]  eta: 0:00:02  Lr: 0.002812  Loss: 1.3739  Acc@1: 83.3333 (65.5680)  Acc@5: 95.8333 (91.3972)  time: 0.2796  data: 0.0009  max mem: 1378\n",
            "Train: Epoch[1/1]  [208/209]  eta: 0:00:00  Lr: 0.002812  Loss: 1.8169  Acc@1: 83.3333 (66.2400)  Acc@5: 100.0000 (91.6000)  time: 0.2711  data: 0.0006  max mem: 1378\n",
            "Train: Epoch[1/1] Total time: 0:00:58 (0.2821 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.8169  Acc@1: 83.3333 (66.2400)  Acc@5: 100.0000 (91.6000)\n",
            "torch.Size([5, 2, 5, 6, 64])\n",
            "torch.Size([5, 2, 1, 5, 6, 64])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:23  Loss: 0.3300 (0.3300)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  Acc@task: 79.1667 (79.1667)  time: 0.5576  data: 0.3823  max mem: 1378\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.4827 (0.4666)  Acc@1: 87.5000 (86.7424)  Acc@5: 100.0000 (98.8636)  Acc@task: 79.1667 (80.6818)  time: 0.2120  data: 0.0356  max mem: 1378\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.3895 (0.4815)  Acc@1: 87.5000 (87.8968)  Acc@5: 100.0000 (97.6190)  Acc@task: 83.3333 (81.7460)  time: 0.1770  data: 0.0007  max mem: 1378\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.3163 (0.4500)  Acc@1: 91.6667 (88.0376)  Acc@5: 100.0000 (97.8495)  Acc@task: 83.3333 (82.1237)  time: 0.1772  data: 0.0004  max mem: 1378\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.2499 (0.4164)  Acc@1: 91.6667 (89.0244)  Acc@5: 100.0000 (98.2724)  Acc@task: 87.5000 (82.8252)  time: 0.1784  data: 0.0003  max mem: 1378\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.2014 (0.4093)  Acc@1: 91.6667 (89.1000)  Acc@5: 100.0000 (98.3000)  Acc@task: 87.5000 (83.1000)  time: 0.1751  data: 0.0003  max mem: 1378\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1879 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@task 83.100 Acc@1 89.100 Acc@5 98.300 loss 0.409\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:21  Loss: 0.8292 (0.8292)  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  Acc@task: 83.3333 (83.3333)  time: 0.5003  data: 0.3330  max mem: 1378\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.5204 (0.6271)  Acc@1: 83.3333 (82.1970)  Acc@5: 100.0000 (97.7273)  Acc@task: 79.1667 (78.7879)  time: 0.2104  data: 0.0349  max mem: 1378\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.5653 (0.6640)  Acc@1: 83.3333 (82.1429)  Acc@5: 100.0000 (96.8254)  Acc@task: 79.1667 (78.5714)  time: 0.1799  data: 0.0034  max mem: 1378\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.6856 (0.6613)  Acc@1: 83.3333 (82.6613)  Acc@5: 95.8333 (96.7742)  Acc@task: 79.1667 (79.0323)  time: 0.1789  data: 0.0012  max mem: 1378\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.5351 (0.6216)  Acc@1: 87.5000 (83.5366)  Acc@5: 100.0000 (97.1545)  Acc@task: 79.1667 (79.2683)  time: 0.1800  data: 0.0004  max mem: 1378\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.4822 (0.6135)  Acc@1: 87.5000 (83.6000)  Acc@5: 100.0000 (97.2000)  Acc@task: 79.1667 (79.3000)  time: 0.1766  data: 0.0004  max mem: 1378\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1885 s / it)\n",
            "* Acc@task 79.300 Acc@1 83.600 Acc@5 97.200 loss 0.614\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:26  Loss: 0.1085 (0.1085)  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)  Acc@task: 87.5000 (87.5000)  time: 0.6372  data: 0.4566  max mem: 1378\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:07  Loss: 0.4858 (0.5773)  Acc@1: 87.5000 (84.0909)  Acc@5: 100.0000 (96.9697)  Acc@task: 83.3333 (81.8182)  time: 0.2198  data: 0.0420  max mem: 1378\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.4582 (0.5095)  Acc@1: 87.5000 (85.9127)  Acc@5: 95.8333 (97.2222)  Acc@task: 83.3333 (82.9365)  time: 0.1781  data: 0.0004  max mem: 1378\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.4146 (0.4842)  Acc@1: 87.5000 (85.8871)  Acc@5: 100.0000 (97.5806)  Acc@task: 83.3333 (82.5269)  time: 0.1790  data: 0.0003  max mem: 1378\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.3882 (0.5116)  Acc@1: 83.3333 (86.0772)  Acc@5: 95.8333 (97.2561)  Acc@task: 83.3333 (82.5203)  time: 0.1801  data: 0.0003  max mem: 1378\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.3882 (0.5164)  Acc@1: 83.3333 (85.8000)  Acc@5: 95.8333 (97.3000)  Acc@task: 83.3333 (82.3000)  time: 0.1768  data: 0.0003  max mem: 1378\n",
            "Test: [Task 3] Total time: 0:00:08 (0.1925 s / it)\n",
            "* Acc@task 82.300 Acc@1 85.800 Acc@5 97.300 loss 0.516\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:34  Loss: 1.0680 (1.0680)  Acc@1: 70.8333 (70.8333)  Acc@5: 95.8333 (95.8333)  Acc@task: 66.6667 (66.6667)  time: 0.8209  data: 0.6295  max mem: 1378\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:07  Loss: 0.5922 (0.5912)  Acc@1: 87.5000 (84.8485)  Acc@5: 95.8333 (96.9697)  Acc@task: 79.1667 (78.7879)  time: 0.2378  data: 0.0582  max mem: 1378\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.5319 (0.6007)  Acc@1: 87.5000 (84.7222)  Acc@5: 95.8333 (96.4286)  Acc@task: 79.1667 (79.9603)  time: 0.1790  data: 0.0008  max mem: 1378\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.4313 (0.5450)  Acc@1: 87.5000 (85.8871)  Acc@5: 100.0000 (96.9086)  Acc@task: 79.1667 (81.1828)  time: 0.1793  data: 0.0005  max mem: 1378\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.4313 (0.5765)  Acc@1: 87.5000 (85.6707)  Acc@5: 95.8333 (96.5447)  Acc@task: 79.1667 (80.5894)  time: 0.1801  data: 0.0003  max mem: 1378\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.3993 (0.5635)  Acc@1: 87.5000 (85.9000)  Acc@5: 100.0000 (96.6000)  Acc@task: 79.1667 (80.7000)  time: 0.1769  data: 0.0003  max mem: 1378\n",
            "Test: [Task 4] Total time: 0:00:08 (0.1960 s / it)\n",
            "* Acc@task 80.700 Acc@1 85.900 Acc@5 96.600 loss 0.563\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:24  Loss: 0.1353 (0.1353)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  Acc@task: 75.0000 (75.0000)  time: 0.5788  data: 0.3927  max mem: 1378\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 0.3358 (0.3778)  Acc@1: 87.5000 (87.8788)  Acc@5: 100.0000 (99.2424)  Acc@task: 79.1667 (79.9242)  time: 0.2156  data: 0.0366  max mem: 1378\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.4261 (0.4506)  Acc@1: 83.3333 (85.3175)  Acc@5: 100.0000 (98.6111)  Acc@task: 79.1667 (78.5714)  time: 0.1785  data: 0.0007  max mem: 1378\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.3931 (0.4250)  Acc@1: 87.5000 (86.5591)  Acc@5: 100.0000 (98.9247)  Acc@task: 79.1667 (79.8387)  time: 0.1791  data: 0.0007  max mem: 1378\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.4079 (0.4534)  Acc@1: 87.5000 (86.5854)  Acc@5: 100.0000 (98.4756)  Acc@task: 79.1667 (78.8618)  time: 0.1805  data: 0.0008  max mem: 1378\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.4079 (0.4663)  Acc@1: 87.5000 (86.2000)  Acc@5: 100.0000 (98.4000)  Acc@task: 75.0000 (78.8000)  time: 0.1766  data: 0.0008  max mem: 1378\n",
            "Test: [Task 5] Total time: 0:00:08 (0.1910 s / it)\n",
            "* Acc@task 78.800 Acc@1 86.200 Acc@5 98.400 loss 0.466\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:33  Loss: 0.2192 (0.2192)  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)  Acc@task: 87.5000 (87.5000)  time: 0.8040  data: 0.6362  max mem: 1378\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:07  Loss: 0.7536 (0.6676)  Acc@1: 83.3333 (84.0909)  Acc@5: 95.8333 (97.3485)  Acc@task: 79.1667 (81.0606)  time: 0.2354  data: 0.0584  max mem: 1378\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:04  Loss: 0.7229 (0.6874)  Acc@1: 83.3333 (82.3413)  Acc@5: 95.8333 (97.0238)  Acc@task: 79.1667 (81.1508)  time: 0.1779  data: 0.0005  max mem: 1378\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 0.6542 (0.7196)  Acc@1: 79.1667 (80.7796)  Acc@5: 95.8333 (97.3118)  Acc@task: 79.1667 (80.9140)  time: 0.1779  data: 0.0004  max mem: 1378\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 0.7377 (0.7396)  Acc@1: 79.1667 (80.0813)  Acc@5: 95.8333 (96.6463)  Acc@task: 79.1667 (80.5894)  time: 0.1792  data: 0.0003  max mem: 1378\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 0.6305 (0.7370)  Acc@1: 79.1667 (79.9000)  Acc@5: 95.8333 (96.7000)  Acc@task: 79.1667 (80.6000)  time: 0.1755  data: 0.0003  max mem: 1378\n",
            "Test: [Task 6] Total time: 0:00:08 (0.1944 s / it)\n",
            "* Acc@task 80.600 Acc@1 79.900 Acc@5 96.700 loss 0.737\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:24  Loss: 3.5722 (3.5722)  Acc@1: 20.8333 (20.8333)  Acc@5: 75.0000 (75.0000)  Acc@task: 75.0000 (75.0000)  time: 0.5738  data: 0.3953  max mem: 1378\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:06  Loss: 3.8493 (3.8414)  Acc@1: 16.6667 (19.6970)  Acc@5: 66.6667 (67.4242)  Acc@task: 75.0000 (73.1061)  time: 0.2134  data: 0.0365  max mem: 1378\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:04  Loss: 3.9009 (3.8578)  Acc@1: 16.6667 (19.2460)  Acc@5: 66.6667 (66.8651)  Acc@task: 70.8333 (72.2222)  time: 0.1779  data: 0.0009  max mem: 1378\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:02  Loss: 3.9710 (3.9558)  Acc@1: 12.5000 (17.0699)  Acc@5: 62.5000 (67.3387)  Acc@task: 70.8333 (72.1774)  time: 0.1789  data: 0.0014  max mem: 1378\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 3.9859 (3.9325)  Acc@1: 12.5000 (16.5650)  Acc@5: 70.8333 (67.2764)  Acc@task: 70.8333 (72.6626)  time: 0.1790  data: 0.0009  max mem: 1378\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 3.8978 (3.8979)  Acc@1: 12.5000 (17.0000)  Acc@5: 70.8333 (67.4000)  Acc@task: 70.8333 (72.7000)  time: 0.1758  data: 0.0008  max mem: 1378\n",
            "Test: [Task 7] Total time: 0:00:07 (0.1891 s / it)\n",
            "* Acc@task 72.700 Acc@1 17.000 Acc@5 67.400 loss 3.898\n",
            "[Average accuracy till task7]\tAcc@task: 79.6429\tAcc@1: 75.3571\tAcc@5: 93.1286\tLoss: 1.0291\tForgetting: 0.9667\tBackward: 47.5333\n",
            "torch.Size([84000, 384])\n",
            "Averaged stats: Lr: 0.005000  Loss: 0.0491  Acc@1: 98.3333 (95.9167)  Acc@5: 100.0000 (99.5695)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:23  Loss: 0.3412 (0.3412)  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)  Acc@task: 79.1667 (79.1667)  time: 0.5614  data: 0.3700  max mem: 1378\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.4040 (0.4787)  Acc@1: 83.3333 (86.3636)  Acc@5: 100.0000 (98.8636)  Acc@task: 79.1667 (80.6818)  time: 0.2120  data: 0.0343  max mem: 1378\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.4040 (0.5014)  Acc@1: 83.3333 (87.6984)  Acc@5: 100.0000 (97.8175)  Acc@task: 83.3333 (81.7460)  time: 0.1767  data: 0.0006  max mem: 1378\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.3491 (0.4664)  Acc@1: 87.5000 (87.9032)  Acc@5: 100.0000 (97.9839)  Acc@task: 83.3333 (82.1237)  time: 0.1772  data: 0.0004  max mem: 1378\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.2707 (0.4344)  Acc@1: 91.6667 (88.6179)  Acc@5: 100.0000 (98.3740)  Acc@task: 87.5000 (82.8252)  time: 0.1786  data: 0.0003  max mem: 1378\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.2208 (0.4269)  Acc@1: 91.6667 (88.7000)  Acc@5: 100.0000 (98.4000)  Acc@task: 87.5000 (83.1000)  time: 0.1751  data: 0.0003  max mem: 1378\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1892 s / it)\n",
            "* Acc@task 83.100 Acc@1 88.700 Acc@5 98.400 loss 0.427\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:37  Loss: 0.8255 (0.8255)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (83.3333)  time: 0.8918  data: 0.6787  max mem: 1378\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:07  Loss: 0.4966 (0.6154)  Acc@1: 79.1667 (82.9545)  Acc@5: 100.0000 (98.4848)  Acc@task: 79.1667 (78.7879)  time: 0.2438  data: 0.0627  max mem: 1378\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.5105 (0.6270)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (97.0238)  Acc@task: 79.1667 (78.5714)  time: 0.1784  data: 0.0008  max mem: 1378\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.5901 (0.6181)  Acc@1: 83.3333 (83.8710)  Acc@5: 95.8333 (97.1774)  Acc@task: 79.1667 (79.0323)  time: 0.1783  data: 0.0004  max mem: 1378\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.5254 (0.5701)  Acc@1: 87.5000 (85.0610)  Acc@5: 100.0000 (97.4594)  Acc@task: 79.1667 (79.2683)  time: 0.1786  data: 0.0003  max mem: 1378\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.4179 (0.5665)  Acc@1: 87.5000 (85.1000)  Acc@5: 100.0000 (97.5000)  Acc@task: 79.1667 (79.3000)  time: 0.1755  data: 0.0003  max mem: 1378\n",
            "Test: [Task 2] Total time: 0:00:08 (0.1965 s / it)\n",
            "* Acc@task 79.300 Acc@1 85.100 Acc@5 97.500 loss 0.566\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:24  Loss: 0.0792 (0.0792)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  Acc@task: 87.5000 (87.5000)  time: 0.5890  data: 0.4192  max mem: 1378\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 0.6819 (0.6682)  Acc@1: 83.3333 (83.7121)  Acc@5: 100.0000 (97.3485)  Acc@task: 83.3333 (81.8182)  time: 0.2153  data: 0.0386  max mem: 1378\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.5052 (0.5884)  Acc@1: 83.3333 (85.5159)  Acc@5: 95.8333 (97.4206)  Acc@task: 83.3333 (82.9365)  time: 0.1777  data: 0.0005  max mem: 1378\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.4940 (0.5560)  Acc@1: 83.3333 (85.6183)  Acc@5: 95.8333 (97.1774)  Acc@task: 83.3333 (82.5269)  time: 0.1782  data: 0.0008  max mem: 1378\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.5195 (0.5734)  Acc@1: 83.3333 (85.6707)  Acc@5: 95.8333 (97.1545)  Acc@task: 83.3333 (82.5203)  time: 0.1789  data: 0.0008  max mem: 1378\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.5195 (0.5743)  Acc@1: 83.3333 (85.3000)  Acc@5: 95.8333 (97.1000)  Acc@task: 83.3333 (82.3000)  time: 0.1753  data: 0.0007  max mem: 1378\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1901 s / it)\n",
            "* Acc@task 82.300 Acc@1 85.300 Acc@5 97.100 loss 0.574\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:33  Loss: 1.0766 (1.0766)  Acc@1: 70.8333 (70.8333)  Acc@5: 91.6667 (91.6667)  Acc@task: 66.6667 (66.6667)  time: 0.7965  data: 0.6267  max mem: 1378\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:07  Loss: 0.5480 (0.5308)  Acc@1: 87.5000 (86.3636)  Acc@5: 95.8333 (97.3485)  Acc@task: 79.1667 (78.7879)  time: 0.2348  data: 0.0576  max mem: 1378\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.5260 (0.5878)  Acc@1: 87.5000 (84.7222)  Acc@5: 95.8333 (96.2302)  Acc@task: 79.1667 (79.9603)  time: 0.1773  data: 0.0005  max mem: 1378\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.4610 (0.5413)  Acc@1: 87.5000 (85.6183)  Acc@5: 95.8333 (96.7742)  Acc@task: 79.1667 (81.1828)  time: 0.1773  data: 0.0004  max mem: 1378\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.4324 (0.5662)  Acc@1: 87.5000 (85.6707)  Acc@5: 100.0000 (96.6463)  Acc@task: 79.1667 (80.5894)  time: 0.1790  data: 0.0004  max mem: 1378\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.4058 (0.5531)  Acc@1: 87.5000 (85.9000)  Acc@5: 100.0000 (96.7000)  Acc@task: 79.1667 (80.7000)  time: 0.1752  data: 0.0004  max mem: 1378\n",
            "Test: [Task 4] Total time: 0:00:08 (0.1941 s / it)\n",
            "* Acc@task 80.700 Acc@1 85.900 Acc@5 96.700 loss 0.553\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:20  Loss: 0.0912 (0.0912)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  Acc@task: 75.0000 (75.0000)  time: 0.4911  data: 0.3255  max mem: 1378\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 0.4021 (0.3976)  Acc@1: 83.3333 (86.7424)  Acc@5: 100.0000 (98.8636)  Acc@task: 79.1667 (79.9242)  time: 0.2062  data: 0.0304  max mem: 1378\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.4442 (0.4753)  Acc@1: 83.3333 (85.3175)  Acc@5: 100.0000 (98.0159)  Acc@task: 79.1667 (78.5714)  time: 0.1777  data: 0.0008  max mem: 1378\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.4229 (0.4397)  Acc@1: 83.3333 (86.1559)  Acc@5: 100.0000 (98.6559)  Acc@task: 79.1667 (79.8387)  time: 0.1782  data: 0.0009  max mem: 1378\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.4009 (0.4667)  Acc@1: 87.5000 (86.6870)  Acc@5: 100.0000 (98.2724)  Acc@task: 79.1667 (78.8618)  time: 0.1794  data: 0.0008  max mem: 1378\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.4160 (0.4772)  Acc@1: 87.5000 (86.4000)  Acc@5: 100.0000 (98.2000)  Acc@task: 75.0000 (78.8000)  time: 0.1761  data: 0.0008  max mem: 1378\n",
            "Test: [Task 5] Total time: 0:00:07 (0.1871 s / it)\n",
            "* Acc@task 78.800 Acc@1 86.400 Acc@5 98.200 loss 0.477\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:23  Loss: 0.5113 (0.5113)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  Acc@task: 87.5000 (87.5000)  time: 0.5497  data: 0.3712  max mem: 1378\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:06  Loss: 0.8481 (0.7991)  Acc@1: 83.3333 (82.1970)  Acc@5: 95.8333 (96.5909)  Acc@task: 79.1667 (81.0606)  time: 0.2119  data: 0.0343  max mem: 1378\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:04  Loss: 0.8182 (0.8029)  Acc@1: 83.3333 (81.1508)  Acc@5: 95.8333 (96.8254)  Acc@task: 79.1667 (81.1508)  time: 0.1778  data: 0.0005  max mem: 1378\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 0.8161 (0.8145)  Acc@1: 79.1667 (79.9731)  Acc@5: 95.8333 (97.0430)  Acc@task: 79.1667 (80.9140)  time: 0.1784  data: 0.0004  max mem: 1378\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 0.8528 (0.8220)  Acc@1: 79.1667 (79.2683)  Acc@5: 95.8333 (96.6463)  Acc@task: 79.1667 (80.5894)  time: 0.1793  data: 0.0004  max mem: 1378\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 0.7082 (0.8193)  Acc@1: 79.1667 (79.3000)  Acc@5: 100.0000 (96.7000)  Acc@task: 79.1667 (80.6000)  time: 0.1758  data: 0.0004  max mem: 1378\n",
            "Test: [Task 6] Total time: 0:00:07 (0.1886 s / it)\n",
            "* Acc@task 80.600 Acc@1 79.300 Acc@5 96.700 loss 0.819\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:24  Loss: 0.7403 (0.7403)  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  Acc@task: 75.0000 (75.0000)  time: 0.5943  data: 0.4286  max mem: 1378\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:06  Loss: 0.6820 (0.6321)  Acc@1: 83.3333 (82.1970)  Acc@5: 95.8333 (96.9697)  Acc@task: 75.0000 (73.1061)  time: 0.2156  data: 0.0404  max mem: 1378\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:04  Loss: 0.7030 (0.7146)  Acc@1: 79.1667 (80.3571)  Acc@5: 95.8333 (96.4286)  Acc@task: 70.8333 (72.2222)  time: 0.1775  data: 0.0016  max mem: 1378\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:02  Loss: 0.7030 (0.7485)  Acc@1: 79.1667 (79.8387)  Acc@5: 95.8333 (95.8333)  Acc@task: 70.8333 (72.1774)  time: 0.1779  data: 0.0012  max mem: 1378\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 0.6887 (0.7494)  Acc@1: 79.1667 (79.6748)  Acc@5: 95.8333 (96.1382)  Acc@task: 70.8333 (72.6626)  time: 0.1793  data: 0.0006  max mem: 1378\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 0.6887 (0.7417)  Acc@1: 79.1667 (79.9000)  Acc@5: 95.8333 (96.2000)  Acc@task: 70.8333 (72.7000)  time: 0.1757  data: 0.0006  max mem: 1378\n",
            "Test: [Task 7] Total time: 0:00:07 (0.1895 s / it)\n",
            "* Acc@task 72.700 Acc@1 79.900 Acc@5 96.200 loss 0.742\n",
            "[Average accuracy till task7]\tAcc@task: 79.6429\tAcc@1: 84.3714\tAcc@5: 97.2571\tLoss: 0.5941\tForgetting: 0.8833\tBackward: 3.4167\n",
            "Loading checkpoint from: ./output/cifar100_full_dino_1epoch_100pct/checkpoint/task8_checkpoint.pth\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/engines/hide_promtp_wtp_and_tap_engine.py:540: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  loss = torch.nn.functional.cross_entropy(sim, torch.range(0, sim.shape[0] - 1).long().to(device))\n",
            "Train: Epoch[1/1]  [  0/209]  eta: 0:02:26  Lr: 0.002812  Loss: 5.4044  Acc@1: 4.1667 (4.1667)  Acc@5: 25.0000 (25.0000)  time: 0.7021  data: 0.3934  max mem: 1378\n",
            "Train: Epoch[1/1]  [ 10/209]  eta: 0:01:03  Lr: 0.002812  Loss: 3.7872  Acc@1: 8.3333 (8.3333)  Acc@5: 54.1667 (50.3788)  time: 0.3191  data: 0.0364  max mem: 1378\n",
            "Train: Epoch[1/1]  [ 20/209]  eta: 0:00:56  Lr: 0.002812  Loss: 3.9313  Acc@1: 12.5000 (13.4921)  Acc@5: 54.1667 (56.7460)  time: 0.2804  data: 0.0006  max mem: 1378\n",
            "Train: Epoch[1/1]  [ 30/209]  eta: 0:00:52  Lr: 0.002812  Loss: 2.9530  Acc@1: 20.8333 (18.0108)  Acc@5: 75.0000 (64.7849)  time: 0.2809  data: 0.0006  max mem: 1378\n",
            "Train: Epoch[1/1]  [ 40/209]  eta: 0:00:49  Lr: 0.002812  Loss: 2.1779  Acc@1: 33.3333 (25.3049)  Acc@5: 83.3333 (69.8171)  time: 0.2820  data: 0.0009  max mem: 1378\n",
            "Train: Epoch[1/1]  [ 50/209]  eta: 0:00:46  Lr: 0.002812  Loss: 2.5738  Acc@1: 50.0000 (31.2909)  Acc@5: 87.5000 (73.6928)  time: 0.2819  data: 0.0008  max mem: 1378\n",
            "Train: Epoch[1/1]  [ 60/209]  eta: 0:00:42  Lr: 0.002812  Loss: 2.3350  Acc@1: 54.1667 (35.9973)  Acc@5: 91.6667 (77.2541)  time: 0.2820  data: 0.0005  max mem: 1378\n",
            "Train: Epoch[1/1]  [ 70/209]  eta: 0:00:39  Lr: 0.002812  Loss: 1.6317  Acc@1: 62.5000 (40.4343)  Acc@5: 91.6667 (79.4014)  time: 0.2814  data: 0.0004  max mem: 1378\n",
            "Train: Epoch[1/1]  [ 80/209]  eta: 0:00:36  Lr: 0.002812  Loss: 1.9090  Acc@1: 70.8333 (44.3930)  Acc@5: 95.8333 (81.5329)  time: 0.2815  data: 0.0006  max mem: 1378\n",
            "Train: Epoch[1/1]  [ 90/209]  eta: 0:00:34  Lr: 0.002812  Loss: 1.8398  Acc@1: 70.8333 (47.6190)  Acc@5: 95.8333 (83.1502)  time: 0.2823  data: 0.0009  max mem: 1378\n",
            "Train: Epoch[1/1]  [100/209]  eta: 0:00:31  Lr: 0.002812  Loss: 1.4666  Acc@1: 75.0000 (50.4125)  Acc@5: 95.8333 (84.3647)  time: 0.2816  data: 0.0007  max mem: 1378\n",
            "Train: Epoch[1/1]  [110/209]  eta: 0:00:28  Lr: 0.002812  Loss: 1.5018  Acc@1: 75.0000 (52.7778)  Acc@5: 95.8333 (85.5856)  time: 0.2814  data: 0.0006  max mem: 1378\n",
            "Train: Epoch[1/1]  [120/209]  eta: 0:00:25  Lr: 0.002812  Loss: 1.6216  Acc@1: 79.1667 (54.6832)  Acc@5: 95.8333 (86.3292)  time: 0.2814  data: 0.0007  max mem: 1378\n",
            "Train: Epoch[1/1]  [130/209]  eta: 0:00:22  Lr: 0.002812  Loss: 1.7782  Acc@1: 79.1667 (56.3295)  Acc@5: 95.8333 (87.0865)  time: 0.2813  data: 0.0010  max mem: 1378\n",
            "Train: Epoch[1/1]  [140/209]  eta: 0:00:19  Lr: 0.002812  Loss: 1.9935  Acc@1: 79.1667 (57.6832)  Acc@5: 100.0000 (87.8546)  time: 0.2816  data: 0.0011  max mem: 1378\n",
            "Train: Epoch[1/1]  [150/209]  eta: 0:00:16  Lr: 0.002812  Loss: 1.7379  Acc@1: 75.0000 (59.1060)  Acc@5: 100.0000 (88.5762)  time: 0.2813  data: 0.0006  max mem: 1378\n",
            "Train: Epoch[1/1]  [160/209]  eta: 0:00:13  Lr: 0.002812  Loss: 1.4353  Acc@1: 79.1667 (60.4037)  Acc@5: 100.0000 (89.1563)  time: 0.2808  data: 0.0004  max mem: 1378\n",
            "Train: Epoch[1/1]  [170/209]  eta: 0:00:11  Lr: 0.002812  Loss: 1.3658  Acc@1: 79.1667 (61.6228)  Acc@5: 100.0000 (89.6930)  time: 0.2806  data: 0.0004  max mem: 1378\n",
            "Train: Epoch[1/1]  [180/209]  eta: 0:00:08  Lr: 0.002812  Loss: 1.8574  Acc@1: 83.3333 (62.6842)  Acc@5: 100.0000 (90.0783)  time: 0.2811  data: 0.0009  max mem: 1378\n",
            "Train: Epoch[1/1]  [190/209]  eta: 0:00:05  Lr: 0.002812  Loss: 1.6749  Acc@1: 83.3333 (63.8089)  Acc@5: 100.0000 (90.5541)  time: 0.2811  data: 0.0010  max mem: 1378\n",
            "Train: Epoch[1/1]  [200/209]  eta: 0:00:02  Lr: 0.002812  Loss: 1.3549  Acc@1: 83.3333 (64.5730)  Acc@5: 100.0000 (90.9204)  time: 0.2806  data: 0.0005  max mem: 1378\n",
            "Train: Epoch[1/1]  [208/209]  eta: 0:00:00  Lr: 0.002812  Loss: 1.1400  Acc@1: 79.1667 (65.1800)  Acc@5: 100.0000 (91.1800)  time: 0.2717  data: 0.0003  max mem: 1378\n",
            "Train: Epoch[1/1] Total time: 0:00:59 (0.2831 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.1400  Acc@1: 79.1667 (65.1800)  Acc@5: 100.0000 (91.1800)\n",
            "torch.Size([5, 2, 5, 6, 64])\n",
            "torch.Size([5, 2, 1, 5, 6, 64])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:29  Loss: 0.3410 (0.3410)  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.7069  data: 0.5289  max mem: 1378\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:07  Loss: 0.4041 (0.4732)  Acc@1: 87.5000 (87.1212)  Acc@5: 100.0000 (99.2424)  Acc@task: 79.1667 (81.0606)  time: 0.2252  data: 0.0486  max mem: 1378\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.4491 (0.5077)  Acc@1: 83.3333 (87.6984)  Acc@5: 100.0000 (97.8175)  Acc@task: 79.1667 (80.9524)  time: 0.1766  data: 0.0005  max mem: 1378\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.3607 (0.4680)  Acc@1: 91.6667 (87.9032)  Acc@5: 100.0000 (97.9839)  Acc@task: 79.1667 (81.7204)  time: 0.1773  data: 0.0004  max mem: 1378\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.2131 (0.4423)  Acc@1: 91.6667 (88.4146)  Acc@5: 100.0000 (98.2724)  Acc@task: 83.3333 (82.7236)  time: 0.1785  data: 0.0003  max mem: 1378\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.1924 (0.4346)  Acc@1: 91.6667 (88.5000)  Acc@5: 100.0000 (98.3000)  Acc@task: 83.3333 (83.0000)  time: 0.1750  data: 0.0003  max mem: 1378\n",
            "Test: [Task 1] Total time: 0:00:08 (0.1932 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@task 83.000 Acc@1 88.500 Acc@5 98.300 loss 0.435\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:37  Loss: 0.8421 (0.8421)  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  Acc@task: 75.0000 (75.0000)  time: 0.9014  data: 0.7141  max mem: 1378\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:07  Loss: 0.4970 (0.6454)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (97.7273)  Acc@task: 79.1667 (76.5152)  time: 0.2426  data: 0.0654  max mem: 1378\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.5406 (0.6699)  Acc@1: 83.3333 (83.1349)  Acc@5: 95.8333 (96.4286)  Acc@task: 75.0000 (75.3968)  time: 0.1768  data: 0.0004  max mem: 1378\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.6181 (0.6496)  Acc@1: 83.3333 (83.6022)  Acc@5: 95.8333 (96.7742)  Acc@task: 75.0000 (76.3441)  time: 0.1777  data: 0.0004  max mem: 1378\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.5256 (0.5974)  Acc@1: 87.5000 (84.8577)  Acc@5: 100.0000 (97.1545)  Acc@task: 75.0000 (76.8293)  time: 0.1790  data: 0.0003  max mem: 1378\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.5101 (0.5931)  Acc@1: 87.5000 (84.9000)  Acc@5: 100.0000 (97.2000)  Acc@task: 75.0000 (76.9000)  time: 0.1754  data: 0.0003  max mem: 1378\n",
            "Test: [Task 2] Total time: 0:00:08 (0.1967 s / it)\n",
            "* Acc@task 76.900 Acc@1 84.900 Acc@5 97.200 loss 0.593\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:22  Loss: 0.0808 (0.0808)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  Acc@task: 87.5000 (87.5000)  time: 0.5288  data: 0.3568  max mem: 1378\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 0.6770 (0.6836)  Acc@1: 83.3333 (83.7121)  Acc@5: 100.0000 (97.3485)  Acc@task: 83.3333 (80.6818)  time: 0.2112  data: 0.0331  max mem: 1378\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.5666 (0.6119)  Acc@1: 83.3333 (85.3175)  Acc@5: 95.8333 (97.2222)  Acc@task: 79.1667 (81.3492)  time: 0.1785  data: 0.0006  max mem: 1378\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.4931 (0.5795)  Acc@1: 83.3333 (85.3495)  Acc@5: 95.8333 (97.0430)  Acc@task: 79.1667 (80.7796)  time: 0.1785  data: 0.0008  max mem: 1378\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.5222 (0.5937)  Acc@1: 83.3333 (85.3659)  Acc@5: 95.8333 (96.9512)  Acc@task: 79.1667 (80.5894)  time: 0.1794  data: 0.0010  max mem: 1378\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.5222 (0.5942)  Acc@1: 83.3333 (85.0000)  Acc@5: 95.8333 (96.9000)  Acc@task: 79.1667 (80.4000)  time: 0.1759  data: 0.0010  max mem: 1378\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1883 s / it)\n",
            "* Acc@task 80.400 Acc@1 85.000 Acc@5 96.900 loss 0.594\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:19  Loss: 0.9153 (0.9153)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  Acc@task: 66.6667 (66.6667)  time: 0.4647  data: 0.2869  max mem: 1378\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:06  Loss: 0.5609 (0.5606)  Acc@1: 87.5000 (85.6061)  Acc@5: 95.8333 (97.3485)  Acc@task: 79.1667 (75.0000)  time: 0.2045  data: 0.0264  max mem: 1378\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.5387 (0.6068)  Acc@1: 83.3333 (84.3254)  Acc@5: 95.8333 (96.2302)  Acc@task: 79.1667 (76.5873)  time: 0.1785  data: 0.0004  max mem: 1378\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.4594 (0.5540)  Acc@1: 83.3333 (85.3495)  Acc@5: 95.8333 (96.7742)  Acc@task: 79.1667 (78.3602)  time: 0.1791  data: 0.0005  max mem: 1378\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.4364 (0.5614)  Acc@1: 87.5000 (85.4675)  Acc@5: 100.0000 (96.7480)  Acc@task: 79.1667 (78.2520)  time: 0.1797  data: 0.0004  max mem: 1378\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.3949 (0.5484)  Acc@1: 87.5000 (85.7000)  Acc@5: 100.0000 (96.8000)  Acc@task: 79.1667 (78.5000)  time: 0.1766  data: 0.0003  max mem: 1378\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1871 s / it)\n",
            "* Acc@task 78.500 Acc@1 85.700 Acc@5 96.800 loss 0.548\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:23  Loss: 0.0869 (0.0869)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.5615  data: 0.4006  max mem: 1378\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 0.3824 (0.3939)  Acc@1: 83.3333 (86.7424)  Acc@5: 100.0000 (99.2424)  Acc@task: 83.3333 (81.8182)  time: 0.2135  data: 0.0368  max mem: 1378\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.4578 (0.4602)  Acc@1: 83.3333 (85.3175)  Acc@5: 100.0000 (98.2143)  Acc@task: 83.3333 (81.3492)  time: 0.1780  data: 0.0009  max mem: 1378\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.4205 (0.4312)  Acc@1: 83.3333 (86.2903)  Acc@5: 100.0000 (98.7903)  Acc@task: 79.1667 (81.8548)  time: 0.1784  data: 0.0011  max mem: 1378\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.4088 (0.4602)  Acc@1: 87.5000 (86.6870)  Acc@5: 100.0000 (98.3740)  Acc@task: 79.1667 (80.6911)  time: 0.1790  data: 0.0005  max mem: 1378\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.4230 (0.4705)  Acc@1: 87.5000 (86.4000)  Acc@5: 100.0000 (98.3000)  Acc@task: 79.1667 (80.7000)  time: 0.1755  data: 0.0005  max mem: 1378\n",
            "Test: [Task 5] Total time: 0:00:07 (0.1887 s / it)\n",
            "* Acc@task 80.700 Acc@1 86.400 Acc@5 98.300 loss 0.471\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:19  Loss: 0.5113 (0.5113)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  Acc@task: 87.5000 (87.5000)  time: 0.4653  data: 0.2979  max mem: 1378\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:06  Loss: 0.7101 (0.6736)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (97.7273)  Acc@task: 79.1667 (81.0606)  time: 0.2057  data: 0.0296  max mem: 1378\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:04  Loss: 0.6735 (0.7087)  Acc@1: 83.3333 (82.5397)  Acc@5: 95.8333 (97.4206)  Acc@task: 79.1667 (80.1587)  time: 0.1790  data: 0.0016  max mem: 1378\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 0.6735 (0.7194)  Acc@1: 79.1667 (81.5860)  Acc@5: 95.8333 (97.5806)  Acc@task: 79.1667 (79.4355)  time: 0.1787  data: 0.0004  max mem: 1378\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 0.6810 (0.7278)  Acc@1: 79.1667 (80.8943)  Acc@5: 100.0000 (97.4594)  Acc@task: 79.1667 (79.0650)  time: 0.1791  data: 0.0003  max mem: 1378\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 0.6810 (0.7271)  Acc@1: 79.1667 (80.9000)  Acc@5: 100.0000 (97.5000)  Acc@task: 79.1667 (79.1000)  time: 0.1759  data: 0.0003  max mem: 1378\n",
            "Test: [Task 6] Total time: 0:00:07 (0.1869 s / it)\n",
            "* Acc@task 79.100 Acc@1 80.900 Acc@5 97.500 loss 0.727\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:30  Loss: 0.7375 (0.7375)  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  Acc@task: 79.1667 (79.1667)  time: 0.7314  data: 0.5271  max mem: 1378\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:07  Loss: 0.5386 (0.5725)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (96.9697)  Acc@task: 79.1667 (76.1364)  time: 0.2286  data: 0.0498  max mem: 1378\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:04  Loss: 0.6400 (0.6342)  Acc@1: 83.3333 (81.3492)  Acc@5: 100.0000 (96.6270)  Acc@task: 79.1667 (77.3810)  time: 0.1780  data: 0.0018  max mem: 1378\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:02  Loss: 0.6644 (0.6669)  Acc@1: 75.0000 (80.6452)  Acc@5: 95.8333 (96.5054)  Acc@task: 75.0000 (76.2097)  time: 0.1782  data: 0.0010  max mem: 1378\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 0.7037 (0.6740)  Acc@1: 75.0000 (80.2846)  Acc@5: 100.0000 (96.8496)  Acc@task: 70.8333 (75.7114)  time: 0.1787  data: 0.0004  max mem: 1378\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 0.7037 (0.6679)  Acc@1: 75.0000 (80.5000)  Acc@5: 100.0000 (96.9000)  Acc@task: 75.0000 (75.8000)  time: 0.1754  data: 0.0003  max mem: 1378\n",
            "Test: [Task 7] Total time: 0:00:08 (0.1927 s / it)\n",
            "* Acc@task 75.800 Acc@1 80.500 Acc@5 96.900 loss 0.668\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:20  Loss: 4.2271 (4.2271)  Acc@1: 16.6667 (16.6667)  Acc@5: 62.5000 (62.5000)  Acc@task: 70.8333 (70.8333)  time: 0.4910  data: 0.3202  max mem: 1378\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:06  Loss: 4.7096 (4.6166)  Acc@1: 12.5000 (10.9848)  Acc@5: 62.5000 (59.8485)  Acc@task: 70.8333 (70.8333)  time: 0.2061  data: 0.0298  max mem: 1378\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:04  Loss: 4.5452 (4.5935)  Acc@1: 12.5000 (12.6984)  Acc@5: 62.5000 (59.5238)  Acc@task: 70.8333 (70.4365)  time: 0.1777  data: 0.0006  max mem: 1378\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:02  Loss: 4.4871 (4.6270)  Acc@1: 8.3333 (11.9624)  Acc@5: 58.3333 (59.5430)  Acc@task: 66.6667 (70.2957)  time: 0.1783  data: 0.0004  max mem: 1378\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 4.7391 (4.6213)  Acc@1: 8.3333 (12.3984)  Acc@5: 58.3333 (59.0447)  Acc@task: 66.6667 (68.5976)  time: 0.1786  data: 0.0003  max mem: 1378\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 4.7391 (4.6179)  Acc@1: 8.3333 (12.5000)  Acc@5: 58.3333 (59.0000)  Acc@task: 66.6667 (68.7000)  time: 0.1754  data: 0.0003  max mem: 1378\n",
            "Test: [Task 8] Total time: 0:00:07 (0.1890 s / it)\n",
            "* Acc@task 68.700 Acc@1 12.500 Acc@5 59.000 loss 4.618\n",
            "[Average accuracy till task8]\tAcc@task: 77.8875\tAcc@1: 75.5500\tAcc@5: 92.6125\tLoss: 1.0817\tForgetting: 0.8714\tBackward: 49.9429\n",
            "torch.Size([96000, 384])\n",
            "Averaged stats: Lr: 0.005000  Loss: 0.0583  Acc@1: 96.6667 (95.5833)  Acc@5: 100.0000 (99.5357)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:25  Loss: 0.2816 (0.2816)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.6186  data: 0.4381  max mem: 1378\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.4212 (0.4465)  Acc@1: 87.5000 (87.8788)  Acc@5: 100.0000 (99.2424)  Acc@task: 79.1667 (81.0606)  time: 0.2175  data: 0.0404  max mem: 1378\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.3595 (0.4909)  Acc@1: 87.5000 (87.6984)  Acc@5: 100.0000 (97.8175)  Acc@task: 79.1667 (80.9524)  time: 0.1770  data: 0.0005  max mem: 1378\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.3074 (0.4503)  Acc@1: 91.6667 (88.0376)  Acc@5: 100.0000 (98.2527)  Acc@task: 79.1667 (81.7204)  time: 0.1773  data: 0.0005  max mem: 1378\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.2188 (0.4174)  Acc@1: 91.6667 (89.1260)  Acc@5: 100.0000 (98.4756)  Acc@task: 83.3333 (82.7236)  time: 0.1784  data: 0.0005  max mem: 1378\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.2073 (0.4102)  Acc@1: 91.6667 (89.2000)  Acc@5: 100.0000 (98.5000)  Acc@task: 83.3333 (83.0000)  time: 0.1747  data: 0.0005  max mem: 1378\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1890 s / it)\n",
            "* Acc@task 83.000 Acc@1 89.200 Acc@5 98.500 loss 0.410\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:24  Loss: 0.9774 (0.9774)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  Acc@task: 75.0000 (75.0000)  time: 0.5812  data: 0.4002  max mem: 1378\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.6184 (0.6599)  Acc@1: 79.1667 (80.6818)  Acc@5: 100.0000 (97.7273)  Acc@task: 79.1667 (76.5152)  time: 0.2142  data: 0.0369  max mem: 1378\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.6184 (0.7221)  Acc@1: 79.1667 (81.3492)  Acc@5: 100.0000 (97.0238)  Acc@task: 75.0000 (75.3968)  time: 0.1768  data: 0.0009  max mem: 1378\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.6537 (0.6929)  Acc@1: 83.3333 (81.9892)  Acc@5: 95.8333 (97.4462)  Acc@task: 75.0000 (76.3441)  time: 0.1770  data: 0.0011  max mem: 1378\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.5434 (0.6402)  Acc@1: 87.5000 (83.6382)  Acc@5: 100.0000 (97.6626)  Acc@task: 75.0000 (76.8293)  time: 0.1785  data: 0.0006  max mem: 1378\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.4466 (0.6348)  Acc@1: 87.5000 (83.7000)  Acc@5: 100.0000 (97.7000)  Acc@task: 75.0000 (76.9000)  time: 0.1748  data: 0.0005  max mem: 1378\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1883 s / it)\n",
            "* Acc@task 76.900 Acc@1 83.700 Acc@5 97.700 loss 0.635\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:20  Loss: 0.1113 (0.1113)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  Acc@task: 87.5000 (87.5000)  time: 0.4912  data: 0.3207  max mem: 1378\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 0.7536 (0.6590)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (97.3485)  Acc@task: 83.3333 (80.6818)  time: 0.2080  data: 0.0318  max mem: 1378\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.4501 (0.5836)  Acc@1: 83.3333 (85.1191)  Acc@5: 95.8333 (97.0238)  Acc@task: 79.1667 (81.3492)  time: 0.1785  data: 0.0017  max mem: 1378\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.4377 (0.5494)  Acc@1: 87.5000 (85.4839)  Acc@5: 95.8333 (96.9086)  Acc@task: 79.1667 (80.7796)  time: 0.1782  data: 0.0005  max mem: 1378\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.4399 (0.5731)  Acc@1: 87.5000 (85.3659)  Acc@5: 95.8333 (96.8496)  Acc@task: 79.1667 (80.5894)  time: 0.1790  data: 0.0004  max mem: 1378\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.4399 (0.5743)  Acc@1: 87.5000 (85.2000)  Acc@5: 95.8333 (96.9000)  Acc@task: 79.1667 (80.4000)  time: 0.1755  data: 0.0004  max mem: 1378\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1876 s / it)\n",
            "* Acc@task 80.400 Acc@1 85.200 Acc@5 96.900 loss 0.574\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:34  Loss: 1.0119 (1.0119)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  Acc@task: 66.6667 (66.6667)  time: 0.8321  data: 0.6148  max mem: 1378\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:07  Loss: 0.6465 (0.6767)  Acc@1: 83.3333 (82.1970)  Acc@5: 95.8333 (96.5909)  Acc@task: 79.1667 (75.0000)  time: 0.2380  data: 0.0564  max mem: 1378\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.6025 (0.6946)  Acc@1: 83.3333 (81.7460)  Acc@5: 95.8333 (96.2302)  Acc@task: 79.1667 (76.5873)  time: 0.1777  data: 0.0013  max mem: 1378\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.5162 (0.6192)  Acc@1: 83.3333 (83.6022)  Acc@5: 95.8333 (96.5054)  Acc@task: 79.1667 (78.3602)  time: 0.1776  data: 0.0012  max mem: 1378\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.4549 (0.6404)  Acc@1: 87.5000 (84.1463)  Acc@5: 100.0000 (96.4431)  Acc@task: 79.1667 (78.2520)  time: 0.1783  data: 0.0003  max mem: 1378\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.4511 (0.6259)  Acc@1: 87.5000 (84.4000)  Acc@5: 100.0000 (96.5000)  Acc@task: 79.1667 (78.5000)  time: 0.1750  data: 0.0003  max mem: 1378\n",
            "Test: [Task 4] Total time: 0:00:08 (0.1948 s / it)\n",
            "* Acc@task 78.500 Acc@1 84.400 Acc@5 96.500 loss 0.626\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:20  Loss: 0.0831 (0.0831)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.4770  data: 0.3123  max mem: 1378\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 0.3812 (0.3826)  Acc@1: 87.5000 (87.8788)  Acc@5: 100.0000 (99.2424)  Acc@task: 83.3333 (81.8182)  time: 0.2105  data: 0.0375  max mem: 1378\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.4743 (0.4739)  Acc@1: 83.3333 (85.7143)  Acc@5: 100.0000 (98.0159)  Acc@task: 83.3333 (81.3492)  time: 0.1817  data: 0.0052  max mem: 1378\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.4356 (0.4356)  Acc@1: 87.5000 (86.5591)  Acc@5: 100.0000 (98.6559)  Acc@task: 79.1667 (81.8548)  time: 0.1791  data: 0.0005  max mem: 1378\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.4270 (0.4741)  Acc@1: 87.5000 (86.5854)  Acc@5: 100.0000 (98.1707)  Acc@task: 79.1667 (80.6911)  time: 0.1788  data: 0.0003  max mem: 1378\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.4277 (0.4874)  Acc@1: 87.5000 (86.3000)  Acc@5: 100.0000 (98.0000)  Acc@task: 79.1667 (80.7000)  time: 0.1759  data: 0.0003  max mem: 1378\n",
            "Test: [Task 5] Total time: 0:00:07 (0.1899 s / it)\n",
            "* Acc@task 80.700 Acc@1 86.300 Acc@5 98.000 loss 0.487\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:40  Loss: 0.5988 (0.5988)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 87.5000 (87.5000)  time: 0.9608  data: 0.7254  max mem: 1378\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:08  Loss: 0.8266 (0.8401)  Acc@1: 83.3333 (80.6818)  Acc@5: 95.8333 (96.5909)  Acc@task: 79.1667 (81.0606)  time: 0.2502  data: 0.0671  max mem: 1378\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:04  Loss: 0.8263 (0.8799)  Acc@1: 83.3333 (79.7619)  Acc@5: 95.8333 (96.4286)  Acc@task: 79.1667 (80.1587)  time: 0.1781  data: 0.0009  max mem: 1378\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 0.8263 (0.8829)  Acc@1: 79.1667 (78.7634)  Acc@5: 95.8333 (96.5054)  Acc@task: 79.1667 (79.4355)  time: 0.1781  data: 0.0004  max mem: 1378\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 0.8786 (0.9131)  Acc@1: 75.0000 (77.5407)  Acc@5: 95.8333 (96.3415)  Acc@task: 79.1667 (79.0650)  time: 0.1790  data: 0.0004  max mem: 1378\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 0.8786 (0.9145)  Acc@1: 75.0000 (77.3000)  Acc@5: 95.8333 (96.2000)  Acc@task: 79.1667 (79.1000)  time: 0.1754  data: 0.0004  max mem: 1378\n",
            "Test: [Task 6] Total time: 0:00:08 (0.1984 s / it)\n",
            "* Acc@task 79.100 Acc@1 77.300 Acc@5 96.200 loss 0.914\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:29  Loss: 0.7778 (0.7778)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 79.1667 (79.1667)  time: 0.6913  data: 0.5233  max mem: 1378\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:07  Loss: 0.5914 (0.6002)  Acc@1: 83.3333 (82.1970)  Acc@5: 100.0000 (96.5909)  Acc@task: 79.1667 (76.1364)  time: 0.2251  data: 0.0480  max mem: 1378\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:04  Loss: 0.6352 (0.6613)  Acc@1: 83.3333 (81.1508)  Acc@5: 95.8333 (96.2302)  Acc@task: 79.1667 (77.3810)  time: 0.1774  data: 0.0005  max mem: 1378\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:02  Loss: 0.7015 (0.7203)  Acc@1: 83.3333 (81.1828)  Acc@5: 95.8333 (95.8333)  Acc@task: 75.0000 (76.2097)  time: 0.1776  data: 0.0007  max mem: 1378\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 0.6956 (0.7089)  Acc@1: 79.1667 (80.9959)  Acc@5: 95.8333 (96.3415)  Acc@task: 70.8333 (75.7114)  time: 0.1789  data: 0.0006  max mem: 1378\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 0.6956 (0.6983)  Acc@1: 79.1667 (81.2000)  Acc@5: 95.8333 (96.4000)  Acc@task: 75.0000 (75.8000)  time: 0.1753  data: 0.0006  max mem: 1378\n",
            "Test: [Task 7] Total time: 0:00:08 (0.1914 s / it)\n",
            "* Acc@task 75.800 Acc@1 81.200 Acc@5 96.400 loss 0.698\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:24  Loss: 0.8851 (0.8851)  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  Acc@task: 70.8333 (70.8333)  time: 0.5881  data: 0.3597  max mem: 1378\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:06  Loss: 0.7099 (0.9110)  Acc@1: 75.0000 (76.1364)  Acc@5: 95.8333 (95.8333)  Acc@task: 70.8333 (70.8333)  time: 0.2154  data: 0.0330  max mem: 1378\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:04  Loss: 0.7474 (0.8409)  Acc@1: 75.0000 (77.1825)  Acc@5: 95.8333 (96.8254)  Acc@task: 70.8333 (70.4365)  time: 0.1777  data: 0.0004  max mem: 1378\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:02  Loss: 0.7633 (0.8018)  Acc@1: 79.1667 (77.9570)  Acc@5: 95.8333 (97.0430)  Acc@task: 66.6667 (70.2957)  time: 0.1781  data: 0.0004  max mem: 1378\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 0.8844 (0.8626)  Acc@1: 75.0000 (76.6260)  Acc@5: 95.8333 (96.0366)  Acc@task: 66.6667 (68.5976)  time: 0.1798  data: 0.0004  max mem: 1378\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 0.8844 (0.8553)  Acc@1: 75.0000 (76.6000)  Acc@5: 95.8333 (96.0000)  Acc@task: 66.6667 (68.7000)  time: 0.1762  data: 0.0004  max mem: 1378\n",
            "Test: [Task 8] Total time: 0:00:07 (0.1896 s / it)\n",
            "* Acc@task 68.700 Acc@1 76.600 Acc@5 96.000 loss 0.855\n",
            "[Average accuracy till task8]\tAcc@task: 77.8875\tAcc@1: 82.9875\tAcc@5: 97.0250\tLoss: 0.6501\tForgetting: 1.4143\tBackward: 2.4571\n",
            "Loading checkpoint from: ./output/cifar100_full_dino_1epoch_100pct/checkpoint/task9_checkpoint.pth\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/engines/hide_promtp_wtp_and_tap_engine.py:540: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  loss = torch.nn.functional.cross_entropy(sim, torch.range(0, sim.shape[0] - 1).long().to(device))\n",
            "Train: Epoch[1/1]  [  0/209]  eta: 0:02:24  Lr: 0.002812  Loss: 4.5275  Acc@1: 0.0000 (0.0000)  Acc@5: 37.5000 (37.5000)  time: 0.6934  data: 0.3730  max mem: 1378\n",
            "Train: Epoch[1/1]  [ 10/209]  eta: 0:01:03  Lr: 0.002812  Loss: 3.3949  Acc@1: 12.5000 (10.6061)  Acc@5: 62.5000 (57.1970)  time: 0.3206  data: 0.0354  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 20/209]  eta: 0:00:57  Lr: 0.002812  Loss: 3.5423  Acc@1: 12.5000 (13.4921)  Acc@5: 66.6667 (64.4841)  time: 0.2829  data: 0.0015  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 30/209]  eta: 0:00:52  Lr: 0.002812  Loss: 2.3599  Acc@1: 29.1667 (21.6398)  Acc@5: 75.0000 (68.8172)  time: 0.2824  data: 0.0009  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 40/209]  eta: 0:00:49  Lr: 0.002812  Loss: 2.4697  Acc@1: 45.8333 (28.7602)  Acc@5: 79.1667 (73.4756)  time: 0.2827  data: 0.0004  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 50/209]  eta: 0:00:46  Lr: 0.002812  Loss: 2.0245  Acc@1: 54.1667 (34.8039)  Acc@5: 87.5000 (77.1242)  time: 0.2831  data: 0.0004  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 60/209]  eta: 0:00:43  Lr: 0.002812  Loss: 2.2572  Acc@1: 58.3333 (39.8224)  Acc@5: 95.8333 (80.0546)  time: 0.2838  data: 0.0006  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 70/209]  eta: 0:00:40  Lr: 0.002812  Loss: 1.9049  Acc@1: 66.6667 (43.6033)  Acc@5: 91.6667 (81.8075)  time: 0.2840  data: 0.0010  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 80/209]  eta: 0:00:37  Lr: 0.002812  Loss: 1.6144  Acc@1: 70.8333 (47.1193)  Acc@5: 91.6667 (83.5905)  time: 0.2834  data: 0.0007  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 90/209]  eta: 0:00:34  Lr: 0.002812  Loss: 1.4305  Acc@1: 75.0000 (50.1832)  Acc@5: 95.8333 (84.9359)  time: 0.2827  data: 0.0004  max mem: 1379\n",
            "Train: Epoch[1/1]  [100/209]  eta: 0:00:31  Lr: 0.002812  Loss: 1.7560  Acc@1: 75.0000 (52.4340)  Acc@5: 95.8333 (85.9323)  time: 0.2822  data: 0.0004  max mem: 1379\n",
            "Train: Epoch[1/1]  [110/209]  eta: 0:00:28  Lr: 0.002812  Loss: 1.4647  Acc@1: 70.8333 (54.5045)  Acc@5: 95.8333 (86.8619)  time: 0.2831  data: 0.0006  max mem: 1379\n",
            "Train: Epoch[1/1]  [120/209]  eta: 0:00:25  Lr: 0.002812  Loss: 1.6195  Acc@1: 79.1667 (56.5427)  Acc@5: 95.8333 (87.6377)  time: 0.2828  data: 0.0008  max mem: 1379\n",
            "Train: Epoch[1/1]  [130/209]  eta: 0:00:22  Lr: 0.002812  Loss: 1.6541  Acc@1: 79.1667 (58.6514)  Acc@5: 95.8333 (88.4224)  time: 0.2822  data: 0.0006  max mem: 1379\n",
            "Train: Epoch[1/1]  [140/209]  eta: 0:00:19  Lr: 0.002812  Loss: 1.4620  Acc@1: 79.1667 (60.0177)  Acc@5: 95.8333 (88.9480)  time: 0.2820  data: 0.0004  max mem: 1379\n",
            "Train: Epoch[1/1]  [150/209]  eta: 0:00:16  Lr: 0.002812  Loss: 1.4263  Acc@1: 83.3333 (61.8929)  Acc@5: 95.8333 (89.5695)  time: 0.2819  data: 0.0004  max mem: 1379\n",
            "Train: Epoch[1/1]  [160/209]  eta: 0:00:13  Lr: 0.002812  Loss: 1.5653  Acc@1: 83.3333 (62.9917)  Acc@5: 100.0000 (90.0621)  time: 0.2825  data: 0.0008  max mem: 1379\n",
            "Train: Epoch[1/1]  [170/209]  eta: 0:00:11  Lr: 0.002812  Loss: 1.4968  Acc@1: 79.1667 (64.0107)  Acc@5: 95.8333 (90.4727)  time: 0.2819  data: 0.0009  max mem: 1379\n",
            "Train: Epoch[1/1]  [180/209]  eta: 0:00:08  Lr: 0.002812  Loss: 1.8785  Acc@1: 83.3333 (65.0092)  Acc@5: 100.0000 (90.8840)  time: 0.2816  data: 0.0005  max mem: 1379\n",
            "Train: Epoch[1/1]  [190/209]  eta: 0:00:05  Lr: 0.002812  Loss: 1.1216  Acc@1: 83.3333 (66.1867)  Acc@5: 100.0000 (91.2522)  time: 0.2814  data: 0.0004  max mem: 1379\n",
            "Train: Epoch[1/1]  [200/209]  eta: 0:00:02  Lr: 0.002812  Loss: 1.7452  Acc@1: 87.5000 (66.9983)  Acc@5: 100.0000 (91.6252)  time: 0.2814  data: 0.0004  max mem: 1379\n",
            "Train: Epoch[1/1]  [208/209]  eta: 0:00:00  Lr: 0.002812  Loss: 1.0744  Acc@1: 87.5000 (67.7600)  Acc@5: 100.0000 (91.9000)  time: 0.2730  data: 0.0003  max mem: 1379\n",
            "Train: Epoch[1/1] Total time: 0:00:59 (0.2845 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.0744  Acc@1: 87.5000 (67.7600)  Acc@5: 100.0000 (91.9000)\n",
            "torch.Size([5, 2, 5, 6, 64])\n",
            "torch.Size([5, 2, 1, 5, 6, 64])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:23  Loss: 0.3441 (0.3441)  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)  Acc@task: 79.1667 (79.1667)  time: 0.5704  data: 0.3933  max mem: 1379\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.4123 (0.4613)  Acc@1: 87.5000 (86.3636)  Acc@5: 100.0000 (98.8636)  Acc@task: 79.1667 (78.0303)  time: 0.2121  data: 0.0372  max mem: 1379\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.3795 (0.4830)  Acc@1: 87.5000 (87.1032)  Acc@5: 100.0000 (98.0159)  Acc@task: 79.1667 (79.5635)  time: 0.1770  data: 0.0014  max mem: 1379\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.3074 (0.4483)  Acc@1: 91.6667 (87.5000)  Acc@5: 100.0000 (98.2527)  Acc@task: 83.3333 (80.2419)  time: 0.1781  data: 0.0009  max mem: 1379\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.2885 (0.4304)  Acc@1: 91.6667 (88.6179)  Acc@5: 100.0000 (98.1707)  Acc@task: 83.3333 (80.7927)  time: 0.1788  data: 0.0004  max mem: 1379\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.2075 (0.4229)  Acc@1: 91.6667 (88.7000)  Acc@5: 100.0000 (98.2000)  Acc@task: 83.3333 (81.1000)  time: 0.1756  data: 0.0004  max mem: 1379\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1882 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@task 81.100 Acc@1 88.700 Acc@5 98.200 loss 0.423\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:24  Loss: 1.0112 (1.0112)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  Acc@task: 66.6667 (66.6667)  time: 0.5879  data: 0.4148  max mem: 1379\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:06  Loss: 0.6185 (0.6463)  Acc@1: 79.1667 (81.4394)  Acc@5: 100.0000 (97.7273)  Acc@task: 75.0000 (75.7576)  time: 0.2147  data: 0.0382  max mem: 1379\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.6185 (0.7287)  Acc@1: 83.3333 (81.3492)  Acc@5: 100.0000 (96.8254)  Acc@task: 75.0000 (74.2064)  time: 0.1776  data: 0.0005  max mem: 1379\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.6585 (0.6997)  Acc@1: 83.3333 (81.8548)  Acc@5: 95.8333 (97.3118)  Acc@task: 75.0000 (75.1344)  time: 0.1785  data: 0.0004  max mem: 1379\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.5433 (0.6479)  Acc@1: 87.5000 (83.2317)  Acc@5: 100.0000 (97.5610)  Acc@task: 75.0000 (75.7114)  time: 0.1798  data: 0.0003  max mem: 1379\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.4910 (0.6442)  Acc@1: 83.3333 (83.2000)  Acc@5: 100.0000 (97.6000)  Acc@task: 75.0000 (75.7000)  time: 0.1763  data: 0.0003  max mem: 1379\n",
            "Test: [Task 2] Total time: 0:00:08 (0.1910 s / it)\n",
            "* Acc@task 75.700 Acc@1 83.200 Acc@5 97.600 loss 0.644\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:41  Loss: 0.1095 (0.1095)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.9795  data: 0.8007  max mem: 1379\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:08  Loss: 0.7551 (0.6380)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (97.7273)  Acc@task: 83.3333 (81.8182)  time: 0.2523  data: 0.0740  max mem: 1379\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.4462 (0.5680)  Acc@1: 83.3333 (84.9206)  Acc@5: 95.8333 (97.2222)  Acc@task: 79.1667 (82.7381)  time: 0.1787  data: 0.0009  max mem: 1379\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.4374 (0.5353)  Acc@1: 87.5000 (85.3495)  Acc@5: 95.8333 (97.0430)  Acc@task: 83.3333 (81.9893)  time: 0.1781  data: 0.0004  max mem: 1379\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.4402 (0.5668)  Acc@1: 87.5000 (85.2642)  Acc@5: 95.8333 (96.9512)  Acc@task: 83.3333 (81.9106)  time: 0.1792  data: 0.0003  max mem: 1379\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.4402 (0.5681)  Acc@1: 87.5000 (85.1000)  Acc@5: 95.8333 (97.0000)  Acc@task: 83.3333 (81.7000)  time: 0.1760  data: 0.0003  max mem: 1379\n",
            "Test: [Task 3] Total time: 0:00:08 (0.2003 s / it)\n",
            "* Acc@task 81.700 Acc@1 85.100 Acc@5 97.000 loss 0.568\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:35  Loss: 0.7871 (0.7871)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (95.8333)  Acc@task: 70.8333 (70.8333)  time: 0.8538  data: 0.6503  max mem: 1379\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:07  Loss: 0.5442 (0.5842)  Acc@1: 87.5000 (84.8485)  Acc@5: 95.8333 (96.9697)  Acc@task: 70.8333 (74.6212)  time: 0.2392  data: 0.0599  max mem: 1379\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.5442 (0.6281)  Acc@1: 83.3333 (83.5317)  Acc@5: 95.8333 (96.8254)  Acc@task: 75.0000 (76.3889)  time: 0.1784  data: 0.0006  max mem: 1379\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.5435 (0.5861)  Acc@1: 83.3333 (84.5430)  Acc@5: 95.8333 (96.9086)  Acc@task: 79.1667 (77.8226)  time: 0.1790  data: 0.0004  max mem: 1379\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.4931 (0.6152)  Acc@1: 83.3333 (84.7561)  Acc@5: 100.0000 (96.7480)  Acc@task: 79.1667 (77.5407)  time: 0.1795  data: 0.0005  max mem: 1379\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.4548 (0.6012)  Acc@1: 87.5000 (85.0000)  Acc@5: 100.0000 (96.8000)  Acc@task: 79.1667 (77.8000)  time: 0.1762  data: 0.0005  max mem: 1379\n",
            "Test: [Task 4] Total time: 0:00:08 (0.1963 s / it)\n",
            "* Acc@task 77.800 Acc@1 85.000 Acc@5 96.800 loss 0.601\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:30  Loss: 0.0589 (0.0589)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.7237  data: 0.5131  max mem: 1379\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:07  Loss: 0.4312 (0.3989)  Acc@1: 87.5000 (88.2576)  Acc@5: 100.0000 (99.2424)  Acc@task: 79.1667 (80.6818)  time: 0.2273  data: 0.0471  max mem: 1379\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.4890 (0.4606)  Acc@1: 87.5000 (86.3095)  Acc@5: 100.0000 (98.2143)  Acc@task: 79.1667 (80.1587)  time: 0.1778  data: 0.0005  max mem: 1379\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.4273 (0.4338)  Acc@1: 87.5000 (86.5591)  Acc@5: 100.0000 (98.6559)  Acc@task: 79.1667 (80.3763)  time: 0.1786  data: 0.0004  max mem: 1379\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.4273 (0.4758)  Acc@1: 87.5000 (86.4837)  Acc@5: 100.0000 (98.1707)  Acc@task: 75.0000 (78.8618)  time: 0.1790  data: 0.0003  max mem: 1379\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.4702 (0.4890)  Acc@1: 87.5000 (86.2000)  Acc@5: 100.0000 (98.0000)  Acc@task: 75.0000 (78.9000)  time: 0.1756  data: 0.0003  max mem: 1379\n",
            "Test: [Task 5] Total time: 0:00:08 (0.1929 s / it)\n",
            "* Acc@task 78.900 Acc@1 86.200 Acc@5 98.000 loss 0.489\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:26  Loss: 0.6216 (0.6216)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.6207  data: 0.4551  max mem: 1379\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:07  Loss: 0.7389 (0.7841)  Acc@1: 83.3333 (82.5758)  Acc@5: 95.8333 (96.9697)  Acc@task: 79.1667 (81.0606)  time: 0.2194  data: 0.0422  max mem: 1379\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:04  Loss: 0.8166 (0.8666)  Acc@1: 83.3333 (80.7540)  Acc@5: 95.8333 (96.6270)  Acc@task: 79.1667 (79.5635)  time: 0.1780  data: 0.0011  max mem: 1379\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 0.8265 (0.8572)  Acc@1: 79.1667 (79.5699)  Acc@5: 95.8333 (96.6398)  Acc@task: 79.1667 (79.3011)  time: 0.1782  data: 0.0012  max mem: 1379\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 0.8953 (0.8971)  Acc@1: 75.0000 (78.1504)  Acc@5: 95.8333 (96.3415)  Acc@task: 79.1667 (78.1504)  time: 0.1789  data: 0.0007  max mem: 1379\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 0.8953 (0.8972)  Acc@1: 75.0000 (77.9000)  Acc@5: 95.8333 (96.3000)  Acc@task: 79.1667 (78.3000)  time: 0.1754  data: 0.0006  max mem: 1379\n",
            "Test: [Task 6] Total time: 0:00:07 (0.1899 s / it)\n",
            "* Acc@task 78.300 Acc@1 77.900 Acc@5 96.300 loss 0.897\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:18  Loss: 0.7292 (0.7292)  Acc@1: 83.3333 (83.3333)  Acc@5: 91.6667 (91.6667)  Acc@task: 70.8333 (70.8333)  time: 0.4496  data: 0.2769  max mem: 1379\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:06  Loss: 0.5689 (0.5832)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (96.5909)  Acc@task: 79.1667 (75.7576)  time: 0.2064  data: 0.0297  max mem: 1379\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:04  Loss: 0.6403 (0.6581)  Acc@1: 83.3333 (81.9444)  Acc@5: 95.8333 (96.2302)  Acc@task: 75.0000 (76.7857)  time: 0.1800  data: 0.0027  max mem: 1379\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:02  Loss: 0.7165 (0.7078)  Acc@1: 83.3333 (81.5860)  Acc@5: 95.8333 (95.9677)  Acc@task: 75.0000 (76.4785)  time: 0.1785  data: 0.0004  max mem: 1379\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 0.6409 (0.6889)  Acc@1: 79.1667 (81.6057)  Acc@5: 95.8333 (96.4431)  Acc@task: 75.0000 (76.4228)  time: 0.1790  data: 0.0003  max mem: 1379\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 0.6409 (0.6799)  Acc@1: 79.1667 (81.8000)  Acc@5: 95.8333 (96.5000)  Acc@task: 75.0000 (76.5000)  time: 0.1758  data: 0.0003  max mem: 1379\n",
            "Test: [Task 7] Total time: 0:00:07 (0.1873 s / it)\n",
            "* Acc@task 76.500 Acc@1 81.800 Acc@5 96.500 loss 0.680\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:32  Loss: 0.8869 (0.8869)  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  Acc@task: 70.8333 (70.8333)  time: 0.7834  data: 0.5590  max mem: 1379\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:07  Loss: 0.7476 (0.8410)  Acc@1: 83.3333 (78.0303)  Acc@5: 100.0000 (96.2121)  Acc@task: 75.0000 (76.1364)  time: 0.2337  data: 0.0530  max mem: 1379\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:04  Loss: 0.7471 (0.7840)  Acc@1: 79.1667 (78.3730)  Acc@5: 95.8333 (96.8254)  Acc@task: 70.8333 (74.4048)  time: 0.1779  data: 0.0018  max mem: 1379\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:02  Loss: 0.6685 (0.7417)  Acc@1: 79.1667 (79.0323)  Acc@5: 95.8333 (97.1774)  Acc@task: 70.8333 (74.8656)  time: 0.1778  data: 0.0008  max mem: 1379\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 0.7873 (0.7682)  Acc@1: 75.0000 (78.4553)  Acc@5: 95.8333 (96.6463)  Acc@task: 70.8333 (73.5772)  time: 0.1785  data: 0.0003  max mem: 1379\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 0.7873 (0.7631)  Acc@1: 75.0000 (78.4000)  Acc@5: 95.8333 (96.6000)  Acc@task: 70.8333 (73.7000)  time: 0.1752  data: 0.0003  max mem: 1379\n",
            "Test: [Task 8] Total time: 0:00:08 (0.1937 s / it)\n",
            "* Acc@task 73.700 Acc@1 78.400 Acc@5 96.600 loss 0.763\n",
            "Test: [Task 9]  [ 0/42]  eta: 0:00:20  Loss: 4.4124 (4.4124)  Acc@1: 8.3333 (8.3333)  Acc@5: 91.6667 (91.6667)  Acc@task: 70.8333 (70.8333)  time: 0.4868  data: 0.3042  max mem: 1379\n",
            "Test: [Task 9]  [10/42]  eta: 0:00:06  Loss: 4.7290 (4.4888)  Acc@1: 8.3333 (9.4697)  Acc@5: 75.0000 (76.1364)  Acc@task: 70.8333 (71.5909)  time: 0.2067  data: 0.0296  max mem: 1379\n",
            "Test: [Task 9]  [20/42]  eta: 0:00:04  Loss: 4.4917 (4.3828)  Acc@1: 8.3333 (9.5238)  Acc@5: 70.8333 (75.3968)  Acc@task: 70.8333 (72.6190)  time: 0.1775  data: 0.0013  max mem: 1379\n",
            "Test: [Task 9]  [30/42]  eta: 0:00:02  Loss: 4.3434 (4.4149)  Acc@1: 8.3333 (9.4086)  Acc@5: 70.8333 (72.3118)  Acc@task: 70.8333 (72.0430)  time: 0.1777  data: 0.0004  max mem: 1379\n",
            "Test: [Task 9]  [40/42]  eta: 0:00:00  Loss: 4.2995 (4.3901)  Acc@1: 12.5000 (10.3659)  Acc@5: 70.8333 (73.0691)  Acc@task: 75.0000 (72.7642)  time: 0.1787  data: 0.0004  max mem: 1379\n",
            "Test: [Task 9]  [41/42]  eta: 0:00:00  Loss: 4.2995 (4.3778)  Acc@1: 12.5000 (10.3000)  Acc@5: 70.8333 (73.2000)  Acc@task: 75.0000 (73.1000)  time: 0.1752  data: 0.0004  max mem: 1379\n",
            "Test: [Task 9] Total time: 0:00:07 (0.1882 s / it)\n",
            "* Acc@task 73.100 Acc@1 10.300 Acc@5 73.200 loss 4.378\n",
            "[Average accuracy till task9]\tAcc@task: 77.4222\tAcc@1: 75.1778\tAcc@5: 94.4667\tLoss: 1.0493\tForgetting: 1.4250\tBackward: 51.4375\n",
            "torch.Size([108000, 384])\n",
            "Averaged stats: Lr: 0.005000  Loss: 0.0813  Acc@1: 98.3333 (96.6146)  Acc@5: 100.0000 (99.8333)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:24  Loss: 0.5000 (0.5000)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  Acc@task: 79.1667 (79.1667)  time: 0.5716  data: 0.3889  max mem: 1379\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.5000 (0.4997)  Acc@1: 87.5000 (86.7424)  Acc@5: 100.0000 (98.4848)  Acc@task: 79.1667 (78.0303)  time: 0.2124  data: 0.0360  max mem: 1379\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.3760 (0.5193)  Acc@1: 87.5000 (86.3095)  Acc@5: 95.8333 (97.2222)  Acc@task: 79.1667 (79.5635)  time: 0.1768  data: 0.0006  max mem: 1379\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.3180 (0.4768)  Acc@1: 87.5000 (87.2312)  Acc@5: 100.0000 (97.7151)  Acc@task: 83.3333 (80.2419)  time: 0.1774  data: 0.0004  max mem: 1379\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.3085 (0.4625)  Acc@1: 91.6667 (88.4146)  Acc@5: 100.0000 (97.7642)  Acc@task: 83.3333 (80.7927)  time: 0.1783  data: 0.0003  max mem: 1379\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.2965 (0.4542)  Acc@1: 91.6667 (88.5000)  Acc@5: 100.0000 (97.8000)  Acc@task: 83.3333 (81.1000)  time: 0.1749  data: 0.0003  max mem: 1379\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1879 s / it)\n",
            "* Acc@task 81.100 Acc@1 88.500 Acc@5 97.800 loss 0.454\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:27  Loss: 0.9778 (0.9778)  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  Acc@task: 66.6667 (66.6667)  time: 0.6526  data: 0.4853  max mem: 1379\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:07  Loss: 0.7087 (0.6801)  Acc@1: 79.1667 (81.0606)  Acc@5: 100.0000 (97.7273)  Acc@task: 75.0000 (75.7576)  time: 0.2194  data: 0.0452  max mem: 1379\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.7087 (0.7290)  Acc@1: 83.3333 (81.7460)  Acc@5: 95.8333 (96.2302)  Acc@task: 75.0000 (74.2064)  time: 0.1763  data: 0.0013  max mem: 1379\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.7224 (0.7041)  Acc@1: 83.3333 (82.2581)  Acc@5: 95.8333 (96.5054)  Acc@task: 75.0000 (75.1344)  time: 0.1778  data: 0.0012  max mem: 1379\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.4672 (0.6485)  Acc@1: 87.5000 (83.6382)  Acc@5: 100.0000 (96.9512)  Acc@task: 75.0000 (75.7114)  time: 0.1787  data: 0.0006  max mem: 1379\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.4586 (0.6423)  Acc@1: 87.5000 (83.7000)  Acc@5: 100.0000 (97.0000)  Acc@task: 75.0000 (75.7000)  time: 0.1754  data: 0.0005  max mem: 1379\n",
            "Test: [Task 2] Total time: 0:00:07 (0.1899 s / it)\n",
            "* Acc@task 75.700 Acc@1 83.700 Acc@5 97.000 loss 0.642\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:18  Loss: 0.0743 (0.0743)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.4481  data: 0.2864  max mem: 1379\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 0.7349 (0.6275)  Acc@1: 83.3333 (84.8485)  Acc@5: 95.8333 (97.7273)  Acc@task: 83.3333 (81.8182)  time: 0.2075  data: 0.0338  max mem: 1379\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.5144 (0.5537)  Acc@1: 87.5000 (85.7143)  Acc@5: 95.8333 (97.4206)  Acc@task: 79.1667 (82.7381)  time: 0.1810  data: 0.0046  max mem: 1379\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.3970 (0.5183)  Acc@1: 87.5000 (86.4247)  Acc@5: 95.8333 (97.3118)  Acc@task: 83.3333 (81.9893)  time: 0.1789  data: 0.0005  max mem: 1379\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.4631 (0.5428)  Acc@1: 87.5000 (86.1789)  Acc@5: 95.8333 (97.1545)  Acc@task: 83.3333 (81.9106)  time: 0.1788  data: 0.0003  max mem: 1379\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.4631 (0.5445)  Acc@1: 87.5000 (86.0000)  Acc@5: 95.8333 (97.2000)  Acc@task: 83.3333 (81.7000)  time: 0.1758  data: 0.0003  max mem: 1379\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1889 s / it)\n",
            "* Acc@task 81.700 Acc@1 86.000 Acc@5 97.200 loss 0.544\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:31  Loss: 0.7189 (0.7189)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 70.8333 (70.8333)  time: 0.7474  data: 0.5459  max mem: 1379\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:07  Loss: 0.6680 (0.5626)  Acc@1: 83.3333 (85.2273)  Acc@5: 100.0000 (97.7273)  Acc@task: 70.8333 (74.6212)  time: 0.2330  data: 0.0559  max mem: 1379\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.5743 (0.6188)  Acc@1: 83.3333 (84.1270)  Acc@5: 95.8333 (96.6270)  Acc@task: 75.0000 (76.3889)  time: 0.1799  data: 0.0039  max mem: 1379\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.5719 (0.5830)  Acc@1: 83.3333 (84.9462)  Acc@5: 95.8333 (96.7742)  Acc@task: 79.1667 (77.8226)  time: 0.1785  data: 0.0006  max mem: 1379\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.5419 (0.6155)  Acc@1: 87.5000 (84.7561)  Acc@5: 95.8333 (96.3415)  Acc@task: 79.1667 (77.5407)  time: 0.1791  data: 0.0003  max mem: 1379\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.4419 (0.6015)  Acc@1: 87.5000 (85.0000)  Acc@5: 95.8333 (96.4000)  Acc@task: 79.1667 (77.8000)  time: 0.1760  data: 0.0003  max mem: 1379\n",
            "Test: [Task 4] Total time: 0:00:08 (0.1942 s / it)\n",
            "* Acc@task 77.800 Acc@1 85.000 Acc@5 96.400 loss 0.601\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:24  Loss: 0.0751 (0.0751)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.5832  data: 0.3984  max mem: 1379\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 0.4297 (0.4051)  Acc@1: 87.5000 (88.6364)  Acc@5: 100.0000 (99.2424)  Acc@task: 79.1667 (80.6818)  time: 0.2141  data: 0.0366  max mem: 1379\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.4851 (0.4886)  Acc@1: 83.3333 (86.3095)  Acc@5: 100.0000 (98.0159)  Acc@task: 79.1667 (80.1587)  time: 0.1774  data: 0.0005  max mem: 1379\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.4670 (0.4800)  Acc@1: 83.3333 (85.8871)  Acc@5: 100.0000 (98.3871)  Acc@task: 79.1667 (80.3763)  time: 0.1787  data: 0.0004  max mem: 1379\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.4790 (0.5330)  Acc@1: 83.3333 (85.5691)  Acc@5: 100.0000 (98.0691)  Acc@task: 75.0000 (78.8618)  time: 0.1799  data: 0.0005  max mem: 1379\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.5495 (0.5460)  Acc@1: 83.3333 (85.3000)  Acc@5: 100.0000 (97.9000)  Acc@task: 75.0000 (78.9000)  time: 0.1765  data: 0.0005  max mem: 1379\n",
            "Test: [Task 5] Total time: 0:00:08 (0.1906 s / it)\n",
            "* Acc@task 78.900 Acc@1 85.300 Acc@5 97.900 loss 0.546\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:37  Loss: 0.4803 (0.4803)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.8997  data: 0.6963  max mem: 1379\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:07  Loss: 0.7140 (0.7009)  Acc@1: 83.3333 (85.2273)  Acc@5: 95.8333 (96.5909)  Acc@task: 79.1667 (81.0606)  time: 0.2449  data: 0.0637  max mem: 1379\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:04  Loss: 0.8110 (0.7915)  Acc@1: 83.3333 (82.5397)  Acc@5: 95.8333 (96.4286)  Acc@task: 79.1667 (79.5635)  time: 0.1783  data: 0.0004  max mem: 1379\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 0.7981 (0.7832)  Acc@1: 79.1667 (81.0484)  Acc@5: 95.8333 (96.7742)  Acc@task: 79.1667 (79.3011)  time: 0.1780  data: 0.0004  max mem: 1379\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 0.7444 (0.8280)  Acc@1: 79.1667 (79.9797)  Acc@5: 95.8333 (96.4431)  Acc@task: 79.1667 (78.1504)  time: 0.1796  data: 0.0004  max mem: 1379\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 0.6944 (0.8249)  Acc@1: 79.1667 (80.0000)  Acc@5: 95.8333 (96.4000)  Acc@task: 79.1667 (78.3000)  time: 0.1759  data: 0.0004  max mem: 1379\n",
            "Test: [Task 6] Total time: 0:00:08 (0.1971 s / it)\n",
            "* Acc@task 78.300 Acc@1 80.000 Acc@5 96.400 loss 0.825\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:20  Loss: 0.7539 (0.7539)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 70.8333 (70.8333)  time: 0.4930  data: 0.3304  max mem: 1379\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:06  Loss: 0.7539 (0.6786)  Acc@1: 79.1667 (79.9242)  Acc@5: 100.0000 (96.9697)  Acc@task: 79.1667 (75.7576)  time: 0.2124  data: 0.0378  max mem: 1379\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:04  Loss: 0.7766 (0.7754)  Acc@1: 79.1667 (78.1746)  Acc@5: 95.8333 (96.6270)  Acc@task: 75.0000 (76.7857)  time: 0.1818  data: 0.0048  max mem: 1379\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:02  Loss: 0.8030 (0.8174)  Acc@1: 75.0000 (77.8226)  Acc@5: 95.8333 (95.9677)  Acc@task: 75.0000 (76.4785)  time: 0.1794  data: 0.0011  max mem: 1379\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 0.7231 (0.8096)  Acc@1: 75.0000 (77.2358)  Acc@5: 95.8333 (96.4431)  Acc@task: 75.0000 (76.4228)  time: 0.1795  data: 0.0008  max mem: 1379\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 0.7231 (0.7992)  Acc@1: 75.0000 (77.5000)  Acc@5: 95.8333 (96.5000)  Acc@task: 75.0000 (76.5000)  time: 0.1765  data: 0.0007  max mem: 1379\n",
            "Test: [Task 7] Total time: 0:00:07 (0.1890 s / it)\n",
            "* Acc@task 76.500 Acc@1 77.500 Acc@5 96.500 loss 0.799\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:26  Loss: 1.0418 (1.0418)  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  Acc@task: 70.8333 (70.8333)  time: 0.6372  data: 0.4578  max mem: 1379\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:07  Loss: 0.8124 (0.9390)  Acc@1: 75.0000 (76.5152)  Acc@5: 95.8333 (95.4545)  Acc@task: 75.0000 (76.1364)  time: 0.2193  data: 0.0422  max mem: 1379\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:04  Loss: 0.8049 (0.9042)  Acc@1: 79.1667 (77.7778)  Acc@5: 95.8333 (95.4365)  Acc@task: 70.8333 (74.4048)  time: 0.1776  data: 0.0006  max mem: 1379\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:02  Loss: 0.7608 (0.8459)  Acc@1: 79.1667 (78.7634)  Acc@5: 95.8333 (95.8333)  Acc@task: 70.8333 (74.8656)  time: 0.1783  data: 0.0005  max mem: 1379\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 0.8515 (0.8803)  Acc@1: 75.0000 (78.0488)  Acc@5: 95.8333 (95.0203)  Acc@task: 70.8333 (73.5772)  time: 0.1788  data: 0.0003  max mem: 1379\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 0.8515 (0.8783)  Acc@1: 75.0000 (78.0000)  Acc@5: 91.6667 (94.9000)  Acc@task: 70.8333 (73.7000)  time: 0.1755  data: 0.0003  max mem: 1379\n",
            "Test: [Task 8] Total time: 0:00:07 (0.1903 s / it)\n",
            "* Acc@task 73.700 Acc@1 78.000 Acc@5 94.900 loss 0.878\n",
            "Test: [Task 9]  [ 0/42]  eta: 0:00:26  Loss: 0.5508 (0.5508)  Acc@1: 91.6667 (91.6667)  Acc@5: 95.8333 (95.8333)  Acc@task: 70.8333 (70.8333)  time: 0.6293  data: 0.4590  max mem: 1379\n",
            "Test: [Task 9]  [10/42]  eta: 0:00:07  Loss: 0.6682 (0.6817)  Acc@1: 79.1667 (79.9242)  Acc@5: 95.8333 (97.7273)  Acc@task: 70.8333 (71.5909)  time: 0.2190  data: 0.0430  max mem: 1379\n",
            "Test: [Task 9]  [20/42]  eta: 0:00:04  Loss: 0.6265 (0.6370)  Acc@1: 83.3333 (81.5476)  Acc@5: 100.0000 (97.8175)  Acc@task: 70.8333 (72.6190)  time: 0.1778  data: 0.0012  max mem: 1379\n",
            "Test: [Task 9]  [30/42]  eta: 0:00:02  Loss: 0.6265 (0.6820)  Acc@1: 83.3333 (80.6452)  Acc@5: 100.0000 (97.3118)  Acc@task: 70.8333 (72.0430)  time: 0.1782  data: 0.0009  max mem: 1379\n",
            "Test: [Task 9]  [40/42]  eta: 0:00:00  Loss: 0.5973 (0.6468)  Acc@1: 79.1667 (81.1992)  Acc@5: 95.8333 (97.2561)  Acc@task: 75.0000 (72.7642)  time: 0.1789  data: 0.0005  max mem: 1379\n",
            "Test: [Task 9]  [41/42]  eta: 0:00:00  Loss: 0.5973 (0.6380)  Acc@1: 79.1667 (81.3000)  Acc@5: 95.8333 (97.3000)  Acc@task: 75.0000 (73.1000)  time: 0.1757  data: 0.0005  max mem: 1379\n",
            "Test: [Task 9] Total time: 0:00:07 (0.1903 s / it)\n",
            "* Acc@task 73.100 Acc@1 81.300 Acc@5 97.300 loss 0.638\n",
            "[Average accuracy till task9]\tAcc@task: 77.4222\tAcc@1: 82.8111\tAcc@5: 96.8222\tLoss: 0.6588\tForgetting: 1.4875\tBackward: 2.1625\n",
            "Loading checkpoint from: ./output/cifar100_full_dino_1epoch_100pct/checkpoint/task10_checkpoint.pth\n",
            "/content/drive/MyDrive/HiDePrompt/HiDePrompt/engines/hide_promtp_wtp_and_tap_engine.py:540: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  loss = torch.nn.functional.cross_entropy(sim, torch.range(0, sim.shape[0] - 1).long().to(device))\n",
            "Train: Epoch[1/1]  [  0/209]  eta: 0:02:54  Lr: 0.002812  Loss: 4.6734  Acc@1: 4.1667 (4.1667)  Acc@5: 62.5000 (62.5000)  time: 0.8361  data: 0.5669  max mem: 1379\n",
            "Train: Epoch[1/1]  [ 10/209]  eta: 0:01:06  Lr: 0.002812  Loss: 3.3063  Acc@1: 12.5000 (14.3939)  Acc@5: 58.3333 (56.8182)  time: 0.3336  data: 0.0519  max mem: 1381\n",
            "Train: Epoch[1/1]  [ 20/209]  eta: 0:00:58  Lr: 0.002812  Loss: 3.3501  Acc@1: 16.6667 (19.6429)  Acc@5: 66.6667 (65.8730)  time: 0.2834  data: 0.0004  max mem: 1381\n",
            "Train: Epoch[1/1]  [ 30/209]  eta: 0:00:54  Lr: 0.002812  Loss: 3.6645  Acc@1: 29.1667 (26.2097)  Acc@5: 75.0000 (70.2957)  time: 0.2845  data: 0.0007  max mem: 1381\n",
            "Train: Epoch[1/1]  [ 40/209]  eta: 0:00:50  Lr: 0.002812  Loss: 2.4409  Acc@1: 50.0000 (33.1301)  Acc@5: 87.5000 (75.1016)  time: 0.2850  data: 0.0010  max mem: 1381\n",
            "Train: Epoch[1/1]  [ 50/209]  eta: 0:00:46  Lr: 0.002812  Loss: 1.8920  Acc@1: 54.1667 (38.2353)  Acc@5: 91.6667 (78.8399)  time: 0.2836  data: 0.0006  max mem: 1381\n",
            "Train: Epoch[1/1]  [ 60/209]  eta: 0:00:43  Lr: 0.002812  Loss: 2.1327  Acc@1: 66.6667 (43.3743)  Acc@5: 95.8333 (81.4208)  time: 0.2832  data: 0.0004  max mem: 1381\n",
            "Train: Epoch[1/1]  [ 70/209]  eta: 0:00:40  Lr: 0.002812  Loss: 1.6214  Acc@1: 75.0000 (47.5352)  Acc@5: 91.6667 (83.3920)  time: 0.2835  data: 0.0004  max mem: 1381\n",
            "Train: Epoch[1/1]  [ 80/209]  eta: 0:00:37  Lr: 0.002812  Loss: 1.7903  Acc@1: 75.0000 (51.3889)  Acc@5: 95.8333 (84.9794)  time: 0.2842  data: 0.0004  max mem: 1381\n",
            "Train: Epoch[1/1]  [ 90/209]  eta: 0:00:34  Lr: 0.002812  Loss: 1.5790  Acc@1: 79.1667 (54.5330)  Acc@5: 95.8333 (86.2180)  time: 0.2849  data: 0.0006  max mem: 1381\n",
            "Train: Epoch[1/1]  [100/209]  eta: 0:00:31  Lr: 0.002812  Loss: 1.7139  Acc@1: 79.1667 (57.0957)  Acc@5: 95.8333 (87.2937)  time: 0.2842  data: 0.0005  max mem: 1381\n",
            "Train: Epoch[1/1]  [110/209]  eta: 0:00:28  Lr: 0.002812  Loss: 1.4149  Acc@1: 79.1667 (59.0465)  Acc@5: 95.8333 (88.1381)  time: 0.2832  data: 0.0005  max mem: 1381\n",
            "Train: Epoch[1/1]  [120/209]  eta: 0:00:25  Lr: 0.002812  Loss: 1.9876  Acc@1: 79.1667 (61.0537)  Acc@5: 100.0000 (88.9807)  time: 0.2836  data: 0.0005  max mem: 1381\n",
            "Train: Epoch[1/1]  [130/209]  eta: 0:00:22  Lr: 0.002812  Loss: 1.4043  Acc@1: 87.5000 (63.0725)  Acc@5: 100.0000 (89.6310)  time: 0.2848  data: 0.0009  max mem: 1381\n",
            "Train: Epoch[1/1]  [140/209]  eta: 0:00:19  Lr: 0.002812  Loss: 1.3364  Acc@1: 87.5000 (64.5095)  Acc@5: 100.0000 (90.2778)  time: 0.2848  data: 0.0008  max mem: 1381\n",
            "Train: Epoch[1/1]  [150/209]  eta: 0:00:16  Lr: 0.002812  Loss: 1.6296  Acc@1: 83.3333 (65.9768)  Acc@5: 100.0000 (90.7837)  time: 0.2838  data: 0.0003  max mem: 1381\n",
            "Train: Epoch[1/1]  [160/209]  eta: 0:00:14  Lr: 0.002812  Loss: 1.1899  Acc@1: 87.5000 (67.5207)  Acc@5: 100.0000 (91.2526)  time: 0.2838  data: 0.0004  max mem: 1381\n",
            "Train: Epoch[1/1]  [170/209]  eta: 0:00:11  Lr: 0.002812  Loss: 1.4511  Acc@1: 87.5000 (68.5429)  Acc@5: 100.0000 (91.5936)  time: 0.2841  data: 0.0007  max mem: 1381\n",
            "Train: Epoch[1/1]  [180/209]  eta: 0:00:08  Lr: 0.002812  Loss: 1.3015  Acc@1: 87.5000 (69.7053)  Acc@5: 100.0000 (92.0350)  time: 0.2851  data: 0.0010  max mem: 1381\n",
            "Train: Epoch[1/1]  [190/209]  eta: 0:00:05  Lr: 0.002812  Loss: 1.2837  Acc@1: 87.5000 (70.6588)  Acc@5: 100.0000 (92.4302)  time: 0.2851  data: 0.0007  max mem: 1381\n",
            "Train: Epoch[1/1]  [200/209]  eta: 0:00:02  Lr: 0.002812  Loss: 1.3022  Acc@1: 87.5000 (71.3516)  Acc@5: 100.0000 (92.7653)  time: 0.2838  data: 0.0004  max mem: 1381\n",
            "Train: Epoch[1/1]  [208/209]  eta: 0:00:00  Lr: 0.002812  Loss: 1.7396  Acc@1: 87.5000 (71.9600)  Acc@5: 100.0000 (92.9400)  time: 0.2752  data: 0.0003  max mem: 1381\n",
            "Train: Epoch[1/1] Total time: 0:00:59 (0.2865 s / it)\n",
            "Averaged stats: Lr: 0.002812  Loss: 1.7396  Acc@1: 87.5000 (71.9600)  Acc@5: 100.0000 (92.9400)\n",
            "torch.Size([5, 2, 5, 6, 64])\n",
            "torch.Size([5, 2, 1, 5, 6, 64])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:25  Loss: 0.4994 (0.4994)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.6031  data: 0.4088  max mem: 1381\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:06  Loss: 0.4994 (0.4869)  Acc@1: 83.3333 (86.7424)  Acc@5: 100.0000 (98.4848)  Acc@task: 75.0000 (79.1667)  time: 0.2147  data: 0.0375  max mem: 1381\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.4088 (0.5276)  Acc@1: 83.3333 (86.3095)  Acc@5: 95.8333 (97.0238)  Acc@task: 79.1667 (79.9603)  time: 0.1763  data: 0.0004  max mem: 1381\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.3615 (0.4973)  Acc@1: 87.5000 (86.8280)  Acc@5: 100.0000 (97.4462)  Acc@task: 83.3333 (79.8387)  time: 0.1779  data: 0.0004  max mem: 1381\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.3615 (0.4771)  Acc@1: 91.6667 (88.1098)  Acc@5: 100.0000 (97.6626)  Acc@task: 83.3333 (80.5894)  time: 0.1790  data: 0.0003  max mem: 1381\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.3180 (0.4684)  Acc@1: 91.6667 (88.2000)  Acc@5: 100.0000 (97.7000)  Acc@task: 83.3333 (80.9000)  time: 0.1756  data: 0.0003  max mem: 1381\n",
            "Test: [Task 1] Total time: 0:00:07 (0.1904 s / it)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. \n",
            "  warnings.warn(  # warn only once\n",
            "* Acc@task 80.900 Acc@1 88.200 Acc@5 97.700 loss 0.468\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:31  Loss: 0.9679 (0.9679)  Acc@1: 79.1667 (79.1667)  Acc@5: 91.6667 (91.6667)  Acc@task: 70.8333 (70.8333)  time: 0.7488  data: 0.5490  max mem: 1381\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:07  Loss: 0.7078 (0.6685)  Acc@1: 79.1667 (81.4394)  Acc@5: 100.0000 (97.7273)  Acc@task: 79.1667 (78.7879)  time: 0.2302  data: 0.0507  max mem: 1381\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.7078 (0.7413)  Acc@1: 83.3333 (81.9444)  Acc@5: 95.8333 (96.0317)  Acc@task: 75.0000 (75.5952)  time: 0.1777  data: 0.0007  max mem: 1381\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.7216 (0.7167)  Acc@1: 83.3333 (82.5269)  Acc@5: 95.8333 (96.5054)  Acc@task: 75.0000 (75.8065)  time: 0.1781  data: 0.0004  max mem: 1381\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.5618 (0.6610)  Acc@1: 87.5000 (83.5366)  Acc@5: 100.0000 (96.9512)  Acc@task: 75.0000 (75.8130)  time: 0.1796  data: 0.0003  max mem: 1381\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.5136 (0.6546)  Acc@1: 87.5000 (83.6000)  Acc@5: 100.0000 (97.0000)  Acc@task: 75.0000 (75.8000)  time: 0.1762  data: 0.0003  max mem: 1381\n",
            "Test: [Task 2] Total time: 0:00:08 (0.1936 s / it)\n",
            "* Acc@task 75.800 Acc@1 83.600 Acc@5 97.000 loss 0.655\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:25  Loss: 0.0725 (0.0725)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  Acc@task: 87.5000 (87.5000)  time: 0.5985  data: 0.4131  max mem: 1381\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 0.7344 (0.6265)  Acc@1: 83.3333 (85.2273)  Acc@5: 95.8333 (97.3485)  Acc@task: 83.3333 (81.0606)  time: 0.2158  data: 0.0381  max mem: 1381\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.5297 (0.5555)  Acc@1: 87.5000 (85.9127)  Acc@5: 95.8333 (97.2222)  Acc@task: 79.1667 (81.9444)  time: 0.1779  data: 0.0007  max mem: 1381\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.4105 (0.5229)  Acc@1: 87.5000 (86.4247)  Acc@5: 95.8333 (97.3118)  Acc@task: 79.1667 (81.1828)  time: 0.1786  data: 0.0013  max mem: 1381\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.4311 (0.5415)  Acc@1: 87.5000 (86.3821)  Acc@5: 95.8333 (97.1545)  Acc@task: 79.1667 (81.4024)  time: 0.1795  data: 0.0013  max mem: 1381\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.4311 (0.5431)  Acc@1: 87.5000 (86.2000)  Acc@5: 95.8333 (97.2000)  Acc@task: 79.1667 (81.2000)  time: 0.1762  data: 0.0012  max mem: 1381\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1899 s / it)\n",
            "* Acc@task 81.200 Acc@1 86.200 Acc@5 97.200 loss 0.543\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:20  Loss: 0.7117 (0.7117)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 62.5000 (62.5000)  time: 0.4825  data: 0.3154  max mem: 1381\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:06  Loss: 0.6350 (0.5824)  Acc@1: 83.3333 (84.8485)  Acc@5: 95.8333 (96.9697)  Acc@task: 70.8333 (68.1818)  time: 0.2073  data: 0.0294  max mem: 1381\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.5745 (0.6297)  Acc@1: 83.3333 (84.1270)  Acc@5: 95.8333 (96.2302)  Acc@task: 70.8333 (71.4286)  time: 0.1788  data: 0.0006  max mem: 1381\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.5490 (0.5912)  Acc@1: 79.1667 (84.9462)  Acc@5: 95.8333 (96.5054)  Acc@task: 75.0000 (73.6559)  time: 0.1789  data: 0.0004  max mem: 1381\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.5460 (0.6211)  Acc@1: 87.5000 (84.8577)  Acc@5: 95.8333 (96.1382)  Acc@task: 75.0000 (73.6789)  time: 0.1800  data: 0.0004  max mem: 1381\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.4316 (0.6069)  Acc@1: 87.5000 (85.1000)  Acc@5: 95.8333 (96.2000)  Acc@task: 75.0000 (74.0000)  time: 0.1766  data: 0.0004  max mem: 1381\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1878 s / it)\n",
            "* Acc@task 74.000 Acc@1 85.100 Acc@5 96.200 loss 0.607\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:19  Loss: 0.1353 (0.1353)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  Acc@task: 79.1667 (79.1667)  time: 0.4623  data: 0.2957  max mem: 1381\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 0.4402 (0.4095)  Acc@1: 87.5000 (88.6364)  Acc@5: 100.0000 (99.2424)  Acc@task: 79.1667 (79.1667)  time: 0.2132  data: 0.0391  max mem: 1381\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.4682 (0.4878)  Acc@1: 83.3333 (86.5079)  Acc@5: 100.0000 (97.8175)  Acc@task: 75.0000 (77.5794)  time: 0.1841  data: 0.0073  max mem: 1381\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.4635 (0.4800)  Acc@1: 83.3333 (85.8871)  Acc@5: 100.0000 (98.2527)  Acc@task: 79.1667 (78.0914)  time: 0.1793  data: 0.0010  max mem: 1381\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.4907 (0.5326)  Acc@1: 83.3333 (85.4675)  Acc@5: 100.0000 (98.0691)  Acc@task: 75.0000 (76.6260)  time: 0.1789  data: 0.0006  max mem: 1381\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.5387 (0.5458)  Acc@1: 83.3333 (85.2000)  Acc@5: 100.0000 (97.9000)  Acc@task: 75.0000 (76.6000)  time: 0.1760  data: 0.0005  max mem: 1381\n",
            "Test: [Task 5] Total time: 0:00:07 (0.1897 s / it)\n",
            "* Acc@task 76.600 Acc@1 85.200 Acc@5 97.900 loss 0.546\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:24  Loss: 0.4799 (0.4799)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)  Acc@task: 87.5000 (87.5000)  time: 0.5771  data: 0.4023  max mem: 1381\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:06  Loss: 0.7302 (0.7078)  Acc@1: 83.3333 (84.8485)  Acc@5: 95.8333 (96.2121)  Acc@task: 79.1667 (79.9242)  time: 0.2147  data: 0.0373  max mem: 1381\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:04  Loss: 0.7846 (0.7780)  Acc@1: 83.3333 (82.3413)  Acc@5: 95.8333 (96.2302)  Acc@task: 79.1667 (78.1746)  time: 0.1777  data: 0.0006  max mem: 1381\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 0.7846 (0.7776)  Acc@1: 79.1667 (80.9140)  Acc@5: 95.8333 (96.5054)  Acc@task: 79.1667 (78.2258)  time: 0.1777  data: 0.0003  max mem: 1381\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 0.7478 (0.8209)  Acc@1: 79.1667 (79.8781)  Acc@5: 95.8333 (96.3415)  Acc@task: 79.1667 (77.4390)  time: 0.1784  data: 0.0003  max mem: 1381\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 0.6957 (0.8179)  Acc@1: 79.1667 (79.9000)  Acc@5: 95.8333 (96.3000)  Acc@task: 79.1667 (77.6000)  time: 0.1745  data: 0.0003  max mem: 1381\n",
            "Test: [Task 6] Total time: 0:00:08 (0.1906 s / it)\n",
            "* Acc@task 77.600 Acc@1 79.900 Acc@5 96.300 loss 0.818\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:38  Loss: 0.7139 (0.7139)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 66.6667 (66.6667)  time: 0.9204  data: 0.7298  max mem: 1381\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:07  Loss: 0.7139 (0.6597)  Acc@1: 79.1667 (79.5455)  Acc@5: 100.0000 (97.3485)  Acc@task: 79.1667 (79.1667)  time: 0.2454  data: 0.0688  max mem: 1381\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:04  Loss: 0.7790 (0.7705)  Acc@1: 75.0000 (77.9762)  Acc@5: 95.8333 (96.8254)  Acc@task: 75.0000 (78.5714)  time: 0.1773  data: 0.0017  max mem: 1381\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:02  Loss: 0.8337 (0.8241)  Acc@1: 75.0000 (77.8226)  Acc@5: 95.8333 (95.9677)  Acc@task: 75.0000 (77.2849)  time: 0.1775  data: 0.0006  max mem: 1381\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 0.8007 (0.8154)  Acc@1: 75.0000 (77.2358)  Acc@5: 95.8333 (96.4431)  Acc@task: 75.0000 (77.1341)  time: 0.1781  data: 0.0003  max mem: 1381\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 0.8007 (0.8048)  Acc@1: 75.0000 (77.5000)  Acc@5: 95.8333 (96.5000)  Acc@task: 75.0000 (77.2000)  time: 0.1748  data: 0.0003  max mem: 1381\n",
            "Test: [Task 7] Total time: 0:00:08 (0.1966 s / it)\n",
            "* Acc@task 77.200 Acc@1 77.500 Acc@5 96.500 loss 0.805\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:20  Loss: 0.9358 (0.9358)  Acc@1: 75.0000 (75.0000)  Acc@5: 100.0000 (100.0000)  Acc@task: 70.8333 (70.8333)  time: 0.4880  data: 0.3177  max mem: 1381\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:06  Loss: 0.8029 (0.8936)  Acc@1: 75.0000 (77.6515)  Acc@5: 95.8333 (95.8333)  Acc@task: 75.0000 (75.3788)  time: 0.2077  data: 0.0317  max mem: 1381\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:04  Loss: 0.8002 (0.8820)  Acc@1: 79.1667 (78.3730)  Acc@5: 95.8333 (95.4365)  Acc@task: 70.8333 (72.8175)  time: 0.1781  data: 0.0018  max mem: 1381\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:02  Loss: 0.6885 (0.8332)  Acc@1: 79.1667 (79.3011)  Acc@5: 95.8333 (95.6989)  Acc@task: 70.8333 (72.9839)  time: 0.1773  data: 0.0007  max mem: 1381\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 0.6635 (0.8691)  Acc@1: 79.1667 (78.6585)  Acc@5: 95.8333 (95.2236)  Acc@task: 70.8333 (71.9512)  time: 0.1781  data: 0.0007  max mem: 1381\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 0.6828 (0.8661)  Acc@1: 79.1667 (78.6000)  Acc@5: 95.8333 (95.1000)  Acc@task: 70.8333 (72.2000)  time: 0.1744  data: 0.0006  max mem: 1381\n",
            "Test: [Task 8] Total time: 0:00:07 (0.1877 s / it)\n",
            "* Acc@task 72.200 Acc@1 78.600 Acc@5 95.100 loss 0.866\n",
            "Test: [Task 9]  [ 0/42]  eta: 0:00:27  Loss: 0.5752 (0.5752)  Acc@1: 91.6667 (91.6667)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (83.3333)  time: 0.6665  data: 0.4659  max mem: 1381\n",
            "Test: [Task 9]  [10/42]  eta: 0:00:07  Loss: 0.6339 (0.6553)  Acc@1: 79.1667 (80.3030)  Acc@5: 95.8333 (97.7273)  Acc@task: 75.0000 (74.6212)  time: 0.2218  data: 0.0428  max mem: 1381\n",
            "Test: [Task 9]  [20/42]  eta: 0:00:04  Loss: 0.4695 (0.5987)  Acc@1: 83.3333 (82.3413)  Acc@5: 100.0000 (98.0159)  Acc@task: 75.0000 (76.1905)  time: 0.1764  data: 0.0004  max mem: 1381\n",
            "Test: [Task 9]  [30/42]  eta: 0:00:02  Loss: 0.5673 (0.6366)  Acc@1: 79.1667 (81.5860)  Acc@5: 100.0000 (97.7151)  Acc@task: 75.0000 (75.5376)  time: 0.1770  data: 0.0005  max mem: 1381\n",
            "Test: [Task 9]  [40/42]  eta: 0:00:00  Loss: 0.5673 (0.5994)  Acc@1: 83.3333 (82.5203)  Acc@5: 95.8333 (97.6626)  Acc@task: 79.1667 (76.5244)  time: 0.1784  data: 0.0004  max mem: 1381\n",
            "Test: [Task 9]  [41/42]  eta: 0:00:00  Loss: 0.5673 (0.5916)  Acc@1: 83.3333 (82.6000)  Acc@5: 95.8333 (97.7000)  Acc@task: 79.1667 (76.8000)  time: 0.1746  data: 0.0004  max mem: 1381\n",
            "Test: [Task 9] Total time: 0:00:07 (0.1900 s / it)\n",
            "* Acc@task 76.800 Acc@1 82.600 Acc@5 97.700 loss 0.592\n",
            "Test: [Task 10]  [ 0/42]  eta: 0:00:19  Loss: 4.6188 (4.6188)  Acc@1: 4.1667 (4.1667)  Acc@5: 83.3333 (83.3333)  Acc@task: 70.8333 (70.8333)  time: 0.4750  data: 0.3012  max mem: 1381\n",
            "Test: [Task 10]  [10/42]  eta: 0:00:06  Loss: 4.7183 (4.6590)  Acc@1: 8.3333 (9.0909)  Acc@5: 70.8333 (72.7273)  Acc@task: 66.6667 (66.2879)  time: 0.2070  data: 0.0329  max mem: 1381\n",
            "Test: [Task 10]  [20/42]  eta: 0:00:04  Loss: 4.7183 (4.6091)  Acc@1: 8.3333 (9.7222)  Acc@5: 70.8333 (72.8175)  Acc@task: 62.5000 (65.8730)  time: 0.1787  data: 0.0036  max mem: 1381\n",
            "Test: [Task 10]  [30/42]  eta: 0:00:02  Loss: 4.6270 (4.6338)  Acc@1: 8.3333 (9.2742)  Acc@5: 75.0000 (73.2527)  Acc@task: 62.5000 (65.4570)  time: 0.1771  data: 0.0009  max mem: 1381\n",
            "Test: [Task 10]  [40/42]  eta: 0:00:00  Loss: 4.6357 (4.6271)  Acc@1: 8.3333 (9.4512)  Acc@5: 75.0000 (72.6626)  Acc@task: 62.5000 (65.0407)  time: 0.1772  data: 0.0005  max mem: 1381\n",
            "Test: [Task 10]  [41/42]  eta: 0:00:00  Loss: 4.6847 (4.6298)  Acc@1: 8.3333 (9.4000)  Acc@5: 75.0000 (72.8000)  Acc@task: 62.5000 (65.2000)  time: 0.1746  data: 0.0005  max mem: 1381\n",
            "Test: [Task 10] Total time: 0:00:07 (0.1864 s / it)\n",
            "* Acc@task 65.200 Acc@1 9.400 Acc@5 72.800 loss 4.630\n",
            "[Average accuracy till task10]\tAcc@task: 75.7500\tAcc@1: 75.6300\tAcc@5: 94.4400\tLoss: 1.0529\tForgetting: 1.5111\tBackward: 53.5333\n",
            "torch.Size([120000, 384])\n",
            "Averaged stats: Lr: 0.005000  Loss: 0.0800  Acc@1: 98.3333 (96.6204)  Acc@5: 100.0000 (99.8796)\n",
            "Test: [Task 1]  [ 0/42]  eta: 0:00:35  Loss: 0.5476 (0.5476)  Acc@1: 79.1667 (79.1667)  Acc@5: 100.0000 (100.0000)  Acc@task: 83.3333 (83.3333)  time: 0.8415  data: 0.6191  max mem: 1381\n",
            "Test: [Task 1]  [10/42]  eta: 0:00:07  Loss: 0.5680 (0.5510)  Acc@1: 83.3333 (82.9545)  Acc@5: 100.0000 (98.4848)  Acc@task: 75.0000 (79.1667)  time: 0.2371  data: 0.0574  max mem: 1381\n",
            "Test: [Task 1]  [20/42]  eta: 0:00:04  Loss: 0.5680 (0.5857)  Acc@1: 83.3333 (83.9286)  Acc@5: 95.8333 (97.4206)  Acc@task: 79.1667 (79.9603)  time: 0.1762  data: 0.0009  max mem: 1381\n",
            "Test: [Task 1]  [30/42]  eta: 0:00:02  Loss: 0.4029 (0.5490)  Acc@1: 87.5000 (85.0806)  Acc@5: 95.8333 (97.4462)  Acc@task: 83.3333 (79.8387)  time: 0.1763  data: 0.0005  max mem: 1381\n",
            "Test: [Task 1]  [40/42]  eta: 0:00:00  Loss: 0.3615 (0.5309)  Acc@1: 91.6667 (85.7724)  Acc@5: 100.0000 (97.5610)  Acc@task: 83.3333 (80.5894)  time: 0.1773  data: 0.0005  max mem: 1381\n",
            "Test: [Task 1]  [41/42]  eta: 0:00:00  Loss: 0.3225 (0.5223)  Acc@1: 91.6667 (85.9000)  Acc@5: 100.0000 (97.6000)  Acc@task: 83.3333 (80.9000)  time: 0.1739  data: 0.0005  max mem: 1381\n",
            "Test: [Task 1] Total time: 0:00:08 (0.1951 s / it)\n",
            "* Acc@task 80.900 Acc@1 85.900 Acc@5 97.600 loss 0.522\n",
            "Test: [Task 2]  [ 0/42]  eta: 0:00:39  Loss: 0.9799 (0.9799)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.6667 (91.6667)  Acc@task: 70.8333 (70.8333)  time: 0.9435  data: 0.7604  max mem: 1381\n",
            "Test: [Task 2]  [10/42]  eta: 0:00:07  Loss: 0.4649 (0.6438)  Acc@1: 83.3333 (81.8182)  Acc@5: 100.0000 (98.1061)  Acc@task: 79.1667 (78.7879)  time: 0.2459  data: 0.0701  max mem: 1381\n",
            "Test: [Task 2]  [20/42]  eta: 0:00:04  Loss: 0.5028 (0.7282)  Acc@1: 83.3333 (82.1429)  Acc@5: 95.8333 (96.6270)  Acc@task: 75.0000 (75.5952)  time: 0.1762  data: 0.0007  max mem: 1381\n",
            "Test: [Task 2]  [30/42]  eta: 0:00:02  Loss: 0.7098 (0.7022)  Acc@1: 83.3333 (83.0645)  Acc@5: 95.8333 (96.5054)  Acc@task: 75.0000 (75.8065)  time: 0.1773  data: 0.0004  max mem: 1381\n",
            "Test: [Task 2]  [40/42]  eta: 0:00:00  Loss: 0.5667 (0.6528)  Acc@1: 87.5000 (83.7398)  Acc@5: 95.8333 (96.9512)  Acc@task: 75.0000 (75.8130)  time: 0.1781  data: 0.0003  max mem: 1381\n",
            "Test: [Task 2]  [41/42]  eta: 0:00:00  Loss: 0.5167 (0.6460)  Acc@1: 87.5000 (83.7000)  Acc@5: 100.0000 (97.0000)  Acc@task: 75.0000 (75.8000)  time: 0.1747  data: 0.0003  max mem: 1381\n",
            "Test: [Task 2] Total time: 0:00:08 (0.1967 s / it)\n",
            "* Acc@task 75.800 Acc@1 83.700 Acc@5 97.000 loss 0.646\n",
            "Test: [Task 3]  [ 0/42]  eta: 0:00:23  Loss: 0.0833 (0.0833)  Acc@1: 95.8333 (95.8333)  Acc@5: 100.0000 (100.0000)  Acc@task: 87.5000 (87.5000)  time: 0.5633  data: 0.3705  max mem: 1381\n",
            "Test: [Task 3]  [10/42]  eta: 0:00:06  Loss: 0.7280 (0.6605)  Acc@1: 83.3333 (82.9545)  Acc@5: 95.8333 (96.9697)  Acc@task: 83.3333 (81.0606)  time: 0.2123  data: 0.0340  max mem: 1381\n",
            "Test: [Task 3]  [20/42]  eta: 0:00:04  Loss: 0.6146 (0.5984)  Acc@1: 83.3333 (84.3254)  Acc@5: 95.8333 (96.8254)  Acc@task: 79.1667 (81.9444)  time: 0.1774  data: 0.0005  max mem: 1381\n",
            "Test: [Task 3]  [30/42]  eta: 0:00:02  Loss: 0.4322 (0.5839)  Acc@1: 83.3333 (84.0054)  Acc@5: 95.8333 (96.9086)  Acc@task: 79.1667 (81.1828)  time: 0.1783  data: 0.0009  max mem: 1381\n",
            "Test: [Task 3]  [40/42]  eta: 0:00:00  Loss: 0.5178 (0.6166)  Acc@1: 83.3333 (83.7398)  Acc@5: 95.8333 (96.6463)  Acc@task: 79.1667 (81.4024)  time: 0.1789  data: 0.0008  max mem: 1381\n",
            "Test: [Task 3]  [41/42]  eta: 0:00:00  Loss: 0.5178 (0.6195)  Acc@1: 83.3333 (83.4000)  Acc@5: 95.8333 (96.5000)  Acc@task: 79.1667 (81.2000)  time: 0.1755  data: 0.0008  max mem: 1381\n",
            "Test: [Task 3] Total time: 0:00:07 (0.1885 s / it)\n",
            "* Acc@task 81.200 Acc@1 83.400 Acc@5 96.500 loss 0.619\n",
            "Test: [Task 4]  [ 0/42]  eta: 0:00:24  Loss: 0.8084 (0.8084)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 62.5000 (62.5000)  time: 0.5852  data: 0.4046  max mem: 1381\n",
            "Test: [Task 4]  [10/42]  eta: 0:00:06  Loss: 0.6146 (0.6066)  Acc@1: 83.3333 (82.9545)  Acc@5: 95.8333 (96.9697)  Acc@task: 70.8333 (68.1818)  time: 0.2148  data: 0.0377  max mem: 1381\n",
            "Test: [Task 4]  [20/42]  eta: 0:00:04  Loss: 0.6036 (0.6259)  Acc@1: 83.3333 (83.1349)  Acc@5: 95.8333 (96.8254)  Acc@task: 70.8333 (71.4286)  time: 0.1777  data: 0.0007  max mem: 1381\n",
            "Test: [Task 4]  [30/42]  eta: 0:00:02  Loss: 0.5000 (0.5807)  Acc@1: 87.5000 (84.4086)  Acc@5: 95.8333 (97.0430)  Acc@task: 75.0000 (73.6559)  time: 0.1787  data: 0.0005  max mem: 1381\n",
            "Test: [Task 4]  [40/42]  eta: 0:00:00  Loss: 0.5000 (0.6038)  Acc@1: 87.5000 (84.6545)  Acc@5: 95.8333 (96.6463)  Acc@task: 75.0000 (73.6789)  time: 0.1794  data: 0.0005  max mem: 1381\n",
            "Test: [Task 4]  [41/42]  eta: 0:00:00  Loss: 0.4937 (0.5899)  Acc@1: 87.5000 (84.9000)  Acc@5: 95.8333 (96.7000)  Acc@task: 75.0000 (74.0000)  time: 0.1759  data: 0.0004  max mem: 1381\n",
            "Test: [Task 4] Total time: 0:00:07 (0.1894 s / it)\n",
            "* Acc@task 74.000 Acc@1 84.900 Acc@5 96.700 loss 0.590\n",
            "Test: [Task 5]  [ 0/42]  eta: 0:00:25  Loss: 0.1714 (0.1714)  Acc@1: 91.6667 (91.6667)  Acc@5: 100.0000 (100.0000)  Acc@task: 79.1667 (79.1667)  time: 0.6035  data: 0.4172  max mem: 1381\n",
            "Test: [Task 5]  [10/42]  eta: 0:00:06  Loss: 0.4824 (0.4753)  Acc@1: 87.5000 (87.1212)  Acc@5: 100.0000 (98.4848)  Acc@task: 79.1667 (79.1667)  time: 0.2160  data: 0.0386  max mem: 1381\n",
            "Test: [Task 5]  [20/42]  eta: 0:00:04  Loss: 0.5827 (0.5836)  Acc@1: 83.3333 (83.3333)  Acc@5: 95.8333 (96.8254)  Acc@task: 75.0000 (77.5794)  time: 0.1780  data: 0.0015  max mem: 1381\n",
            "Test: [Task 5]  [30/42]  eta: 0:00:02  Loss: 0.5725 (0.5603)  Acc@1: 83.3333 (84.4086)  Acc@5: 95.8333 (97.4462)  Acc@task: 79.1667 (78.0914)  time: 0.1787  data: 0.0016  max mem: 1381\n",
            "Test: [Task 5]  [40/42]  eta: 0:00:00  Loss: 0.5306 (0.6107)  Acc@1: 83.3333 (84.1463)  Acc@5: 95.8333 (97.0528)  Acc@task: 75.0000 (76.6260)  time: 0.1791  data: 0.0007  max mem: 1381\n",
            "Test: [Task 5]  [41/42]  eta: 0:00:00  Loss: 0.5838 (0.6278)  Acc@1: 83.3333 (83.9000)  Acc@5: 95.8333 (96.7000)  Acc@task: 75.0000 (76.6000)  time: 0.1759  data: 0.0006  max mem: 1381\n",
            "Test: [Task 5] Total time: 0:00:07 (0.1899 s / it)\n",
            "* Acc@task 76.600 Acc@1 83.900 Acc@5 96.700 loss 0.628\n",
            "Test: [Task 6]  [ 0/42]  eta: 0:00:26  Loss: 0.4298 (0.4298)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 87.5000 (87.5000)  time: 0.6407  data: 0.4672  max mem: 1381\n",
            "Test: [Task 6]  [10/42]  eta: 0:00:07  Loss: 0.5995 (0.6298)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (98.4848)  Acc@task: 79.1667 (79.9242)  time: 0.2212  data: 0.0429  max mem: 1381\n",
            "Test: [Task 6]  [20/42]  eta: 0:00:04  Loss: 0.7780 (0.7557)  Acc@1: 83.3333 (81.5476)  Acc@5: 95.8333 (97.2222)  Acc@task: 79.1667 (78.1746)  time: 0.1779  data: 0.0005  max mem: 1381\n",
            "Test: [Task 6]  [30/42]  eta: 0:00:02  Loss: 0.8439 (0.7485)  Acc@1: 83.3333 (81.1828)  Acc@5: 95.8333 (97.4462)  Acc@task: 79.1667 (78.2258)  time: 0.1776  data: 0.0005  max mem: 1381\n",
            "Test: [Task 6]  [40/42]  eta: 0:00:00  Loss: 0.6949 (0.7889)  Acc@1: 79.1667 (79.9797)  Acc@5: 95.8333 (97.1545)  Acc@task: 79.1667 (77.4390)  time: 0.1789  data: 0.0004  max mem: 1381\n",
            "Test: [Task 6]  [41/42]  eta: 0:00:00  Loss: 0.6938 (0.7854)  Acc@1: 79.1667 (79.9000)  Acc@5: 95.8333 (97.1000)  Acc@task: 79.1667 (77.6000)  time: 0.1753  data: 0.0004  max mem: 1381\n",
            "Test: [Task 6] Total time: 0:00:08 (0.1913 s / it)\n",
            "* Acc@task 77.600 Acc@1 79.900 Acc@5 97.100 loss 0.785\n",
            "Test: [Task 7]  [ 0/42]  eta: 0:00:37  Loss: 0.6812 (0.6812)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (95.8333)  Acc@task: 66.6667 (66.6667)  time: 0.8892  data: 0.6969  max mem: 1381\n",
            "Test: [Task 7]  [10/42]  eta: 0:00:07  Loss: 0.6812 (0.6699)  Acc@1: 79.1667 (79.9242)  Acc@5: 100.0000 (96.9697)  Acc@task: 79.1667 (79.1667)  time: 0.2422  data: 0.0647  max mem: 1381\n",
            "Test: [Task 7]  [20/42]  eta: 0:00:04  Loss: 0.8366 (0.7882)  Acc@1: 79.1667 (78.9683)  Acc@5: 95.8333 (96.0317)  Acc@task: 75.0000 (78.5714)  time: 0.1776  data: 0.0012  max mem: 1381\n",
            "Test: [Task 7]  [30/42]  eta: 0:00:02  Loss: 0.9628 (0.8472)  Acc@1: 75.0000 (78.3602)  Acc@5: 95.8333 (95.4301)  Acc@task: 75.0000 (77.2849)  time: 0.1784  data: 0.0007  max mem: 1381\n",
            "Test: [Task 7]  [40/42]  eta: 0:00:00  Loss: 0.9628 (0.8453)  Acc@1: 75.0000 (77.7439)  Acc@5: 95.8333 (95.8333)  Acc@task: 75.0000 (77.1341)  time: 0.1786  data: 0.0004  max mem: 1381\n",
            "Test: [Task 7]  [41/42]  eta: 0:00:00  Loss: 0.9628 (0.8357)  Acc@1: 75.0000 (78.0000)  Acc@5: 95.8333 (95.9000)  Acc@task: 75.0000 (77.2000)  time: 0.1751  data: 0.0004  max mem: 1381\n",
            "Test: [Task 7] Total time: 0:00:08 (0.1961 s / it)\n",
            "* Acc@task 77.200 Acc@1 78.000 Acc@5 95.900 loss 0.836\n",
            "Test: [Task 8]  [ 0/42]  eta: 0:00:20  Loss: 1.4201 (1.4201)  Acc@1: 75.0000 (75.0000)  Acc@5: 87.5000 (87.5000)  Acc@task: 70.8333 (70.8333)  time: 0.4785  data: 0.2996  max mem: 1381\n",
            "Test: [Task 8]  [10/42]  eta: 0:00:06  Loss: 1.1879 (1.1959)  Acc@1: 75.0000 (74.2424)  Acc@5: 95.8333 (93.1818)  Acc@task: 75.0000 (75.3788)  time: 0.2047  data: 0.0283  max mem: 1381\n",
            "Test: [Task 8]  [20/42]  eta: 0:00:04  Loss: 1.1033 (1.1257)  Acc@1: 75.0000 (74.2064)  Acc@5: 95.8333 (93.2540)  Acc@task: 70.8333 (72.8175)  time: 0.1778  data: 0.0008  max mem: 1381\n",
            "Test: [Task 8]  [30/42]  eta: 0:00:02  Loss: 0.9202 (1.0844)  Acc@1: 75.0000 (73.9247)  Acc@5: 95.8333 (93.5484)  Acc@task: 70.8333 (72.9839)  time: 0.1786  data: 0.0005  max mem: 1381\n",
            "Test: [Task 8]  [40/42]  eta: 0:00:00  Loss: 0.9202 (1.1008)  Acc@1: 70.8333 (73.6789)  Acc@5: 95.8333 (93.1911)  Acc@task: 70.8333 (71.9512)  time: 0.1787  data: 0.0005  max mem: 1381\n",
            "Test: [Task 8]  [41/42]  eta: 0:00:00  Loss: 1.0754 (1.1017)  Acc@1: 70.8333 (73.7000)  Acc@5: 91.6667 (93.1000)  Acc@task: 70.8333 (72.2000)  time: 0.1754  data: 0.0005  max mem: 1381\n",
            "Test: [Task 8] Total time: 0:00:07 (0.1876 s / it)\n",
            "* Acc@task 72.200 Acc@1 73.700 Acc@5 93.100 loss 1.102\n",
            "Test: [Task 9]  [ 0/42]  eta: 0:00:36  Loss: 0.5605 (0.5605)  Acc@1: 91.6667 (91.6667)  Acc@5: 95.8333 (95.8333)  Acc@task: 83.3333 (83.3333)  time: 0.8708  data: 0.6657  max mem: 1381\n",
            "Test: [Task 9]  [10/42]  eta: 0:00:07  Loss: 0.6544 (0.6463)  Acc@1: 87.5000 (83.3333)  Acc@5: 95.8333 (96.5909)  Acc@task: 75.0000 (74.6212)  time: 0.2404  data: 0.0610  max mem: 1381\n",
            "Test: [Task 9]  [20/42]  eta: 0:00:04  Loss: 0.5135 (0.6214)  Acc@1: 83.3333 (83.9286)  Acc@5: 100.0000 (97.8175)  Acc@task: 75.0000 (76.1905)  time: 0.1769  data: 0.0005  max mem: 1381\n",
            "Test: [Task 9]  [30/42]  eta: 0:00:02  Loss: 0.5341 (0.6534)  Acc@1: 83.3333 (83.7366)  Acc@5: 100.0000 (97.5806)  Acc@task: 75.0000 (75.5376)  time: 0.1776  data: 0.0004  max mem: 1381\n",
            "Test: [Task 9]  [40/42]  eta: 0:00:00  Loss: 0.5187 (0.6083)  Acc@1: 87.5000 (84.5528)  Acc@5: 100.0000 (97.7642)  Acc@task: 79.1667 (76.5244)  time: 0.1786  data: 0.0004  max mem: 1381\n",
            "Test: [Task 9]  [41/42]  eta: 0:00:00  Loss: 0.5187 (0.5994)  Acc@1: 87.5000 (84.7000)  Acc@5: 100.0000 (97.8000)  Acc@task: 79.1667 (76.8000)  time: 0.1753  data: 0.0004  max mem: 1381\n",
            "Test: [Task 9] Total time: 0:00:08 (0.1954 s / it)\n",
            "* Acc@task 76.800 Acc@1 84.700 Acc@5 97.800 loss 0.599\n",
            "Test: [Task 10]  [ 0/42]  eta: 0:00:25  Loss: 0.5612 (0.5612)  Acc@1: 83.3333 (83.3333)  Acc@5: 100.0000 (100.0000)  Acc@task: 70.8333 (70.8333)  time: 0.6036  data: 0.4238  max mem: 1381\n",
            "Test: [Task 10]  [10/42]  eta: 0:00:06  Loss: 0.8207 (0.8570)  Acc@1: 79.1667 (79.1667)  Acc@5: 95.8333 (96.2121)  Acc@task: 66.6667 (66.2879)  time: 0.2160  data: 0.0390  max mem: 1381\n",
            "Test: [Task 10]  [20/42]  eta: 0:00:04  Loss: 0.8953 (0.8710)  Acc@1: 75.0000 (76.7857)  Acc@5: 95.8333 (95.8333)  Acc@task: 62.5000 (65.8730)  time: 0.1768  data: 0.0006  max mem: 1381\n",
            "Test: [Task 10]  [30/42]  eta: 0:00:02  Loss: 0.8953 (0.9045)  Acc@1: 75.0000 (76.7473)  Acc@5: 95.8333 (95.6989)  Acc@task: 62.5000 (65.4570)  time: 0.1774  data: 0.0011  max mem: 1381\n",
            "Test: [Task 10]  [40/42]  eta: 0:00:00  Loss: 0.8171 (0.8826)  Acc@1: 75.0000 (77.0325)  Acc@5: 95.8333 (96.1382)  Acc@task: 62.5000 (65.0407)  time: 0.1783  data: 0.0010  max mem: 1381\n",
            "Test: [Task 10]  [41/42]  eta: 0:00:00  Loss: 0.7042 (0.8778)  Acc@1: 75.0000 (77.0000)  Acc@5: 100.0000 (96.2000)  Acc@task: 62.5000 (65.2000)  time: 0.1750  data: 0.0010  max mem: 1381\n",
            "Test: [Task 10] Total time: 0:00:07 (0.1890 s / it)\n",
            "* Acc@task 65.200 Acc@1 77.000 Acc@5 96.200 loss 0.878\n",
            "[Average accuracy till task10]\tAcc@task: 75.7500\tAcc@1: 81.5100\tAcc@5: 96.4600\tLoss: 0.7205\tForgetting: 2.5000\tBackward: 1.1222\n",
            "Total training time: 0:28:47\n",
            "[rank0]:[W1003 15:49:27.261465361 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
          ]
        }
      ],
      "source": [
        "!torchrun --nproc_per_node=1 main.py cifar100_hideprompt_5e --original_model vit_small_patch16_224.dino --model vit_small_patch16_224.dino --batch-size 24 --epochs 1 --seed 20 --ca_lr 0.005 --crct_epochs 1 --prompt_momentum 0.1 --reg 0.1 --length 5 --larger_prompt_lr --data-path ./datasets/ --trained_original_model ./output/cifar100_full_dino_1epoch_25pct --output_dir ./output/cifar100_full_dino_1epoch_final_25pct --pct 0.25\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}